{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxTOT6Zzvb9b"
      },
      "source": [
        "https://github.com/raghav64/SemiSuper_GAN/blob/master/SSGAN.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiJzfP_d88xN"
      },
      "outputs": [],
      "source": [
        "\n",
        "                ############ Imports ############\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxbIEmw49wuP"
      },
      "outputs": [],
      "source": [
        "                ############ Initializations ############\n",
        "\n",
        "num_classes = 10\n",
        "channels = 1\n",
        "height = 64\n",
        "width = 64\n",
        "# MNIST was resized to 64 * 64 for discriminator and generator architecture fitting\n",
        "latent = 100\n",
        "epsilon = 1e-7\n",
        "labeled_rate = 0.2 # For initial testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVepUf_2khFX"
      },
      "outputs": [],
      "source": [
        "                ############ Importing MNIST data ############\n",
        "\n",
        "def get_data():\n",
        "    from tensorflow.examples.tutorials.mnist import input_data\n",
        "    mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True, reshape=[])\n",
        "    return mnist\n",
        "\n",
        "                ############ Normalizing data ############\n",
        "# Scaling in range (-1,1) for generator tanh output\n",
        "def scale(x):\n",
        "    # normalize data\n",
        "    x = (x - 0.5) / 0.5\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7ldhkfXbfjW"
      },
      "source": [
        "Discriminator and Generator architecture should mirror each other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX3LbUcp_rDJ"
      },
      "outputs": [],
      "source": [
        "              ############ Defining Discriminator ############\n",
        "\n",
        "def discriminator(x, dropout_rate = 0., is_training = True, reuse = False):\n",
        "    # input x -> n+1 classes\n",
        "\n",
        "    with tf.variable_scope('Discriminator', reuse = reuse):\n",
        "\n",
        "      # x = ?*64*64*1\n",
        "\n",
        "      print('Discriminator architecture: ')\n",
        "      #Layer 1\n",
        "      conv1 = tf.layers.conv2d(x, 128, kernel_size = [4,4], strides = [2,2],\n",
        "                              padding = 'same', activation = tf.nn.leaky_relu, name = 'conv1') # ?*32*32*128\n",
        "      print(conv1.shape)\n",
        "      #No batch-norm for input layer\n",
        "      dropout1 = tf.nn.dropout(conv1, dropout_rate)\n",
        "\n",
        "      #Layer2\n",
        "      conv2 = tf.layers.conv2d(dropout1, 256, kernel_size = [4,4], strides = [2,2],\n",
        "                              padding = 'same', activation = tf.nn.leaky_relu, name = 'conv2') # ?*16*16*256\n",
        "      batch2 = tf.layers.batch_normalization(conv2, training = is_training)\n",
        "      dropout2 = tf.nn.dropout(batch2, dropout_rate)\n",
        "      print(conv2.shape)\n",
        "\n",
        "      #Layer3\n",
        "      conv3 = tf.layers.conv2d(dropout2, 512, kernel_size = [4,4], strides = [4,4],\n",
        "                              padding = 'same', activation = tf.nn.leaky_relu, name = 'conv3') # ?*4*4*512\n",
        "      batch3 = tf.layers.batch_normalization(conv3, training = is_training)\n",
        "      dropout3 = tf.nn.dropout(batch3, dropout_rate)\n",
        "      print(conv3.shape)\n",
        "\n",
        "      # Layer 4\n",
        "      conv4 = tf.layers.conv2d(dropout3, 1024, kernel_size=[3,3], strides=[1,1],\n",
        "                               padding='valid',activation = tf.nn.leaky_relu, name='conv4') # ?*2*2*1024\n",
        "      # No batch-norm as this layer's op will be used in feature matching loss\n",
        "      # No dropout as feature matching needs to be definite on logits\n",
        "      print(conv4.shape)\n",
        "\n",
        "      # Layer 5\n",
        "      # Note: Applying Global average pooling\n",
        "\n",
        "      flatten = tf.reduce_mean(conv4, axis = [1,2])\n",
        "      logits_D = tf.layers.dense(flatten, (1 + num_classes))\n",
        "      out_D = tf.nn.softmax(logits_D)\n",
        "\n",
        "    return flatten,logits_D,out_D\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "635RFlWgFk8A"
      },
      "outputs": [],
      "source": [
        "                ############ Defining Generator ############\n",
        "\n",
        "def generator(z, dropout_rate = 0., is_training = True, reuse = False):\n",
        "    # input latent z -> image x\n",
        "\n",
        "    with tf.variable_scope('Generator', reuse = reuse):\n",
        "      print('\\n Generator architecture: ')\n",
        "\n",
        "      #Layer 1\n",
        "      deconv1 = tf.layers.conv2d_transpose(z, 512, kernel_size = [4,4],\n",
        "                                         strides = [1,1], padding = 'valid',\n",
        "                                        activation = tf.nn.relu, name = 'deconv1') # ?*4*4*512\n",
        "      batch1 = tf.layers.batch_normalization(deconv1, training = is_training)\n",
        "      dropout1 = tf.nn.dropout(batch1, dropout_rate)\n",
        "      print(deconv1.shape)\n",
        "\n",
        "      #Layer 2\n",
        "      deconv2 = tf.layers.conv2d_transpose(dropout1, 256, kernel_size = [4,4],\n",
        "                                         strides = [4,4], padding = 'same',\n",
        "                                        activation = tf.nn.relu, name = 'deconv2')# ?*16*16*256\n",
        "      batch2 = tf.layers.batch_normalization(deconv2, training = is_training)\n",
        "      dropout2 = tf.nn.dropout(batch2, dropout_rate)\n",
        "      print(deconv2.shape)\n",
        "\n",
        "      #Layer 3\n",
        "      deconv3 = tf.layers.conv2d_transpose(dropout2, 128, kernel_size = [4,4],\n",
        "                                         strides = [2,2], padding = 'same',\n",
        "                                        activation = tf.nn.relu, name = 'deconv3')# ?*32*32*256\n",
        "      batch3 = tf.layers.batch_normalization(deconv3, training = is_training)\n",
        "      dropout3 = tf.nn.dropout(batch3, dropout_rate)\n",
        "      print(deconv3.shape)\n",
        "\n",
        "      #Output layer\n",
        "      deconv4 = tf.layers.conv2d_transpose(dropout3, 1, kernel_size = [4,4],\n",
        "                                        strides = [2,2], padding = 'same',\n",
        "                                        activation = None, name = 'deconv4')# ?*64*64*1\n",
        "      out = tf.nn.tanh(deconv4)\n",
        "      print(deconv4.shape)\n",
        "\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNYagCnMJaLS"
      },
      "outputs": [],
      "source": [
        "                ############ Building model ############\n",
        "\n",
        "def build_GAN(x_real, z, dropout_rate, is_training):\n",
        "\n",
        "    fake_images = generator(z, dropout_rate, is_training)\n",
        "\n",
        "    D_real_features, D_real_logits, D_real_prob = discriminator(x_real, dropout_rate,\n",
        "                                                              is_training)\n",
        "\n",
        "    D_fake_features, D_fake_logits, D_fake_prob = discriminator(fake_images, dropout_rate,\n",
        "                                                                is_training, reuse = True)\n",
        "    #Setting reuse=True this time for using variables trained in real batch training\n",
        "\n",
        "    return D_real_features, D_real_logits, D_real_prob, D_fake_features, D_fake_logits, D_fake_prob, fake_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4JqKKmsawd1"
      },
      "outputs": [],
      "source": [
        "                ############ Preparing Mask ############\n",
        "\n",
        "# Preparing a binary label_mask to be multiplied with real labels\n",
        "def get_labeled_mask(labeled_rate, batch_size):\n",
        "    labeled_mask = np.zeros([batch_size], dtype = np.float32)\n",
        "    labeled_count = np.int(batch_size * labeled_rate)\n",
        "    labeled_mask[range(labeled_count)] = 1.0\n",
        "    np.random.shuffle(labeled_mask)\n",
        "    return labeled_mask\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYDk-3Cd0q9j"
      },
      "outputs": [],
      "source": [
        "               ############ Preparing Extended label ############\n",
        "\n",
        "def prepare_extended_label(label):\n",
        "    # add extra label for fake data\n",
        "    extended_label = tf.concat([tf.zeros([tf.shape(label)[0], 1]), label], axis = 1)\n",
        "\n",
        "    return extended_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGyS5eI1JeE7"
      },
      "outputs": [],
      "source": [
        "            ############ Defining losses ############\n",
        "\n",
        "# The total loss inculcates  D_L_Unsupervised + D_L_Supervised + G_feature_matching loss + G_R/F loss\n",
        "\n",
        "def loss_accuracy(D_real_features, D_real_logit, D_real_prob, D_fake_features,\n",
        "                  D_fake_logit, D_fake_prob, extended_label, labeled_mask):\n",
        "\n",
        "                    ### Discriminator loss ###\n",
        "\n",
        "    # Supervised loss -> which class the real data belongs to\n",
        "\n",
        "    temp = tf.nn.softmax_cross_entropy_with_logits_v2(logits = D_real_logit,\n",
        "                                                  labels = extended_label)\n",
        "    # Don't confuse labeled_rate with labeled_mask\n",
        "    # Labeled_mask and temp are of same size = batch_size where temp is softmax\n",
        "    # cross_entropy calculated over whole batch\n",
        "\n",
        "    D_L_Supervised = tf.reduce_sum(tf.multiply(temp,labeled_mask)) / tf.reduce_sum(labeled_mask)\n",
        "\n",
        "    # Multiplying temp with labeled_mask gives supervised loss on labeled_mask\n",
        "    # data only, calculating mean by dividing by no of labeled samples\n",
        "\n",
        "    # Unsupervised loss -> R/F\n",
        "\n",
        "    D_L_RealUnsupervised = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits = D_real_logit[:, 0], labels = tf.zeros_like(D_real_logit[:, 0], dtype=tf.float32)))\n",
        "\n",
        "    D_L_FakeUnsupervised = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits = D_fake_logit[:, 0], labels = tf.ones_like(D_fake_logit[:, 0], dtype=tf.float32)))\n",
        "\n",
        "    D_L = D_L_Supervised + D_L_RealUnsupervised + D_L_FakeUnsupervised\n",
        "\n",
        "\n",
        "                    ### Generator loss ###\n",
        "\n",
        "    # G_L_1 -> Fake data wanna be real\n",
        "\n",
        "    G_L_1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits = D_fake_logit[:, 0],labels = tf.zeros_like(D_fake_logit[:, 0], dtype=tf.float32)))\n",
        "\n",
        "    # G_L_2 -> Feature matching\n",
        "    data_moments = tf.reduce_mean(D_real_features, axis = 0)\n",
        "    sample_moments = tf.reduce_mean(D_fake_features, axis = 0)\n",
        "    G_L_2 = tf.reduce_mean(tf.square(data_moments-sample_moments))\n",
        "\n",
        "\n",
        "    G_L = G_L_1 + G_L_2\n",
        "\n",
        "    prediction = tf.equal(tf.argmax(D_real_prob[:, 1:], 1),\n",
        "                                  tf.argmax(extended_label[:, 1:], 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
        "\n",
        "    return D_L, G_L, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v8g1_2ZJhkF"
      },
      "outputs": [],
      "source": [
        "            ############ Defining Optimizer ############\n",
        "\n",
        "def optimizer(D_Loss, G_Loss, learning_rate, beta1):\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        all_vars = tf.trainable_variables()\n",
        "        D_vars = [var for var in all_vars if var.name.startswith('Discriminator')]\n",
        "        G_vars = [var for var in all_vars if var.name.startswith('Generator')]\n",
        "\n",
        "        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1,\n",
        "                                             name = 'd_optimiser').minimize(D_Loss, var_list=D_vars)\n",
        "        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1,\n",
        "                                             name = 'g_optimiser').minimize(G_Loss, var_list=G_vars)\n",
        "\n",
        "    return d_train_opt, g_train_opt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbrCsvtrM4VF"
      },
      "outputs": [],
      "source": [
        "            ############ Plotting Results ############\n",
        "\n",
        "def show_result(test_images, num_epoch, show = True, save = False, path = 'result.png'):\n",
        "\n",
        "    size_figure_grid = 5\n",
        "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
        "    for i in range(0, size_figure_grid):\n",
        "      for j in range(0, size_figure_grid):\n",
        "        ax[i, j].get_xaxis().set_visible(False)\n",
        "        ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "    for k in range(size_figure_grid*size_figure_grid):\n",
        "        i = k // size_figure_grid\n",
        "        j = k % size_figure_grid\n",
        "        ax[i, j].cla()\n",
        "        ax[i, j].imshow(np.reshape(test_images[k], (64, 64)), cmap='gray')\n",
        "\n",
        "    label = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0.04, label, ha='center')\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
        "\n",
        "\n",
        "    x = range(len(hist['D_losses']))\n",
        "\n",
        "    y1 = hist['D_losses']\n",
        "    y2 = hist['G_losses']\n",
        "\n",
        "    plt.plot(x, y1, label='D_loss')\n",
        "    plt.plot(x, y2, label='G_loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELqpD5tSJl22"
      },
      "outputs": [],
      "source": [
        "            ############ TRAINING ############\n",
        "\n",
        "def train_GAN(batch_size, epochs):\n",
        "\n",
        "    train_hist = {}\n",
        "    train_hist['D_losses'] = []\n",
        "    train_hist['G_losses'] = []\n",
        "\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "\n",
        "    x = tf.placeholder(tf.float32, shape = [None, height ,width, channels], name = 'x')\n",
        "    z = tf.placeholder(tf.float32, shape = [None, 1, 1, latent], name = 'z')\n",
        "    label = tf.placeholder(tf.float32, name = 'label', shape = [None, num_classes])\n",
        "    labeled_mask = tf.placeholder(tf.float32, name = 'labeled_mask', shape = [None])\n",
        "    dropout_rate = tf.placeholder(tf.float32, name = 'dropout_rate')\n",
        "    is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
        "\n",
        "    lr_rate = 2e-4\n",
        "\n",
        "    model = build_GAN(x, z, dropout_rate, is_training)\n",
        "    D_real_features, D_real_logit, D_real_prob, D_fake_features, D_fake_logit, D_fake_prob, fake_data = model\n",
        "\n",
        "    extended_label = prepare_extended_label(label)\n",
        "\n",
        "    # Fake_data of size = batch_size*28*28*1\n",
        "\n",
        "    loss_acc = loss_accuracy(D_real_features, D_real_logit, D_real_prob,\n",
        "                                  D_fake_features, D_fake_logit, D_fake_prob,\n",
        "                                  extended_label, labeled_mask)\n",
        "    D_L, G_L, accuracy = loss_acc\n",
        "\n",
        "    D_optimizer, G_optimizer = optimizer(D_L, G_L, lr_rate, beta1 = 0.5)\n",
        "\n",
        "    print ('...Training begins...')\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "\n",
        "      mnist_data = get_data()\n",
        "      no_of_batches = int (mnist_data.train.images.shape[0]/batch_size) + 1\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "\n",
        "        train_accuracies, train_D_losses, train_G_losses = [], [], []\n",
        "\n",
        "        for it in range(no_of_batches):\n",
        "\n",
        "          batch = mnist_data.train.next_batch(batch_size, shuffle = False)\n",
        "          # batch[0] has shape: batch_size*28*28*1\n",
        "\n",
        "          batch_reshaped = tf.image.resize_images(batch[0], [64, 64]).eval()\n",
        "\n",
        "          # Reshaping the whole batch into batch_size*64*64*1 for disc/gen architecture\n",
        "          batch_z = np.random.normal(0, 1, (batch_size, 1, 1, latent))\n",
        "\n",
        "          mask = get_labeled_mask(labeled_rate, batch_size)\n",
        "\n",
        "          train_feed_dict = {x : scale(batch_reshaped), z : batch_z,\n",
        "                                   label : batch[1], labeled_mask : mask,\n",
        "                                   dropout_rate : 0.7,\n",
        "                                   is_training : True}\n",
        "          #The label provided in dict are one hot encoded in 10 classes\n",
        "\n",
        "          D_optimizer.run(feed_dict = train_feed_dict)\n",
        "          G_optimizer.run(feed_dict = train_feed_dict)\n",
        "\n",
        "          train_D_loss = D_L.eval(feed_dict = train_feed_dict)\n",
        "          train_G_loss = G_L.eval(feed_dict = train_feed_dict)\n",
        "          train_accuracy = accuracy.eval(feed_dict = train_feed_dict)\n",
        "\n",
        "          train_D_losses.append(train_D_loss)\n",
        "          train_G_losses.append(train_G_loss)\n",
        "          train_accuracies.append(train_accuracy)\n",
        "          print('Batch evaluated: ' +str(it+1))\n",
        "\n",
        "        tr_GL = np.mean(train_G_losses)\n",
        "        tr_DL = np.mean(train_D_losses)\n",
        "        tr_acc = np.mean(train_accuracies)\n",
        "\n",
        "        print ('After epoch: '+ str(epoch+1) + ' Generator loss: '\n",
        "                       + str(tr_GL) + ' Discriminator loss: ' + str(tr_DL) + ' Accuracy: ' + str(tr_acc))\n",
        "\n",
        "        gen_samples = fake_data.eval(feed_dict = {z : np.random.normal(0, 1, (25, 1, 1, latent)), dropout_rate : 0.7, is_training : False})\n",
        "        # Dont train batch-norm while plotting => is_training = False\n",
        "        test_images = tf.image.resize_images(gen_samples, [64, 64]).eval()\n",
        "        show_result(test_images, (epoch + 1), show = True, save = False, path = '')\n",
        "\n",
        "        train_hist['D_losses'].append(np.mean(train_D_losses))\n",
        "        train_hist['G_losses'].append(np.mean(train_G_losses))\n",
        "\n",
        "      show_train_hist(train_hist, show=True, save = True, path = 'train_hist.png')\n",
        "      sess.close()\n",
        "\n",
        "    return train_D_losses,train_G_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-CHzkGsWbi6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Input, Reshape, Dense  # Add more layers as per your GAN architecture\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_GAN(batch_size, epochs):\n",
        "    # Initialize GAN components\n",
        "    generator, discriminator = build_GAN()\n",
        "    # Optimizers\n",
        "    gen_optimizer = tf.optimizers.Adam(2e-4)\n",
        "    disc_optimizer = tf.optimizers.Adam(2e-4)\n",
        "\n",
        "    # Training history\n",
        "    train_hist = {'D_losses': [], 'G_losses': []}\n",
        "\n",
        "    # Load and preprocess MNIST data\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    x_train = x_train.astype('float32') / 255.0\n",
        "    x_train = np.expand_dims(x_train, axis=-1)\n",
        "\n",
        "    x_test = x_train.astype('float32') / 255.0\n",
        "    x_test = np.expand_dims(x_train, axis=-1)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for i in range(0, x_train.shape[0], batch_size):\n",
        "            batch_images = x_train[i:i + batch_size]\n",
        "\n",
        "            # Sample random noise for G\n",
        "            noise = np.random.normal(0, 1, size=(batch_size, latent))\n",
        "\n",
        "            with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "                # Generate images\n",
        "                generated_images = generator(noise, training=True)\n",
        "\n",
        "                # Discriminator output\n",
        "                real_output = discriminator(batch_images, training=True)\n",
        "                fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "                # Calculate loss\n",
        "                gen_loss = generator_loss(fake_output)\n",
        "                disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "            # Gradient updates\n",
        "            gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "            gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "            gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "            disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "            print(f\"Epoch {epoch + 1}, Batch {i // batch_size + 1}, G Loss: {gen_loss}, D Loss: {disc_loss}\")\n",
        "\n",
        "            # Save losses for history\n",
        "            train_hist['D_losses'].append(disc_loss.numpy())\n",
        "            train_hist['G_losses'].append(gen_loss.numpy())\n",
        "\n",
        "        # Display or save generated images per epoch\n",
        "\n",
        "    # Display training history\n",
        "    show_train_hist(train_hist, show=True, save=True, path='train_hist.png')\n",
        "\n",
        "def generator_loss(fake_output):\n",
        "    return tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S9uW7Pvoi_EQ",
        "outputId": "3808944b-7f33-4de3-80e4-a9455b203c7f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 1, G Loss: 0.723347008228302, D Loss: 1.3647778034210205\n",
            "Epoch 1, Batch 2, G Loss: 0.7245930433273315, D Loss: 1.364215612411499\n",
            "Epoch 1, Batch 3, G Loss: 0.7256934642791748, D Loss: 1.3632328510284424\n",
            "Epoch 1, Batch 4, G Loss: 0.7267054319381714, D Loss: 1.3624966144561768\n",
            "Epoch 1, Batch 5, G Loss: 0.7276740670204163, D Loss: 1.3619359731674194\n",
            "Epoch 1, Batch 6, G Loss: 0.7285840511322021, D Loss: 1.3621695041656494\n",
            "Epoch 1, Batch 7, G Loss: 0.7294853925704956, D Loss: 1.3614630699157715\n",
            "Epoch 1, Batch 8, G Loss: 0.7303432822227478, D Loss: 1.3603527545928955\n",
            "Epoch 1, Batch 9, G Loss: 0.7311723828315735, D Loss: 1.3589134216308594\n",
            "Epoch 1, Batch 10, G Loss: 0.7319869995117188, D Loss: 1.3607807159423828\n",
            "Epoch 1, Batch 11, G Loss: 0.7327480912208557, D Loss: 1.361006736755371\n",
            "Epoch 1, Batch 12, G Loss: 0.7335492372512817, D Loss: 1.3579380512237549\n",
            "Epoch 1, Batch 13, G Loss: 0.7342745065689087, D Loss: 1.3571871519088745\n",
            "Epoch 1, Batch 14, G Loss: 0.7349775433540344, D Loss: 1.3566486835479736\n",
            "Epoch 1, Batch 15, G Loss: 0.7356340289115906, D Loss: 1.356546401977539\n",
            "Epoch 1, Batch 16, G Loss: 0.7363572120666504, D Loss: 1.3575628995895386\n",
            "Epoch 1, Batch 17, G Loss: 0.7370149493217468, D Loss: 1.3561155796051025\n",
            "Epoch 1, Batch 18, G Loss: 0.737626850605011, D Loss: 1.3554928302764893\n",
            "Epoch 1, Batch 19, G Loss: 0.7381828427314758, D Loss: 1.354569911956787\n",
            "Epoch 1, Batch 20, G Loss: 0.7388260364532471, D Loss: 1.3545262813568115\n",
            "Epoch 1, Batch 21, G Loss: 0.7393875122070312, D Loss: 1.3544163703918457\n",
            "Epoch 1, Batch 22, G Loss: 0.7399775981903076, D Loss: 1.3531990051269531\n",
            "Epoch 1, Batch 23, G Loss: 0.7404115796089172, D Loss: 1.3536468744277954\n",
            "Epoch 1, Batch 24, G Loss: 0.7410011887550354, D Loss: 1.3517463207244873\n",
            "Epoch 1, Batch 25, G Loss: 0.7414652109146118, D Loss: 1.3510258197784424\n",
            "Epoch 1, Batch 26, G Loss: 0.7418927550315857, D Loss: 1.3505831956863403\n",
            "Epoch 1, Batch 27, G Loss: 0.7424820065498352, D Loss: 1.350518822669983\n",
            "Epoch 1, Batch 28, G Loss: 0.7427039742469788, D Loss: 1.3503187894821167\n",
            "Epoch 1, Batch 29, G Loss: 0.7434994578361511, D Loss: 1.3519160747528076\n",
            "Epoch 1, Batch 30, G Loss: 0.7440541982650757, D Loss: 1.3498417139053345\n",
            "Epoch 1, Batch 31, G Loss: 0.7446442246437073, D Loss: 1.3492107391357422\n",
            "Epoch 1, Batch 32, G Loss: 0.7450816631317139, D Loss: 1.3497164249420166\n",
            "Epoch 1, Batch 33, G Loss: 0.7456657886505127, D Loss: 1.348497986793518\n",
            "Epoch 1, Batch 34, G Loss: 0.7459312677383423, D Loss: 1.3490625619888306\n",
            "Epoch 1, Batch 35, G Loss: 0.7464233636856079, D Loss: 1.3477377891540527\n",
            "Epoch 1, Batch 36, G Loss: 0.7467818260192871, D Loss: 1.3496159315109253\n",
            "Epoch 1, Batch 37, G Loss: 0.7471663951873779, D Loss: 1.3504955768585205\n",
            "Epoch 1, Batch 38, G Loss: 0.747480571269989, D Loss: 1.3490551710128784\n",
            "Epoch 1, Batch 39, G Loss: 0.7480537295341492, D Loss: 1.3476653099060059\n",
            "Epoch 1, Batch 40, G Loss: 0.7484654188156128, D Loss: 1.348228931427002\n",
            "Epoch 1, Batch 41, G Loss: 0.7488541007041931, D Loss: 1.3482835292816162\n",
            "Epoch 1, Batch 42, G Loss: 0.7491689920425415, D Loss: 1.3487911224365234\n",
            "Epoch 1, Batch 43, G Loss: 0.7495796084403992, D Loss: 1.3492414951324463\n",
            "Epoch 1, Batch 44, G Loss: 0.7498906850814819, D Loss: 1.3478243350982666\n",
            "Epoch 1, Batch 45, G Loss: 0.7500678896903992, D Loss: 1.3473860025405884\n",
            "Epoch 1, Batch 46, G Loss: 0.7502068281173706, D Loss: 1.3459312915802002\n",
            "Epoch 1, Batch 47, G Loss: 0.750211775302887, D Loss: 1.348564624786377\n",
            "Epoch 1, Batch 48, G Loss: 0.7505002617835999, D Loss: 1.3505029678344727\n",
            "Epoch 1, Batch 49, G Loss: 0.7505496144294739, D Loss: 1.3488081693649292\n",
            "Epoch 1, Batch 50, G Loss: 0.7510859966278076, D Loss: 1.347792625427246\n",
            "Epoch 1, Batch 51, G Loss: 0.7509782910346985, D Loss: 1.346625804901123\n",
            "Epoch 1, Batch 52, G Loss: 0.7512722611427307, D Loss: 1.345658302307129\n",
            "Epoch 1, Batch 53, G Loss: 0.7512190937995911, D Loss: 1.3429332971572876\n",
            "Epoch 1, Batch 54, G Loss: 0.7512968182563782, D Loss: 1.3448035717010498\n",
            "Epoch 1, Batch 55, G Loss: 0.751717209815979, D Loss: 1.3470572233200073\n",
            "Epoch 1, Batch 56, G Loss: 0.7520050406455994, D Loss: 1.3437979221343994\n",
            "Epoch 1, Batch 57, G Loss: 0.7519585490226746, D Loss: 1.3437201976776123\n",
            "Epoch 1, Batch 58, G Loss: 0.7519879937171936, D Loss: 1.3419514894485474\n",
            "Epoch 1, Batch 59, G Loss: 0.7516180276870728, D Loss: 1.3441860675811768\n",
            "Epoch 1, Batch 60, G Loss: 0.7518307566642761, D Loss: 1.344916820526123\n",
            "Epoch 1, Batch 61, G Loss: 0.7520911693572998, D Loss: 1.3453822135925293\n",
            "Epoch 1, Batch 62, G Loss: 0.7514274716377258, D Loss: 1.3487427234649658\n",
            "Epoch 1, Batch 63, G Loss: 0.7520850300788879, D Loss: 1.3485260009765625\n",
            "Epoch 1, Batch 64, G Loss: 0.7521364092826843, D Loss: 1.345458984375\n",
            "Epoch 1, Batch 65, G Loss: 0.7523365616798401, D Loss: 1.3460521697998047\n",
            "Epoch 1, Batch 66, G Loss: 0.7521402835845947, D Loss: 1.3465821743011475\n",
            "Epoch 1, Batch 67, G Loss: 0.7518063187599182, D Loss: 1.3518180847167969\n",
            "Epoch 1, Batch 68, G Loss: 0.7514844536781311, D Loss: 1.3507698774337769\n",
            "Epoch 1, Batch 69, G Loss: 0.7511982917785645, D Loss: 1.3479602336883545\n",
            "Epoch 1, Batch 70, G Loss: 0.7512913346290588, D Loss: 1.3469346761703491\n",
            "Epoch 1, Batch 71, G Loss: 0.7512714862823486, D Loss: 1.3468568325042725\n",
            "Epoch 1, Batch 72, G Loss: 0.751068651676178, D Loss: 1.345475196838379\n",
            "Epoch 1, Batch 73, G Loss: 0.751554548740387, D Loss: 1.3453080654144287\n",
            "Epoch 1, Batch 74, G Loss: 0.7506868243217468, D Loss: 1.349208116531372\n",
            "Epoch 1, Batch 75, G Loss: 0.7502716779708862, D Loss: 1.3486158847808838\n",
            "Epoch 1, Batch 76, G Loss: 0.7509191632270813, D Loss: 1.3485206365585327\n",
            "Epoch 1, Batch 77, G Loss: 0.7503456473350525, D Loss: 1.3486902713775635\n",
            "Epoch 1, Batch 78, G Loss: 0.7504631280899048, D Loss: 1.3477652072906494\n",
            "Epoch 1, Batch 79, G Loss: 0.7503011226654053, D Loss: 1.3490523099899292\n",
            "Epoch 1, Batch 80, G Loss: 0.7499120235443115, D Loss: 1.3510997295379639\n",
            "Epoch 1, Batch 81, G Loss: 0.7490142583847046, D Loss: 1.3556394577026367\n",
            "Epoch 1, Batch 82, G Loss: 0.748982310295105, D Loss: 1.3527946472167969\n",
            "Epoch 1, Batch 83, G Loss: 0.7483431696891785, D Loss: 1.3514190912246704\n",
            "Epoch 1, Batch 84, G Loss: 0.7483266592025757, D Loss: 1.3486963510513306\n",
            "Epoch 1, Batch 85, G Loss: 0.7487478256225586, D Loss: 1.351132869720459\n",
            "Epoch 1, Batch 86, G Loss: 0.7480591535568237, D Loss: 1.3523149490356445\n",
            "Epoch 1, Batch 87, G Loss: 0.7480250597000122, D Loss: 1.3522577285766602\n",
            "Epoch 1, Batch 88, G Loss: 0.7466396689414978, D Loss: 1.3524315357208252\n",
            "Epoch 1, Batch 89, G Loss: 0.7464737296104431, D Loss: 1.3519034385681152\n",
            "Epoch 1, Batch 90, G Loss: 0.7470185160636902, D Loss: 1.351137399673462\n",
            "Epoch 1, Batch 91, G Loss: 0.7464091181755066, D Loss: 1.3515640497207642\n",
            "Epoch 1, Batch 92, G Loss: 0.7457694411277771, D Loss: 1.35213303565979\n",
            "Epoch 1, Batch 93, G Loss: 0.7445412278175354, D Loss: 1.3533928394317627\n",
            "Epoch 1, Batch 94, G Loss: 0.7446486949920654, D Loss: 1.3546550273895264\n",
            "Epoch 1, Batch 95, G Loss: 0.7439934611320496, D Loss: 1.3552272319793701\n",
            "Epoch 1, Batch 96, G Loss: 0.7433591485023499, D Loss: 1.3568497896194458\n",
            "Epoch 1, Batch 97, G Loss: 0.7433622479438782, D Loss: 1.3558688163757324\n",
            "Epoch 1, Batch 98, G Loss: 0.7430087327957153, D Loss: 1.3573105335235596\n",
            "Epoch 1, Batch 99, G Loss: 0.7433692812919617, D Loss: 1.3569440841674805\n",
            "Epoch 1, Batch 100, G Loss: 0.7420488595962524, D Loss: 1.3554754257202148\n",
            "Epoch 1, Batch 101, G Loss: 0.7422223091125488, D Loss: 1.3542972803115845\n",
            "Epoch 1, Batch 102, G Loss: 0.7413289546966553, D Loss: 1.360551357269287\n",
            "Epoch 1, Batch 103, G Loss: 0.7408068776130676, D Loss: 1.3611754179000854\n",
            "Epoch 1, Batch 104, G Loss: 0.7396298050880432, D Loss: 1.3603020906448364\n",
            "Epoch 1, Batch 105, G Loss: 0.7399352788925171, D Loss: 1.3591487407684326\n",
            "Epoch 1, Batch 106, G Loss: 0.738903820514679, D Loss: 1.3620309829711914\n",
            "Epoch 1, Batch 107, G Loss: 0.7386671304702759, D Loss: 1.3584144115447998\n",
            "Epoch 1, Batch 108, G Loss: 0.7390568256378174, D Loss: 1.3551044464111328\n",
            "Epoch 1, Batch 109, G Loss: 0.7380626201629639, D Loss: 1.3589897155761719\n",
            "Epoch 1, Batch 110, G Loss: 0.7375556826591492, D Loss: 1.360589861869812\n",
            "Epoch 1, Batch 111, G Loss: 0.7377158999443054, D Loss: 1.3635857105255127\n",
            "Epoch 1, Batch 112, G Loss: 0.734810471534729, D Loss: 1.3648993968963623\n",
            "Epoch 1, Batch 113, G Loss: 0.7368878722190857, D Loss: 1.3605124950408936\n",
            "Epoch 1, Batch 114, G Loss: 0.7357449531555176, D Loss: 1.358917236328125\n",
            "Epoch 1, Batch 115, G Loss: 0.7347406148910522, D Loss: 1.3620549440383911\n",
            "Epoch 1, Batch 116, G Loss: 0.7349565625190735, D Loss: 1.3610944747924805\n",
            "Epoch 1, Batch 117, G Loss: 0.7337146401405334, D Loss: 1.3620113134384155\n",
            "Epoch 1, Batch 118, G Loss: 0.7331150770187378, D Loss: 1.3616760969161987\n",
            "Epoch 1, Batch 119, G Loss: 0.7330390214920044, D Loss: 1.360799789428711\n",
            "Epoch 1, Batch 120, G Loss: 0.7312014698982239, D Loss: 1.3627068996429443\n",
            "Epoch 1, Batch 121, G Loss: 0.7314614057540894, D Loss: 1.3615678548812866\n",
            "Epoch 1, Batch 122, G Loss: 0.7314322590827942, D Loss: 1.3630890846252441\n",
            "Epoch 1, Batch 123, G Loss: 0.7303754687309265, D Loss: 1.364466905593872\n",
            "Epoch 1, Batch 124, G Loss: 0.7297371029853821, D Loss: 1.3645809888839722\n",
            "Epoch 1, Batch 125, G Loss: 0.728657603263855, D Loss: 1.3668086528778076\n",
            "Epoch 1, Batch 126, G Loss: 0.7288928031921387, D Loss: 1.3681961297988892\n",
            "Epoch 1, Batch 127, G Loss: 0.7279744148254395, D Loss: 1.3688406944274902\n",
            "Epoch 1, Batch 128, G Loss: 0.7269851565361023, D Loss: 1.3680992126464844\n",
            "Epoch 1, Batch 129, G Loss: 0.7269673347473145, D Loss: 1.3674583435058594\n",
            "Epoch 1, Batch 130, G Loss: 0.7274752259254456, D Loss: 1.3667237758636475\n",
            "Epoch 1, Batch 131, G Loss: 0.7259840965270996, D Loss: 1.367297887802124\n",
            "Epoch 1, Batch 132, G Loss: 0.7249014973640442, D Loss: 1.3684780597686768\n",
            "Epoch 1, Batch 133, G Loss: 0.7236122488975525, D Loss: 1.3697348833084106\n",
            "Epoch 1, Batch 134, G Loss: 0.7235828042030334, D Loss: 1.3704050779342651\n",
            "Epoch 1, Batch 135, G Loss: 0.7242509722709656, D Loss: 1.371668815612793\n",
            "Epoch 1, Batch 136, G Loss: 0.7241231203079224, D Loss: 1.3710068464279175\n",
            "Epoch 1, Batch 137, G Loss: 0.7223839163780212, D Loss: 1.3703787326812744\n",
            "Epoch 1, Batch 138, G Loss: 0.7224810123443604, D Loss: 1.368466854095459\n",
            "Epoch 1, Batch 139, G Loss: 0.7221007943153381, D Loss: 1.3689484596252441\n",
            "Epoch 1, Batch 140, G Loss: 0.7208213210105896, D Loss: 1.3693410158157349\n",
            "Epoch 1, Batch 141, G Loss: 0.7195473313331604, D Loss: 1.3734166622161865\n",
            "Epoch 1, Batch 142, G Loss: 0.7189409732818604, D Loss: 1.3733857870101929\n",
            "Epoch 1, Batch 143, G Loss: 0.7190081477165222, D Loss: 1.3739542961120605\n",
            "Epoch 1, Batch 144, G Loss: 0.7194971442222595, D Loss: 1.371946096420288\n",
            "Epoch 1, Batch 145, G Loss: 0.7178879380226135, D Loss: 1.3739876747131348\n",
            "Epoch 1, Batch 146, G Loss: 0.7176142334938049, D Loss: 1.375190258026123\n",
            "Epoch 1, Batch 147, G Loss: 0.7172475457191467, D Loss: 1.3744066953659058\n",
            "Epoch 1, Batch 148, G Loss: 0.716649055480957, D Loss: 1.3752795457839966\n",
            "Epoch 1, Batch 149, G Loss: 0.716408371925354, D Loss: 1.373892068862915\n",
            "Epoch 1, Batch 150, G Loss: 0.7154650092124939, D Loss: 1.3747785091400146\n",
            "Epoch 1, Batch 151, G Loss: 0.7164461016654968, D Loss: 1.3789021968841553\n",
            "Epoch 1, Batch 152, G Loss: 0.7139467597007751, D Loss: 1.3816921710968018\n",
            "Epoch 1, Batch 153, G Loss: 0.7135491371154785, D Loss: 1.3818494081497192\n",
            "Epoch 1, Batch 154, G Loss: 0.713360071182251, D Loss: 1.3808658123016357\n",
            "Epoch 1, Batch 155, G Loss: 0.7141808271408081, D Loss: 1.380361795425415\n",
            "Epoch 1, Batch 156, G Loss: 0.713300883769989, D Loss: 1.3804118633270264\n",
            "Epoch 1, Batch 157, G Loss: 0.7120528817176819, D Loss: 1.3804359436035156\n",
            "Epoch 1, Batch 158, G Loss: 0.7127087116241455, D Loss: 1.37919282913208\n",
            "Epoch 1, Batch 159, G Loss: 0.7102763056755066, D Loss: 1.3793307542800903\n",
            "Epoch 1, Batch 160, G Loss: 0.71102374792099, D Loss: 1.3796360492706299\n",
            "Epoch 1, Batch 161, G Loss: 0.7103955149650574, D Loss: 1.3809349536895752\n",
            "Epoch 1, Batch 162, G Loss: 0.7094787359237671, D Loss: 1.3822168111801147\n",
            "Epoch 1, Batch 163, G Loss: 0.7083770036697388, D Loss: 1.3811671733856201\n",
            "Epoch 1, Batch 164, G Loss: 0.7086554765701294, D Loss: 1.3818955421447754\n",
            "Epoch 1, Batch 165, G Loss: 0.7076272964477539, D Loss: 1.3800098896026611\n",
            "Epoch 1, Batch 166, G Loss: 0.7074547410011292, D Loss: 1.381369948387146\n",
            "Epoch 1, Batch 167, G Loss: 0.7073900103569031, D Loss: 1.3825619220733643\n",
            "Epoch 1, Batch 168, G Loss: 0.707086980342865, D Loss: 1.3834033012390137\n",
            "Epoch 1, Batch 169, G Loss: 0.7059704661369324, D Loss: 1.3827662467956543\n",
            "Epoch 1, Batch 170, G Loss: 0.7063940167427063, D Loss: 1.379624366760254\n",
            "Epoch 1, Batch 171, G Loss: 0.7054678201675415, D Loss: 1.3877589702606201\n",
            "Epoch 1, Batch 172, G Loss: 0.7047119140625, D Loss: 1.3881287574768066\n",
            "Epoch 1, Batch 173, G Loss: 0.7047229409217834, D Loss: 1.3839507102966309\n",
            "Epoch 1, Batch 174, G Loss: 0.7043304443359375, D Loss: 1.3841490745544434\n",
            "Epoch 1, Batch 175, G Loss: 0.7037057876586914, D Loss: 1.3839924335479736\n",
            "Epoch 1, Batch 176, G Loss: 0.7034367322921753, D Loss: 1.3842060565948486\n",
            "Epoch 1, Batch 177, G Loss: 0.7037658095359802, D Loss: 1.3854615688323975\n",
            "Epoch 1, Batch 178, G Loss: 0.7040296196937561, D Loss: 1.3880236148834229\n",
            "Epoch 1, Batch 179, G Loss: 0.7022550702095032, D Loss: 1.3873355388641357\n",
            "Epoch 1, Batch 180, G Loss: 0.7015726566314697, D Loss: 1.3885774612426758\n",
            "Epoch 1, Batch 181, G Loss: 0.7013909220695496, D Loss: 1.3888828754425049\n",
            "Epoch 1, Batch 182, G Loss: 0.7001808881759644, D Loss: 1.387399673461914\n",
            "Epoch 1, Batch 183, G Loss: 0.7010930776596069, D Loss: 1.387026071548462\n",
            "Epoch 1, Batch 184, G Loss: 0.6997783184051514, D Loss: 1.3864684104919434\n",
            "Epoch 1, Batch 185, G Loss: 0.7005195021629333, D Loss: 1.3843164443969727\n",
            "Epoch 1, Batch 186, G Loss: 0.6993892192840576, D Loss: 1.386383056640625\n",
            "Epoch 1, Batch 187, G Loss: 0.6997937560081482, D Loss: 1.3864357471466064\n",
            "Epoch 1, Batch 188, G Loss: 0.6990979909896851, D Loss: 1.3872010707855225\n",
            "Epoch 1, Batch 189, G Loss: 0.6983062028884888, D Loss: 1.3873274326324463\n",
            "Epoch 1, Batch 190, G Loss: 0.6987642049789429, D Loss: 1.3865211009979248\n",
            "Epoch 1, Batch 191, G Loss: 0.6984179615974426, D Loss: 1.386915683746338\n",
            "Epoch 1, Batch 192, G Loss: 0.6982102394104004, D Loss: 1.3870019912719727\n",
            "Epoch 1, Batch 193, G Loss: 0.6977773904800415, D Loss: 1.3886528015136719\n",
            "Epoch 1, Batch 194, G Loss: 0.6965183615684509, D Loss: 1.3882791996002197\n",
            "Epoch 1, Batch 195, G Loss: 0.697284460067749, D Loss: 1.384859561920166\n",
            "Epoch 1, Batch 196, G Loss: 0.696424663066864, D Loss: 1.386789321899414\n",
            "Epoch 1, Batch 197, G Loss: 0.6961715221405029, D Loss: 1.3873589038848877\n",
            "Epoch 1, Batch 198, G Loss: 0.6959770321846008, D Loss: 1.3902324438095093\n",
            "Epoch 1, Batch 199, G Loss: 0.6961161494255066, D Loss: 1.3907017707824707\n",
            "Epoch 1, Batch 200, G Loss: 0.6961772441864014, D Loss: 1.3878031969070435\n",
            "Epoch 1, Batch 201, G Loss: 0.695426344871521, D Loss: 1.3885033130645752\n",
            "Epoch 1, Batch 202, G Loss: 0.6955467462539673, D Loss: 1.3852468729019165\n",
            "Epoch 1, Batch 203, G Loss: 0.6947830319404602, D Loss: 1.3851370811462402\n",
            "Epoch 1, Batch 204, G Loss: 0.6941203474998474, D Loss: 1.3895690441131592\n",
            "Epoch 1, Batch 205, G Loss: 0.694580614566803, D Loss: 1.3877654075622559\n",
            "Epoch 1, Batch 206, G Loss: 0.694138765335083, D Loss: 1.3868975639343262\n",
            "Epoch 1, Batch 207, G Loss: 0.6936551928520203, D Loss: 1.3869653940200806\n",
            "Epoch 1, Batch 208, G Loss: 0.6931666135787964, D Loss: 1.3864455223083496\n",
            "Epoch 1, Batch 209, G Loss: 0.6927555799484253, D Loss: 1.389394998550415\n",
            "Epoch 1, Batch 210, G Loss: 0.6938266158103943, D Loss: 1.3888683319091797\n",
            "Epoch 1, Batch 211, G Loss: 0.6928757429122925, D Loss: 1.3876831531524658\n",
            "Epoch 1, Batch 212, G Loss: 0.6937194466590881, D Loss: 1.3885555267333984\n",
            "Epoch 1, Batch 213, G Loss: 0.6925414204597473, D Loss: 1.3902060985565186\n",
            "Epoch 1, Batch 214, G Loss: 0.6927075982093811, D Loss: 1.3890337944030762\n",
            "Epoch 1, Batch 215, G Loss: 0.6922228336334229, D Loss: 1.3884880542755127\n",
            "Epoch 1, Batch 216, G Loss: 0.6916938424110413, D Loss: 1.388274908065796\n",
            "Epoch 1, Batch 217, G Loss: 0.6907010078430176, D Loss: 1.3883450031280518\n",
            "Epoch 1, Batch 218, G Loss: 0.6915690302848816, D Loss: 1.3874841928482056\n",
            "Epoch 1, Batch 219, G Loss: 0.6917625069618225, D Loss: 1.3899989128112793\n",
            "Epoch 1, Batch 220, G Loss: 0.6910426616668701, D Loss: 1.3904250860214233\n",
            "Epoch 1, Batch 221, G Loss: 0.6906165480613708, D Loss: 1.3914376497268677\n",
            "Epoch 1, Batch 222, G Loss: 0.691041886806488, D Loss: 1.3905179500579834\n",
            "Epoch 1, Batch 223, G Loss: 0.6907055974006653, D Loss: 1.3905208110809326\n",
            "Epoch 1, Batch 224, G Loss: 0.690801203250885, D Loss: 1.3894129991531372\n",
            "Epoch 1, Batch 225, G Loss: 0.690258800983429, D Loss: 1.3891024589538574\n",
            "Epoch 1, Batch 226, G Loss: 0.6905730366706848, D Loss: 1.3893581628799438\n",
            "Epoch 1, Batch 227, G Loss: 0.6900250911712646, D Loss: 1.3898053169250488\n",
            "Epoch 1, Batch 228, G Loss: 0.6898730397224426, D Loss: 1.387254238128662\n",
            "Epoch 1, Batch 229, G Loss: 0.6896299123764038, D Loss: 1.3896087408065796\n",
            "Epoch 1, Batch 230, G Loss: 0.6903303265571594, D Loss: 1.3902699947357178\n",
            "Epoch 1, Batch 231, G Loss: 0.6902849078178406, D Loss: 1.3883174657821655\n",
            "Epoch 1, Batch 232, G Loss: 0.6897733807563782, D Loss: 1.3872147798538208\n",
            "Epoch 1, Batch 233, G Loss: 0.6891494989395142, D Loss: 1.3868298530578613\n",
            "Epoch 1, Batch 234, G Loss: 0.6895376443862915, D Loss: 1.3864185810089111\n",
            "Epoch 1, Batch 235, G Loss: 0.6888142824172974, D Loss: 1.3873909711837769\n",
            "Epoch 1, Batch 236, G Loss: 0.6881445050239563, D Loss: 1.388662338256836\n",
            "Epoch 1, Batch 237, G Loss: 0.6884557604789734, D Loss: 1.388033151626587\n",
            "Epoch 1, Batch 238, G Loss: 0.6889103651046753, D Loss: 1.3887767791748047\n",
            "Epoch 1, Batch 239, G Loss: 0.6884382367134094, D Loss: 1.3885306119918823\n",
            "Epoch 1, Batch 240, G Loss: 0.6882560849189758, D Loss: 1.3859379291534424\n",
            "Epoch 1, Batch 241, G Loss: 0.688348114490509, D Loss: 1.387218713760376\n",
            "Epoch 1, Batch 242, G Loss: 0.6887821555137634, D Loss: 1.3869898319244385\n",
            "Epoch 1, Batch 243, G Loss: 0.6876955628395081, D Loss: 1.3878095149993896\n",
            "Epoch 1, Batch 244, G Loss: 0.6879338026046753, D Loss: 1.3873696327209473\n",
            "Epoch 1, Batch 245, G Loss: 0.6874924302101135, D Loss: 1.3876903057098389\n",
            "Epoch 1, Batch 246, G Loss: 0.6878920793533325, D Loss: 1.3870508670806885\n",
            "Epoch 1, Batch 247, G Loss: 0.6875059008598328, D Loss: 1.3866214752197266\n",
            "Epoch 1, Batch 248, G Loss: 0.6878726482391357, D Loss: 1.386373519897461\n",
            "Epoch 1, Batch 249, G Loss: 0.6877237558364868, D Loss: 1.3870457410812378\n",
            "Epoch 1, Batch 250, G Loss: 0.6875414848327637, D Loss: 1.3872829675674438\n",
            "Epoch 1, Batch 251, G Loss: 0.6877560019493103, D Loss: 1.3880482912063599\n",
            "Epoch 1, Batch 252, G Loss: 0.6876659989356995, D Loss: 1.3872816562652588\n",
            "Epoch 1, Batch 253, G Loss: 0.6871249079704285, D Loss: 1.3885993957519531\n",
            "Epoch 1, Batch 254, G Loss: 0.687126636505127, D Loss: 1.3888871669769287\n",
            "Epoch 1, Batch 255, G Loss: 0.6871292591094971, D Loss: 1.3871898651123047\n",
            "Epoch 1, Batch 256, G Loss: 0.6870712637901306, D Loss: 1.386446237564087\n",
            "Epoch 1, Batch 257, G Loss: 0.6866711974143982, D Loss: 1.3871110677719116\n",
            "Epoch 1, Batch 258, G Loss: 0.6869843006134033, D Loss: 1.3878231048583984\n",
            "Epoch 1, Batch 259, G Loss: 0.6870123147964478, D Loss: 1.387585163116455\n",
            "Epoch 1, Batch 260, G Loss: 0.6869906187057495, D Loss: 1.3873486518859863\n",
            "Epoch 1, Batch 261, G Loss: 0.686689555644989, D Loss: 1.3869638442993164\n",
            "Epoch 1, Batch 262, G Loss: 0.6868926882743835, D Loss: 1.3859143257141113\n",
            "Epoch 1, Batch 263, G Loss: 0.686450719833374, D Loss: 1.3857719898223877\n",
            "Epoch 1, Batch 264, G Loss: 0.6871798038482666, D Loss: 1.3860565423965454\n",
            "Epoch 1, Batch 265, G Loss: 0.6868085265159607, D Loss: 1.3870420455932617\n",
            "Epoch 1, Batch 266, G Loss: 0.6865238547325134, D Loss: 1.3859264850616455\n",
            "Epoch 1, Batch 267, G Loss: 0.6866793632507324, D Loss: 1.3862358331680298\n",
            "Epoch 1, Batch 268, G Loss: 0.6864299178123474, D Loss: 1.385801911354065\n",
            "Epoch 1, Batch 269, G Loss: 0.6867154240608215, D Loss: 1.3862948417663574\n",
            "Epoch 1, Batch 270, G Loss: 0.6861070394515991, D Loss: 1.3867167234420776\n",
            "Epoch 1, Batch 271, G Loss: 0.6859056949615479, D Loss: 1.3866848945617676\n",
            "Epoch 1, Batch 272, G Loss: 0.6863502264022827, D Loss: 1.3861010074615479\n",
            "Epoch 1, Batch 273, G Loss: 0.6868345141410828, D Loss: 1.385366678237915\n",
            "Epoch 1, Batch 274, G Loss: 0.6864810585975647, D Loss: 1.3858988285064697\n",
            "Epoch 1, Batch 275, G Loss: 0.6867414116859436, D Loss: 1.3848556280136108\n",
            "Epoch 1, Batch 276, G Loss: 0.686589241027832, D Loss: 1.3846772909164429\n",
            "Epoch 1, Batch 277, G Loss: 0.6863746047019958, D Loss: 1.3855035305023193\n",
            "Epoch 1, Batch 278, G Loss: 0.686316967010498, D Loss: 1.385359525680542\n",
            "Epoch 1, Batch 279, G Loss: 0.6858361959457397, D Loss: 1.38545560836792\n",
            "Epoch 1, Batch 280, G Loss: 0.6863733530044556, D Loss: 1.3847262859344482\n",
            "Epoch 1, Batch 281, G Loss: 0.6858212947845459, D Loss: 1.384652853012085\n",
            "Epoch 1, Batch 282, G Loss: 0.6863505840301514, D Loss: 1.3836369514465332\n",
            "Epoch 1, Batch 283, G Loss: 0.6861014366149902, D Loss: 1.3855044841766357\n",
            "Epoch 1, Batch 284, G Loss: 0.6863831281661987, D Loss: 1.385119915008545\n",
            "Epoch 1, Batch 285, G Loss: 0.6862752437591553, D Loss: 1.3842318058013916\n",
            "Epoch 1, Batch 286, G Loss: 0.6860479116439819, D Loss: 1.384861946105957\n",
            "Epoch 1, Batch 287, G Loss: 0.6862606406211853, D Loss: 1.385035753250122\n",
            "Epoch 1, Batch 288, G Loss: 0.6866493821144104, D Loss: 1.384065866470337\n",
            "Epoch 1, Batch 289, G Loss: 0.6862214803695679, D Loss: 1.383893370628357\n",
            "Epoch 1, Batch 290, G Loss: 0.6863071918487549, D Loss: 1.3837847709655762\n",
            "Epoch 1, Batch 291, G Loss: 0.6865925788879395, D Loss: 1.3836300373077393\n",
            "Epoch 1, Batch 292, G Loss: 0.6860289573669434, D Loss: 1.3839797973632812\n",
            "Epoch 1, Batch 293, G Loss: 0.6859710812568665, D Loss: 1.3834974765777588\n",
            "Epoch 1, Batch 294, G Loss: 0.686260998249054, D Loss: 1.3828644752502441\n",
            "Epoch 1, Batch 295, G Loss: 0.6865715384483337, D Loss: 1.3826518058776855\n",
            "Epoch 1, Batch 296, G Loss: 0.6863841414451599, D Loss: 1.3829741477966309\n",
            "Epoch 1, Batch 297, G Loss: 0.6859776973724365, D Loss: 1.3829468488693237\n",
            "Epoch 1, Batch 298, G Loss: 0.6863725781440735, D Loss: 1.382434606552124\n",
            "Epoch 1, Batch 299, G Loss: 0.6865010261535645, D Loss: 1.382347822189331\n",
            "Epoch 1, Batch 300, G Loss: 0.686310887336731, D Loss: 1.3822355270385742\n",
            "Epoch 1, Batch 301, G Loss: 0.686165452003479, D Loss: 1.3821918964385986\n",
            "Epoch 1, Batch 302, G Loss: 0.6863283514976501, D Loss: 1.3821868896484375\n",
            "Epoch 1, Batch 303, G Loss: 0.6863551139831543, D Loss: 1.3819037675857544\n",
            "Epoch 1, Batch 304, G Loss: 0.6863903403282166, D Loss: 1.3818011283874512\n",
            "Epoch 1, Batch 305, G Loss: 0.6864244341850281, D Loss: 1.3815586566925049\n",
            "Epoch 1, Batch 306, G Loss: 0.6860503554344177, D Loss: 1.3819098472595215\n",
            "Epoch 1, Batch 307, G Loss: 0.686626136302948, D Loss: 1.3812308311462402\n",
            "Epoch 1, Batch 308, G Loss: 0.6863085031509399, D Loss: 1.3813979625701904\n",
            "Epoch 1, Batch 309, G Loss: 0.6867983937263489, D Loss: 1.3806874752044678\n",
            "Epoch 1, Batch 310, G Loss: 0.6865555047988892, D Loss: 1.3808469772338867\n",
            "Epoch 1, Batch 311, G Loss: 0.6866647601127625, D Loss: 1.380709171295166\n",
            "Epoch 1, Batch 312, G Loss: 0.6863272786140442, D Loss: 1.3810019493103027\n",
            "Epoch 1, Batch 313, G Loss: 0.6867088079452515, D Loss: 1.3803765773773193\n",
            "Epoch 1, Batch 314, G Loss: 0.6866555213928223, D Loss: 1.3803291320800781\n",
            "Epoch 1, Batch 315, G Loss: 0.6865231394767761, D Loss: 1.380427598953247\n",
            "Epoch 1, Batch 316, G Loss: 0.6865452527999878, D Loss: 1.3802485466003418\n",
            "Epoch 1, Batch 317, G Loss: 0.6866772770881653, D Loss: 1.3798253536224365\n",
            "Epoch 1, Batch 318, G Loss: 0.6865826845169067, D Loss: 1.3800122737884521\n",
            "Epoch 1, Batch 319, G Loss: 0.6866346001625061, D Loss: 1.3799493312835693\n",
            "Epoch 1, Batch 320, G Loss: 0.6867083311080933, D Loss: 1.3796892166137695\n",
            "Epoch 1, Batch 321, G Loss: 0.6870338320732117, D Loss: 1.3792856931686401\n",
            "Epoch 1, Batch 322, G Loss: 0.6866692304611206, D Loss: 1.3794840574264526\n",
            "Epoch 1, Batch 323, G Loss: 0.6869824528694153, D Loss: 1.3790736198425293\n",
            "Epoch 1, Batch 324, G Loss: 0.6871711611747742, D Loss: 1.3787410259246826\n",
            "Epoch 1, Batch 325, G Loss: 0.6869864463806152, D Loss: 1.3788957595825195\n",
            "Epoch 1, Batch 326, G Loss: 0.6870750784873962, D Loss: 1.3789639472961426\n",
            "Epoch 1, Batch 327, G Loss: 0.6874878406524658, D Loss: 1.3786420822143555\n",
            "Epoch 1, Batch 328, G Loss: 0.6869831681251526, D Loss: 1.37834632396698\n",
            "Epoch 1, Batch 329, G Loss: 0.6872698664665222, D Loss: 1.3777759075164795\n",
            "Epoch 1, Batch 330, G Loss: 0.6868741512298584, D Loss: 1.3782167434692383\n",
            "Epoch 1, Batch 331, G Loss: 0.6872994303703308, D Loss: 1.3778197765350342\n",
            "Epoch 1, Batch 332, G Loss: 0.6873143911361694, D Loss: 1.3779282569885254\n",
            "Epoch 1, Batch 333, G Loss: 0.6872533559799194, D Loss: 1.377638816833496\n",
            "Epoch 1, Batch 334, G Loss: 0.6874722838401794, D Loss: 1.3769278526306152\n",
            "Epoch 1, Batch 335, G Loss: 0.6874849200248718, D Loss: 1.3769593238830566\n",
            "Epoch 1, Batch 336, G Loss: 0.6878151893615723, D Loss: 1.3772556781768799\n",
            "Epoch 1, Batch 337, G Loss: 0.6875824928283691, D Loss: 1.3774549961090088\n",
            "Epoch 1, Batch 338, G Loss: 0.687559187412262, D Loss: 1.3766396045684814\n",
            "Epoch 1, Batch 339, G Loss: 0.6877357363700867, D Loss: 1.3766298294067383\n",
            "Epoch 1, Batch 340, G Loss: 0.687854528427124, D Loss: 1.3763079643249512\n",
            "Epoch 1, Batch 341, G Loss: 0.6878255009651184, D Loss: 1.3764309883117676\n",
            "Epoch 1, Batch 342, G Loss: 0.6878575086593628, D Loss: 1.3761851787567139\n",
            "Epoch 1, Batch 343, G Loss: 0.688280463218689, D Loss: 1.3760688304901123\n",
            "Epoch 1, Batch 344, G Loss: 0.6882888078689575, D Loss: 1.3763388395309448\n",
            "Epoch 1, Batch 345, G Loss: 0.6880093216896057, D Loss: 1.376103401184082\n",
            "Epoch 1, Batch 346, G Loss: 0.6881222128868103, D Loss: 1.3757600784301758\n",
            "Epoch 1, Batch 347, G Loss: 0.6880780458450317, D Loss: 1.3757619857788086\n",
            "Epoch 1, Batch 348, G Loss: 0.688264787197113, D Loss: 1.3753559589385986\n",
            "Epoch 1, Batch 349, G Loss: 0.6883691549301147, D Loss: 1.3753070831298828\n",
            "Epoch 1, Batch 350, G Loss: 0.6884432435035706, D Loss: 1.3753809928894043\n",
            "Epoch 1, Batch 351, G Loss: 0.6883891224861145, D Loss: 1.3753557205200195\n",
            "Epoch 1, Batch 352, G Loss: 0.6884981989860535, D Loss: 1.374567985534668\n",
            "Epoch 1, Batch 353, G Loss: 0.6886075735092163, D Loss: 1.3743739128112793\n",
            "Epoch 1, Batch 354, G Loss: 0.6883918642997742, D Loss: 1.3746137619018555\n",
            "Epoch 1, Batch 355, G Loss: 0.688877284526825, D Loss: 1.3744624853134155\n",
            "Epoch 1, Batch 356, G Loss: 0.6887496709823608, D Loss: 1.3747119903564453\n",
            "Epoch 1, Batch 357, G Loss: 0.6888832449913025, D Loss: 1.374248743057251\n",
            "Epoch 1, Batch 358, G Loss: 0.6886690258979797, D Loss: 1.3741605281829834\n",
            "Epoch 1, Batch 359, G Loss: 0.6889803409576416, D Loss: 1.3741869926452637\n",
            "Epoch 1, Batch 360, G Loss: 0.6889897584915161, D Loss: 1.3750371932983398\n",
            "Epoch 1, Batch 361, G Loss: 0.6890693306922913, D Loss: 1.374372959136963\n",
            "Epoch 1, Batch 362, G Loss: 0.6891615986824036, D Loss: 1.3737289905548096\n",
            "Epoch 1, Batch 363, G Loss: 0.6893041133880615, D Loss: 1.3728684186935425\n",
            "Epoch 1, Batch 364, G Loss: 0.6893149614334106, D Loss: 1.3730721473693848\n",
            "Epoch 1, Batch 365, G Loss: 0.6893840432167053, D Loss: 1.3721251487731934\n",
            "Epoch 1, Batch 366, G Loss: 0.689548909664154, D Loss: 1.3718657493591309\n",
            "Epoch 1, Batch 367, G Loss: 0.6894482970237732, D Loss: 1.3720529079437256\n",
            "Epoch 1, Batch 368, G Loss: 0.6897534728050232, D Loss: 1.3720571994781494\n",
            "Epoch 1, Batch 369, G Loss: 0.689641535282135, D Loss: 1.3725967407226562\n",
            "Epoch 1, Batch 370, G Loss: 0.6898145079612732, D Loss: 1.3732695579528809\n",
            "Epoch 1, Batch 371, G Loss: 0.6899579763412476, D Loss: 1.3710957765579224\n",
            "Epoch 1, Batch 372, G Loss: 0.6899312734603882, D Loss: 1.3706457614898682\n",
            "Epoch 1, Batch 373, G Loss: 0.6900362372398376, D Loss: 1.369706392288208\n",
            "Epoch 1, Batch 374, G Loss: 0.6902363896369934, D Loss: 1.3703391551971436\n",
            "Epoch 1, Batch 375, G Loss: 0.6900242567062378, D Loss: 1.3709683418273926\n",
            "Epoch 1, Batch 376, G Loss: 0.6902222633361816, D Loss: 1.3696322441101074\n",
            "Epoch 1, Batch 377, G Loss: 0.6903144717216492, D Loss: 1.3689758777618408\n",
            "Epoch 1, Batch 378, G Loss: 0.6902473568916321, D Loss: 1.369277834892273\n",
            "Epoch 1, Batch 379, G Loss: 0.6904500126838684, D Loss: 1.369407057762146\n",
            "Epoch 1, Batch 380, G Loss: 0.6905047297477722, D Loss: 1.3693599700927734\n",
            "Epoch 1, Batch 381, G Loss: 0.6905639171600342, D Loss: 1.3691096305847168\n",
            "Epoch 1, Batch 382, G Loss: 0.6904807686805725, D Loss: 1.3693575859069824\n",
            "Epoch 1, Batch 383, G Loss: 0.6907299757003784, D Loss: 1.3718451261520386\n",
            "Epoch 1, Batch 384, G Loss: 0.6907027959823608, D Loss: 1.3699871301651\n",
            "Epoch 1, Batch 385, G Loss: 0.6908986568450928, D Loss: 1.3688830137252808\n",
            "Epoch 1, Batch 386, G Loss: 0.6909240484237671, D Loss: 1.369570016860962\n",
            "Epoch 1, Batch 387, G Loss: 0.691078782081604, D Loss: 1.3688846826553345\n",
            "Epoch 1, Batch 388, G Loss: 0.6909665465354919, D Loss: 1.369316577911377\n",
            "Epoch 1, Batch 389, G Loss: 0.6912437081336975, D Loss: 1.3688879013061523\n",
            "Epoch 1, Batch 390, G Loss: 0.6914578080177307, D Loss: 1.3693939447402954\n",
            "Epoch 1, Batch 391, G Loss: 0.6915654540061951, D Loss: 1.3697247505187988\n",
            "Epoch 1, Batch 392, G Loss: 0.6914143562316895, D Loss: 1.3685588836669922\n",
            "Epoch 1, Batch 393, G Loss: 0.6916518807411194, D Loss: 1.3687279224395752\n",
            "Epoch 1, Batch 394, G Loss: 0.691632866859436, D Loss: 1.3685622215270996\n",
            "Epoch 1, Batch 395, G Loss: 0.6917263865470886, D Loss: 1.3679826259613037\n",
            "Epoch 1, Batch 396, G Loss: 0.6917338371276855, D Loss: 1.3686765432357788\n",
            "Epoch 1, Batch 397, G Loss: 0.6917686462402344, D Loss: 1.366985559463501\n",
            "Epoch 1, Batch 398, G Loss: 0.69198077917099, D Loss: 1.3670263290405273\n",
            "Epoch 1, Batch 399, G Loss: 0.6920212507247925, D Loss: 1.3677805662155151\n",
            "Epoch 1, Batch 400, G Loss: 0.6921195983886719, D Loss: 1.3658474683761597\n",
            "Epoch 1, Batch 401, G Loss: 0.6921987533569336, D Loss: 1.3649137020111084\n",
            "Epoch 1, Batch 402, G Loss: 0.6923550963401794, D Loss: 1.3660783767700195\n",
            "Epoch 1, Batch 403, G Loss: 0.6925066709518433, D Loss: 1.3660171031951904\n",
            "Epoch 1, Batch 404, G Loss: 0.6924383044242859, D Loss: 1.366347074508667\n",
            "Epoch 1, Batch 405, G Loss: 0.6926447153091431, D Loss: 1.3670427799224854\n",
            "Epoch 1, Batch 406, G Loss: 0.692645788192749, D Loss: 1.366866946220398\n",
            "Epoch 1, Batch 407, G Loss: 0.6927699446678162, D Loss: 1.3677897453308105\n",
            "Epoch 1, Batch 408, G Loss: 0.6929252743721008, D Loss: 1.3672075271606445\n",
            "Epoch 1, Batch 409, G Loss: 0.692931056022644, D Loss: 1.367146611213684\n",
            "Epoch 1, Batch 410, G Loss: 0.6930246949195862, D Loss: 1.3647513389587402\n",
            "Epoch 1, Batch 411, G Loss: 0.6929929852485657, D Loss: 1.36500883102417\n",
            "Epoch 1, Batch 412, G Loss: 0.6931455135345459, D Loss: 1.366255283355713\n",
            "Epoch 1, Batch 413, G Loss: 0.6933832168579102, D Loss: 1.366051197052002\n",
            "Epoch 1, Batch 414, G Loss: 0.6933407187461853, D Loss: 1.3640952110290527\n",
            "Epoch 1, Batch 415, G Loss: 0.6933573484420776, D Loss: 1.3629587888717651\n",
            "Epoch 1, Batch 416, G Loss: 0.6935553550720215, D Loss: 1.3645389080047607\n",
            "Epoch 1, Batch 417, G Loss: 0.6937190294265747, D Loss: 1.3649353981018066\n",
            "Epoch 1, Batch 418, G Loss: 0.6936523914337158, D Loss: 1.3649330139160156\n",
            "Epoch 1, Batch 419, G Loss: 0.6937445998191833, D Loss: 1.3656787872314453\n",
            "Epoch 1, Batch 420, G Loss: 0.6939191222190857, D Loss: 1.364816427230835\n",
            "Epoch 1, Batch 421, G Loss: 0.6940664052963257, D Loss: 1.3655571937561035\n",
            "Epoch 1, Batch 422, G Loss: 0.6938357949256897, D Loss: 1.3630043268203735\n",
            "Epoch 1, Batch 423, G Loss: 0.6942082643508911, D Loss: 1.3633670806884766\n",
            "Epoch 1, Batch 424, G Loss: 0.6942362189292908, D Loss: 1.3648804426193237\n",
            "Epoch 1, Batch 425, G Loss: 0.694236159324646, D Loss: 1.3636665344238281\n",
            "Epoch 1, Batch 426, G Loss: 0.694331705570221, D Loss: 1.3617537021636963\n",
            "Epoch 1, Batch 427, G Loss: 0.6946251392364502, D Loss: 1.361297845840454\n",
            "Epoch 1, Batch 428, G Loss: 0.6945282220840454, D Loss: 1.3616540431976318\n",
            "Epoch 1, Batch 429, G Loss: 0.6945968866348267, D Loss: 1.3626494407653809\n",
            "Epoch 1, Batch 430, G Loss: 0.6948551535606384, D Loss: 1.361628770828247\n",
            "Epoch 1, Batch 431, G Loss: 0.6947835087776184, D Loss: 1.3619931936264038\n",
            "Epoch 1, Batch 432, G Loss: 0.6949387192726135, D Loss: 1.3614921569824219\n",
            "Epoch 1, Batch 433, G Loss: 0.6951733827590942, D Loss: 1.3627585172653198\n",
            "Epoch 1, Batch 434, G Loss: 0.6952020525932312, D Loss: 1.3607563972473145\n",
            "Epoch 1, Batch 435, G Loss: 0.6952967047691345, D Loss: 1.3584448099136353\n",
            "Epoch 1, Batch 436, G Loss: 0.6954238414764404, D Loss: 1.357593297958374\n",
            "Epoch 1, Batch 437, G Loss: 0.695441484451294, D Loss: 1.3607723712921143\n",
            "Epoch 1, Batch 438, G Loss: 0.6955378651618958, D Loss: 1.3608198165893555\n",
            "Epoch 1, Batch 439, G Loss: 0.69561767578125, D Loss: 1.3607850074768066\n",
            "Epoch 1, Batch 440, G Loss: 0.6955865621566772, D Loss: 1.3614137172698975\n",
            "Epoch 1, Batch 441, G Loss: 0.6957643628120422, D Loss: 1.363008737564087\n",
            "Epoch 1, Batch 442, G Loss: 0.6957529783248901, D Loss: 1.3611218929290771\n",
            "Epoch 1, Batch 443, G Loss: 0.6959254741668701, D Loss: 1.361255168914795\n",
            "Epoch 1, Batch 444, G Loss: 0.6960464715957642, D Loss: 1.3577759265899658\n",
            "Epoch 1, Batch 445, G Loss: 0.6962294578552246, D Loss: 1.3596177101135254\n",
            "Epoch 1, Batch 446, G Loss: 0.6962476372718811, D Loss: 1.3631258010864258\n",
            "Epoch 1, Batch 447, G Loss: 0.6963962912559509, D Loss: 1.3612444400787354\n",
            "Epoch 1, Batch 448, G Loss: 0.6964437365531921, D Loss: 1.3593966960906982\n",
            "Epoch 1, Batch 449, G Loss: 0.6965751051902771, D Loss: 1.359627366065979\n",
            "Epoch 1, Batch 450, G Loss: 0.6965858936309814, D Loss: 1.3583037853240967\n",
            "Epoch 1, Batch 451, G Loss: 0.6968088150024414, D Loss: 1.361011028289795\n",
            "Epoch 1, Batch 452, G Loss: 0.6968355774879456, D Loss: 1.3620262145996094\n",
            "Epoch 1, Batch 453, G Loss: 0.6969447731971741, D Loss: 1.360565185546875\n",
            "Epoch 1, Batch 454, G Loss: 0.6970295310020447, D Loss: 1.360180139541626\n",
            "Epoch 1, Batch 455, G Loss: 0.6971026062965393, D Loss: 1.360257625579834\n",
            "Epoch 1, Batch 456, G Loss: 0.6970770955085754, D Loss: 1.3574318885803223\n",
            "Epoch 1, Batch 457, G Loss: 0.6972852349281311, D Loss: 1.3553622961044312\n",
            "Epoch 1, Batch 458, G Loss: 0.6974308490753174, D Loss: 1.3615686893463135\n",
            "Epoch 1, Batch 459, G Loss: 0.6974964737892151, D Loss: 1.3632643222808838\n",
            "Epoch 1, Batch 460, G Loss: 0.6975922584533691, D Loss: 1.3583972454071045\n",
            "Epoch 1, Batch 461, G Loss: 0.6977018117904663, D Loss: 1.3571932315826416\n",
            "Epoch 1, Batch 462, G Loss: 0.6977692246437073, D Loss: 1.3586037158966064\n",
            "Epoch 1, Batch 463, G Loss: 0.697868287563324, D Loss: 1.3569761514663696\n",
            "Epoch 1, Batch 464, G Loss: 0.6979967951774597, D Loss: 1.3494946956634521\n",
            "Epoch 1, Batch 465, G Loss: 0.6980364918708801, D Loss: 1.3458480834960938\n",
            "Epoch 1, Batch 466, G Loss: 0.6981289982795715, D Loss: 1.3584074974060059\n",
            "Epoch 1, Batch 467, G Loss: 0.6982932090759277, D Loss: 1.355605125427246\n",
            "Epoch 1, Batch 468, G Loss: 0.6983205080032349, D Loss: 1.3534934520721436\n",
            "Epoch 1, Batch 469, G Loss: 0.6983358263969421, D Loss: 1.3600051403045654\n",
            "Epoch 2, Batch 1, G Loss: 0.6984489560127258, D Loss: 1.3567261695861816\n",
            "Epoch 2, Batch 2, G Loss: 0.6987047791481018, D Loss: 1.3560564517974854\n",
            "Epoch 2, Batch 3, G Loss: 0.6987630724906921, D Loss: 1.3570594787597656\n",
            "Epoch 2, Batch 4, G Loss: 0.6989932656288147, D Loss: 1.3571304082870483\n",
            "Epoch 2, Batch 5, G Loss: 0.6990066766738892, D Loss: 1.3568347692489624\n",
            "Epoch 2, Batch 6, G Loss: 0.6990519165992737, D Loss: 1.3540898561477661\n",
            "Epoch 2, Batch 7, G Loss: 0.699144184589386, D Loss: 1.3542944192886353\n",
            "Epoch 2, Batch 8, G Loss: 0.6992484927177429, D Loss: 1.3557195663452148\n",
            "Epoch 2, Batch 9, G Loss: 0.6994203329086304, D Loss: 1.3580434322357178\n",
            "Epoch 2, Batch 10, G Loss: 0.6995415687561035, D Loss: 1.350641131401062\n",
            "Epoch 2, Batch 11, G Loss: 0.6995734572410583, D Loss: 1.3484065532684326\n",
            "Epoch 2, Batch 12, G Loss: 0.6996937990188599, D Loss: 1.3553309440612793\n",
            "Epoch 2, Batch 13, G Loss: 0.6997506022453308, D Loss: 1.3558073043823242\n",
            "Epoch 2, Batch 14, G Loss: 0.6998922228813171, D Loss: 1.3556475639343262\n",
            "Epoch 2, Batch 15, G Loss: 0.6999209523200989, D Loss: 1.3544819355010986\n",
            "Epoch 2, Batch 16, G Loss: 0.699982762336731, D Loss: 1.350191593170166\n",
            "Epoch 2, Batch 17, G Loss: 0.7000814080238342, D Loss: 1.3525631427764893\n",
            "Epoch 2, Batch 18, G Loss: 0.7002711892127991, D Loss: 1.3527687788009644\n",
            "Epoch 2, Batch 19, G Loss: 0.7003214955329895, D Loss: 1.353955626487732\n",
            "Epoch 2, Batch 20, G Loss: 0.7004066705703735, D Loss: 1.3526296615600586\n",
            "Epoch 2, Batch 21, G Loss: 0.7005797624588013, D Loss: 1.3515779972076416\n",
            "Epoch 2, Batch 22, G Loss: 0.7006388902664185, D Loss: 1.3532044887542725\n",
            "Epoch 2, Batch 23, G Loss: 0.7007596492767334, D Loss: 1.35117506980896\n",
            "Epoch 2, Batch 24, G Loss: 0.7008862495422363, D Loss: 1.3543431758880615\n",
            "Epoch 2, Batch 25, G Loss: 0.7010141611099243, D Loss: 1.3549425601959229\n",
            "Epoch 2, Batch 26, G Loss: 0.701141357421875, D Loss: 1.3548293113708496\n",
            "Epoch 2, Batch 27, G Loss: 0.7012571096420288, D Loss: 1.3536710739135742\n",
            "Epoch 2, Batch 28, G Loss: 0.7012783885002136, D Loss: 1.3536840677261353\n",
            "Epoch 2, Batch 29, G Loss: 0.7012763619422913, D Loss: 1.3488781452178955\n",
            "Epoch 2, Batch 30, G Loss: 0.7015317678451538, D Loss: 1.3524558544158936\n",
            "Epoch 2, Batch 31, G Loss: 0.7015271782875061, D Loss: 1.35298490524292\n",
            "Epoch 2, Batch 32, G Loss: 0.7017051577568054, D Loss: 1.3511236906051636\n",
            "Epoch 2, Batch 33, G Loss: 0.7017587423324585, D Loss: 1.3527569770812988\n",
            "Epoch 2, Batch 34, G Loss: 0.7019491195678711, D Loss: 1.351123332977295\n",
            "Epoch 2, Batch 35, G Loss: 0.7020027041435242, D Loss: 1.3530848026275635\n",
            "Epoch 2, Batch 36, G Loss: 0.7021152973175049, D Loss: 1.3484716415405273\n",
            "Epoch 2, Batch 37, G Loss: 0.7022088766098022, D Loss: 1.346008062362671\n",
            "Epoch 2, Batch 38, G Loss: 0.7023053169250488, D Loss: 1.3484992980957031\n",
            "Epoch 2, Batch 39, G Loss: 0.7024784684181213, D Loss: 1.3503282070159912\n",
            "Epoch 2, Batch 40, G Loss: 0.7025256752967834, D Loss: 1.3487725257873535\n",
            "Epoch 2, Batch 41, G Loss: 0.7026461958885193, D Loss: 1.3481215238571167\n",
            "Epoch 2, Batch 42, G Loss: 0.7026890516281128, D Loss: 1.3467808961868286\n",
            "Epoch 2, Batch 43, G Loss: 0.7027066349983215, D Loss: 1.3452967405319214\n",
            "Epoch 2, Batch 44, G Loss: 0.7029482126235962, D Loss: 1.3474777936935425\n",
            "Epoch 2, Batch 45, G Loss: 0.703021228313446, D Loss: 1.3481003046035767\n",
            "Epoch 2, Batch 46, G Loss: 0.7031233906745911, D Loss: 1.3506314754486084\n",
            "Epoch 2, Batch 47, G Loss: 0.7030789852142334, D Loss: 1.3456642627716064\n",
            "Epoch 2, Batch 48, G Loss: 0.7033331990242004, D Loss: 1.341261863708496\n",
            "Epoch 2, Batch 49, G Loss: 0.7034642100334167, D Loss: 1.3443520069122314\n",
            "Epoch 2, Batch 50, G Loss: 0.7036100625991821, D Loss: 1.3452130556106567\n",
            "Epoch 2, Batch 51, G Loss: 0.7035506963729858, D Loss: 1.3478331565856934\n",
            "Epoch 2, Batch 52, G Loss: 0.7036917805671692, D Loss: 1.3491404056549072\n",
            "Epoch 2, Batch 53, G Loss: 0.7039449214935303, D Loss: 1.3542406558990479\n",
            "Epoch 2, Batch 54, G Loss: 0.7039614915847778, D Loss: 1.350836992263794\n",
            "Epoch 2, Batch 55, G Loss: 0.7041921019554138, D Loss: 1.3459219932556152\n",
            "Epoch 2, Batch 56, G Loss: 0.7041923403739929, D Loss: 1.351525068283081\n",
            "Epoch 2, Batch 57, G Loss: 0.7041375041007996, D Loss: 1.3518824577331543\n",
            "Epoch 2, Batch 58, G Loss: 0.7044243216514587, D Loss: 1.3547849655151367\n",
            "Epoch 2, Batch 59, G Loss: 0.7045838236808777, D Loss: 1.3512202501296997\n",
            "Epoch 2, Batch 60, G Loss: 0.7046493887901306, D Loss: 1.3495311737060547\n",
            "Epoch 2, Batch 61, G Loss: 0.7045531868934631, D Loss: 1.3484448194503784\n",
            "Epoch 2, Batch 62, G Loss: 0.7047063112258911, D Loss: 1.3435065746307373\n",
            "Epoch 2, Batch 63, G Loss: 0.7050046324729919, D Loss: 1.3427157402038574\n",
            "Epoch 2, Batch 64, G Loss: 0.7051254510879517, D Loss: 1.3481009006500244\n",
            "Epoch 2, Batch 65, G Loss: 0.7050715088844299, D Loss: 1.3469754457473755\n",
            "Epoch 2, Batch 66, G Loss: 0.7053120136260986, D Loss: 1.3461644649505615\n",
            "Epoch 2, Batch 67, G Loss: 0.7052513957023621, D Loss: 1.3376193046569824\n",
            "Epoch 2, Batch 68, G Loss: 0.7055106163024902, D Loss: 1.3397729396820068\n",
            "Epoch 2, Batch 69, G Loss: 0.7055466175079346, D Loss: 1.3450772762298584\n",
            "Epoch 2, Batch 70, G Loss: 0.7057297229766846, D Loss: 1.3464833498001099\n",
            "Epoch 2, Batch 71, G Loss: 0.705769419670105, D Loss: 1.3465503454208374\n",
            "Epoch 2, Batch 72, G Loss: 0.7057782411575317, D Loss: 1.3491488695144653\n",
            "Epoch 2, Batch 73, G Loss: 0.7059496641159058, D Loss: 1.3484983444213867\n",
            "Epoch 2, Batch 74, G Loss: 0.705915629863739, D Loss: 1.3434081077575684\n",
            "Epoch 2, Batch 75, G Loss: 0.7062469720840454, D Loss: 1.3447277545928955\n",
            "Epoch 2, Batch 76, G Loss: 0.7063423991203308, D Loss: 1.343910574913025\n",
            "Epoch 2, Batch 77, G Loss: 0.7063789367675781, D Loss: 1.3446002006530762\n",
            "Epoch 2, Batch 78, G Loss: 0.7064245343208313, D Loss: 1.3459559679031372\n",
            "Epoch 2, Batch 79, G Loss: 0.7065361738204956, D Loss: 1.3438853025436401\n",
            "Epoch 2, Batch 80, G Loss: 0.7066933512687683, D Loss: 1.3408551216125488\n",
            "Epoch 2, Batch 81, G Loss: 0.7066802978515625, D Loss: 1.3345482349395752\n",
            "Epoch 2, Batch 82, G Loss: 0.7067856192588806, D Loss: 1.3392622470855713\n",
            "Epoch 2, Batch 83, G Loss: 0.7070205807685852, D Loss: 1.3421926498413086\n",
            "Epoch 2, Batch 84, G Loss: 0.7071322798728943, D Loss: 1.3465909957885742\n",
            "Epoch 2, Batch 85, G Loss: 0.7071817517280579, D Loss: 1.341883897781372\n",
            "Epoch 2, Batch 86, G Loss: 0.7073599100112915, D Loss: 1.3408169746398926\n",
            "Epoch 2, Batch 87, G Loss: 0.7074777483940125, D Loss: 1.3408982753753662\n",
            "Epoch 2, Batch 88, G Loss: 0.70745849609375, D Loss: 1.3427248001098633\n",
            "Epoch 2, Batch 89, G Loss: 0.7077423334121704, D Loss: 1.3435300588607788\n",
            "Epoch 2, Batch 90, G Loss: 0.707694411277771, D Loss: 1.3439929485321045\n",
            "Epoch 2, Batch 91, G Loss: 0.7077456116676331, D Loss: 1.3440732955932617\n",
            "Epoch 2, Batch 92, G Loss: 0.7080584168434143, D Loss: 1.3436331748962402\n",
            "Epoch 2, Batch 93, G Loss: 0.7080780267715454, D Loss: 1.3433406352996826\n",
            "Epoch 2, Batch 94, G Loss: 0.7082184553146362, D Loss: 1.3407994508743286\n",
            "Epoch 2, Batch 95, G Loss: 0.7082862257957458, D Loss: 1.3406234979629517\n",
            "Epoch 2, Batch 96, G Loss: 0.7082270383834839, D Loss: 1.3388422727584839\n",
            "Epoch 2, Batch 97, G Loss: 0.708518385887146, D Loss: 1.340214729309082\n",
            "Epoch 2, Batch 98, G Loss: 0.7087126970291138, D Loss: 1.3382484912872314\n",
            "Epoch 2, Batch 99, G Loss: 0.7087746262550354, D Loss: 1.338285207748413\n",
            "Epoch 2, Batch 100, G Loss: 0.708823561668396, D Loss: 1.342577338218689\n",
            "Epoch 2, Batch 101, G Loss: 0.7089505791664124, D Loss: 1.3440721035003662\n",
            "Epoch 2, Batch 102, G Loss: 0.7090387940406799, D Loss: 1.3348238468170166\n",
            "Epoch 2, Batch 103, G Loss: 0.7090924978256226, D Loss: 1.3344075679779053\n",
            "Epoch 2, Batch 104, G Loss: 0.7092229127883911, D Loss: 1.3374475240707397\n",
            "Epoch 2, Batch 105, G Loss: 0.7092953324317932, D Loss: 1.338648796081543\n",
            "Epoch 2, Batch 106, G Loss: 0.7093261480331421, D Loss: 1.3351900577545166\n",
            "Epoch 2, Batch 107, G Loss: 0.7092387676239014, D Loss: 1.3414981365203857\n",
            "Epoch 2, Batch 108, G Loss: 0.7096079587936401, D Loss: 1.3459151983261108\n",
            "Epoch 2, Batch 109, G Loss: 0.7094324827194214, D Loss: 1.3410698175430298\n",
            "Epoch 2, Batch 110, G Loss: 0.7098033428192139, D Loss: 1.3387031555175781\n",
            "Epoch 2, Batch 111, G Loss: 0.7097311019897461, D Loss: 1.3334503173828125\n",
            "Epoch 2, Batch 112, G Loss: 0.7100967764854431, D Loss: 1.335330605506897\n",
            "Epoch 2, Batch 113, G Loss: 0.7101097702980042, D Loss: 1.3394438028335571\n",
            "Epoch 2, Batch 114, G Loss: 0.7101376056671143, D Loss: 1.3438334465026855\n",
            "Epoch 2, Batch 115, G Loss: 0.7103830575942993, D Loss: 1.3396756649017334\n",
            "Epoch 2, Batch 116, G Loss: 0.7103928327560425, D Loss: 1.3407727479934692\n",
            "Epoch 2, Batch 117, G Loss: 0.7104148268699646, D Loss: 1.3409948348999023\n",
            "Epoch 2, Batch 118, G Loss: 0.7106595039367676, D Loss: 1.3420684337615967\n",
            "Epoch 2, Batch 119, G Loss: 0.7107442617416382, D Loss: 1.3435176610946655\n",
            "Epoch 2, Batch 120, G Loss: 0.710650622844696, D Loss: 1.343066930770874\n",
            "Epoch 2, Batch 121, G Loss: 0.710902214050293, D Loss: 1.3441295623779297\n",
            "Epoch 2, Batch 122, G Loss: 0.7109929919242859, D Loss: 1.3412625789642334\n",
            "Epoch 2, Batch 123, G Loss: 0.7111362814903259, D Loss: 1.3402762413024902\n",
            "Epoch 2, Batch 124, G Loss: 0.7111328840255737, D Loss: 1.3411118984222412\n",
            "Epoch 2, Batch 125, G Loss: 0.7109541893005371, D Loss: 1.3390109539031982\n",
            "Epoch 2, Batch 126, G Loss: 0.7111515998840332, D Loss: 1.3359785079956055\n",
            "Epoch 2, Batch 127, G Loss: 0.7111073732376099, D Loss: 1.3362932205200195\n",
            "Epoch 2, Batch 128, G Loss: 0.711376428604126, D Loss: 1.338879942893982\n",
            "Epoch 2, Batch 129, G Loss: 0.7114735245704651, D Loss: 1.3398361206054688\n",
            "Epoch 2, Batch 130, G Loss: 0.7116565108299255, D Loss: 1.339996576309204\n",
            "Epoch 2, Batch 131, G Loss: 0.7117551565170288, D Loss: 1.3411314487457275\n",
            "Epoch 2, Batch 132, G Loss: 0.7118539810180664, D Loss: 1.3406758308410645\n",
            "Epoch 2, Batch 133, G Loss: 0.7118377089500427, D Loss: 1.340210199356079\n",
            "Epoch 2, Batch 134, G Loss: 0.7116847038269043, D Loss: 1.339046835899353\n",
            "Epoch 2, Batch 135, G Loss: 0.7119095325469971, D Loss: 1.3352775573730469\n",
            "Epoch 2, Batch 136, G Loss: 0.7120798826217651, D Loss: 1.336221694946289\n",
            "Epoch 2, Batch 137, G Loss: 0.7120598554611206, D Loss: 1.3400013446807861\n",
            "Epoch 2, Batch 138, G Loss: 0.7120938897132874, D Loss: 1.3430999517440796\n",
            "Epoch 2, Batch 139, G Loss: 0.7121495604515076, D Loss: 1.342458724975586\n",
            "Epoch 2, Batch 140, G Loss: 0.7123048901557922, D Loss: 1.34346604347229\n",
            "Epoch 2, Batch 141, G Loss: 0.7123399972915649, D Loss: 1.3380045890808105\n",
            "Epoch 2, Batch 142, G Loss: 0.7125241160392761, D Loss: 1.3387809991836548\n",
            "Epoch 2, Batch 143, G Loss: 0.7124101519584656, D Loss: 1.337620735168457\n",
            "Epoch 2, Batch 144, G Loss: 0.7124689221382141, D Loss: 1.3403561115264893\n",
            "Epoch 2, Batch 145, G Loss: 0.7125173807144165, D Loss: 1.3392677307128906\n",
            "Epoch 2, Batch 146, G Loss: 0.7123509049415588, D Loss: 1.337634563446045\n",
            "Epoch 2, Batch 147, G Loss: 0.7125593423843384, D Loss: 1.3392930030822754\n",
            "Epoch 2, Batch 148, G Loss: 0.7126126885414124, D Loss: 1.3384904861450195\n",
            "Epoch 2, Batch 149, G Loss: 0.7129279971122742, D Loss: 1.3410453796386719\n",
            "Epoch 2, Batch 150, G Loss: 0.712590754032135, D Loss: 1.341202974319458\n",
            "Epoch 2, Batch 151, G Loss: 0.7128581404685974, D Loss: 1.3309839963912964\n",
            "Epoch 2, Batch 152, G Loss: 0.7128478288650513, D Loss: 1.3298876285552979\n",
            "Epoch 2, Batch 153, G Loss: 0.7127458453178406, D Loss: 1.3300700187683105\n",
            "Epoch 2, Batch 154, G Loss: 0.7126790881156921, D Loss: 1.3320006132125854\n",
            "Epoch 2, Batch 155, G Loss: 0.7131848931312561, D Loss: 1.330683708190918\n",
            "Epoch 2, Batch 156, G Loss: 0.7131525278091431, D Loss: 1.331908941268921\n",
            "Epoch 2, Batch 157, G Loss: 0.7131937742233276, D Loss: 1.3338338136672974\n",
            "Epoch 2, Batch 158, G Loss: 0.7131327390670776, D Loss: 1.3347322940826416\n",
            "Epoch 2, Batch 159, G Loss: 0.7130627632141113, D Loss: 1.3388313055038452\n",
            "Epoch 2, Batch 160, G Loss: 0.7130925059318542, D Loss: 1.336317777633667\n",
            "Epoch 2, Batch 161, G Loss: 0.7134615778923035, D Loss: 1.3342646360397339\n",
            "Epoch 2, Batch 162, G Loss: 0.7131025195121765, D Loss: 1.333370327949524\n",
            "Epoch 2, Batch 163, G Loss: 0.7134879231452942, D Loss: 1.3369749784469604\n",
            "Epoch 2, Batch 164, G Loss: 0.7134585976600647, D Loss: 1.3346160650253296\n",
            "Epoch 2, Batch 165, G Loss: 0.7133849859237671, D Loss: 1.3403592109680176\n",
            "Epoch 2, Batch 166, G Loss: 0.7132486701011658, D Loss: 1.3380087614059448\n",
            "Epoch 2, Batch 167, G Loss: 0.7133232951164246, D Loss: 1.3354501724243164\n",
            "Epoch 2, Batch 168, G Loss: 0.7133222222328186, D Loss: 1.3340553045272827\n",
            "Epoch 2, Batch 169, G Loss: 0.7133927345275879, D Loss: 1.3375132083892822\n",
            "Epoch 2, Batch 170, G Loss: 0.7132649421691895, D Loss: 1.3434208631515503\n",
            "Epoch 2, Batch 171, G Loss: 0.7133273482322693, D Loss: 1.3273487091064453\n",
            "Epoch 2, Batch 172, G Loss: 0.7134413719177246, D Loss: 1.3277509212493896\n",
            "Epoch 2, Batch 173, G Loss: 0.7133656740188599, D Loss: 1.3368247747421265\n",
            "Epoch 2, Batch 174, G Loss: 0.713464081287384, D Loss: 1.3368127346038818\n",
            "Epoch 2, Batch 175, G Loss: 0.7131316661834717, D Loss: 1.3385653495788574\n",
            "Epoch 2, Batch 176, G Loss: 0.713257908821106, D Loss: 1.3381646871566772\n",
            "Epoch 2, Batch 177, G Loss: 0.7132211923599243, D Loss: 1.334329605102539\n",
            "Epoch 2, Batch 178, G Loss: 0.7134639024734497, D Loss: 1.3272943496704102\n",
            "Epoch 2, Batch 179, G Loss: 0.7131995558738708, D Loss: 1.332689642906189\n",
            "Epoch 2, Batch 180, G Loss: 0.713135838508606, D Loss: 1.3310662508010864\n",
            "Epoch 2, Batch 181, G Loss: 0.7131462097167969, D Loss: 1.3303446769714355\n",
            "Epoch 2, Batch 182, G Loss: 0.7133285999298096, D Loss: 1.3359251022338867\n",
            "Epoch 2, Batch 183, G Loss: 0.7131617069244385, D Loss: 1.3345427513122559\n",
            "Epoch 2, Batch 184, G Loss: 0.7131949663162231, D Loss: 1.3384994268417358\n",
            "Epoch 2, Batch 185, G Loss: 0.713038444519043, D Loss: 1.341907024383545\n",
            "Epoch 2, Batch 186, G Loss: 0.713062584400177, D Loss: 1.3391950130462646\n",
            "Epoch 2, Batch 187, G Loss: 0.713151752948761, D Loss: 1.3375318050384521\n",
            "Epoch 2, Batch 188, G Loss: 0.7128260731697083, D Loss: 1.3371551036834717\n",
            "Epoch 2, Batch 189, G Loss: 0.7130439281463623, D Loss: 1.3381597995758057\n",
            "Epoch 2, Batch 190, G Loss: 0.7128562927246094, D Loss: 1.338928461074829\n",
            "Epoch 2, Batch 191, G Loss: 0.7127830386161804, D Loss: 1.3384933471679688\n",
            "Epoch 2, Batch 192, G Loss: 0.7125031352043152, D Loss: 1.3386499881744385\n",
            "Epoch 2, Batch 193, G Loss: 0.7123928666114807, D Loss: 1.3352017402648926\n",
            "Epoch 2, Batch 194, G Loss: 0.712640643119812, D Loss: 1.3385882377624512\n",
            "Epoch 2, Batch 195, G Loss: 0.7122189402580261, D Loss: 1.3456637859344482\n",
            "Epoch 2, Batch 196, G Loss: 0.712342381477356, D Loss: 1.3424041271209717\n",
            "Epoch 2, Batch 197, G Loss: 0.7121347784996033, D Loss: 1.3415241241455078\n",
            "Epoch 2, Batch 198, G Loss: 0.712190568447113, D Loss: 1.333702564239502\n",
            "Epoch 2, Batch 199, G Loss: 0.711915135383606, D Loss: 1.3317885398864746\n",
            "Epoch 2, Batch 200, G Loss: 0.7116109132766724, D Loss: 1.3395209312438965\n",
            "Epoch 2, Batch 201, G Loss: 0.7115540504455566, D Loss: 1.339377522468567\n",
            "Epoch 2, Batch 202, G Loss: 0.711452066898346, D Loss: 1.3478730916976929\n",
            "Epoch 2, Batch 203, G Loss: 0.711493968963623, D Loss: 1.3499951362609863\n",
            "Epoch 2, Batch 204, G Loss: 0.7112390995025635, D Loss: 1.3391108512878418\n",
            "Epoch 2, Batch 205, G Loss: 0.7109065055847168, D Loss: 1.3430031538009644\n",
            "Epoch 2, Batch 206, G Loss: 0.7110258340835571, D Loss: 1.346437931060791\n",
            "Epoch 2, Batch 207, G Loss: 0.7108137011528015, D Loss: 1.3475661277770996\n",
            "Epoch 2, Batch 208, G Loss: 0.7106013298034668, D Loss: 1.350492238998413\n",
            "Epoch 2, Batch 209, G Loss: 0.7099936604499817, D Loss: 1.3430843353271484\n",
            "Epoch 2, Batch 210, G Loss: 0.7101529240608215, D Loss: 1.3410061597824097\n",
            "Epoch 2, Batch 211, G Loss: 0.7099665403366089, D Loss: 1.3473520278930664\n",
            "Epoch 2, Batch 212, G Loss: 0.709770679473877, D Loss: 1.3420149087905884\n",
            "Epoch 2, Batch 213, G Loss: 0.7095537781715393, D Loss: 1.340519666671753\n",
            "Epoch 2, Batch 214, G Loss: 0.7094080448150635, D Loss: 1.3433724641799927\n",
            "Epoch 2, Batch 215, G Loss: 0.708835780620575, D Loss: 1.3468475341796875\n",
            "Epoch 2, Batch 216, G Loss: 0.7089300155639648, D Loss: 1.3487813472747803\n",
            "Epoch 2, Batch 217, G Loss: 0.7086440324783325, D Loss: 1.3517580032348633\n",
            "Epoch 2, Batch 218, G Loss: 0.7086745500564575, D Loss: 1.351507306098938\n",
            "Epoch 2, Batch 219, G Loss: 0.7080663442611694, D Loss: 1.3425638675689697\n",
            "Epoch 2, Batch 220, G Loss: 0.7079581618309021, D Loss: 1.3433237075805664\n",
            "Epoch 2, Batch 221, G Loss: 0.7078645825386047, D Loss: 1.3411660194396973\n",
            "Epoch 2, Batch 222, G Loss: 0.7076317667961121, D Loss: 1.342592477798462\n",
            "Epoch 2, Batch 223, G Loss: 0.7072643637657166, D Loss: 1.3435269594192505\n",
            "Epoch 2, Batch 224, G Loss: 0.7071552872657776, D Loss: 1.3468363285064697\n",
            "Epoch 2, Batch 225, G Loss: 0.7069463729858398, D Loss: 1.3500232696533203\n",
            "Epoch 2, Batch 226, G Loss: 0.706266462802887, D Loss: 1.348262071609497\n",
            "Epoch 2, Batch 227, G Loss: 0.7061487436294556, D Loss: 1.3485493659973145\n",
            "Epoch 2, Batch 228, G Loss: 0.7060959339141846, D Loss: 1.358216643333435\n",
            "Epoch 2, Batch 229, G Loss: 0.7055089473724365, D Loss: 1.350461483001709\n",
            "Epoch 2, Batch 230, G Loss: 0.7055198550224304, D Loss: 1.3447725772857666\n",
            "Epoch 2, Batch 231, G Loss: 0.7051374316215515, D Loss: 1.352376103401184\n",
            "Epoch 2, Batch 232, G Loss: 0.7051454782485962, D Loss: 1.3583691120147705\n",
            "Epoch 2, Batch 233, G Loss: 0.704632043838501, D Loss: 1.36246657371521\n",
            "Epoch 2, Batch 234, G Loss: 0.7044065594673157, D Loss: 1.3625612258911133\n",
            "Epoch 2, Batch 235, G Loss: 0.7043019533157349, D Loss: 1.361725091934204\n",
            "Epoch 2, Batch 236, G Loss: 0.7040306925773621, D Loss: 1.3591258525848389\n",
            "Epoch 2, Batch 237, G Loss: 0.7037770748138428, D Loss: 1.3605029582977295\n",
            "Epoch 2, Batch 238, G Loss: 0.7034133672714233, D Loss: 1.3555171489715576\n",
            "Epoch 2, Batch 239, G Loss: 0.7032402157783508, D Loss: 1.3585877418518066\n",
            "Epoch 2, Batch 240, G Loss: 0.7029635310173035, D Loss: 1.370807409286499\n",
            "Epoch 2, Batch 241, G Loss: 0.7025512456893921, D Loss: 1.3649232387542725\n",
            "Epoch 2, Batch 242, G Loss: 0.7023138999938965, D Loss: 1.363752841949463\n",
            "Epoch 2, Batch 243, G Loss: 0.7021573185920715, D Loss: 1.3648912906646729\n",
            "Epoch 2, Batch 244, G Loss: 0.7018833756446838, D Loss: 1.3661388158798218\n",
            "Epoch 2, Batch 245, G Loss: 0.7015839219093323, D Loss: 1.3666285276412964\n",
            "Epoch 2, Batch 246, G Loss: 0.7013099789619446, D Loss: 1.3679295778274536\n",
            "Epoch 2, Batch 247, G Loss: 0.7010244131088257, D Loss: 1.371948003768921\n",
            "Epoch 2, Batch 248, G Loss: 0.7009173035621643, D Loss: 1.3713304996490479\n",
            "Epoch 2, Batch 249, G Loss: 0.7006241083145142, D Loss: 1.3684343099594116\n",
            "Epoch 2, Batch 250, G Loss: 0.7003284096717834, D Loss: 1.3684921264648438\n",
            "Epoch 2, Batch 251, G Loss: 0.7001498341560364, D Loss: 1.3635532855987549\n",
            "Epoch 2, Batch 252, G Loss: 0.6997535228729248, D Loss: 1.368173599243164\n",
            "Epoch 2, Batch 253, G Loss: 0.699646532535553, D Loss: 1.3634638786315918\n",
            "Epoch 2, Batch 254, G Loss: 0.6993944644927979, D Loss: 1.3618791103363037\n",
            "Epoch 2, Batch 255, G Loss: 0.6990442872047424, D Loss: 1.371410846710205\n",
            "Epoch 2, Batch 256, G Loss: 0.6990219354629517, D Loss: 1.3754255771636963\n",
            "Epoch 2, Batch 257, G Loss: 0.6988632678985596, D Loss: 1.373682975769043\n",
            "Epoch 2, Batch 258, G Loss: 0.6986684203147888, D Loss: 1.3679311275482178\n",
            "Epoch 2, Batch 259, G Loss: 0.6983228325843811, D Loss: 1.3688771724700928\n",
            "Epoch 2, Batch 260, G Loss: 0.6981082558631897, D Loss: 1.3705271482467651\n",
            "Epoch 2, Batch 261, G Loss: 0.6981558799743652, D Loss: 1.3740079402923584\n",
            "Epoch 2, Batch 262, G Loss: 0.6979537010192871, D Loss: 1.3788261413574219\n",
            "Epoch 2, Batch 263, G Loss: 0.697783887386322, D Loss: 1.3823928833007812\n",
            "Epoch 2, Batch 264, G Loss: 0.6976615786552429, D Loss: 1.3753572702407837\n",
            "Epoch 2, Batch 265, G Loss: 0.6974260210990906, D Loss: 1.3712413311004639\n",
            "Epoch 2, Batch 266, G Loss: 0.6972828507423401, D Loss: 1.3804665803909302\n",
            "Epoch 2, Batch 267, G Loss: 0.6972459554672241, D Loss: 1.377099871635437\n",
            "Epoch 2, Batch 268, G Loss: 0.6971530318260193, D Loss: 1.381094217300415\n",
            "Epoch 2, Batch 269, G Loss: 0.6968587636947632, D Loss: 1.3757236003875732\n",
            "Epoch 2, Batch 270, G Loss: 0.6967162489891052, D Loss: 1.3769664764404297\n",
            "Epoch 2, Batch 271, G Loss: 0.6968694925308228, D Loss: 1.3776403665542603\n",
            "Epoch 2, Batch 272, G Loss: 0.696509063243866, D Loss: 1.3786578178405762\n",
            "Epoch 2, Batch 273, G Loss: 0.6965696215629578, D Loss: 1.379583716392517\n",
            "Epoch 2, Batch 274, G Loss: 0.6965087056159973, D Loss: 1.3775885105133057\n",
            "Epoch 2, Batch 275, G Loss: 0.6965783834457397, D Loss: 1.3835229873657227\n",
            "Epoch 2, Batch 276, G Loss: 0.6964322924613953, D Loss: 1.3857510089874268\n",
            "Epoch 2, Batch 277, G Loss: 0.6964235901832581, D Loss: 1.3796117305755615\n",
            "Epoch 2, Batch 278, G Loss: 0.6962745785713196, D Loss: 1.3811829090118408\n",
            "Epoch 2, Batch 279, G Loss: 0.6961744427680969, D Loss: 1.3840796947479248\n",
            "Epoch 2, Batch 280, G Loss: 0.6961891055107117, D Loss: 1.3845891952514648\n",
            "Epoch 2, Batch 281, G Loss: 0.6963834166526794, D Loss: 1.3898379802703857\n",
            "Epoch 2, Batch 282, G Loss: 0.6958369016647339, D Loss: 1.3949635028839111\n",
            "Epoch 2, Batch 283, G Loss: 0.6956260204315186, D Loss: 1.3774373531341553\n",
            "Epoch 2, Batch 284, G Loss: 0.6959673762321472, D Loss: 1.377345323562622\n",
            "Epoch 2, Batch 285, G Loss: 0.6962438225746155, D Loss: 1.3866573572158813\n",
            "Epoch 2, Batch 286, G Loss: 0.6959192752838135, D Loss: 1.3814972639083862\n",
            "Epoch 2, Batch 287, G Loss: 0.6958462595939636, D Loss: 1.3751254081726074\n",
            "Epoch 2, Batch 288, G Loss: 0.6959946155548096, D Loss: 1.379934549331665\n",
            "Epoch 2, Batch 289, G Loss: 0.6959324479103088, D Loss: 1.3867406845092773\n",
            "Epoch 2, Batch 290, G Loss: 0.695975661277771, D Loss: 1.3849799633026123\n",
            "Epoch 2, Batch 291, G Loss: 0.6961599588394165, D Loss: 1.3804457187652588\n",
            "Epoch 2, Batch 292, G Loss: 0.6958063840866089, D Loss: 1.3840256929397583\n",
            "Epoch 2, Batch 293, G Loss: 0.6960092782974243, D Loss: 1.3897614479064941\n",
            "Epoch 2, Batch 294, G Loss: 0.6960751414299011, D Loss: 1.3941562175750732\n",
            "Epoch 2, Batch 295, G Loss: 0.6961318850517273, D Loss: 1.3908138275146484\n",
            "Epoch 2, Batch 296, G Loss: 0.696165144443512, D Loss: 1.3867413997650146\n",
            "Epoch 2, Batch 297, G Loss: 0.6958630681037903, D Loss: 1.3926217555999756\n",
            "Epoch 2, Batch 298, G Loss: 0.6963093280792236, D Loss: 1.3926408290863037\n",
            "Epoch 2, Batch 299, G Loss: 0.6966550946235657, D Loss: 1.3888648748397827\n",
            "Epoch 2, Batch 300, G Loss: 0.6961336135864258, D Loss: 1.394355058670044\n",
            "Epoch 2, Batch 301, G Loss: 0.6964446306228638, D Loss: 1.395526647567749\n",
            "Epoch 2, Batch 302, G Loss: 0.6965212225914001, D Loss: 1.3886327743530273\n",
            "Epoch 2, Batch 303, G Loss: 0.6968429088592529, D Loss: 1.392699956893921\n",
            "Epoch 2, Batch 304, G Loss: 0.6964194774627686, D Loss: 1.391660451889038\n",
            "Epoch 2, Batch 305, G Loss: 0.6966003179550171, D Loss: 1.3961281776428223\n",
            "Epoch 2, Batch 306, G Loss: 0.6965875625610352, D Loss: 1.389140009880066\n",
            "Epoch 2, Batch 307, G Loss: 0.6970908641815186, D Loss: 1.3879841566085815\n",
            "Epoch 2, Batch 308, G Loss: 0.697490394115448, D Loss: 1.389728307723999\n",
            "Epoch 2, Batch 309, G Loss: 0.6972053647041321, D Loss: 1.390453815460205\n",
            "Epoch 2, Batch 310, G Loss: 0.6971480846405029, D Loss: 1.390606164932251\n",
            "Epoch 2, Batch 311, G Loss: 0.6973314881324768, D Loss: 1.400321364402771\n",
            "Epoch 2, Batch 312, G Loss: 0.6973815560340881, D Loss: 1.3989585638046265\n",
            "Epoch 2, Batch 313, G Loss: 0.6976485252380371, D Loss: 1.3935871124267578\n",
            "Epoch 2, Batch 314, G Loss: 0.6979057192802429, D Loss: 1.392478585243225\n",
            "Epoch 2, Batch 315, G Loss: 0.6974290609359741, D Loss: 1.3972703218460083\n",
            "Epoch 2, Batch 316, G Loss: 0.6980620622634888, D Loss: 1.3940238952636719\n",
            "Epoch 2, Batch 317, G Loss: 0.6982895135879517, D Loss: 1.3885494470596313\n",
            "Epoch 2, Batch 318, G Loss: 0.6981601119041443, D Loss: 1.3956308364868164\n",
            "Epoch 2, Batch 319, G Loss: 0.6980956792831421, D Loss: 1.399177074432373\n",
            "Epoch 2, Batch 320, G Loss: 0.6984062194824219, D Loss: 1.397346019744873\n",
            "Epoch 2, Batch 321, G Loss: 0.6984771490097046, D Loss: 1.3981471061706543\n",
            "Epoch 2, Batch 322, G Loss: 0.69865882396698, D Loss: 1.395738124847412\n",
            "Epoch 2, Batch 323, G Loss: 0.6986154317855835, D Loss: 1.3965210914611816\n",
            "Epoch 2, Batch 324, G Loss: 0.6989725828170776, D Loss: 1.394585132598877\n",
            "Epoch 2, Batch 325, G Loss: 0.6990537047386169, D Loss: 1.3973617553710938\n",
            "Epoch 2, Batch 326, G Loss: 0.6992185115814209, D Loss: 1.4013543128967285\n",
            "Epoch 2, Batch 327, G Loss: 0.6992131471633911, D Loss: 1.404632806777954\n",
            "Epoch 2, Batch 328, G Loss: 0.6994023323059082, D Loss: 1.3938608169555664\n",
            "Epoch 2, Batch 329, G Loss: 0.6993705630302429, D Loss: 1.3913072347640991\n",
            "Epoch 2, Batch 330, G Loss: 0.6997706890106201, D Loss: 1.3937101364135742\n",
            "Epoch 2, Batch 331, G Loss: 0.6996667981147766, D Loss: 1.3957469463348389\n",
            "Epoch 2, Batch 332, G Loss: 0.699669361114502, D Loss: 1.398209571838379\n",
            "Epoch 2, Batch 333, G Loss: 0.6992263793945312, D Loss: 1.396333932876587\n",
            "Epoch 2, Batch 334, G Loss: 0.6998736262321472, D Loss: 1.3915115594863892\n",
            "Epoch 2, Batch 335, G Loss: 0.7001075148582458, D Loss: 1.3930509090423584\n",
            "Epoch 2, Batch 336, G Loss: 0.6997836232185364, D Loss: 1.401139736175537\n",
            "Epoch 2, Batch 337, G Loss: 0.7003428936004639, D Loss: 1.401588797569275\n",
            "Epoch 2, Batch 338, G Loss: 0.7006335854530334, D Loss: 1.393949270248413\n",
            "Epoch 2, Batch 339, G Loss: 0.7007043957710266, D Loss: 1.3967591524124146\n",
            "Epoch 2, Batch 340, G Loss: 0.7007409930229187, D Loss: 1.3958349227905273\n",
            "Epoch 2, Batch 341, G Loss: 0.7008872628211975, D Loss: 1.3974668979644775\n",
            "Epoch 2, Batch 342, G Loss: 0.7011550068855286, D Loss: 1.3965407609939575\n",
            "Epoch 2, Batch 343, G Loss: 0.701312780380249, D Loss: 1.3995945453643799\n",
            "Epoch 2, Batch 344, G Loss: 0.7010011672973633, D Loss: 1.4028239250183105\n",
            "Epoch 2, Batch 345, G Loss: 0.7013250589370728, D Loss: 1.3991329669952393\n",
            "Epoch 2, Batch 346, G Loss: 0.7013684511184692, D Loss: 1.398255467414856\n",
            "Epoch 2, Batch 347, G Loss: 0.7013245224952698, D Loss: 1.3989064693450928\n",
            "Epoch 2, Batch 348, G Loss: 0.7014457583427429, D Loss: 1.3980203866958618\n",
            "Epoch 2, Batch 349, G Loss: 0.7015479207038879, D Loss: 1.3989728689193726\n",
            "Epoch 2, Batch 350, G Loss: 0.701860249042511, D Loss: 1.4000728130340576\n",
            "Epoch 2, Batch 351, G Loss: 0.7021744251251221, D Loss: 1.3998081684112549\n",
            "Epoch 2, Batch 352, G Loss: 0.7018932700157166, D Loss: 1.3969277143478394\n",
            "Epoch 2, Batch 353, G Loss: 0.7021510601043701, D Loss: 1.3967320919036865\n",
            "Epoch 2, Batch 354, G Loss: 0.7021939158439636, D Loss: 1.3973307609558105\n",
            "Epoch 2, Batch 355, G Loss: 0.702370285987854, D Loss: 1.399552583694458\n",
            "Epoch 2, Batch 356, G Loss: 0.7022636532783508, D Loss: 1.4008606672286987\n",
            "Epoch 2, Batch 357, G Loss: 0.7025574445724487, D Loss: 1.3993966579437256\n",
            "Epoch 2, Batch 358, G Loss: 0.7024889588356018, D Loss: 1.3984601497650146\n",
            "Epoch 2, Batch 359, G Loss: 0.7024762034416199, D Loss: 1.4006476402282715\n",
            "Epoch 2, Batch 360, G Loss: 0.7026957273483276, D Loss: 1.4049477577209473\n",
            "Epoch 2, Batch 361, G Loss: 0.7029950022697449, D Loss: 1.4022090435028076\n",
            "Epoch 2, Batch 362, G Loss: 0.7029136419296265, D Loss: 1.4002153873443604\n",
            "Epoch 2, Batch 363, G Loss: 0.7030222415924072, D Loss: 1.3973228931427002\n",
            "Epoch 2, Batch 364, G Loss: 0.703149676322937, D Loss: 1.3985633850097656\n",
            "Epoch 2, Batch 365, G Loss: 0.7030652165412903, D Loss: 1.3954052925109863\n",
            "Epoch 2, Batch 366, G Loss: 0.7036011219024658, D Loss: 1.3947956562042236\n",
            "Epoch 2, Batch 367, G Loss: 0.7030739188194275, D Loss: 1.3963418006896973\n",
            "Epoch 2, Batch 368, G Loss: 0.7033770680427551, D Loss: 1.3976142406463623\n",
            "Epoch 2, Batch 369, G Loss: 0.7030156254768372, D Loss: 1.4002430438995361\n",
            "Epoch 2, Batch 370, G Loss: 0.7040532827377319, D Loss: 1.402455449104309\n",
            "Epoch 2, Batch 371, G Loss: 0.7037172913551331, D Loss: 1.3956284523010254\n",
            "Epoch 2, Batch 372, G Loss: 0.7038329243659973, D Loss: 1.3941514492034912\n",
            "Epoch 2, Batch 373, G Loss: 0.7036293148994446, D Loss: 1.3918735980987549\n",
            "Epoch 2, Batch 374, G Loss: 0.7038583755493164, D Loss: 1.3949711322784424\n",
            "Epoch 2, Batch 375, G Loss: 0.704253077507019, D Loss: 1.3962950706481934\n",
            "Epoch 2, Batch 376, G Loss: 0.7041358351707458, D Loss: 1.3930251598358154\n",
            "Epoch 2, Batch 377, G Loss: 0.703753650188446, D Loss: 1.392120361328125\n",
            "Epoch 2, Batch 378, G Loss: 0.7038955688476562, D Loss: 1.3931941986083984\n",
            "Epoch 2, Batch 379, G Loss: 0.7040435671806335, D Loss: 1.3945237398147583\n",
            "Epoch 2, Batch 380, G Loss: 0.7042150497436523, D Loss: 1.3947715759277344\n",
            "Epoch 2, Batch 381, G Loss: 0.7043047547340393, D Loss: 1.3945392370224\n",
            "Epoch 2, Batch 382, G Loss: 0.7044163346290588, D Loss: 1.3952414989471436\n",
            "Epoch 2, Batch 383, G Loss: 0.7046913504600525, D Loss: 1.4033279418945312\n",
            "Epoch 2, Batch 384, G Loss: 0.7046111226081848, D Loss: 1.3982867002487183\n",
            "Epoch 2, Batch 385, G Loss: 0.7045714855194092, D Loss: 1.3960464000701904\n",
            "Epoch 2, Batch 386, G Loss: 0.7046122550964355, D Loss: 1.398268222808838\n",
            "Epoch 2, Batch 387, G Loss: 0.704627513885498, D Loss: 1.3971152305603027\n",
            "Epoch 2, Batch 388, G Loss: 0.7050107717514038, D Loss: 1.3977422714233398\n",
            "Epoch 2, Batch 389, G Loss: 0.704809308052063, D Loss: 1.3978784084320068\n",
            "Epoch 2, Batch 390, G Loss: 0.705053448677063, D Loss: 1.3996449708938599\n",
            "Epoch 2, Batch 391, G Loss: 0.7049735188484192, D Loss: 1.4010274410247803\n",
            "Epoch 2, Batch 392, G Loss: 0.7051039934158325, D Loss: 1.3976407051086426\n",
            "Epoch 2, Batch 393, G Loss: 0.7048454880714417, D Loss: 1.3993581533432007\n",
            "Epoch 2, Batch 394, G Loss: 0.7049860954284668, D Loss: 1.3989354372024536\n",
            "Epoch 2, Batch 395, G Loss: 0.7053754925727844, D Loss: 1.3974299430847168\n",
            "Epoch 2, Batch 396, G Loss: 0.705170750617981, D Loss: 1.399430751800537\n",
            "Epoch 2, Batch 397, G Loss: 0.7053917050361633, D Loss: 1.395451545715332\n",
            "Epoch 2, Batch 398, G Loss: 0.7053934335708618, D Loss: 1.3961210250854492\n",
            "Epoch 2, Batch 399, G Loss: 0.7051935195922852, D Loss: 1.3985073566436768\n",
            "Epoch 2, Batch 400, G Loss: 0.705342173576355, D Loss: 1.394355297088623\n",
            "Epoch 2, Batch 401, G Loss: 0.7054809927940369, D Loss: 1.3924578428268433\n",
            "Epoch 2, Batch 402, G Loss: 0.7053353190422058, D Loss: 1.395833969116211\n",
            "Epoch 2, Batch 403, G Loss: 0.7056155800819397, D Loss: 1.3958780765533447\n",
            "Epoch 2, Batch 404, G Loss: 0.7054740786552429, D Loss: 1.3969027996063232\n",
            "Epoch 2, Batch 405, G Loss: 0.7055373191833496, D Loss: 1.398756504058838\n",
            "Epoch 2, Batch 406, G Loss: 0.7055768370628357, D Loss: 1.398502230644226\n",
            "Epoch 2, Batch 407, G Loss: 0.7056833505630493, D Loss: 1.4006155729293823\n",
            "Epoch 2, Batch 408, G Loss: 0.705577552318573, D Loss: 1.3999311923980713\n",
            "Epoch 2, Batch 409, G Loss: 0.705680251121521, D Loss: 1.3996851444244385\n",
            "Epoch 2, Batch 410, G Loss: 0.70548015832901, D Loss: 1.3955732583999634\n",
            "Epoch 2, Batch 411, G Loss: 0.7054969072341919, D Loss: 1.3961372375488281\n",
            "Epoch 2, Batch 412, G Loss: 0.7058039903640747, D Loss: 1.3985000848770142\n",
            "Epoch 2, Batch 413, G Loss: 0.7054837942123413, D Loss: 1.399046540260315\n",
            "Epoch 2, Batch 414, G Loss: 0.705718994140625, D Loss: 1.395300269126892\n",
            "Epoch 2, Batch 415, G Loss: 0.7059028744697571, D Loss: 1.393164038658142\n",
            "Epoch 2, Batch 416, G Loss: 0.7056021690368652, D Loss: 1.3967750072479248\n",
            "Epoch 2, Batch 417, G Loss: 0.7061065435409546, D Loss: 1.3972899913787842\n",
            "Epoch 2, Batch 418, G Loss: 0.705656886100769, D Loss: 1.3976988792419434\n",
            "Epoch 2, Batch 419, G Loss: 0.7055264115333557, D Loss: 1.399406909942627\n",
            "Epoch 2, Batch 420, G Loss: 0.7058524489402771, D Loss: 1.3981049060821533\n",
            "Epoch 2, Batch 421, G Loss: 0.7057608962059021, D Loss: 1.3996264934539795\n",
            "Epoch 2, Batch 422, G Loss: 0.7059001326560974, D Loss: 1.3951194286346436\n",
            "Epoch 2, Batch 423, G Loss: 0.7055318355560303, D Loss: 1.3966310024261475\n",
            "Epoch 2, Batch 424, G Loss: 0.705507218837738, D Loss: 1.3990979194641113\n",
            "Epoch 2, Batch 425, G Loss: 0.7057226300239563, D Loss: 1.3970096111297607\n",
            "Epoch 2, Batch 426, G Loss: 0.7057378888130188, D Loss: 1.394242286682129\n",
            "Epoch 2, Batch 427, G Loss: 0.7056097984313965, D Loss: 1.394195795059204\n",
            "Epoch 2, Batch 428, G Loss: 0.7057927846908569, D Loss: 1.3943278789520264\n",
            "Epoch 2, Batch 429, G Loss: 0.7056529521942139, D Loss: 1.3962594270706177\n",
            "Epoch 2, Batch 430, G Loss: 0.7058941721916199, D Loss: 1.3950989246368408\n",
            "Epoch 2, Batch 431, G Loss: 0.7058601975440979, D Loss: 1.3954999446868896\n",
            "Epoch 2, Batch 432, G Loss: 0.7057077884674072, D Loss: 1.3952525854110718\n",
            "Epoch 2, Batch 433, G Loss: 0.705666184425354, D Loss: 1.3973485231399536\n",
            "Epoch 2, Batch 434, G Loss: 0.7056056261062622, D Loss: 1.3947283029556274\n",
            "Epoch 2, Batch 435, G Loss: 0.7057530879974365, D Loss: 1.3915185928344727\n",
            "Epoch 2, Batch 436, G Loss: 0.7057326436042786, D Loss: 1.3907413482666016\n",
            "Epoch 2, Batch 437, G Loss: 0.7057487368583679, D Loss: 1.3951480388641357\n",
            "Epoch 2, Batch 438, G Loss: 0.7058220505714417, D Loss: 1.395219087600708\n",
            "Epoch 2, Batch 439, G Loss: 0.7055569887161255, D Loss: 1.3956282138824463\n",
            "Epoch 2, Batch 440, G Loss: 0.7056305408477783, D Loss: 1.39646315574646\n",
            "Epoch 2, Batch 441, G Loss: 0.7060747146606445, D Loss: 1.3980486392974854\n",
            "Epoch 2, Batch 442, G Loss: 0.7057633399963379, D Loss: 1.3962292671203613\n",
            "Epoch 2, Batch 443, G Loss: 0.7055541276931763, D Loss: 1.39680814743042\n",
            "Epoch 2, Batch 444, G Loss: 0.7056978344917297, D Loss: 1.3924286365509033\n",
            "Epoch 2, Batch 445, G Loss: 0.7057541608810425, D Loss: 1.3949084281921387\n",
            "Epoch 2, Batch 446, G Loss: 0.7054913640022278, D Loss: 1.3995647430419922\n",
            "Epoch 2, Batch 447, G Loss: 0.7056930065155029, D Loss: 1.397279977798462\n",
            "Epoch 2, Batch 448, G Loss: 0.7058051228523254, D Loss: 1.3950703144073486\n",
            "Epoch 2, Batch 449, G Loss: 0.705718457698822, D Loss: 1.3954896926879883\n",
            "Epoch 2, Batch 450, G Loss: 0.7059155106544495, D Loss: 1.394015908241272\n",
            "Epoch 2, Batch 451, G Loss: 0.705700695514679, D Loss: 1.3974190950393677\n",
            "Epoch 2, Batch 452, G Loss: 0.7054686546325684, D Loss: 1.3987457752227783\n",
            "Epoch 2, Batch 453, G Loss: 0.7054822444915771, D Loss: 1.397290587425232\n",
            "Epoch 2, Batch 454, G Loss: 0.7056677937507629, D Loss: 1.3966621160507202\n",
            "Epoch 2, Batch 455, G Loss: 0.7054595947265625, D Loss: 1.3971805572509766\n",
            "Epoch 2, Batch 456, G Loss: 0.7058408856391907, D Loss: 1.3936086893081665\n",
            "Epoch 2, Batch 457, G Loss: 0.705455482006073, D Loss: 1.3920326232910156\n",
            "Epoch 2, Batch 458, G Loss: 0.7055153250694275, D Loss: 1.3984997272491455\n",
            "Epoch 2, Batch 459, G Loss: 0.7055678367614746, D Loss: 1.4002792835235596\n",
            "Epoch 2, Batch 460, G Loss: 0.7052380442619324, D Loss: 1.39605712890625\n",
            "Epoch 2, Batch 461, G Loss: 0.7057297229766846, D Loss: 1.3940277099609375\n",
            "Epoch 2, Batch 462, G Loss: 0.7053842544555664, D Loss: 1.3960623741149902\n",
            "Epoch 2, Batch 463, G Loss: 0.7054166197776794, D Loss: 1.3941264152526855\n",
            "Epoch 2, Batch 464, G Loss: 0.7056102156639099, D Loss: 1.3869990110397339\n",
            "Epoch 2, Batch 465, G Loss: 0.7052045464515686, D Loss: 1.3837101459503174\n",
            "Epoch 2, Batch 466, G Loss: 0.7052465081214905, D Loss: 1.396092414855957\n",
            "Epoch 2, Batch 467, G Loss: 0.7052403688430786, D Loss: 1.3933994770050049\n",
            "Epoch 2, Batch 468, G Loss: 0.7053583860397339, D Loss: 1.3911998271942139\n",
            "Epoch 2, Batch 469, G Loss: 0.7053389549255371, D Loss: 1.3977007865905762\n",
            "Epoch 3, Batch 1, G Loss: 0.7051737904548645, D Loss: 1.3946478366851807\n",
            "Epoch 3, Batch 2, G Loss: 0.7052795886993408, D Loss: 1.3941237926483154\n",
            "Epoch 3, Batch 3, G Loss: 0.705194890499115, D Loss: 1.3950562477111816\n",
            "Epoch 3, Batch 4, G Loss: 0.7050719261169434, D Loss: 1.395505428314209\n",
            "Epoch 3, Batch 5, G Loss: 0.7051546573638916, D Loss: 1.3951555490493774\n",
            "Epoch 3, Batch 6, G Loss: 0.705048680305481, D Loss: 1.3929038047790527\n",
            "Epoch 3, Batch 7, G Loss: 0.7050023078918457, D Loss: 1.3931105136871338\n",
            "Epoch 3, Batch 8, G Loss: 0.7048777341842651, D Loss: 1.394499659538269\n",
            "Epoch 3, Batch 9, G Loss: 0.7048903703689575, D Loss: 1.3966913223266602\n",
            "Epoch 3, Batch 10, G Loss: 0.7047538757324219, D Loss: 1.3904199600219727\n",
            "Epoch 3, Batch 11, G Loss: 0.7047085165977478, D Loss: 1.388561725616455\n",
            "Epoch 3, Batch 12, G Loss: 0.7045159339904785, D Loss: 1.3947460651397705\n",
            "Epoch 3, Batch 13, G Loss: 0.7046306133270264, D Loss: 1.3951138257980347\n",
            "Epoch 3, Batch 14, G Loss: 0.7048156261444092, D Loss: 1.394819736480713\n",
            "Epoch 3, Batch 15, G Loss: 0.7046476006507874, D Loss: 1.393869400024414\n",
            "Epoch 3, Batch 16, G Loss: 0.7047151327133179, D Loss: 1.3903546333312988\n",
            "Epoch 3, Batch 17, G Loss: 0.704473078250885, D Loss: 1.3926678895950317\n",
            "Epoch 3, Batch 18, G Loss: 0.704486608505249, D Loss: 1.3931078910827637\n",
            "Epoch 3, Batch 19, G Loss: 0.7043716907501221, D Loss: 1.394089698791504\n",
            "Epoch 3, Batch 20, G Loss: 0.7043634653091431, D Loss: 1.3931726217269897\n",
            "Epoch 3, Batch 21, G Loss: 0.7043794989585876, D Loss: 1.392478346824646\n",
            "Epoch 3, Batch 22, G Loss: 0.704443097114563, D Loss: 1.3934175968170166\n",
            "Epoch 3, Batch 23, G Loss: 0.7044073343276978, D Loss: 1.39223051071167\n",
            "Epoch 3, Batch 24, G Loss: 0.704253613948822, D Loss: 1.3947293758392334\n",
            "Epoch 3, Batch 25, G Loss: 0.7041305899620056, D Loss: 1.3953251838684082\n",
            "Epoch 3, Batch 26, G Loss: 0.704203724861145, D Loss: 1.3948333263397217\n",
            "Epoch 3, Batch 27, G Loss: 0.7040913701057434, D Loss: 1.3942787647247314\n",
            "Epoch 3, Batch 28, G Loss: 0.7039840221405029, D Loss: 1.394437313079834\n",
            "Epoch 3, Batch 29, G Loss: 0.7039875388145447, D Loss: 1.391426682472229\n",
            "Epoch 3, Batch 30, G Loss: 0.7042258381843567, D Loss: 1.393576741218567\n",
            "Epoch 3, Batch 31, G Loss: 0.703862726688385, D Loss: 1.3941218852996826\n",
            "Epoch 3, Batch 32, G Loss: 0.7039549946784973, D Loss: 1.3928852081298828\n",
            "Epoch 3, Batch 33, G Loss: 0.7039446830749512, D Loss: 1.3937445878982544\n",
            "Epoch 3, Batch 34, G Loss: 0.7038559317588806, D Loss: 1.3930935859680176\n",
            "Epoch 3, Batch 35, G Loss: 0.7037238478660583, D Loss: 1.394456386566162\n",
            "Epoch 3, Batch 36, G Loss: 0.7036557793617249, D Loss: 1.391406774520874\n",
            "Epoch 3, Batch 37, G Loss: 0.7036974430084229, D Loss: 1.389959454536438\n",
            "Epoch 3, Batch 38, G Loss: 0.7036639451980591, D Loss: 1.3917369842529297\n",
            "Epoch 3, Batch 39, G Loss: 0.7035720348358154, D Loss: 1.392723560333252\n",
            "Epoch 3, Batch 40, G Loss: 0.7036973834037781, D Loss: 1.391815185546875\n",
            "Epoch 3, Batch 41, G Loss: 0.7035415172576904, D Loss: 1.3916335105895996\n",
            "Epoch 3, Batch 42, G Loss: 0.7035584449768066, D Loss: 1.3909451961517334\n",
            "Epoch 3, Batch 43, G Loss: 0.703545868396759, D Loss: 1.3897662162780762\n",
            "Epoch 3, Batch 44, G Loss: 0.7033249139785767, D Loss: 1.3915116786956787\n",
            "Epoch 3, Batch 45, G Loss: 0.7032758593559265, D Loss: 1.3919246196746826\n",
            "Epoch 3, Batch 46, G Loss: 0.7030510306358337, D Loss: 1.3934869766235352\n",
            "Epoch 3, Batch 47, G Loss: 0.7030543088912964, D Loss: 1.3905630111694336\n",
            "Epoch 3, Batch 48, G Loss: 0.7029591202735901, D Loss: 1.3882439136505127\n",
            "Epoch 3, Batch 49, G Loss: 0.7030279040336609, D Loss: 1.3899033069610596\n",
            "Epoch 3, Batch 50, G Loss: 0.7029521465301514, D Loss: 1.3904848098754883\n",
            "Epoch 3, Batch 51, G Loss: 0.702904224395752, D Loss: 1.3921054601669312\n",
            "Epoch 3, Batch 52, G Loss: 0.7028699517250061, D Loss: 1.392716884613037\n",
            "Epoch 3, Batch 53, G Loss: 0.7026835680007935, D Loss: 1.3957719802856445\n",
            "Epoch 3, Batch 54, G Loss: 0.7027189135551453, D Loss: 1.3938519954681396\n",
            "Epoch 3, Batch 55, G Loss: 0.7028366327285767, D Loss: 1.3912136554718018\n",
            "Epoch 3, Batch 56, G Loss: 0.7026709914207458, D Loss: 1.3942790031433105\n",
            "Epoch 3, Batch 57, G Loss: 0.7027249932289124, D Loss: 1.3944127559661865\n",
            "Epoch 3, Batch 58, G Loss: 0.7024979591369629, D Loss: 1.3960859775543213\n",
            "Epoch 3, Batch 59, G Loss: 0.7024322152137756, D Loss: 1.3942878246307373\n",
            "Epoch 3, Batch 60, G Loss: 0.7024162411689758, D Loss: 1.393428087234497\n",
            "Epoch 3, Batch 61, G Loss: 0.7024571895599365, D Loss: 1.3927936553955078\n",
            "Epoch 3, Batch 62, G Loss: 0.7023986577987671, D Loss: 1.3903952836990356\n",
            "Epoch 3, Batch 63, G Loss: 0.702299952507019, D Loss: 1.3900679349899292\n",
            "Epoch 3, Batch 64, G Loss: 0.7023134231567383, D Loss: 1.392700433731079\n",
            "Epoch 3, Batch 65, G Loss: 0.7023088335990906, D Loss: 1.392244815826416\n",
            "Epoch 3, Batch 66, G Loss: 0.7022607922554016, D Loss: 1.391834020614624\n",
            "Epoch 3, Batch 67, G Loss: 0.7021393179893494, D Loss: 1.3875494003295898\n",
            "Epoch 3, Batch 68, G Loss: 0.7020409107208252, D Loss: 1.3889495134353638\n",
            "Epoch 3, Batch 69, G Loss: 0.7021157145500183, D Loss: 1.3913564682006836\n",
            "Epoch 3, Batch 70, G Loss: 0.7019650936126709, D Loss: 1.3921902179718018\n",
            "Epoch 3, Batch 71, G Loss: 0.7018609046936035, D Loss: 1.392359972000122\n",
            "Epoch 3, Batch 72, G Loss: 0.7019567489624023, D Loss: 1.393519401550293\n",
            "Epoch 3, Batch 73, G Loss: 0.7019317150115967, D Loss: 1.3930435180664062\n",
            "Epoch 3, Batch 74, G Loss: 0.701844334602356, D Loss: 1.391047477722168\n",
            "Epoch 3, Batch 75, G Loss: 0.7018252611160278, D Loss: 1.3915367126464844\n",
            "Epoch 3, Batch 76, G Loss: 0.7018544673919678, D Loss: 1.3911142349243164\n",
            "Epoch 3, Batch 77, G Loss: 0.7016822099685669, D Loss: 1.3918194770812988\n",
            "Epoch 3, Batch 78, G Loss: 0.7017204165458679, D Loss: 1.392240047454834\n",
            "Epoch 3, Batch 79, G Loss: 0.7016622424125671, D Loss: 1.391237497329712\n",
            "Epoch 3, Batch 80, G Loss: 0.7014448046684265, D Loss: 1.3900593519210815\n",
            "Epoch 3, Batch 81, G Loss: 0.7013585567474365, D Loss: 1.3874351978302002\n",
            "Epoch 3, Batch 82, G Loss: 0.7013257145881653, D Loss: 1.3897894620895386\n",
            "Epoch 3, Batch 83, G Loss: 0.7013037800788879, D Loss: 1.3909368515014648\n",
            "Epoch 3, Batch 84, G Loss: 0.7012527585029602, D Loss: 1.392762303352356\n",
            "Epoch 3, Batch 85, G Loss: 0.701259434223175, D Loss: 1.3907208442687988\n",
            "Epoch 3, Batch 86, G Loss: 0.7012525200843811, D Loss: 1.39011549949646\n",
            "Epoch 3, Batch 87, G Loss: 0.7011178731918335, D Loss: 1.3904637098312378\n",
            "Epoch 3, Batch 88, G Loss: 0.7010918259620667, D Loss: 1.3911845684051514\n",
            "Epoch 3, Batch 89, G Loss: 0.7009920477867126, D Loss: 1.391575813293457\n",
            "Epoch 3, Batch 90, G Loss: 0.7010357975959778, D Loss: 1.3915725946426392\n",
            "Epoch 3, Batch 91, G Loss: 0.7009162902832031, D Loss: 1.391624093055725\n",
            "Epoch 3, Batch 92, G Loss: 0.700777530670166, D Loss: 1.3915619850158691\n",
            "Epoch 3, Batch 93, G Loss: 0.7008374333381653, D Loss: 1.3914613723754883\n",
            "Epoch 3, Batch 94, G Loss: 0.7007085084915161, D Loss: 1.3906606435775757\n",
            "Epoch 3, Batch 95, G Loss: 0.7007038593292236, D Loss: 1.3905096054077148\n",
            "Epoch 3, Batch 96, G Loss: 0.7006323933601379, D Loss: 1.3900878429412842\n",
            "Epoch 3, Batch 97, G Loss: 0.7006378769874573, D Loss: 1.3905317783355713\n",
            "Epoch 3, Batch 98, G Loss: 0.7005337476730347, D Loss: 1.390162706375122\n",
            "Epoch 3, Batch 99, G Loss: 0.7003588080406189, D Loss: 1.3904681205749512\n",
            "Epoch 3, Batch 100, G Loss: 0.7005037069320679, D Loss: 1.391568660736084\n",
            "Epoch 3, Batch 101, G Loss: 0.7003791928291321, D Loss: 1.392174243927002\n",
            "Epoch 3, Batch 102, G Loss: 0.7002608776092529, D Loss: 1.3890399932861328\n",
            "Epoch 3, Batch 103, G Loss: 0.7003499269485474, D Loss: 1.388688564300537\n",
            "Epoch 3, Batch 104, G Loss: 0.7001882791519165, D Loss: 1.3901633024215698\n",
            "Epoch 3, Batch 105, G Loss: 0.7002174258232117, D Loss: 1.3903937339782715\n",
            "Epoch 3, Batch 106, G Loss: 0.7001495957374573, D Loss: 1.3891842365264893\n",
            "Epoch 3, Batch 107, G Loss: 0.7001222968101501, D Loss: 1.3911653757095337\n",
            "Epoch 3, Batch 108, G Loss: 0.6999267339706421, D Loss: 1.3928642272949219\n",
            "Epoch 3, Batch 109, G Loss: 0.6999669075012207, D Loss: 1.391322135925293\n",
            "Epoch 3, Batch 110, G Loss: 0.6998562812805176, D Loss: 1.390519142150879\n",
            "Epoch 3, Batch 111, G Loss: 0.6998189687728882, D Loss: 1.3887441158294678\n",
            "Epoch 3, Batch 112, G Loss: 0.6997162103652954, D Loss: 1.3894472122192383\n",
            "Epoch 3, Batch 113, G Loss: 0.6997482180595398, D Loss: 1.390876293182373\n",
            "Epoch 3, Batch 114, G Loss: 0.6996031999588013, D Loss: 1.3924005031585693\n",
            "Epoch 3, Batch 115, G Loss: 0.6998153328895569, D Loss: 1.3907533884048462\n",
            "Epoch 3, Batch 116, G Loss: 0.6995771527290344, D Loss: 1.3910934925079346\n",
            "Epoch 3, Batch 117, G Loss: 0.6994185447692871, D Loss: 1.3913345336914062\n",
            "Epoch 3, Batch 118, G Loss: 0.6995106935501099, D Loss: 1.3915677070617676\n",
            "Epoch 3, Batch 119, G Loss: 0.6995108723640442, D Loss: 1.3921606540679932\n",
            "Epoch 3, Batch 120, G Loss: 0.6994200944900513, D Loss: 1.3920531272888184\n",
            "Epoch 3, Batch 121, G Loss: 0.6993260383605957, D Loss: 1.3923988342285156\n",
            "Epoch 3, Batch 122, G Loss: 0.6994112133979797, D Loss: 1.391472578048706\n",
            "Epoch 3, Batch 123, G Loss: 0.6993017196655273, D Loss: 1.3914623260498047\n",
            "Epoch 3, Batch 124, G Loss: 0.6993041038513184, D Loss: 1.391679286956787\n",
            "Epoch 3, Batch 125, G Loss: 0.6992456912994385, D Loss: 1.3905680179595947\n",
            "Epoch 3, Batch 126, G Loss: 0.6993855237960815, D Loss: 1.389683723449707\n",
            "Epoch 3, Batch 127, G Loss: 0.6992451548576355, D Loss: 1.3898847103118896\n",
            "Epoch 3, Batch 128, G Loss: 0.6992074847221375, D Loss: 1.3906941413879395\n",
            "Epoch 3, Batch 129, G Loss: 0.6991155743598938, D Loss: 1.3910883665084839\n",
            "Epoch 3, Batch 130, G Loss: 0.6991782188415527, D Loss: 1.391219973564148\n",
            "Epoch 3, Batch 131, G Loss: 0.6990813612937927, D Loss: 1.3913129568099976\n",
            "Epoch 3, Batch 132, G Loss: 0.699079692363739, D Loss: 1.3916857242584229\n",
            "Epoch 3, Batch 133, G Loss: 0.6992348432540894, D Loss: 1.3907358646392822\n",
            "Epoch 3, Batch 134, G Loss: 0.6991902589797974, D Loss: 1.3905630111694336\n",
            "Epoch 3, Batch 135, G Loss: 0.6990618705749512, D Loss: 1.3900625705718994\n",
            "Epoch 3, Batch 136, G Loss: 0.6988525390625, D Loss: 1.39046311378479\n",
            "Epoch 3, Batch 137, G Loss: 0.6989810466766357, D Loss: 1.391037940979004\n",
            "Epoch 3, Batch 138, G Loss: 0.6990493535995483, D Loss: 1.3919180631637573\n",
            "Epoch 3, Batch 139, G Loss: 0.6989957690238953, D Loss: 1.391446828842163\n",
            "Epoch 3, Batch 140, G Loss: 0.6989070177078247, D Loss: 1.3915787935256958\n",
            "Epoch 3, Batch 141, G Loss: 0.6988581418991089, D Loss: 1.3904774188995361\n",
            "Epoch 3, Batch 142, G Loss: 0.6988995671272278, D Loss: 1.3904786109924316\n",
            "Epoch 3, Batch 143, G Loss: 0.698838472366333, D Loss: 1.390143871307373\n",
            "Epoch 3, Batch 144, G Loss: 0.6986930966377258, D Loss: 1.390971302986145\n",
            "Epoch 3, Batch 145, G Loss: 0.6987752914428711, D Loss: 1.3906631469726562\n",
            "Epoch 3, Batch 146, G Loss: 0.698681652545929, D Loss: 1.390730381011963\n",
            "Epoch 3, Batch 147, G Loss: 0.698698103427887, D Loss: 1.3908240795135498\n",
            "Epoch 3, Batch 148, G Loss: 0.6987557411193848, D Loss: 1.3905762434005737\n",
            "Epoch 3, Batch 149, G Loss: 0.6986371874809265, D Loss: 1.3912115097045898\n",
            "Epoch 3, Batch 150, G Loss: 0.698710560798645, D Loss: 1.3910799026489258\n",
            "Epoch 3, Batch 151, G Loss: 0.6987225413322449, D Loss: 1.3889521360397339\n",
            "Epoch 3, Batch 152, G Loss: 0.6986164450645447, D Loss: 1.388751745223999\n",
            "Epoch 3, Batch 153, G Loss: 0.6986122727394104, D Loss: 1.3885982036590576\n",
            "Epoch 3, Batch 154, G Loss: 0.6986244916915894, D Loss: 1.3889222145080566\n",
            "Epoch 3, Batch 155, G Loss: 0.6985902786254883, D Loss: 1.3889110088348389\n",
            "Epoch 3, Batch 156, G Loss: 0.6983659863471985, D Loss: 1.3892872333526611\n",
            "Epoch 3, Batch 157, G Loss: 0.698484480381012, D Loss: 1.3895395994186401\n",
            "Epoch 3, Batch 158, G Loss: 0.6982772350311279, D Loss: 1.3898786306381226\n",
            "Epoch 3, Batch 159, G Loss: 0.698279857635498, D Loss: 1.3905632495880127\n",
            "Epoch 3, Batch 160, G Loss: 0.69837486743927, D Loss: 1.3898239135742188\n",
            "Epoch 3, Batch 161, G Loss: 0.6983452439308167, D Loss: 1.3896268606185913\n",
            "Epoch 3, Batch 162, G Loss: 0.6981872916221619, D Loss: 1.3892652988433838\n",
            "Epoch 3, Batch 163, G Loss: 0.698236346244812, D Loss: 1.3899483680725098\n",
            "Epoch 3, Batch 164, G Loss: 0.6981998085975647, D Loss: 1.3896061182022095\n",
            "Epoch 3, Batch 165, G Loss: 0.6980984807014465, D Loss: 1.3906171321868896\n",
            "Epoch 3, Batch 166, G Loss: 0.6979137063026428, D Loss: 1.3903745412826538\n",
            "Epoch 3, Batch 167, G Loss: 0.6980955004692078, D Loss: 1.3899142742156982\n",
            "Epoch 3, Batch 168, G Loss: 0.6979986429214478, D Loss: 1.389458417892456\n",
            "Epoch 3, Batch 169, G Loss: 0.6979303956031799, D Loss: 1.3902171850204468\n",
            "Epoch 3, Batch 170, G Loss: 0.6978639960289001, D Loss: 1.3911027908325195\n",
            "Epoch 3, Batch 171, G Loss: 0.697931706905365, D Loss: 1.3881092071533203\n",
            "Epoch 3, Batch 172, G Loss: 0.6979038715362549, D Loss: 1.3881936073303223\n",
            "Epoch 3, Batch 173, G Loss: 0.6977643370628357, D Loss: 1.3900765180587769\n",
            "Epoch 3, Batch 174, G Loss: 0.6977545022964478, D Loss: 1.389996886253357\n",
            "Epoch 3, Batch 175, G Loss: 0.6975947618484497, D Loss: 1.390254259109497\n",
            "Epoch 3, Batch 176, G Loss: 0.6976432204246521, D Loss: 1.3899987936019897\n",
            "Epoch 3, Batch 177, G Loss: 0.6976763606071472, D Loss: 1.3892135620117188\n",
            "Epoch 3, Batch 178, G Loss: 0.6975376605987549, D Loss: 1.3882229328155518\n",
            "Epoch 3, Batch 179, G Loss: 0.6975687742233276, D Loss: 1.38887619972229\n",
            "Epoch 3, Batch 180, G Loss: 0.6975298523902893, D Loss: 1.3887385129928589\n",
            "Epoch 3, Batch 181, G Loss: 0.6974449753761292, D Loss: 1.3886380195617676\n",
            "Epoch 3, Batch 182, G Loss: 0.6973264813423157, D Loss: 1.389366626739502\n",
            "Epoch 3, Batch 183, G Loss: 0.6973456740379333, D Loss: 1.3892197608947754\n",
            "Epoch 3, Batch 184, G Loss: 0.6973233222961426, D Loss: 1.3897336721420288\n",
            "Epoch 3, Batch 185, G Loss: 0.697241485118866, D Loss: 1.3905658721923828\n",
            "Epoch 3, Batch 186, G Loss: 0.6972405314445496, D Loss: 1.3901474475860596\n",
            "Epoch 3, Batch 187, G Loss: 0.6971883177757263, D Loss: 1.3898489475250244\n",
            "Epoch 3, Batch 188, G Loss: 0.6971930861473083, D Loss: 1.3893613815307617\n",
            "Epoch 3, Batch 189, G Loss: 0.6972191333770752, D Loss: 1.3893616199493408\n",
            "Epoch 3, Batch 190, G Loss: 0.6970767378807068, D Loss: 1.3896605968475342\n",
            "Epoch 3, Batch 191, G Loss: 0.6970023512840271, D Loss: 1.389609932899475\n",
            "Epoch 3, Batch 192, G Loss: 0.6968966126441956, D Loss: 1.3896414041519165\n",
            "Epoch 3, Batch 193, G Loss: 0.6969838738441467, D Loss: 1.3889884948730469\n",
            "Epoch 3, Batch 194, G Loss: 0.6969461441040039, D Loss: 1.389266014099121\n",
            "Epoch 3, Batch 195, G Loss: 0.6968139410018921, D Loss: 1.390331745147705\n",
            "Epoch 3, Batch 196, G Loss: 0.6969180703163147, D Loss: 1.3897701501846313\n",
            "Epoch 3, Batch 197, G Loss: 0.6968048810958862, D Loss: 1.3900140523910522\n",
            "Epoch 3, Batch 198, G Loss: 0.6968262195587158, D Loss: 1.3886613845825195\n",
            "Epoch 3, Batch 199, G Loss: 0.6967470049858093, D Loss: 1.388185977935791\n",
            "Epoch 3, Batch 200, G Loss: 0.6967485547065735, D Loss: 1.3892121315002441\n",
            "Epoch 3, Batch 201, G Loss: 0.6967120170593262, D Loss: 1.3893152475357056\n",
            "Epoch 3, Batch 202, G Loss: 0.6967120170593262, D Loss: 1.3902013301849365\n",
            "Epoch 3, Batch 203, G Loss: 0.6966152787208557, D Loss: 1.3905651569366455\n",
            "Epoch 3, Batch 204, G Loss: 0.6966512799263, D Loss: 1.3890167474746704\n",
            "Epoch 3, Batch 205, G Loss: 0.6965707540512085, D Loss: 1.3894879817962646\n",
            "Epoch 3, Batch 206, G Loss: 0.6964852213859558, D Loss: 1.3900976181030273\n",
            "Epoch 3, Batch 207, G Loss: 0.696596086025238, D Loss: 1.3900011777877808\n",
            "Epoch 3, Batch 208, G Loss: 0.6964996457099915, D Loss: 1.390478253364563\n",
            "Epoch 3, Batch 209, G Loss: 0.6965622901916504, D Loss: 1.3892782926559448\n",
            "Epoch 3, Batch 210, G Loss: 0.6964306235313416, D Loss: 1.3890752792358398\n",
            "Epoch 3, Batch 211, G Loss: 0.6964230537414551, D Loss: 1.389828085899353\n",
            "Epoch 3, Batch 212, G Loss: 0.6964711546897888, D Loss: 1.3892204761505127\n",
            "Epoch 3, Batch 213, G Loss: 0.696459174156189, D Loss: 1.3890409469604492\n",
            "Epoch 3, Batch 214, G Loss: 0.6964001059532166, D Loss: 1.389282464981079\n",
            "Epoch 3, Batch 215, G Loss: 0.6965250968933105, D Loss: 1.389414668083191\n",
            "Epoch 3, Batch 216, G Loss: 0.6965296268463135, D Loss: 1.3894660472869873\n",
            "Epoch 3, Batch 217, G Loss: 0.6964055299758911, D Loss: 1.3899359703063965\n",
            "Epoch 3, Batch 218, G Loss: 0.696246862411499, D Loss: 1.390087366104126\n",
            "Epoch 3, Batch 219, G Loss: 0.6962568163871765, D Loss: 1.3886733055114746\n",
            "Epoch 3, Batch 220, G Loss: 0.6963108777999878, D Loss: 1.3885763883590698\n",
            "Epoch 3, Batch 221, G Loss: 0.6962331533432007, D Loss: 1.3886172771453857\n",
            "Epoch 3, Batch 222, G Loss: 0.6962740421295166, D Loss: 1.388486623764038\n",
            "Epoch 3, Batch 223, G Loss: 0.696247398853302, D Loss: 1.388403296470642\n",
            "Epoch 3, Batch 224, G Loss: 0.6962382197380066, D Loss: 1.3887509107589722\n",
            "Epoch 3, Batch 225, G Loss: 0.696150004863739, D Loss: 1.389264702796936\n",
            "Epoch 3, Batch 226, G Loss: 0.6960998773574829, D Loss: 1.3890286684036255\n",
            "Epoch 3, Batch 227, G Loss: 0.6961153149604797, D Loss: 1.3890419006347656\n",
            "Epoch 3, Batch 228, G Loss: 0.6961089372634888, D Loss: 1.3900623321533203\n",
            "Epoch 3, Batch 229, G Loss: 0.6961150169372559, D Loss: 1.3889102935791016\n",
            "Epoch 3, Batch 230, G Loss: 0.695977509021759, D Loss: 1.3882818222045898\n",
            "Epoch 3, Batch 231, G Loss: 0.6959211230278015, D Loss: 1.3889493942260742\n",
            "Epoch 3, Batch 232, G Loss: 0.695967435836792, D Loss: 1.3895353078842163\n",
            "Epoch 3, Batch 233, G Loss: 0.6958675980567932, D Loss: 1.3897862434387207\n",
            "Epoch 3, Batch 234, G Loss: 0.6959078907966614, D Loss: 1.38973867893219\n",
            "Epoch 3, Batch 235, G Loss: 0.6959795355796814, D Loss: 1.389702320098877\n",
            "Epoch 3, Batch 236, G Loss: 0.6958102583885193, D Loss: 1.3893518447875977\n",
            "Epoch 3, Batch 237, G Loss: 0.6959180235862732, D Loss: 1.3892971277236938\n",
            "Epoch 3, Batch 238, G Loss: 0.6958579421043396, D Loss: 1.3888283967971802\n",
            "Epoch 3, Batch 239, G Loss: 0.6958099603652954, D Loss: 1.3891427516937256\n",
            "Epoch 3, Batch 240, G Loss: 0.6957990527153015, D Loss: 1.3902671337127686\n",
            "Epoch 3, Batch 241, G Loss: 0.6958341002464294, D Loss: 1.3894915580749512\n",
            "Epoch 3, Batch 242, G Loss: 0.6958661675453186, D Loss: 1.3891546726226807\n",
            "Epoch 3, Batch 243, G Loss: 0.6957875490188599, D Loss: 1.3893145322799683\n",
            "Epoch 3, Batch 244, G Loss: 0.6958909034729004, D Loss: 1.3894119262695312\n",
            "Epoch 3, Batch 245, G Loss: 0.6957772970199585, D Loss: 1.3893284797668457\n",
            "Epoch 3, Batch 246, G Loss: 0.6957897543907166, D Loss: 1.3894245624542236\n",
            "Epoch 3, Batch 247, G Loss: 0.6958036422729492, D Loss: 1.3896708488464355\n",
            "Epoch 3, Batch 248, G Loss: 0.6958027482032776, D Loss: 1.38960599899292\n",
            "Epoch 3, Batch 249, G Loss: 0.6958013772964478, D Loss: 1.3890814781188965\n",
            "Epoch 3, Batch 250, G Loss: 0.6958750486373901, D Loss: 1.3890033960342407\n",
            "Epoch 3, Batch 251, G Loss: 0.6958481073379517, D Loss: 1.3886882066726685\n",
            "Epoch 3, Batch 252, G Loss: 0.6958492994308472, D Loss: 1.3889682292938232\n",
            "Epoch 3, Batch 253, G Loss: 0.6957562565803528, D Loss: 1.3884673118591309\n",
            "Epoch 3, Batch 254, G Loss: 0.6958059072494507, D Loss: 1.3882319927215576\n",
            "Epoch 3, Batch 255, G Loss: 0.6958436965942383, D Loss: 1.3889977931976318\n",
            "Epoch 3, Batch 256, G Loss: 0.6957501769065857, D Loss: 1.3893849849700928\n",
            "Epoch 3, Batch 257, G Loss: 0.6957868337631226, D Loss: 1.389024257659912\n",
            "Epoch 3, Batch 258, G Loss: 0.6957334280014038, D Loss: 1.3887221813201904\n",
            "Epoch 3, Batch 259, G Loss: 0.6956589221954346, D Loss: 1.38862144947052\n",
            "Epoch 3, Batch 260, G Loss: 0.695732593536377, D Loss: 1.3887461423873901\n",
            "Epoch 3, Batch 261, G Loss: 0.6956648826599121, D Loss: 1.3889718055725098\n",
            "Epoch 3, Batch 262, G Loss: 0.6955970525741577, D Loss: 1.3892065286636353\n",
            "Epoch 3, Batch 263, G Loss: 0.6956074237823486, D Loss: 1.3894047737121582\n",
            "Epoch 3, Batch 264, G Loss: 0.6955918073654175, D Loss: 1.3887219429016113\n",
            "Epoch 3, Batch 265, G Loss: 0.6955991983413696, D Loss: 1.3883333206176758\n",
            "Epoch 3, Batch 266, G Loss: 0.6954538226127625, D Loss: 1.3891444206237793\n",
            "Epoch 3, Batch 267, G Loss: 0.6955708265304565, D Loss: 1.388847827911377\n",
            "Epoch 3, Batch 268, G Loss: 0.6954874992370605, D Loss: 1.3889808654785156\n",
            "Epoch 3, Batch 269, G Loss: 0.6954940557479858, D Loss: 1.3885858058929443\n",
            "Epoch 3, Batch 270, G Loss: 0.6954281330108643, D Loss: 1.3887875080108643\n",
            "Epoch 3, Batch 271, G Loss: 0.69554603099823, D Loss: 1.3886449337005615\n",
            "Epoch 3, Batch 272, G Loss: 0.695476233959198, D Loss: 1.3886902332305908\n",
            "Epoch 3, Batch 273, G Loss: 0.6953739523887634, D Loss: 1.3887029886245728\n",
            "Epoch 3, Batch 274, G Loss: 0.695374608039856, D Loss: 1.388545036315918\n",
            "Epoch 3, Batch 275, G Loss: 0.6953865885734558, D Loss: 1.3889518976211548\n",
            "Epoch 3, Batch 276, G Loss: 0.6953445672988892, D Loss: 1.388998031616211\n",
            "Epoch 3, Batch 277, G Loss: 0.695253849029541, D Loss: 1.388606071472168\n",
            "Epoch 3, Batch 278, G Loss: 0.6952334046363831, D Loss: 1.3888028860092163\n",
            "Epoch 3, Batch 279, G Loss: 0.695324182510376, D Loss: 1.3887813091278076\n",
            "Epoch 3, Batch 280, G Loss: 0.6952769160270691, D Loss: 1.3887128829956055\n",
            "Epoch 3, Batch 281, G Loss: 0.6952135562896729, D Loss: 1.3891137838363647\n",
            "Epoch 3, Batch 282, G Loss: 0.6952114701271057, D Loss: 1.3892875909805298\n",
            "Epoch 3, Batch 283, G Loss: 0.6951948404312134, D Loss: 1.3881927728652954\n",
            "Epoch 3, Batch 284, G Loss: 0.6951761245727539, D Loss: 1.3883081674575806\n",
            "Epoch 3, Batch 285, G Loss: 0.6952163577079773, D Loss: 1.3887615203857422\n",
            "Epoch 3, Batch 286, G Loss: 0.6951060891151428, D Loss: 1.3884758949279785\n",
            "Epoch 3, Batch 287, G Loss: 0.695065975189209, D Loss: 1.3881025314331055\n",
            "Epoch 3, Batch 288, G Loss: 0.6950655579566956, D Loss: 1.388211965560913\n",
            "Epoch 3, Batch 289, G Loss: 0.695046067237854, D Loss: 1.3885502815246582\n",
            "Epoch 3, Batch 290, G Loss: 0.6949777603149414, D Loss: 1.3883366584777832\n",
            "Epoch 3, Batch 291, G Loss: 0.6948883533477783, D Loss: 1.3880434036254883\n",
            "Epoch 3, Batch 292, G Loss: 0.694889485836029, D Loss: 1.3884122371673584\n",
            "Epoch 3, Batch 293, G Loss: 0.6948980093002319, D Loss: 1.3885610103607178\n",
            "Epoch 3, Batch 294, G Loss: 0.6947773694992065, D Loss: 1.3888957500457764\n",
            "Epoch 3, Batch 295, G Loss: 0.6947975158691406, D Loss: 1.3886816501617432\n",
            "Epoch 3, Batch 296, G Loss: 0.694728434085846, D Loss: 1.3885478973388672\n",
            "Epoch 3, Batch 297, G Loss: 0.6946988701820374, D Loss: 1.388634443283081\n",
            "Epoch 3, Batch 298, G Loss: 0.6946401000022888, D Loss: 1.388715147972107\n",
            "Epoch 3, Batch 299, G Loss: 0.6945596933364868, D Loss: 1.3886477947235107\n",
            "Epoch 3, Batch 300, G Loss: 0.6945428848266602, D Loss: 1.388746738433838\n",
            "Epoch 3, Batch 301, G Loss: 0.6945967674255371, D Loss: 1.3886523246765137\n",
            "Epoch 3, Batch 302, G Loss: 0.6945721507072449, D Loss: 1.3884530067443848\n",
            "Epoch 3, Batch 303, G Loss: 0.6944905519485474, D Loss: 1.3886516094207764\n",
            "Epoch 3, Batch 304, G Loss: 0.694469690322876, D Loss: 1.3885505199432373\n",
            "Epoch 3, Batch 305, G Loss: 0.6944637894630432, D Loss: 1.3887150287628174\n",
            "Epoch 3, Batch 306, G Loss: 0.6944103837013245, D Loss: 1.3883190155029297\n",
            "Epoch 3, Batch 307, G Loss: 0.6943898797035217, D Loss: 1.3883346319198608\n",
            "Epoch 3, Batch 308, G Loss: 0.6943840384483337, D Loss: 1.3883752822875977\n",
            "Epoch 3, Batch 309, G Loss: 0.6944641470909119, D Loss: 1.3880600929260254\n",
            "Epoch 3, Batch 310, G Loss: 0.6944079995155334, D Loss: 1.3881807327270508\n",
            "Epoch 3, Batch 311, G Loss: 0.6942805051803589, D Loss: 1.3889260292053223\n",
            "Epoch 3, Batch 312, G Loss: 0.6942819952964783, D Loss: 1.3889267444610596\n",
            "Epoch 3, Batch 313, G Loss: 0.6943138241767883, D Loss: 1.3884384632110596\n",
            "Epoch 3, Batch 314, G Loss: 0.6943425536155701, D Loss: 1.3884066343307495\n",
            "Epoch 3, Batch 315, G Loss: 0.69423907995224, D Loss: 1.3886520862579346\n",
            "Epoch 3, Batch 316, G Loss: 0.6942453384399414, D Loss: 1.3885321617126465\n",
            "Epoch 3, Batch 317, G Loss: 0.6942718625068665, D Loss: 1.3881936073303223\n",
            "Epoch 3, Batch 318, G Loss: 0.6942311525344849, D Loss: 1.3885364532470703\n",
            "Epoch 3, Batch 319, G Loss: 0.6941847801208496, D Loss: 1.3886979818344116\n",
            "Epoch 3, Batch 320, G Loss: 0.6943090558052063, D Loss: 1.3884007930755615\n",
            "Epoch 3, Batch 321, G Loss: 0.6941282153129578, D Loss: 1.3885889053344727\n",
            "Epoch 3, Batch 322, G Loss: 0.6941739320755005, D Loss: 1.3885583877563477\n",
            "Epoch 3, Batch 323, G Loss: 0.6941622495651245, D Loss: 1.3885383605957031\n",
            "Epoch 3, Batch 324, G Loss: 0.6941927075386047, D Loss: 1.3885606527328491\n",
            "Epoch 3, Batch 325, G Loss: 0.6941526532173157, D Loss: 1.388458490371704\n",
            "Epoch 3, Batch 326, G Loss: 0.6941484808921814, D Loss: 1.3886574506759644\n",
            "Epoch 3, Batch 327, G Loss: 0.6942440867424011, D Loss: 1.388580560684204\n",
            "Epoch 3, Batch 328, G Loss: 0.6941608786582947, D Loss: 1.3882544040679932\n",
            "Epoch 3, Batch 329, G Loss: 0.694158136844635, D Loss: 1.3882973194122314\n",
            "Epoch 3, Batch 330, G Loss: 0.6941235661506653, D Loss: 1.3882265090942383\n",
            "Epoch 3, Batch 331, G Loss: 0.6941691637039185, D Loss: 1.3883050680160522\n",
            "Epoch 3, Batch 332, G Loss: 0.6941297650337219, D Loss: 1.3885595798492432\n",
            "Epoch 3, Batch 333, G Loss: 0.6941627860069275, D Loss: 1.38837468624115\n",
            "Epoch 3, Batch 334, G Loss: 0.6942074298858643, D Loss: 1.3881888389587402\n",
            "Epoch 3, Batch 335, G Loss: 0.6941514015197754, D Loss: 1.388321042060852\n",
            "Epoch 3, Batch 336, G Loss: 0.6941296458244324, D Loss: 1.3885365724563599\n",
            "Epoch 3, Batch 337, G Loss: 0.6941136717796326, D Loss: 1.388312816619873\n",
            "Epoch 3, Batch 338, G Loss: 0.6941121816635132, D Loss: 1.3883438110351562\n",
            "Epoch 3, Batch 339, G Loss: 0.6941410899162292, D Loss: 1.3882691860198975\n",
            "Epoch 3, Batch 340, G Loss: 0.6942028999328613, D Loss: 1.3883161544799805\n",
            "Epoch 3, Batch 341, G Loss: 0.6941410899162292, D Loss: 1.3883534669876099\n",
            "Epoch 3, Batch 342, G Loss: 0.6940780878067017, D Loss: 1.388270378112793\n",
            "Epoch 3, Batch 343, G Loss: 0.6940659284591675, D Loss: 1.3884295225143433\n",
            "Epoch 3, Batch 344, G Loss: 0.6940510272979736, D Loss: 1.388461947441101\n",
            "Epoch 3, Batch 345, G Loss: 0.6941317915916443, D Loss: 1.388443112373352\n",
            "Epoch 3, Batch 346, G Loss: 0.6941069960594177, D Loss: 1.3884685039520264\n",
            "Epoch 3, Batch 347, G Loss: 0.6941174864768982, D Loss: 1.3883297443389893\n",
            "Epoch 3, Batch 348, G Loss: 0.6940346360206604, D Loss: 1.3883916139602661\n",
            "Epoch 3, Batch 349, G Loss: 0.6940422058105469, D Loss: 1.3883193731307983\n",
            "Epoch 3, Batch 350, G Loss: 0.6940757632255554, D Loss: 1.3884007930755615\n",
            "Epoch 3, Batch 351, G Loss: 0.6941012740135193, D Loss: 1.388302206993103\n",
            "Epoch 3, Batch 352, G Loss: 0.6941142082214355, D Loss: 1.3881622552871704\n",
            "Epoch 3, Batch 353, G Loss: 0.6940401196479797, D Loss: 1.3882808685302734\n",
            "Epoch 3, Batch 354, G Loss: 0.6940165758132935, D Loss: 1.388432264328003\n",
            "Epoch 3, Batch 355, G Loss: 0.6940689086914062, D Loss: 1.3883895874023438\n",
            "Epoch 3, Batch 356, G Loss: 0.6940765380859375, D Loss: 1.388409972190857\n",
            "Epoch 3, Batch 357, G Loss: 0.6941224932670593, D Loss: 1.3882012367248535\n",
            "Epoch 3, Batch 358, G Loss: 0.6940441727638245, D Loss: 1.3883723020553589\n",
            "Epoch 3, Batch 359, G Loss: 0.6940637826919556, D Loss: 1.3883827924728394\n",
            "Epoch 3, Batch 360, G Loss: 0.6941623091697693, D Loss: 1.388335943222046\n",
            "Epoch 3, Batch 361, G Loss: 0.6940747499465942, D Loss: 1.3882039785385132\n",
            "Epoch 3, Batch 362, G Loss: 0.6940861940383911, D Loss: 1.3882707357406616\n",
            "Epoch 3, Batch 363, G Loss: 0.6940712332725525, D Loss: 1.3883073329925537\n",
            "Epoch 3, Batch 364, G Loss: 0.6941748857498169, D Loss: 1.3881444931030273\n",
            "Epoch 3, Batch 365, G Loss: 0.694196343421936, D Loss: 1.3881666660308838\n",
            "Epoch 3, Batch 366, G Loss: 0.6941009163856506, D Loss: 1.3882299661636353\n",
            "Epoch 3, Batch 367, G Loss: 0.6941779851913452, D Loss: 1.388129472732544\n",
            "Epoch 3, Batch 368, G Loss: 0.6941472291946411, D Loss: 1.3881475925445557\n",
            "Epoch 3, Batch 369, G Loss: 0.6941607594490051, D Loss: 1.3881120681762695\n",
            "Epoch 3, Batch 370, G Loss: 0.6941255927085876, D Loss: 1.3881168365478516\n",
            "Epoch 3, Batch 371, G Loss: 0.69412761926651, D Loss: 1.388223648071289\n",
            "Epoch 3, Batch 372, G Loss: 0.6941207647323608, D Loss: 1.3881778717041016\n",
            "Epoch 3, Batch 373, G Loss: 0.694148063659668, D Loss: 1.3881542682647705\n",
            "Epoch 3, Batch 374, G Loss: 0.6941588521003723, D Loss: 1.3880281448364258\n",
            "Epoch 3, Batch 375, G Loss: 0.694067120552063, D Loss: 1.3880643844604492\n",
            "Epoch 3, Batch 376, G Loss: 0.6940914988517761, D Loss: 1.3879761695861816\n",
            "Epoch 3, Batch 377, G Loss: 0.6940737962722778, D Loss: 1.3880246877670288\n",
            "Epoch 3, Batch 378, G Loss: 0.6940521001815796, D Loss: 1.3880529403686523\n",
            "Epoch 3, Batch 379, G Loss: 0.6940468549728394, D Loss: 1.387988567352295\n",
            "Epoch 3, Batch 380, G Loss: 0.693956196308136, D Loss: 1.388144850730896\n",
            "Epoch 3, Batch 381, G Loss: 0.6939000487327576, D Loss: 1.388171672821045\n",
            "Epoch 3, Batch 382, G Loss: 0.6938579082489014, D Loss: 1.3880659341812134\n",
            "Epoch 3, Batch 383, G Loss: 0.6939444541931152, D Loss: 1.387852430343628\n",
            "Epoch 3, Batch 384, G Loss: 0.6938173174858093, D Loss: 1.388153076171875\n",
            "Epoch 3, Batch 385, G Loss: 0.6938809752464294, D Loss: 1.3881139755249023\n",
            "Epoch 3, Batch 386, G Loss: 0.6937945485115051, D Loss: 1.3880562782287598\n",
            "Epoch 3, Batch 387, G Loss: 0.6938043832778931, D Loss: 1.3881261348724365\n",
            "Epoch 3, Batch 388, G Loss: 0.6938275098800659, D Loss: 1.3880655765533447\n",
            "Epoch 3, Batch 389, G Loss: 0.6937622427940369, D Loss: 1.388145923614502\n",
            "Epoch 3, Batch 390, G Loss: 0.6937276124954224, D Loss: 1.3880064487457275\n",
            "Epoch 3, Batch 391, G Loss: 0.6937786936759949, D Loss: 1.3878893852233887\n",
            "Epoch 3, Batch 392, G Loss: 0.6938608288764954, D Loss: 1.3878344297409058\n",
            "Epoch 3, Batch 393, G Loss: 0.6938127875328064, D Loss: 1.3880088329315186\n",
            "Epoch 3, Batch 394, G Loss: 0.6938225626945496, D Loss: 1.3880244493484497\n",
            "Epoch 3, Batch 395, G Loss: 0.6937838792800903, D Loss: 1.3880424499511719\n",
            "Epoch 3, Batch 396, G Loss: 0.6938194632530212, D Loss: 1.3878774642944336\n",
            "Epoch 3, Batch 397, G Loss: 0.6938529014587402, D Loss: 1.3879268169403076\n",
            "Epoch 3, Batch 398, G Loss: 0.6937604546546936, D Loss: 1.387860655784607\n",
            "Epoch 3, Batch 399, G Loss: 0.6938141584396362, D Loss: 1.3879163265228271\n",
            "Epoch 3, Batch 400, G Loss: 0.6938440799713135, D Loss: 1.3879408836364746\n",
            "Epoch 3, Batch 401, G Loss: 0.6938520073890686, D Loss: 1.3879289627075195\n",
            "Epoch 3, Batch 402, G Loss: 0.6938391923904419, D Loss: 1.387924313545227\n",
            "Epoch 3, Batch 403, G Loss: 0.6938014030456543, D Loss: 1.3879492282867432\n",
            "Epoch 3, Batch 404, G Loss: 0.6937687397003174, D Loss: 1.3880656957626343\n",
            "Epoch 3, Batch 405, G Loss: 0.6937893629074097, D Loss: 1.3878931999206543\n",
            "Epoch 3, Batch 406, G Loss: 0.6938149333000183, D Loss: 1.3878391981124878\n",
            "Epoch 3, Batch 407, G Loss: 0.6937709450721741, D Loss: 1.3878324031829834\n",
            "Epoch 3, Batch 408, G Loss: 0.6938014030456543, D Loss: 1.3877761363983154\n",
            "Epoch 3, Batch 409, G Loss: 0.6937772631645203, D Loss: 1.3877350091934204\n",
            "Epoch 3, Batch 410, G Loss: 0.6937804222106934, D Loss: 1.3878660202026367\n",
            "Epoch 3, Batch 411, G Loss: 0.6937845945358276, D Loss: 1.387866497039795\n",
            "Epoch 3, Batch 412, G Loss: 0.6937416195869446, D Loss: 1.3877942562103271\n",
            "Epoch 3, Batch 413, G Loss: 0.6938522458076477, D Loss: 1.3876736164093018\n",
            "Epoch 3, Batch 414, G Loss: 0.693767249584198, D Loss: 1.3879613876342773\n",
            "Epoch 3, Batch 415, G Loss: 0.6937785148620605, D Loss: 1.388026237487793\n",
            "Epoch 3, Batch 416, G Loss: 0.6938212513923645, D Loss: 1.3877661228179932\n",
            "Epoch 3, Batch 417, G Loss: 0.6938265562057495, D Loss: 1.3877553939819336\n",
            "Epoch 3, Batch 418, G Loss: 0.6937848329544067, D Loss: 1.3877243995666504\n",
            "Epoch 3, Batch 419, G Loss: 0.6937894821166992, D Loss: 1.387672781944275\n",
            "Epoch 3, Batch 420, G Loss: 0.6938419938087463, D Loss: 1.3877930641174316\n",
            "Epoch 3, Batch 421, G Loss: 0.6938473582267761, D Loss: 1.3876240253448486\n",
            "Epoch 3, Batch 422, G Loss: 0.6937543749809265, D Loss: 1.3880314826965332\n",
            "Epoch 3, Batch 423, G Loss: 0.6938102841377258, D Loss: 1.3877882957458496\n",
            "Epoch 3, Batch 424, G Loss: 0.6938127875328064, D Loss: 1.3875806331634521\n",
            "Epoch 3, Batch 425, G Loss: 0.6938046813011169, D Loss: 1.3877075910568237\n",
            "Epoch 3, Batch 426, G Loss: 0.6938586831092834, D Loss: 1.387769103050232\n",
            "Epoch 3, Batch 427, G Loss: 0.6938716173171997, D Loss: 1.3877489566802979\n",
            "Epoch 3, Batch 428, G Loss: 0.6938241720199585, D Loss: 1.3876889944076538\n",
            "Epoch 3, Batch 429, G Loss: 0.6938027143478394, D Loss: 1.3876707553863525\n",
            "Epoch 3, Batch 430, G Loss: 0.6937047243118286, D Loss: 1.3879034519195557\n",
            "Epoch 3, Batch 431, G Loss: 0.6937829852104187, D Loss: 1.3877089023590088\n",
            "Epoch 3, Batch 432, G Loss: 0.693782389163971, D Loss: 1.3877328634262085\n",
            "Epoch 3, Batch 433, G Loss: 0.6937570571899414, D Loss: 1.387528657913208\n",
            "Epoch 3, Batch 434, G Loss: 0.69375079870224, D Loss: 1.387699842453003\n",
            "Epoch 3, Batch 435, G Loss: 0.6937657594680786, D Loss: 1.387817144393921\n",
            "Epoch 3, Batch 436, G Loss: 0.6936913132667542, D Loss: 1.3879690170288086\n",
            "Epoch 3, Batch 437, G Loss: 0.6936861872673035, D Loss: 1.387716293334961\n",
            "Epoch 3, Batch 438, G Loss: 0.6937009692192078, D Loss: 1.387600064277649\n",
            "Epoch 3, Batch 439, G Loss: 0.6937040090560913, D Loss: 1.387575626373291\n",
            "Epoch 3, Batch 440, G Loss: 0.6936131715774536, D Loss: 1.3876255750656128\n",
            "Epoch 3, Batch 441, G Loss: 0.6936332583427429, D Loss: 1.3873612880706787\n",
            "Epoch 3, Batch 442, G Loss: 0.693644642829895, D Loss: 1.3876066207885742\n",
            "Epoch 3, Batch 443, G Loss: 0.6936774849891663, D Loss: 1.3875136375427246\n",
            "Epoch 3, Batch 444, G Loss: 0.693672239780426, D Loss: 1.3877334594726562\n",
            "Epoch 3, Batch 445, G Loss: 0.6936343312263489, D Loss: 1.3876194953918457\n",
            "Epoch 3, Batch 446, G Loss: 0.6936925053596497, D Loss: 1.3873403072357178\n",
            "Epoch 3, Batch 447, G Loss: 0.6937019228935242, D Loss: 1.3874837160110474\n",
            "Epoch 3, Batch 448, G Loss: 0.6936403512954712, D Loss: 1.3877134323120117\n",
            "Epoch 3, Batch 449, G Loss: 0.6936503052711487, D Loss: 1.3876070976257324\n",
            "Epoch 3, Batch 450, G Loss: 0.6936919689178467, D Loss: 1.3877785205841064\n",
            "Epoch 3, Batch 451, G Loss: 0.6936691403388977, D Loss: 1.3874753713607788\n",
            "Epoch 3, Batch 452, G Loss: 0.693671703338623, D Loss: 1.3873416185379028\n",
            "Epoch 3, Batch 453, G Loss: 0.6936863660812378, D Loss: 1.3874716758728027\n",
            "Epoch 3, Batch 454, G Loss: 0.6936911940574646, D Loss: 1.3874537944793701\n",
            "Epoch 3, Batch 455, G Loss: 0.6936941146850586, D Loss: 1.3875349760055542\n",
            "Epoch 3, Batch 456, G Loss: 0.6937474012374878, D Loss: 1.3876893520355225\n",
            "Epoch 3, Batch 457, G Loss: 0.6937586665153503, D Loss: 1.387808084487915\n",
            "Epoch 3, Batch 458, G Loss: 0.6936545968055725, D Loss: 1.3873158693313599\n",
            "Epoch 3, Batch 459, G Loss: 0.693731963634491, D Loss: 1.3871238231658936\n",
            "Epoch 3, Batch 460, G Loss: 0.6938003301620483, D Loss: 1.387628436088562\n",
            "Epoch 3, Batch 461, G Loss: 0.6938105821609497, D Loss: 1.3875701427459717\n",
            "Epoch 3, Batch 462, G Loss: 0.6938179731369019, D Loss: 1.387542724609375\n",
            "Epoch 3, Batch 463, G Loss: 0.6938227415084839, D Loss: 1.3875043392181396\n",
            "Epoch 3, Batch 464, G Loss: 0.6938581466674805, D Loss: 1.3882451057434082\n",
            "Epoch 3, Batch 465, G Loss: 0.6938400864601135, D Loss: 1.388391137123108\n",
            "Epoch 3, Batch 466, G Loss: 0.6938305497169495, D Loss: 1.387425422668457\n",
            "Epoch 3, Batch 467, G Loss: 0.6937952637672424, D Loss: 1.3875758647918701\n",
            "Epoch 3, Batch 468, G Loss: 0.6937873363494873, D Loss: 1.3876423835754395\n",
            "Epoch 3, Batch 469, G Loss: 0.693708598613739, D Loss: 1.387366771697998\n",
            "Epoch 4, Batch 1, G Loss: 0.6937482953071594, D Loss: 1.3874276876449585\n",
            "Epoch 4, Batch 2, G Loss: 0.6937543153762817, D Loss: 1.3874356746673584\n",
            "Epoch 4, Batch 3, G Loss: 0.6936966180801392, D Loss: 1.3873357772827148\n",
            "Epoch 4, Batch 4, G Loss: 0.6937069296836853, D Loss: 1.387326955795288\n",
            "Epoch 4, Batch 5, G Loss: 0.6936770081520081, D Loss: 1.3873811960220337\n",
            "Epoch 4, Batch 6, G Loss: 0.6937413215637207, D Loss: 1.3875267505645752\n",
            "Epoch 4, Batch 7, G Loss: 0.6936920881271362, D Loss: 1.3875008821487427\n",
            "Epoch 4, Batch 8, G Loss: 0.6936241984367371, D Loss: 1.3874104022979736\n",
            "Epoch 4, Batch 9, G Loss: 0.6936812400817871, D Loss: 1.387190580368042\n",
            "Epoch 4, Batch 10, G Loss: 0.693720817565918, D Loss: 1.3875970840454102\n",
            "Epoch 4, Batch 11, G Loss: 0.6936595439910889, D Loss: 1.3877718448638916\n",
            "Epoch 4, Batch 12, G Loss: 0.6935870051383972, D Loss: 1.387336254119873\n",
            "Epoch 4, Batch 13, G Loss: 0.6935366988182068, D Loss: 1.3873398303985596\n",
            "Epoch 4, Batch 14, G Loss: 0.69353848695755, D Loss: 1.3873183727264404\n",
            "Epoch 4, Batch 15, G Loss: 0.6935778260231018, D Loss: 1.387260913848877\n",
            "Epoch 4, Batch 16, G Loss: 0.6935691237449646, D Loss: 1.3875446319580078\n",
            "Epoch 4, Batch 17, G Loss: 0.6935192942619324, D Loss: 1.3874354362487793\n",
            "Epoch 4, Batch 18, G Loss: 0.6934944987297058, D Loss: 1.3874711990356445\n",
            "Epoch 4, Batch 19, G Loss: 0.6935242414474487, D Loss: 1.38730788230896\n",
            "Epoch 4, Batch 20, G Loss: 0.6934725046157837, D Loss: 1.387436866760254\n",
            "Epoch 4, Batch 21, G Loss: 0.6934646368026733, D Loss: 1.3874940872192383\n",
            "Epoch 4, Batch 22, G Loss: 0.693366527557373, D Loss: 1.3873544931411743\n",
            "Epoch 4, Batch 23, G Loss: 0.6933783888816833, D Loss: 1.387542963027954\n",
            "Epoch 4, Batch 24, G Loss: 0.6934007406234741, D Loss: 1.3872398138046265\n",
            "Epoch 4, Batch 25, G Loss: 0.6933722496032715, D Loss: 1.387207269668579\n",
            "Epoch 4, Batch 26, G Loss: 0.6934145092964172, D Loss: 1.387035608291626\n",
            "Epoch 4, Batch 27, G Loss: 0.6934765577316284, D Loss: 1.3871179819107056\n",
            "Epoch 4, Batch 28, G Loss: 0.6934546828269958, D Loss: 1.3871673345565796\n",
            "Epoch 4, Batch 29, G Loss: 0.6935147643089294, D Loss: 1.3876131772994995\n",
            "Epoch 4, Batch 30, G Loss: 0.6935187578201294, D Loss: 1.3872623443603516\n",
            "Epoch 4, Batch 31, G Loss: 0.6935396790504456, D Loss: 1.387163519859314\n",
            "Epoch 4, Batch 32, G Loss: 0.6935041546821594, D Loss: 1.3873403072357178\n",
            "Epoch 4, Batch 33, G Loss: 0.6935586929321289, D Loss: 1.3870784044265747\n",
            "Epoch 4, Batch 34, G Loss: 0.6935732364654541, D Loss: 1.3872787952423096\n",
            "Epoch 4, Batch 35, G Loss: 0.6935641765594482, D Loss: 1.3871235847473145\n",
            "Epoch 4, Batch 36, G Loss: 0.6935734152793884, D Loss: 1.3873882293701172\n",
            "Epoch 4, Batch 37, G Loss: 0.6935561299324036, D Loss: 1.3876229524612427\n",
            "Epoch 4, Batch 38, G Loss: 0.6935800313949585, D Loss: 1.3874379396438599\n",
            "Epoch 4, Batch 39, G Loss: 0.6936001181602478, D Loss: 1.3871443271636963\n",
            "Epoch 4, Batch 40, G Loss: 0.6936047673225403, D Loss: 1.3873121738433838\n",
            "Epoch 4, Batch 41, G Loss: 0.6935398578643799, D Loss: 1.387399673461914\n",
            "Epoch 4, Batch 42, G Loss: 0.6935012340545654, D Loss: 1.3875627517700195\n",
            "Epoch 4, Batch 43, G Loss: 0.6935548782348633, D Loss: 1.387500286102295\n",
            "Epoch 4, Batch 44, G Loss: 0.693511962890625, D Loss: 1.3873648643493652\n",
            "Epoch 4, Batch 45, G Loss: 0.6934641003608704, D Loss: 1.3873240947723389\n",
            "Epoch 4, Batch 46, G Loss: 0.6934218406677246, D Loss: 1.387085199356079\n",
            "Epoch 4, Batch 47, G Loss: 0.6934119462966919, D Loss: 1.3874258995056152\n",
            "Epoch 4, Batch 48, G Loss: 0.6933929920196533, D Loss: 1.3876943588256836\n",
            "Epoch 4, Batch 49, G Loss: 0.6934120655059814, D Loss: 1.3873859643936157\n",
            "Epoch 4, Batch 50, G Loss: 0.6933290362358093, D Loss: 1.387343406677246\n",
            "Epoch 4, Batch 51, G Loss: 0.6933035850524902, D Loss: 1.387197732925415\n",
            "Epoch 4, Batch 52, G Loss: 0.6932715773582458, D Loss: 1.387031078338623\n",
            "Epoch 4, Batch 53, G Loss: 0.6932182908058167, D Loss: 1.386690616607666\n",
            "Epoch 4, Batch 54, G Loss: 0.6932197213172913, D Loss: 1.3869067430496216\n",
            "Epoch 4, Batch 55, G Loss: 0.6931546926498413, D Loss: 1.3872724771499634\n",
            "Epoch 4, Batch 56, G Loss: 0.6931736469268799, D Loss: 1.3868627548217773\n",
            "Epoch 4, Batch 57, G Loss: 0.6932405829429626, D Loss: 1.3867897987365723\n",
            "Epoch 4, Batch 58, G Loss: 0.6931124329566956, D Loss: 1.386657476425171\n",
            "Epoch 4, Batch 59, G Loss: 0.6931542158126831, D Loss: 1.3868262767791748\n",
            "Epoch 4, Batch 60, G Loss: 0.6931643486022949, D Loss: 1.3869342803955078\n",
            "Epoch 4, Batch 61, G Loss: 0.6932060718536377, D Loss: 1.3870115280151367\n",
            "Epoch 4, Batch 62, G Loss: 0.6932443976402283, D Loss: 1.3872909545898438\n",
            "Epoch 4, Batch 63, G Loss: 0.693270206451416, D Loss: 1.3872570991516113\n",
            "Epoch 4, Batch 64, G Loss: 0.6932567954063416, D Loss: 1.3868894577026367\n",
            "Epoch 4, Batch 65, G Loss: 0.6932128667831421, D Loss: 1.38706636428833\n",
            "Epoch 4, Batch 66, G Loss: 0.6932646036148071, D Loss: 1.3870081901550293\n",
            "Epoch 4, Batch 67, G Loss: 0.6932423114776611, D Loss: 1.387531042098999\n",
            "Epoch 4, Batch 68, G Loss: 0.6932425498962402, D Loss: 1.387399673461914\n",
            "Epoch 4, Batch 69, G Loss: 0.6932100057601929, D Loss: 1.3870513439178467\n",
            "Epoch 4, Batch 70, G Loss: 0.693239688873291, D Loss: 1.386894702911377\n",
            "Epoch 4, Batch 71, G Loss: 0.6931739449501038, D Loss: 1.3869514465332031\n",
            "Epoch 4, Batch 72, G Loss: 0.6931451559066772, D Loss: 1.3868188858032227\n",
            "Epoch 4, Batch 73, G Loss: 0.6932057738304138, D Loss: 1.386721134185791\n",
            "Epoch 4, Batch 74, G Loss: 0.6932083368301392, D Loss: 1.387133240699768\n",
            "Epoch 4, Batch 75, G Loss: 0.6931654214859009, D Loss: 1.3869901895523071\n",
            "Epoch 4, Batch 76, G Loss: 0.6931740641593933, D Loss: 1.3870124816894531\n",
            "Epoch 4, Batch 77, G Loss: 0.6931809186935425, D Loss: 1.3870209455490112\n",
            "Epoch 4, Batch 78, G Loss: 0.6931945085525513, D Loss: 1.3868846893310547\n",
            "Epoch 4, Batch 79, G Loss: 0.6931871771812439, D Loss: 1.3869678974151611\n",
            "Epoch 4, Batch 80, G Loss: 0.693152904510498, D Loss: 1.3871554136276245\n",
            "Epoch 4, Batch 81, G Loss: 0.693151593208313, D Loss: 1.387563943862915\n",
            "Epoch 4, Batch 82, G Loss: 0.6931859850883484, D Loss: 1.3872650861740112\n",
            "Epoch 4, Batch 83, G Loss: 0.6931704878807068, D Loss: 1.387012004852295\n",
            "Epoch 4, Batch 84, G Loss: 0.6931306719779968, D Loss: 1.3867273330688477\n",
            "Epoch 4, Batch 85, G Loss: 0.693144679069519, D Loss: 1.3869737386703491\n",
            "Epoch 4, Batch 86, G Loss: 0.6930865049362183, D Loss: 1.3870177268981934\n",
            "Epoch 4, Batch 87, G Loss: 0.6930531859397888, D Loss: 1.3870562314987183\n",
            "Epoch 4, Batch 88, G Loss: 0.6930872797966003, D Loss: 1.386892557144165\n",
            "Epoch 4, Batch 89, G Loss: 0.6930891275405884, D Loss: 1.3867831230163574\n",
            "Epoch 4, Batch 90, G Loss: 0.6930640935897827, D Loss: 1.386745572090149\n",
            "Epoch 4, Batch 91, G Loss: 0.6930168271064758, D Loss: 1.3867642879486084\n",
            "Epoch 4, Batch 92, G Loss: 0.6930816173553467, D Loss: 1.3866736888885498\n",
            "Epoch 4, Batch 93, G Loss: 0.6930227279663086, D Loss: 1.386786699295044\n",
            "Epoch 4, Batch 94, G Loss: 0.6930645704269409, D Loss: 1.3868980407714844\n",
            "Epoch 4, Batch 95, G Loss: 0.6930803656578064, D Loss: 1.386866569519043\n",
            "Epoch 4, Batch 96, G Loss: 0.6930900812149048, D Loss: 1.3870267868041992\n",
            "Epoch 4, Batch 97, G Loss: 0.6930453181266785, D Loss: 1.386928677558899\n",
            "Epoch 4, Batch 98, G Loss: 0.6930536031723022, D Loss: 1.3870803117752075\n",
            "Epoch 4, Batch 99, G Loss: 0.6930497884750366, D Loss: 1.3870964050292969\n",
            "Epoch 4, Batch 100, G Loss: 0.6930622458457947, D Loss: 1.3867340087890625\n",
            "Epoch 4, Batch 101, G Loss: 0.6929987668991089, D Loss: 1.3866782188415527\n",
            "Epoch 4, Batch 102, G Loss: 0.6930087208747864, D Loss: 1.3872380256652832\n",
            "Epoch 4, Batch 103, G Loss: 0.6930221319198608, D Loss: 1.3872148990631104\n",
            "Epoch 4, Batch 104, G Loss: 0.6930327415466309, D Loss: 1.3870441913604736\n",
            "Epoch 4, Batch 105, G Loss: 0.6929852962493896, D Loss: 1.3869616985321045\n",
            "Epoch 4, Batch 106, G Loss: 0.6929824352264404, D Loss: 1.38713800907135\n",
            "Epoch 4, Batch 107, G Loss: 0.6929419636726379, D Loss: 1.3867452144622803\n",
            "Epoch 4, Batch 108, G Loss: 0.6929371356964111, D Loss: 1.3864344358444214\n",
            "Epoch 4, Batch 109, G Loss: 0.6929389238357544, D Loss: 1.3867790699005127\n",
            "Epoch 4, Batch 110, G Loss: 0.692927896976471, D Loss: 1.3868601322174072\n",
            "Epoch 4, Batch 111, G Loss: 0.6929405331611633, D Loss: 1.3871428966522217\n",
            "Epoch 4, Batch 112, G Loss: 0.6928580403327942, D Loss: 1.3870620727539062\n",
            "Epoch 4, Batch 113, G Loss: 0.6928721070289612, D Loss: 1.3868234157562256\n",
            "Epoch 4, Batch 114, G Loss: 0.6928726434707642, D Loss: 1.3865535259246826\n",
            "Epoch 4, Batch 115, G Loss: 0.6928882002830505, D Loss: 1.3867313861846924\n",
            "Epoch 4, Batch 116, G Loss: 0.6929107904434204, D Loss: 1.3865997791290283\n",
            "Epoch 4, Batch 117, G Loss: 0.6928542852401733, D Loss: 1.386649489402771\n",
            "Epoch 4, Batch 118, G Loss: 0.6928707957267761, D Loss: 1.3865513801574707\n",
            "Epoch 4, Batch 119, G Loss: 0.6929216384887695, D Loss: 1.386458396911621\n",
            "Epoch 4, Batch 120, G Loss: 0.6929199695587158, D Loss: 1.3865119218826294\n",
            "Epoch 4, Batch 121, G Loss: 0.6929671764373779, D Loss: 1.3863835334777832\n",
            "Epoch 4, Batch 122, G Loss: 0.6929857730865479, D Loss: 1.3865580558776855\n",
            "Epoch 4, Batch 123, G Loss: 0.6930206418037415, D Loss: 1.38663649559021\n",
            "Epoch 4, Batch 124, G Loss: 0.6930594444274902, D Loss: 1.3865787982940674\n",
            "Epoch 4, Batch 125, G Loss: 0.6930623054504395, D Loss: 1.3866456747055054\n",
            "Epoch 4, Batch 126, G Loss: 0.6931055784225464, D Loss: 1.3868249654769897\n",
            "Epoch 4, Batch 127, G Loss: 0.6931381821632385, D Loss: 1.3868019580841064\n",
            "Epoch 4, Batch 128, G Loss: 0.6931906938552856, D Loss: 1.386598825454712\n",
            "Epoch 4, Batch 129, G Loss: 0.6932047009468079, D Loss: 1.3865431547164917\n",
            "Epoch 4, Batch 130, G Loss: 0.6931905150413513, D Loss: 1.3865861892700195\n",
            "Epoch 4, Batch 131, G Loss: 0.6931865215301514, D Loss: 1.3864740133285522\n",
            "Epoch 4, Batch 132, G Loss: 0.6932461261749268, D Loss: 1.3865530490875244\n",
            "Epoch 4, Batch 133, G Loss: 0.6932704448699951, D Loss: 1.3864636421203613\n",
            "Epoch 4, Batch 134, G Loss: 0.6932880878448486, D Loss: 1.3865864276885986\n",
            "Epoch 4, Batch 135, G Loss: 0.6933327317237854, D Loss: 1.386852741241455\n",
            "Epoch 4, Batch 136, G Loss: 0.6933581829071045, D Loss: 1.3867616653442383\n",
            "Epoch 4, Batch 137, G Loss: 0.6933460831642151, D Loss: 1.3865011930465698\n",
            "Epoch 4, Batch 138, G Loss: 0.6933636665344238, D Loss: 1.3863277435302734\n",
            "Epoch 4, Batch 139, G Loss: 0.6934112310409546, D Loss: 1.386276125907898\n",
            "Epoch 4, Batch 140, G Loss: 0.6934394836425781, D Loss: 1.3861583471298218\n",
            "Epoch 4, Batch 141, G Loss: 0.6934621930122375, D Loss: 1.3865625858306885\n",
            "Epoch 4, Batch 142, G Loss: 0.6934868693351746, D Loss: 1.386474609375\n",
            "Epoch 4, Batch 143, G Loss: 0.6935362815856934, D Loss: 1.3865246772766113\n",
            "Epoch 4, Batch 144, G Loss: 0.6935676336288452, D Loss: 1.3863393068313599\n",
            "Epoch 4, Batch 145, G Loss: 0.6936290264129639, D Loss: 1.3863821029663086\n",
            "Epoch 4, Batch 146, G Loss: 0.6936192512512207, D Loss: 1.386619210243225\n",
            "Epoch 4, Batch 147, G Loss: 0.6936695575714111, D Loss: 1.3864136934280396\n",
            "Epoch 4, Batch 148, G Loss: 0.6937041878700256, D Loss: 1.386458158493042\n",
            "Epoch 4, Batch 149, G Loss: 0.6937417984008789, D Loss: 1.386227011680603\n",
            "Epoch 4, Batch 150, G Loss: 0.6937514543533325, D Loss: 1.3862557411193848\n",
            "Epoch 4, Batch 151, G Loss: 0.6937915086746216, D Loss: 1.3869802951812744\n",
            "Epoch 4, Batch 152, G Loss: 0.6938112378120422, D Loss: 1.387052059173584\n",
            "Epoch 4, Batch 153, G Loss: 0.6938079595565796, D Loss: 1.3870289325714111\n",
            "Epoch 4, Batch 154, G Loss: 0.693840503692627, D Loss: 1.386866807937622\n",
            "Epoch 4, Batch 155, G Loss: 0.6938502788543701, D Loss: 1.3869472742080688\n",
            "Epoch 4, Batch 156, G Loss: 0.6938164234161377, D Loss: 1.386884331703186\n",
            "Epoch 4, Batch 157, G Loss: 0.6938210129737854, D Loss: 1.3867278099060059\n",
            "Epoch 4, Batch 158, G Loss: 0.6938093304634094, D Loss: 1.3866872787475586\n",
            "Epoch 4, Batch 159, G Loss: 0.6938334703445435, D Loss: 1.3863600492477417\n",
            "Epoch 4, Batch 160, G Loss: 0.69383704662323, D Loss: 1.3865289688110352\n",
            "Epoch 4, Batch 161, G Loss: 0.6938254833221436, D Loss: 1.3866899013519287\n",
            "Epoch 4, Batch 162, G Loss: 0.6938270330429077, D Loss: 1.386744737625122\n",
            "Epoch 4, Batch 163, G Loss: 0.6938316226005554, D Loss: 1.3864548206329346\n",
            "Epoch 4, Batch 164, G Loss: 0.6938382983207703, D Loss: 1.3866397142410278\n",
            "Epoch 4, Batch 165, G Loss: 0.6938334703445435, D Loss: 1.3862156867980957\n",
            "Epoch 4, Batch 166, G Loss: 0.6938053965568542, D Loss: 1.38644278049469\n",
            "Epoch 4, Batch 167, G Loss: 0.6938323974609375, D Loss: 1.3866301774978638\n",
            "Epoch 4, Batch 168, G Loss: 0.6938283443450928, D Loss: 1.3866976499557495\n",
            "Epoch 4, Batch 169, G Loss: 0.6938514113426208, D Loss: 1.3864545822143555\n",
            "Epoch 4, Batch 170, G Loss: 0.6938506364822388, D Loss: 1.3860373497009277\n",
            "Epoch 4, Batch 171, G Loss: 0.6938735246658325, D Loss: 1.3871700763702393\n",
            "Epoch 4, Batch 172, G Loss: 0.6938635110855103, D Loss: 1.3871493339538574\n",
            "Epoch 4, Batch 173, G Loss: 0.6938546895980835, D Loss: 1.3865442276000977\n",
            "Epoch 4, Batch 174, G Loss: 0.6938589215278625, D Loss: 1.3865208625793457\n",
            "Epoch 4, Batch 175, G Loss: 0.6938663125038147, D Loss: 1.3863952159881592\n",
            "Epoch 4, Batch 176, G Loss: 0.693873941898346, D Loss: 1.3863868713378906\n",
            "Epoch 4, Batch 177, G Loss: 0.6938250660896301, D Loss: 1.3866993188858032\n",
            "Epoch 4, Batch 178, G Loss: 0.6938247680664062, D Loss: 1.3871792554855347\n",
            "Epoch 4, Batch 179, G Loss: 0.6938346028327942, D Loss: 1.3867751359939575\n",
            "Epoch 4, Batch 180, G Loss: 0.6937992572784424, D Loss: 1.3869315385818481\n",
            "Epoch 4, Batch 181, G Loss: 0.693778932094574, D Loss: 1.3869709968566895\n",
            "Epoch 4, Batch 182, G Loss: 0.6937291026115417, D Loss: 1.3865536451339722\n",
            "Epoch 4, Batch 183, G Loss: 0.6937210559844971, D Loss: 1.3866708278656006\n",
            "Epoch 4, Batch 184, G Loss: 0.6936813592910767, D Loss: 1.3863978385925293\n",
            "Epoch 4, Batch 185, G Loss: 0.6936684250831604, D Loss: 1.3862197399139404\n",
            "Epoch 4, Batch 186, G Loss: 0.6936590075492859, D Loss: 1.386404037475586\n",
            "Epoch 4, Batch 187, G Loss: 0.6935980916023254, D Loss: 1.3865511417388916\n",
            "Epoch 4, Batch 188, G Loss: 0.6935926079750061, D Loss: 1.3865525722503662\n",
            "Epoch 4, Batch 189, G Loss: 0.6936302781105042, D Loss: 1.3864102363586426\n",
            "Epoch 4, Batch 190, G Loss: 0.6935996413230896, D Loss: 1.3864208459854126\n",
            "Epoch 4, Batch 191, G Loss: 0.6935898065567017, D Loss: 1.386457920074463\n",
            "Epoch 4, Batch 192, G Loss: 0.6935767531394958, D Loss: 1.386499047279358\n",
            "Epoch 4, Batch 193, G Loss: 0.6936124563217163, D Loss: 1.3867218494415283\n",
            "Epoch 4, Batch 194, G Loss: 0.6935815811157227, D Loss: 1.3864850997924805\n",
            "Epoch 4, Batch 195, G Loss: 0.6936131715774536, D Loss: 1.3860223293304443\n",
            "Epoch 4, Batch 196, G Loss: 0.693622887134552, D Loss: 1.3862519264221191\n",
            "Epoch 4, Batch 197, G Loss: 0.6936302781105042, D Loss: 1.3863823413848877\n",
            "Epoch 4, Batch 198, G Loss: 0.6936411261558533, D Loss: 1.3868775367736816\n",
            "Epoch 4, Batch 199, G Loss: 0.6936706900596619, D Loss: 1.3869824409484863\n",
            "Epoch 4, Batch 200, G Loss: 0.6936818361282349, D Loss: 1.386500358581543\n",
            "Epoch 4, Batch 201, G Loss: 0.6936924457550049, D Loss: 1.3865437507629395\n",
            "Epoch 4, Batch 202, G Loss: 0.6936886310577393, D Loss: 1.3859775066375732\n",
            "Epoch 4, Batch 203, G Loss: 0.6937010884284973, D Loss: 1.3858506679534912\n",
            "Epoch 4, Batch 204, G Loss: 0.6937366127967834, D Loss: 1.3865922689437866\n",
            "Epoch 4, Batch 205, G Loss: 0.6937382221221924, D Loss: 1.386378526687622\n",
            "Epoch 4, Batch 206, G Loss: 0.6937596797943115, D Loss: 1.386157512664795\n",
            "Epoch 4, Batch 207, G Loss: 0.6938077211380005, D Loss: 1.38606858253479\n",
            "Epoch 4, Batch 208, G Loss: 0.6938113570213318, D Loss: 1.3859204053878784\n",
            "Epoch 4, Batch 209, G Loss: 0.6938267350196838, D Loss: 1.3864762783050537\n",
            "Epoch 4, Batch 210, G Loss: 0.6938593983650208, D Loss: 1.3866055011749268\n",
            "Epoch 4, Batch 211, G Loss: 0.6938478350639343, D Loss: 1.3862230777740479\n",
            "Epoch 4, Batch 212, G Loss: 0.6938987374305725, D Loss: 1.386602759361267\n",
            "Epoch 4, Batch 213, G Loss: 0.69391930103302, D Loss: 1.3867337703704834\n",
            "Epoch 4, Batch 214, G Loss: 0.6939264535903931, D Loss: 1.3865435123443604\n",
            "Epoch 4, Batch 215, G Loss: 0.6939147114753723, D Loss: 1.386364221572876\n",
            "Epoch 4, Batch 216, G Loss: 0.6939091682434082, D Loss: 1.3862299919128418\n",
            "Epoch 4, Batch 217, G Loss: 0.6939468383789062, D Loss: 1.3860344886779785\n",
            "Epoch 4, Batch 218, G Loss: 0.693950355052948, D Loss: 1.3860840797424316\n",
            "Epoch 4, Batch 219, G Loss: 0.6939666867256165, D Loss: 1.3867381811141968\n",
            "Epoch 4, Batch 220, G Loss: 0.693967878818512, D Loss: 1.3867130279541016\n",
            "Epoch 4, Batch 221, G Loss: 0.694000244140625, D Loss: 1.386913776397705\n",
            "Epoch 4, Batch 222, G Loss: 0.6940172910690308, D Loss: 1.386824131011963\n",
            "Epoch 4, Batch 223, G Loss: 0.6940333843231201, D Loss: 1.3867707252502441\n",
            "Epoch 4, Batch 224, G Loss: 0.6940328478813171, D Loss: 1.386565923690796\n",
            "Epoch 4, Batch 225, G Loss: 0.6940292119979858, D Loss: 1.386401653289795\n",
            "Epoch 4, Batch 226, G Loss: 0.694034993648529, D Loss: 1.3865796327590942\n",
            "Epoch 4, Batch 227, G Loss: 0.6940146684646606, D Loss: 1.3866102695465088\n",
            "Epoch 4, Batch 228, G Loss: 0.6939693093299866, D Loss: 1.3859888315200806\n",
            "Epoch 4, Batch 229, G Loss: 0.693994402885437, D Loss: 1.3865388631820679\n",
            "Epoch 4, Batch 230, G Loss: 0.6939541697502136, D Loss: 1.3869750499725342\n",
            "Epoch 4, Batch 231, G Loss: 0.6939301490783691, D Loss: 1.3864874839782715\n",
            "Epoch 4, Batch 232, G Loss: 0.6939026713371277, D Loss: 1.386102557182312\n",
            "Epoch 4, Batch 233, G Loss: 0.6939163208007812, D Loss: 1.3858274221420288\n",
            "Epoch 4, Batch 234, G Loss: 0.6938756704330444, D Loss: 1.3858966827392578\n",
            "Epoch 4, Batch 235, G Loss: 0.693891167640686, D Loss: 1.3859896659851074\n",
            "Epoch 4, Batch 236, G Loss: 0.6939002871513367, D Loss: 1.3861842155456543\n",
            "Epoch 4, Batch 237, G Loss: 0.6939059495925903, D Loss: 1.38612699508667\n",
            "Epoch 4, Batch 238, G Loss: 0.6939148902893066, D Loss: 1.3865203857421875\n",
            "Epoch 4, Batch 239, G Loss: 0.6938899159431458, D Loss: 1.3863749504089355\n",
            "Epoch 4, Batch 240, G Loss: 0.6939114332199097, D Loss: 1.3855470418930054\n",
            "Epoch 4, Batch 241, G Loss: 0.6939080953598022, D Loss: 1.3860148191452026\n",
            "Epoch 4, Batch 242, G Loss: 0.6938967108726501, D Loss: 1.3861384391784668\n",
            "Epoch 4, Batch 243, G Loss: 0.6939183473587036, D Loss: 1.3860843181610107\n",
            "Epoch 4, Batch 244, G Loss: 0.6939110159873962, D Loss: 1.3860796689987183\n",
            "Epoch 4, Batch 245, G Loss: 0.6939488649368286, D Loss: 1.3860493898391724\n",
            "Epoch 4, Batch 246, G Loss: 0.693946123123169, D Loss: 1.3860257863998413\n",
            "Epoch 4, Batch 247, G Loss: 0.6939653754234314, D Loss: 1.3857725858688354\n",
            "Epoch 4, Batch 248, G Loss: 0.6939910054206848, D Loss: 1.3858451843261719\n",
            "Epoch 4, Batch 249, G Loss: 0.6940022706985474, D Loss: 1.3860859870910645\n",
            "Epoch 4, Batch 250, G Loss: 0.694050669670105, D Loss: 1.3861165046691895\n",
            "Epoch 4, Batch 251, G Loss: 0.6941016912460327, D Loss: 1.3865077495574951\n",
            "Epoch 4, Batch 252, G Loss: 0.6940671801567078, D Loss: 1.3862757682800293\n",
            "Epoch 4, Batch 253, G Loss: 0.6940679550170898, D Loss: 1.386648416519165\n",
            "Epoch 4, Batch 254, G Loss: 0.6940902471542358, D Loss: 1.3868077993392944\n",
            "Epoch 4, Batch 255, G Loss: 0.6940795183181763, D Loss: 1.386165738105774\n",
            "Epoch 4, Batch 256, G Loss: 0.6940719485282898, D Loss: 1.385903000831604\n",
            "Epoch 4, Batch 257, G Loss: 0.6941045522689819, D Loss: 1.3860492706298828\n",
            "Epoch 4, Batch 258, G Loss: 0.6941075325012207, D Loss: 1.386568307876587\n",
            "Epoch 4, Batch 259, G Loss: 0.6941120624542236, D Loss: 1.3865457773208618\n",
            "Epoch 4, Batch 260, G Loss: 0.6941091418266296, D Loss: 1.3864928483963013\n",
            "Epoch 4, Batch 261, G Loss: 0.6941335201263428, D Loss: 1.3862061500549316\n",
            "Epoch 4, Batch 262, G Loss: 0.6941128373146057, D Loss: 1.38588547706604\n",
            "Epoch 4, Batch 263, G Loss: 0.6941058039665222, D Loss: 1.3856587409973145\n",
            "Epoch 4, Batch 264, G Loss: 0.6941019892692566, D Loss: 1.3862438201904297\n",
            "Epoch 4, Batch 265, G Loss: 0.6941215395927429, D Loss: 1.386610746383667\n",
            "Epoch 4, Batch 266, G Loss: 0.6941128969192505, D Loss: 1.3859447240829468\n",
            "Epoch 4, Batch 267, G Loss: 0.6941495537757874, D Loss: 1.386246681213379\n",
            "Epoch 4, Batch 268, G Loss: 0.6941701173782349, D Loss: 1.3859493732452393\n",
            "Epoch 4, Batch 269, G Loss: 0.6941877007484436, D Loss: 1.3864595890045166\n",
            "Epoch 4, Batch 270, G Loss: 0.6941839456558228, D Loss: 1.386439561843872\n",
            "Epoch 4, Batch 271, G Loss: 0.6942369341850281, D Loss: 1.386340618133545\n",
            "Epoch 4, Batch 272, G Loss: 0.6942288279533386, D Loss: 1.3863563537597656\n",
            "Epoch 4, Batch 273, G Loss: 0.6942395567893982, D Loss: 1.3862814903259277\n",
            "Epoch 4, Batch 274, G Loss: 0.6942102313041687, D Loss: 1.3865106105804443\n",
            "Epoch 4, Batch 275, G Loss: 0.6942093372344971, D Loss: 1.3860561847686768\n",
            "Epoch 4, Batch 276, G Loss: 0.6942068338394165, D Loss: 1.3859117031097412\n",
            "Epoch 4, Batch 277, G Loss: 0.694189727306366, D Loss: 1.3864482641220093\n",
            "Epoch 4, Batch 278, G Loss: 0.6941723823547363, D Loss: 1.3863954544067383\n",
            "Epoch 4, Batch 279, G Loss: 0.6941680908203125, D Loss: 1.3861887454986572\n",
            "Epoch 4, Batch 280, G Loss: 0.6941588521003723, D Loss: 1.3861563205718994\n",
            "Epoch 4, Batch 281, G Loss: 0.6941418051719666, D Loss: 1.385737419128418\n",
            "Epoch 4, Batch 282, G Loss: 0.6941642165184021, D Loss: 1.3854200839996338\n",
            "Epoch 4, Batch 283, G Loss: 0.6941925287246704, D Loss: 1.3869231939315796\n",
            "Epoch 4, Batch 284, G Loss: 0.6941995620727539, D Loss: 1.3869438171386719\n",
            "Epoch 4, Batch 285, G Loss: 0.6942071914672852, D Loss: 1.3861494064331055\n",
            "Epoch 4, Batch 286, G Loss: 0.6942420601844788, D Loss: 1.3866550922393799\n",
            "Epoch 4, Batch 287, G Loss: 0.694298267364502, D Loss: 1.387214183807373\n",
            "Epoch 4, Batch 288, G Loss: 0.6943342089653015, D Loss: 1.386804461479187\n",
            "Epoch 4, Batch 289, G Loss: 0.6943262815475464, D Loss: 1.3863110542297363\n",
            "Epoch 4, Batch 290, G Loss: 0.6943659782409668, D Loss: 1.3864459991455078\n",
            "Epoch 4, Batch 291, G Loss: 0.6943703293800354, D Loss: 1.3868190050125122\n",
            "Epoch 4, Batch 292, G Loss: 0.6943542957305908, D Loss: 1.386654019355774\n",
            "Epoch 4, Batch 293, G Loss: 0.694340169429779, D Loss: 1.3861703872680664\n",
            "Epoch 4, Batch 294, G Loss: 0.6943725943565369, D Loss: 1.3858264684677124\n",
            "Epoch 4, Batch 295, G Loss: 0.6943960189819336, D Loss: 1.3861346244812012\n",
            "Epoch 4, Batch 296, G Loss: 0.6944162845611572, D Loss: 1.3865084648132324\n",
            "Epoch 4, Batch 297, G Loss: 0.6944252848625183, D Loss: 1.3860716819763184\n",
            "Epoch 4, Batch 298, G Loss: 0.6944480538368225, D Loss: 1.3860355615615845\n",
            "Epoch 4, Batch 299, G Loss: 0.6944425702095032, D Loss: 1.3863515853881836\n",
            "Epoch 4, Batch 300, G Loss: 0.6944488883018494, D Loss: 1.3859868049621582\n",
            "Epoch 4, Batch 301, G Loss: 0.6944699883460999, D Loss: 1.385875940322876\n",
            "Epoch 4, Batch 302, G Loss: 0.694507896900177, D Loss: 1.3864842653274536\n",
            "Epoch 4, Batch 303, G Loss: 0.6945421099662781, D Loss: 1.386115312576294\n",
            "Epoch 4, Batch 304, G Loss: 0.6945338845252991, D Loss: 1.386314034461975\n",
            "Epoch 4, Batch 305, G Loss: 0.6945664286613464, D Loss: 1.385920763015747\n",
            "Epoch 4, Batch 306, G Loss: 0.6945785880088806, D Loss: 1.3865183591842651\n",
            "Epoch 4, Batch 307, G Loss: 0.6945657730102539, D Loss: 1.3866033554077148\n",
            "Epoch 4, Batch 308, G Loss: 0.6945627927780151, D Loss: 1.386423110961914\n",
            "Epoch 4, Batch 309, G Loss: 0.6945479512214661, D Loss: 1.3863954544067383\n",
            "Epoch 4, Batch 310, G Loss: 0.6945205926895142, D Loss: 1.3864740133285522\n",
            "Epoch 4, Batch 311, G Loss: 0.6945763230323792, D Loss: 1.3856360912322998\n",
            "Epoch 4, Batch 312, G Loss: 0.6945579051971436, D Loss: 1.3858184814453125\n",
            "Epoch 4, Batch 313, G Loss: 0.6945727467536926, D Loss: 1.3862172365188599\n",
            "Epoch 4, Batch 314, G Loss: 0.6945635080337524, D Loss: 1.3863298892974854\n",
            "Epoch 4, Batch 315, G Loss: 0.6945804953575134, D Loss: 1.386000394821167\n",
            "Epoch 4, Batch 316, G Loss: 0.6945640444755554, D Loss: 1.3862378597259521\n",
            "Epoch 4, Batch 317, G Loss: 0.6945832967758179, D Loss: 1.3866653442382812\n",
            "Epoch 4, Batch 318, G Loss: 0.6945691704750061, D Loss: 1.3861212730407715\n",
            "Epoch 4, Batch 319, G Loss: 0.694557249546051, D Loss: 1.3858458995819092\n",
            "Epoch 4, Batch 320, G Loss: 0.6945226192474365, D Loss: 1.385999083518982\n",
            "Epoch 4, Batch 321, G Loss: 0.6945213675498962, D Loss: 1.385937213897705\n",
            "Epoch 4, Batch 322, G Loss: 0.6945303082466125, D Loss: 1.3861417770385742\n",
            "Epoch 4, Batch 323, G Loss: 0.6944910287857056, D Loss: 1.3861294984817505\n",
            "Epoch 4, Batch 324, G Loss: 0.6945000290870667, D Loss: 1.386274814605713\n",
            "Epoch 4, Batch 325, G Loss: 0.694482147693634, D Loss: 1.3860137462615967\n",
            "Epoch 4, Batch 326, G Loss: 0.6944637894630432, D Loss: 1.385668158531189\n",
            "Epoch 4, Batch 327, G Loss: 0.6944636702537537, D Loss: 1.385379433631897\n",
            "Epoch 4, Batch 328, G Loss: 0.6944562792778015, D Loss: 1.3863368034362793\n",
            "Epoch 4, Batch 329, G Loss: 0.6944435238838196, D Loss: 1.3866206407546997\n",
            "Epoch 4, Batch 330, G Loss: 0.6944150924682617, D Loss: 1.3863599300384521\n",
            "Epoch 4, Batch 331, G Loss: 0.6944435238838196, D Loss: 1.386204719543457\n",
            "Epoch 4, Batch 332, G Loss: 0.6944600343704224, D Loss: 1.3860197067260742\n",
            "Epoch 4, Batch 333, G Loss: 0.6944402456283569, D Loss: 1.3862946033477783\n",
            "Epoch 4, Batch 334, G Loss: 0.6944746971130371, D Loss: 1.3866333961486816\n",
            "Epoch 4, Batch 335, G Loss: 0.6944399476051331, D Loss: 1.386514663696289\n",
            "Epoch 4, Batch 336, G Loss: 0.6944296956062317, D Loss: 1.3858304023742676\n",
            "Epoch 4, Batch 337, G Loss: 0.6944234371185303, D Loss: 1.3856756687164307\n",
            "Epoch 4, Batch 338, G Loss: 0.6944066882133484, D Loss: 1.3864316940307617\n",
            "Epoch 4, Batch 339, G Loss: 0.6944238543510437, D Loss: 1.3861373662948608\n",
            "Epoch 4, Batch 340, G Loss: 0.6944234371185303, D Loss: 1.3862645626068115\n",
            "Epoch 4, Batch 341, G Loss: 0.6943989396095276, D Loss: 1.38611900806427\n",
            "Epoch 4, Batch 342, G Loss: 0.694395899772644, D Loss: 1.3861641883850098\n",
            "Epoch 4, Batch 343, G Loss: 0.6943629384040833, D Loss: 1.3859148025512695\n",
            "Epoch 4, Batch 344, G Loss: 0.694402277469635, D Loss: 1.3856263160705566\n",
            "Epoch 4, Batch 345, G Loss: 0.6943855881690979, D Loss: 1.385993480682373\n",
            "Epoch 4, Batch 346, G Loss: 0.6944020986557007, D Loss: 1.3860843181610107\n",
            "Epoch 4, Batch 347, G Loss: 0.694398820400238, D Loss: 1.3860232830047607\n",
            "Epoch 4, Batch 348, G Loss: 0.6944137811660767, D Loss: 1.386095404624939\n",
            "Epoch 4, Batch 349, G Loss: 0.694398820400238, D Loss: 1.3860002756118774\n",
            "Epoch 4, Batch 350, G Loss: 0.6943862438201904, D Loss: 1.3858873844146729\n",
            "Epoch 4, Batch 351, G Loss: 0.6943837404251099, D Loss: 1.385871410369873\n",
            "Epoch 4, Batch 352, G Loss: 0.6943821907043457, D Loss: 1.386199951171875\n",
            "Epoch 4, Batch 353, G Loss: 0.6943687796592712, D Loss: 1.3862203359603882\n",
            "Epoch 4, Batch 354, G Loss: 0.6943751573562622, D Loss: 1.3861796855926514\n",
            "Epoch 4, Batch 355, G Loss: 0.6943709850311279, D Loss: 1.3859326839447021\n",
            "Epoch 4, Batch 356, G Loss: 0.6943530440330505, D Loss: 1.385838270187378\n",
            "Epoch 4, Batch 357, G Loss: 0.6943536996841431, D Loss: 1.3859295845031738\n",
            "Epoch 4, Batch 358, G Loss: 0.6943557262420654, D Loss: 1.3860657215118408\n",
            "Epoch 4, Batch 359, G Loss: 0.6943482160568237, D Loss: 1.3858590126037598\n",
            "Epoch 4, Batch 360, G Loss: 0.6943468451499939, D Loss: 1.3853846788406372\n",
            "Epoch 4, Batch 361, G Loss: 0.6943491101264954, D Loss: 1.385602355003357\n",
            "Epoch 4, Batch 362, G Loss: 0.6943585276603699, D Loss: 1.3858596086502075\n",
            "Epoch 4, Batch 363, G Loss: 0.6943776607513428, D Loss: 1.3861632347106934\n",
            "Epoch 4, Batch 364, G Loss: 0.6943792104721069, D Loss: 1.386011004447937\n",
            "Epoch 4, Batch 365, G Loss: 0.6943753957748413, D Loss: 1.386399745941162\n",
            "Epoch 4, Batch 366, G Loss: 0.6943677067756653, D Loss: 1.386397361755371\n",
            "Epoch 4, Batch 367, G Loss: 0.6943555474281311, D Loss: 1.3863089084625244\n",
            "Epoch 4, Batch 368, G Loss: 0.6943516135215759, D Loss: 1.3861205577850342\n",
            "Epoch 4, Batch 369, G Loss: 0.6943267583847046, D Loss: 1.3858914375305176\n",
            "Epoch 4, Batch 370, G Loss: 0.6943051218986511, D Loss: 1.385488510131836\n",
            "Epoch 4, Batch 371, G Loss: 0.69430011510849, D Loss: 1.3863575458526611\n",
            "Epoch 4, Batch 372, G Loss: 0.6942970156669617, D Loss: 1.386502981185913\n",
            "Epoch 4, Batch 373, G Loss: 0.6942809224128723, D Loss: 1.3868122100830078\n",
            "Epoch 4, Batch 374, G Loss: 0.6942275762557983, D Loss: 1.3864305019378662\n",
            "Epoch 4, Batch 375, G Loss: 0.6942184567451477, D Loss: 1.386196255683899\n",
            "Epoch 4, Batch 376, G Loss: 0.6941817402839661, D Loss: 1.3866052627563477\n",
            "Epoch 4, Batch 377, G Loss: 0.694146990776062, D Loss: 1.3867928981781006\n",
            "Epoch 4, Batch 378, G Loss: 0.6941009759902954, D Loss: 1.3866641521453857\n",
            "Epoch 4, Batch 379, G Loss: 0.6940597295761108, D Loss: 1.3864824771881104\n",
            "Epoch 4, Batch 380, G Loss: 0.6940131783485413, D Loss: 1.3864578008651733\n",
            "Epoch 4, Batch 381, G Loss: 0.6939631700515747, D Loss: 1.3864856958389282\n",
            "Epoch 4, Batch 382, G Loss: 0.6939150094985962, D Loss: 1.3863673210144043\n",
            "Epoch 4, Batch 383, G Loss: 0.6938442587852478, D Loss: 1.3853868246078491\n",
            "Epoch 4, Batch 384, G Loss: 0.6938245892524719, D Loss: 1.386030673980713\n",
            "Epoch 4, Batch 385, G Loss: 0.6937842965126038, D Loss: 1.3863260746002197\n",
            "Epoch 4, Batch 386, G Loss: 0.6937548518180847, D Loss: 1.386030673980713\n",
            "Epoch 4, Batch 387, G Loss: 0.693745493888855, D Loss: 1.3861745595932007\n",
            "Epoch 4, Batch 388, G Loss: 0.6937121152877808, D Loss: 1.386047124862671\n",
            "Epoch 4, Batch 389, G Loss: 0.6936713457107544, D Loss: 1.3860907554626465\n",
            "Epoch 4, Batch 390, G Loss: 0.693659245967865, D Loss: 1.3857988119125366\n",
            "Epoch 4, Batch 391, G Loss: 0.693657398223877, D Loss: 1.385617971420288\n",
            "Epoch 4, Batch 392, G Loss: 0.6936460733413696, D Loss: 1.3860142230987549\n",
            "Epoch 4, Batch 393, G Loss: 0.6936169266700745, D Loss: 1.3859107494354248\n",
            "Epoch 4, Batch 394, G Loss: 0.6936189532279968, D Loss: 1.3859460353851318\n",
            "Epoch 4, Batch 395, G Loss: 0.6936265826225281, D Loss: 1.3860552310943604\n",
            "Epoch 4, Batch 396, G Loss: 0.693635106086731, D Loss: 1.3858001232147217\n",
            "Epoch 4, Batch 397, G Loss: 0.6936405897140503, D Loss: 1.3862667083740234\n",
            "Epoch 4, Batch 398, G Loss: 0.6936191916465759, D Loss: 1.3861720561981201\n",
            "Epoch 4, Batch 399, G Loss: 0.6936348080635071, D Loss: 1.3859196901321411\n",
            "Epoch 4, Batch 400, G Loss: 0.6936255693435669, D Loss: 1.3864312171936035\n",
            "Epoch 4, Batch 401, G Loss: 0.6935996413230896, D Loss: 1.3866698741912842\n",
            "Epoch 4, Batch 402, G Loss: 0.6936069130897522, D Loss: 1.3862533569335938\n",
            "Epoch 4, Batch 403, G Loss: 0.6936075091362, D Loss: 1.3861894607543945\n",
            "Epoch 4, Batch 404, G Loss: 0.6935887336730957, D Loss: 1.3861150741577148\n",
            "Epoch 4, Batch 405, G Loss: 0.693550705909729, D Loss: 1.3858642578125\n",
            "Epoch 4, Batch 406, G Loss: 0.6935603618621826, D Loss: 1.3858654499053955\n",
            "Epoch 4, Batch 407, G Loss: 0.6935439109802246, D Loss: 1.385585069656372\n",
            "Epoch 4, Batch 408, G Loss: 0.6935717463493347, D Loss: 1.3856556415557861\n",
            "Epoch 4, Batch 409, G Loss: 0.6935860514640808, D Loss: 1.3856436014175415\n",
            "Epoch 4, Batch 410, G Loss: 0.6935829520225525, D Loss: 1.3862513303756714\n",
            "Epoch 4, Batch 411, G Loss: 0.693592369556427, D Loss: 1.3861796855926514\n",
            "Epoch 4, Batch 412, G Loss: 0.6936274766921997, D Loss: 1.3857698440551758\n",
            "Epoch 4, Batch 413, G Loss: 0.6936264038085938, D Loss: 1.3857581615447998\n",
            "Epoch 4, Batch 414, G Loss: 0.6936278343200684, D Loss: 1.3862769603729248\n",
            "Epoch 4, Batch 415, G Loss: 0.6936414837837219, D Loss: 1.3865458965301514\n",
            "Epoch 4, Batch 416, G Loss: 0.6936665773391724, D Loss: 1.386040210723877\n",
            "Epoch 4, Batch 417, G Loss: 0.6936430931091309, D Loss: 1.385911464691162\n",
            "Epoch 4, Batch 418, G Loss: 0.6936375498771667, D Loss: 1.385910987854004\n",
            "Epoch 4, Batch 419, G Loss: 0.6936336159706116, D Loss: 1.385697841644287\n",
            "Epoch 4, Batch 420, G Loss: 0.6936599016189575, D Loss: 1.3858448266983032\n",
            "Epoch 4, Batch 421, G Loss: 0.6936612129211426, D Loss: 1.3856067657470703\n",
            "Epoch 4, Batch 422, G Loss: 0.6936550736427307, D Loss: 1.386305332183838\n",
            "Epoch 4, Batch 423, G Loss: 0.6936823129653931, D Loss: 1.386074185371399\n",
            "Epoch 4, Batch 424, G Loss: 0.69370037317276, D Loss: 1.3856620788574219\n",
            "Epoch 4, Batch 425, G Loss: 0.6937255859375, D Loss: 1.38593327999115\n",
            "Epoch 4, Batch 426, G Loss: 0.6937195062637329, D Loss: 1.3863694667816162\n",
            "Epoch 4, Batch 427, G Loss: 0.6937354803085327, D Loss: 1.3863861560821533\n",
            "Epoch 4, Batch 428, G Loss: 0.6937316656112671, D Loss: 1.3863080739974976\n",
            "Epoch 4, Batch 429, G Loss: 0.6937364935874939, D Loss: 1.3860399723052979\n",
            "Epoch 4, Batch 430, G Loss: 0.6937314867973328, D Loss: 1.3862180709838867\n",
            "Epoch 4, Batch 431, G Loss: 0.6937428712844849, D Loss: 1.3861104249954224\n",
            "Epoch 4, Batch 432, G Loss: 0.6937344074249268, D Loss: 1.386181354522705\n",
            "Epoch 4, Batch 433, G Loss: 0.6937185525894165, D Loss: 1.385826826095581\n",
            "Epoch 4, Batch 434, G Loss: 0.6937159299850464, D Loss: 1.3862614631652832\n",
            "Epoch 4, Batch 435, G Loss: 0.6937159299850464, D Loss: 1.3867319822311401\n",
            "Epoch 4, Batch 436, G Loss: 0.6937012672424316, D Loss: 1.3868821859359741\n",
            "Epoch 4, Batch 437, G Loss: 0.6936714053153992, D Loss: 1.3861593008041382\n",
            "Epoch 4, Batch 438, G Loss: 0.6936478614807129, D Loss: 1.3861079216003418\n",
            "Epoch 4, Batch 439, G Loss: 0.6936312317848206, D Loss: 1.3860797882080078\n",
            "Epoch 4, Batch 440, G Loss: 0.6936178207397461, D Loss: 1.3859305381774902\n",
            "Epoch 4, Batch 441, G Loss: 0.6935882568359375, D Loss: 1.3855441808700562\n",
            "Epoch 4, Batch 442, G Loss: 0.6936020255088806, D Loss: 1.3859233856201172\n",
            "Epoch 4, Batch 443, G Loss: 0.693609356880188, D Loss: 1.3858332633972168\n",
            "Epoch 4, Batch 444, G Loss: 0.6935867071151733, D Loss: 1.3865398168563843\n",
            "Epoch 4, Batch 445, G Loss: 0.6935669183731079, D Loss: 1.386122226715088\n",
            "Epoch 4, Batch 446, G Loss: 0.693572998046875, D Loss: 1.3853743076324463\n",
            "Epoch 4, Batch 447, G Loss: 0.693566620349884, D Loss: 1.3857284784317017\n",
            "Epoch 4, Batch 448, G Loss: 0.693574070930481, D Loss: 1.3860855102539062\n",
            "Epoch 4, Batch 449, G Loss: 0.6935908794403076, D Loss: 1.3859798908233643\n",
            "Epoch 4, Batch 450, G Loss: 0.6935852766036987, D Loss: 1.3862502574920654\n",
            "Epoch 4, Batch 451, G Loss: 0.693598210811615, D Loss: 1.3856257200241089\n",
            "Epoch 4, Batch 452, G Loss: 0.6936023235321045, D Loss: 1.3854047060012817\n",
            "Epoch 4, Batch 453, G Loss: 0.6935986876487732, D Loss: 1.385685920715332\n",
            "Epoch 4, Batch 454, G Loss: 0.6936169862747192, D Loss: 1.3857251405715942\n",
            "Epoch 4, Batch 455, G Loss: 0.6936637759208679, D Loss: 1.385667085647583\n",
            "Epoch 4, Batch 456, G Loss: 0.6936628222465515, D Loss: 1.386244773864746\n",
            "Epoch 4, Batch 457, G Loss: 0.6936689019203186, D Loss: 1.3866074085235596\n",
            "Epoch 4, Batch 458, G Loss: 0.6936887502670288, D Loss: 1.3853223323822021\n",
            "Epoch 4, Batch 459, G Loss: 0.6936934590339661, D Loss: 1.3849830627441406\n",
            "Epoch 4, Batch 460, G Loss: 0.6937155723571777, D Loss: 1.3859138488769531\n",
            "Epoch 4, Batch 461, G Loss: 0.6937413811683655, D Loss: 1.386104702949524\n",
            "Epoch 4, Batch 462, G Loss: 0.6937602162361145, D Loss: 1.3858119249343872\n",
            "Epoch 4, Batch 463, G Loss: 0.6937782764434814, D Loss: 1.3860877752304077\n",
            "Epoch 4, Batch 464, G Loss: 0.6937962770462036, D Loss: 1.3875123262405396\n",
            "Epoch 4, Batch 465, G Loss: 0.6937857866287231, D Loss: 1.3881781101226807\n",
            "Epoch 4, Batch 466, G Loss: 0.6937431693077087, D Loss: 1.3857488632202148\n",
            "Epoch 4, Batch 467, G Loss: 0.6937209367752075, D Loss: 1.386225700378418\n",
            "Epoch 4, Batch 468, G Loss: 0.6936874389648438, D Loss: 1.386595606803894\n",
            "Epoch 4, Batch 469, G Loss: 0.6936601400375366, D Loss: 1.3853874206542969\n",
            "Epoch 5, Batch 1, G Loss: 0.6936547160148621, D Loss: 1.3859407901763916\n",
            "Epoch 5, Batch 2, G Loss: 0.6936278343200684, D Loss: 1.3860208988189697\n",
            "Epoch 5, Batch 3, G Loss: 0.6936123967170715, D Loss: 1.3858213424682617\n",
            "Epoch 5, Batch 4, G Loss: 0.6936355829238892, D Loss: 1.385735273361206\n",
            "Epoch 5, Batch 5, G Loss: 0.6936246752738953, D Loss: 1.3857907056808472\n",
            "Epoch 5, Batch 6, G Loss: 0.6936334371566772, D Loss: 1.386260986328125\n",
            "Epoch 5, Batch 7, G Loss: 0.6936196088790894, D Loss: 1.3862130641937256\n",
            "Epoch 5, Batch 8, G Loss: 0.6935977339744568, D Loss: 1.3859453201293945\n",
            "Epoch 5, Batch 9, G Loss: 0.6936097145080566, D Loss: 1.3854782581329346\n",
            "Epoch 5, Batch 10, G Loss: 0.6936063170433044, D Loss: 1.386746883392334\n",
            "Epoch 5, Batch 11, G Loss: 0.693611741065979, D Loss: 1.3870978355407715\n",
            "Epoch 5, Batch 12, G Loss: 0.6935628652572632, D Loss: 1.3858871459960938\n",
            "Epoch 5, Batch 13, G Loss: 0.6935446858406067, D Loss: 1.3857818841934204\n",
            "Epoch 5, Batch 14, G Loss: 0.6935370564460754, D Loss: 1.3857691287994385\n",
            "Epoch 5, Batch 15, G Loss: 0.6935150623321533, D Loss: 1.3859610557556152\n",
            "Epoch 5, Batch 16, G Loss: 0.6935083270072937, D Loss: 1.386662483215332\n",
            "Epoch 5, Batch 17, G Loss: 0.6934900283813477, D Loss: 1.3862406015396118\n",
            "Epoch 5, Batch 18, G Loss: 0.6934727430343628, D Loss: 1.3861695528030396\n",
            "Epoch 5, Batch 19, G Loss: 0.6934633851051331, D Loss: 1.3859498500823975\n",
            "Epoch 5, Batch 20, G Loss: 0.6934448480606079, D Loss: 1.3861489295959473\n",
            "Epoch 5, Batch 21, G Loss: 0.6934274435043335, D Loss: 1.3862844705581665\n",
            "Epoch 5, Batch 22, G Loss: 0.6934067010879517, D Loss: 1.3859984874725342\n",
            "Epoch 5, Batch 23, G Loss: 0.6934073567390442, D Loss: 1.386286973953247\n",
            "Epoch 5, Batch 24, G Loss: 0.6933863162994385, D Loss: 1.3857581615447998\n",
            "Epoch 5, Batch 25, G Loss: 0.6933820843696594, D Loss: 1.385634422302246\n",
            "Epoch 5, Batch 26, G Loss: 0.6933871507644653, D Loss: 1.3856101036071777\n",
            "Epoch 5, Batch 27, G Loss: 0.6933813095092773, D Loss: 1.3857872486114502\n",
            "Epoch 5, Batch 28, G Loss: 0.693398118019104, D Loss: 1.3857662677764893\n",
            "Epoch 5, Batch 29, G Loss: 0.6934181451797485, D Loss: 1.3865070343017578\n",
            "Epoch 5, Batch 30, G Loss: 0.6934182047843933, D Loss: 1.3859012126922607\n",
            "Epoch 5, Batch 31, G Loss: 0.6934216022491455, D Loss: 1.3858165740966797\n",
            "Epoch 5, Batch 32, G Loss: 0.6934201121330261, D Loss: 1.3860796689987183\n",
            "Epoch 5, Batch 33, G Loss: 0.6934226155281067, D Loss: 1.3858131170272827\n",
            "Epoch 5, Batch 34, G Loss: 0.6934440732002258, D Loss: 1.3860197067260742\n",
            "Epoch 5, Batch 35, G Loss: 0.6934356689453125, D Loss: 1.385717511177063\n",
            "Epoch 5, Batch 36, G Loss: 0.6934422254562378, D Loss: 1.3863983154296875\n",
            "Epoch 5, Batch 37, G Loss: 0.6934593319892883, D Loss: 1.3867361545562744\n",
            "Epoch 5, Batch 38, G Loss: 0.6934476494789124, D Loss: 1.3863389492034912\n",
            "Epoch 5, Batch 39, G Loss: 0.6934367418289185, D Loss: 1.3860218524932861\n",
            "Epoch 5, Batch 40, G Loss: 0.6934453248977661, D Loss: 1.386224389076233\n",
            "Epoch 5, Batch 41, G Loss: 0.6934316158294678, D Loss: 1.3862985372543335\n",
            "Epoch 5, Batch 42, G Loss: 0.6934001445770264, D Loss: 1.386502981185913\n",
            "Epoch 5, Batch 43, G Loss: 0.6933960914611816, D Loss: 1.386691927909851\n",
            "Epoch 5, Batch 44, G Loss: 0.6933738589286804, D Loss: 1.3863266706466675\n",
            "Epoch 5, Batch 45, G Loss: 0.6933484077453613, D Loss: 1.3862190246582031\n",
            "Epoch 5, Batch 46, G Loss: 0.6933173537254333, D Loss: 1.3858393430709839\n",
            "Epoch 5, Batch 47, G Loss: 0.693312406539917, D Loss: 1.3865305185317993\n",
            "Epoch 5, Batch 48, G Loss: 0.6932913064956665, D Loss: 1.387101173400879\n",
            "Epoch 5, Batch 49, G Loss: 0.6932449340820312, D Loss: 1.386651873588562\n",
            "Epoch 5, Batch 50, G Loss: 0.6932139992713928, D Loss: 1.386490821838379\n",
            "Epoch 5, Batch 51, G Loss: 0.6931722164154053, D Loss: 1.3861315250396729\n",
            "Epoch 5, Batch 52, G Loss: 0.693126916885376, D Loss: 1.3859362602233887\n",
            "Epoch 5, Batch 53, G Loss: 0.693102240562439, D Loss: 1.3852145671844482\n",
            "Epoch 5, Batch 54, G Loss: 0.693098247051239, D Loss: 1.385662317276001\n",
            "Epoch 5, Batch 55, G Loss: 0.6930856704711914, D Loss: 1.3862882852554321\n",
            "Epoch 5, Batch 56, G Loss: 0.6930957436561584, D Loss: 1.3855305910110474\n",
            "Epoch 5, Batch 57, G Loss: 0.6930965781211853, D Loss: 1.3854892253875732\n",
            "Epoch 5, Batch 58, G Loss: 0.6931014060974121, D Loss: 1.3850798606872559\n",
            "Epoch 5, Batch 59, G Loss: 0.6931206583976746, D Loss: 1.3855255842208862\n",
            "Epoch 5, Batch 60, G Loss: 0.6931493282318115, D Loss: 1.385735034942627\n",
            "Epoch 5, Batch 61, G Loss: 0.6931878924369812, D Loss: 1.3858805894851685\n",
            "Epoch 5, Batch 62, G Loss: 0.6932226419448853, D Loss: 1.386491060256958\n",
            "Epoch 5, Batch 63, G Loss: 0.693242073059082, D Loss: 1.3865517377853394\n",
            "Epoch 5, Batch 64, G Loss: 0.6932276487350464, D Loss: 1.3858587741851807\n",
            "Epoch 5, Batch 65, G Loss: 0.6932629346847534, D Loss: 1.385986328125\n",
            "Epoch 5, Batch 66, G Loss: 0.6932610273361206, D Loss: 1.3860669136047363\n",
            "Epoch 5, Batch 67, G Loss: 0.6932812333106995, D Loss: 1.3871548175811768\n",
            "Epoch 5, Batch 68, G Loss: 0.6932736039161682, D Loss: 1.3868385553359985\n",
            "Epoch 5, Batch 69, G Loss: 0.6932700276374817, D Loss: 1.3861403465270996\n",
            "Epoch 5, Batch 70, G Loss: 0.693261444568634, D Loss: 1.3859305381774902\n",
            "Epoch 5, Batch 71, G Loss: 0.6932276487350464, D Loss: 1.3859397172927856\n",
            "Epoch 5, Batch 72, G Loss: 0.6932322978973389, D Loss: 1.385596513748169\n",
            "Epoch 5, Batch 73, G Loss: 0.6932429671287537, D Loss: 1.3856500387191772\n",
            "Epoch 5, Batch 74, G Loss: 0.6932623982429504, D Loss: 1.386277198791504\n",
            "Epoch 5, Batch 75, G Loss: 0.6932612657546997, D Loss: 1.3860701322555542\n",
            "Epoch 5, Batch 76, G Loss: 0.6932637691497803, D Loss: 1.3861591815948486\n",
            "Epoch 5, Batch 77, G Loss: 0.6932536363601685, D Loss: 1.3860819339752197\n",
            "Epoch 5, Batch 78, G Loss: 0.6932405829429626, D Loss: 1.3859224319458008\n",
            "Epoch 5, Batch 79, G Loss: 0.6932629346847534, D Loss: 1.386136531829834\n",
            "Epoch 5, Batch 80, G Loss: 0.6932711601257324, D Loss: 1.386476755142212\n",
            "Epoch 5, Batch 81, G Loss: 0.6932712197303772, D Loss: 1.3872268199920654\n",
            "Epoch 5, Batch 82, G Loss: 0.6932385563850403, D Loss: 1.386646032333374\n",
            "Epoch 5, Batch 83, G Loss: 0.693234920501709, D Loss: 1.3862383365631104\n",
            "Epoch 5, Batch 84, G Loss: 0.6932029128074646, D Loss: 1.385711908340454\n",
            "Epoch 5, Batch 85, G Loss: 0.6931907534599304, D Loss: 1.386254072189331\n",
            "Epoch 5, Batch 86, G Loss: 0.6931770443916321, D Loss: 1.3863565921783447\n",
            "Epoch 5, Batch 87, G Loss: 0.6931662559509277, D Loss: 1.3863219022750854\n",
            "Epoch 5, Batch 88, G Loss: 0.693149983882904, D Loss: 1.3861122131347656\n",
            "Epoch 5, Batch 89, G Loss: 0.6931379437446594, D Loss: 1.3859835863113403\n",
            "Epoch 5, Batch 90, G Loss: 0.6931247115135193, D Loss: 1.3859374523162842\n",
            "Epoch 5, Batch 91, G Loss: 0.6931014060974121, D Loss: 1.3859367370605469\n",
            "Epoch 5, Batch 92, G Loss: 0.6931058168411255, D Loss: 1.3859350681304932\n",
            "Epoch 5, Batch 93, G Loss: 0.6931108832359314, D Loss: 1.3859586715698242\n",
            "Epoch 5, Batch 94, G Loss: 0.6931123733520508, D Loss: 1.386218547821045\n",
            "Epoch 5, Batch 95, G Loss: 0.6931037306785583, D Loss: 1.386230707168579\n",
            "Epoch 5, Batch 96, G Loss: 0.6930964589118958, D Loss: 1.3864285945892334\n",
            "Epoch 5, Batch 97, G Loss: 0.6930851936340332, D Loss: 1.3862431049346924\n",
            "Epoch 5, Batch 98, G Loss: 0.6930812001228333, D Loss: 1.386425256729126\n",
            "Epoch 5, Batch 99, G Loss: 0.6930674314498901, D Loss: 1.3864083290100098\n",
            "Epoch 5, Batch 100, G Loss: 0.6930412650108337, D Loss: 1.3859472274780273\n",
            "Epoch 5, Batch 101, G Loss: 0.6930290460586548, D Loss: 1.3857741355895996\n",
            "Epoch 5, Batch 102, G Loss: 0.6930313110351562, D Loss: 1.386733055114746\n",
            "Epoch 5, Batch 103, G Loss: 0.6930273175239563, D Loss: 1.386756181716919\n",
            "Epoch 5, Batch 104, G Loss: 0.6930060982704163, D Loss: 1.386418104171753\n",
            "Epoch 5, Batch 105, G Loss: 0.6929783821105957, D Loss: 1.386290431022644\n",
            "Epoch 5, Batch 106, G Loss: 0.6929646730422974, D Loss: 1.386629581451416\n",
            "Epoch 5, Batch 107, G Loss: 0.6929458975791931, D Loss: 1.3859829902648926\n",
            "Epoch 5, Batch 108, G Loss: 0.6929166913032532, D Loss: 1.385512113571167\n",
            "Epoch 5, Batch 109, G Loss: 0.6929201483726501, D Loss: 1.3860034942626953\n",
            "Epoch 5, Batch 110, G Loss: 0.6929099559783936, D Loss: 1.3862075805664062\n",
            "Epoch 5, Batch 111, G Loss: 0.6929110884666443, D Loss: 1.3867244720458984\n",
            "Epoch 5, Batch 112, G Loss: 0.6928823590278625, D Loss: 1.386516809463501\n",
            "Epoch 5, Batch 113, G Loss: 0.6928642988204956, D Loss: 1.3861103057861328\n",
            "Epoch 5, Batch 114, G Loss: 0.6928564310073853, D Loss: 1.385676622390747\n",
            "Epoch 5, Batch 115, G Loss: 0.6928567886352539, D Loss: 1.3860445022583008\n",
            "Epoch 5, Batch 116, G Loss: 0.6928491592407227, D Loss: 1.3859434127807617\n",
            "Epoch 5, Batch 117, G Loss: 0.6928407549858093, D Loss: 1.3859272003173828\n",
            "Epoch 5, Batch 118, G Loss: 0.6928598880767822, D Loss: 1.385788083076477\n",
            "Epoch 5, Batch 119, G Loss: 0.6928552389144897, D Loss: 1.385663390159607\n",
            "Epoch 5, Batch 120, G Loss: 0.6928814649581909, D Loss: 1.385704517364502\n",
            "Epoch 5, Batch 121, G Loss: 0.6929011344909668, D Loss: 1.3855834007263184\n",
            "Epoch 5, Batch 122, G Loss: 0.6929337382316589, D Loss: 1.3858376741409302\n",
            "Epoch 5, Batch 123, G Loss: 0.6929692625999451, D Loss: 1.3859093189239502\n",
            "Epoch 5, Batch 124, G Loss: 0.6930010914802551, D Loss: 1.3858301639556885\n",
            "Epoch 5, Batch 125, G Loss: 0.6930305361747742, D Loss: 1.386047124862671\n",
            "Epoch 5, Batch 126, G Loss: 0.6930618286132812, D Loss: 1.3863129615783691\n",
            "Epoch 5, Batch 127, G Loss: 0.6930931806564331, D Loss: 1.3862767219543457\n",
            "Epoch 5, Batch 128, G Loss: 0.6931174993515015, D Loss: 1.3860069513320923\n",
            "Epoch 5, Batch 129, G Loss: 0.6931266784667969, D Loss: 1.3859175443649292\n",
            "Epoch 5, Batch 130, G Loss: 0.693150520324707, D Loss: 1.3858814239501953\n",
            "Epoch 5, Batch 131, G Loss: 0.6931757926940918, D Loss: 1.3857669830322266\n",
            "Epoch 5, Batch 132, G Loss: 0.6932061910629272, D Loss: 1.3857907056808472\n",
            "Epoch 5, Batch 133, G Loss: 0.6932250261306763, D Loss: 1.3858482837677002\n",
            "Epoch 5, Batch 134, G Loss: 0.6932672262191772, D Loss: 1.3859597444534302\n",
            "Epoch 5, Batch 135, G Loss: 0.6933031678199768, D Loss: 1.3862829208374023\n",
            "Epoch 5, Batch 136, G Loss: 0.6933251023292542, D Loss: 1.3861777782440186\n",
            "Epoch 5, Batch 137, G Loss: 0.6933338642120361, D Loss: 1.3858311176300049\n",
            "Epoch 5, Batch 138, G Loss: 0.6933619379997253, D Loss: 1.3855276107788086\n",
            "Epoch 5, Batch 139, G Loss: 0.6933888792991638, D Loss: 1.3855834007263184\n",
            "Epoch 5, Batch 140, G Loss: 0.6934118270874023, D Loss: 1.385481834411621\n",
            "Epoch 5, Batch 141, G Loss: 0.6934491395950317, D Loss: 1.3859894275665283\n",
            "Epoch 5, Batch 142, G Loss: 0.6934829950332642, D Loss: 1.3858976364135742\n",
            "Epoch 5, Batch 143, G Loss: 0.6935269236564636, D Loss: 1.3860031366348267\n",
            "Epoch 5, Batch 144, G Loss: 0.6935455203056335, D Loss: 1.385749101638794\n",
            "Epoch 5, Batch 145, G Loss: 0.6935766339302063, D Loss: 1.3858438730239868\n",
            "Epoch 5, Batch 146, G Loss: 0.6936101913452148, D Loss: 1.3860054016113281\n",
            "Epoch 5, Batch 147, G Loss: 0.6936342716217041, D Loss: 1.3858296871185303\n",
            "Epoch 5, Batch 148, G Loss: 0.6936561465263367, D Loss: 1.3859033584594727\n",
            "Epoch 5, Batch 149, G Loss: 0.6936945915222168, D Loss: 1.3856157064437866\n",
            "Epoch 5, Batch 150, G Loss: 0.6937158703804016, D Loss: 1.385636806488037\n",
            "Epoch 5, Batch 151, G Loss: 0.6937377452850342, D Loss: 1.3866055011749268\n",
            "Epoch 5, Batch 152, G Loss: 0.6937612295150757, D Loss: 1.3867087364196777\n",
            "Epoch 5, Batch 153, G Loss: 0.6937832236289978, D Loss: 1.3866827487945557\n",
            "Epoch 5, Batch 154, G Loss: 0.6937573552131653, D Loss: 1.3865172863006592\n",
            "Epoch 5, Batch 155, G Loss: 0.6937370896339417, D Loss: 1.386597990989685\n",
            "Epoch 5, Batch 156, G Loss: 0.6937190294265747, D Loss: 1.3864774703979492\n",
            "Epoch 5, Batch 157, G Loss: 0.6936863660812378, D Loss: 1.3862890005111694\n",
            "Epoch 5, Batch 158, G Loss: 0.6936653256416321, D Loss: 1.3861985206604004\n",
            "Epoch 5, Batch 159, G Loss: 0.693629264831543, D Loss: 1.385829210281372\n",
            "Epoch 5, Batch 160, G Loss: 0.6936097145080566, D Loss: 1.386063814163208\n",
            "Epoch 5, Batch 161, G Loss: 0.6935901045799255, D Loss: 1.3862218856811523\n",
            "Epoch 5, Batch 162, G Loss: 0.6935720443725586, D Loss: 1.3863422870635986\n",
            "Epoch 5, Batch 163, G Loss: 0.6935445070266724, D Loss: 1.3859814405441284\n",
            "Epoch 5, Batch 164, G Loss: 0.6935181617736816, D Loss: 1.3862066268920898\n",
            "Epoch 5, Batch 165, G Loss: 0.6935114860534668, D Loss: 1.3856723308563232\n",
            "Epoch 5, Batch 166, G Loss: 0.6935004591941833, D Loss: 1.385899543762207\n",
            "Epoch 5, Batch 167, G Loss: 0.693468451499939, D Loss: 1.3861408233642578\n",
            "Epoch 5, Batch 168, G Loss: 0.6934758424758911, D Loss: 1.3862519264221191\n",
            "Epoch 5, Batch 169, G Loss: 0.6934475302696228, D Loss: 1.3859474658966064\n",
            "Epoch 5, Batch 170, G Loss: 0.693444550037384, D Loss: 1.385432481765747\n",
            "Epoch 5, Batch 171, G Loss: 0.6934438943862915, D Loss: 1.3868672847747803\n",
            "Epoch 5, Batch 172, G Loss: 0.6934308409690857, D Loss: 1.3868229389190674\n",
            "Epoch 5, Batch 173, G Loss: 0.6933906078338623, D Loss: 1.3860352039337158\n",
            "Epoch 5, Batch 174, G Loss: 0.6933908462524414, D Loss: 1.3860068321228027\n",
            "Epoch 5, Batch 175, G Loss: 0.6933690309524536, D Loss: 1.385892391204834\n",
            "Epoch 5, Batch 176, G Loss: 0.6933515071868896, D Loss: 1.3859249353408813\n",
            "Epoch 5, Batch 177, G Loss: 0.6933398842811584, D Loss: 1.38627290725708\n",
            "Epoch 5, Batch 178, G Loss: 0.6933366656303406, D Loss: 1.386857509613037\n",
            "Epoch 5, Batch 179, G Loss: 0.6933209300041199, D Loss: 1.3864086866378784\n",
            "Epoch 5, Batch 180, G Loss: 0.6932836771011353, D Loss: 1.3865635395050049\n",
            "Epoch 5, Batch 181, G Loss: 0.6932530999183655, D Loss: 1.3866255283355713\n",
            "Epoch 5, Batch 182, G Loss: 0.693211555480957, D Loss: 1.3861485719680786\n",
            "Epoch 5, Batch 183, G Loss: 0.6931840777397156, D Loss: 1.3862757682800293\n",
            "Epoch 5, Batch 184, G Loss: 0.6931373476982117, D Loss: 1.3859635591506958\n",
            "Epoch 5, Batch 185, G Loss: 0.693121612071991, D Loss: 1.3856885433197021\n",
            "Epoch 5, Batch 186, G Loss: 0.6931123733520508, D Loss: 1.3859052658081055\n",
            "Epoch 5, Batch 187, G Loss: 0.6930878162384033, D Loss: 1.3860507011413574\n",
            "Epoch 5, Batch 188, G Loss: 0.6930856704711914, D Loss: 1.3861162662506104\n",
            "Epoch 5, Batch 189, G Loss: 0.6930696368217468, D Loss: 1.386035680770874\n",
            "Epoch 5, Batch 190, G Loss: 0.6930762529373169, D Loss: 1.3859829902648926\n",
            "Epoch 5, Batch 191, G Loss: 0.6930615305900574, D Loss: 1.3860411643981934\n",
            "Epoch 5, Batch 192, G Loss: 0.6930745840072632, D Loss: 1.3860418796539307\n",
            "Epoch 5, Batch 193, G Loss: 0.6930571794509888, D Loss: 1.386352300643921\n",
            "Epoch 5, Batch 194, G Loss: 0.6930564045906067, D Loss: 1.3860678672790527\n",
            "Epoch 5, Batch 195, G Loss: 0.6930534243583679, D Loss: 1.3855477571487427\n",
            "Epoch 5, Batch 196, G Loss: 0.6930707097053528, D Loss: 1.3857996463775635\n",
            "Epoch 5, Batch 197, G Loss: 0.6930798888206482, D Loss: 1.3858884572982788\n",
            "Epoch 5, Batch 198, G Loss: 0.6930985450744629, D Loss: 1.3865103721618652\n",
            "Epoch 5, Batch 199, G Loss: 0.6931020617485046, D Loss: 1.3867027759552002\n",
            "Epoch 5, Batch 200, G Loss: 0.6930935978889465, D Loss: 1.38612699508667\n",
            "Epoch 5, Batch 201, G Loss: 0.6930823922157288, D Loss: 1.386155605316162\n",
            "Epoch 5, Batch 202, G Loss: 0.693083643913269, D Loss: 1.3855090141296387\n",
            "Epoch 5, Batch 203, G Loss: 0.6930983662605286, D Loss: 1.3853435516357422\n",
            "Epoch 5, Batch 204, G Loss: 0.6931136846542358, D Loss: 1.3862276077270508\n",
            "Epoch 5, Batch 205, G Loss: 0.6931353211402893, D Loss: 1.3859611749649048\n",
            "Epoch 5, Batch 206, G Loss: 0.6931454539299011, D Loss: 1.3857007026672363\n",
            "Epoch 5, Batch 207, G Loss: 0.693169355392456, D Loss: 1.385641098022461\n",
            "Epoch 5, Batch 208, G Loss: 0.6931933760643005, D Loss: 1.3854385614395142\n",
            "Epoch 5, Batch 209, G Loss: 0.6932312846183777, D Loss: 1.3860794305801392\n",
            "Epoch 5, Batch 210, G Loss: 0.6932892799377441, D Loss: 1.3862223625183105\n",
            "Epoch 5, Batch 211, G Loss: 0.6932795643806458, D Loss: 1.3857812881469727\n",
            "Epoch 5, Batch 212, G Loss: 0.6932988166809082, D Loss: 1.386237382888794\n",
            "Epoch 5, Batch 213, G Loss: 0.6933204531669617, D Loss: 1.386387825012207\n",
            "Epoch 5, Batch 214, G Loss: 0.69334876537323, D Loss: 1.3861701488494873\n",
            "Epoch 5, Batch 215, G Loss: 0.6933528184890747, D Loss: 1.3859611749649048\n",
            "Epoch 5, Batch 216, G Loss: 0.693361759185791, D Loss: 1.3858253955841064\n",
            "Epoch 5, Batch 217, G Loss: 0.6933798789978027, D Loss: 1.3856191635131836\n",
            "Epoch 5, Batch 218, G Loss: 0.6933925151824951, D Loss: 1.3856630325317383\n",
            "Epoch 5, Batch 219, G Loss: 0.6934300661087036, D Loss: 1.3864388465881348\n",
            "Epoch 5, Batch 220, G Loss: 0.6934399008750916, D Loss: 1.386420488357544\n",
            "Epoch 5, Batch 221, G Loss: 0.6934483051300049, D Loss: 1.3866111040115356\n",
            "Epoch 5, Batch 222, G Loss: 0.6934553384780884, D Loss: 1.386528730392456\n",
            "Epoch 5, Batch 223, G Loss: 0.6934425830841064, D Loss: 1.3865034580230713\n",
            "Epoch 5, Batch 224, G Loss: 0.6934198141098022, D Loss: 1.38627290725708\n",
            "Epoch 5, Batch 225, G Loss: 0.6933857798576355, D Loss: 1.3860797882080078\n",
            "Epoch 5, Batch 226, G Loss: 0.6933847665786743, D Loss: 1.3862712383270264\n",
            "Epoch 5, Batch 227, G Loss: 0.6933586001396179, D Loss: 1.38629150390625\n",
            "Epoch 5, Batch 228, G Loss: 0.6933251023292542, D Loss: 1.3855712413787842\n",
            "Epoch 5, Batch 229, G Loss: 0.6933172345161438, D Loss: 1.3862500190734863\n",
            "Epoch 5, Batch 230, G Loss: 0.6933176517486572, D Loss: 1.3867055177688599\n",
            "Epoch 5, Batch 231, G Loss: 0.6932985186576843, D Loss: 1.3861687183380127\n",
            "Epoch 5, Batch 232, G Loss: 0.6932711005210876, D Loss: 1.3857338428497314\n",
            "Epoch 5, Batch 233, G Loss: 0.6932589411735535, D Loss: 1.3854889869689941\n",
            "Epoch 5, Batch 234, G Loss: 0.6932640671730042, D Loss: 1.3855173587799072\n",
            "Epoch 5, Batch 235, G Loss: 0.6932824850082397, D Loss: 1.385608196258545\n",
            "Epoch 5, Batch 236, G Loss: 0.6932801008224487, D Loss: 1.385880708694458\n",
            "Epoch 5, Batch 237, G Loss: 0.6933076977729797, D Loss: 1.3858153820037842\n",
            "Epoch 5, Batch 238, G Loss: 0.6933308839797974, D Loss: 1.38624906539917\n",
            "Epoch 5, Batch 239, G Loss: 0.6933627724647522, D Loss: 1.3860363960266113\n",
            "Epoch 5, Batch 240, G Loss: 0.6933581233024597, D Loss: 1.385162115097046\n",
            "Epoch 5, Batch 241, G Loss: 0.6933841109275818, D Loss: 1.3856806755065918\n",
            "Epoch 5, Batch 242, G Loss: 0.6934216618537903, D Loss: 1.385804533958435\n",
            "Epoch 5, Batch 243, G Loss: 0.6934425234794617, D Loss: 1.3857616186141968\n",
            "Epoch 5, Batch 244, G Loss: 0.6934852004051208, D Loss: 1.3857008218765259\n",
            "Epoch 5, Batch 245, G Loss: 0.6935107111930847, D Loss: 1.3857288360595703\n",
            "Epoch 5, Batch 246, G Loss: 0.6935487985610962, D Loss: 1.3856737613677979\n",
            "Epoch 5, Batch 247, G Loss: 0.6935704350471497, D Loss: 1.3854188919067383\n",
            "Epoch 5, Batch 248, G Loss: 0.693622350692749, D Loss: 1.385490894317627\n",
            "Epoch 5, Batch 249, G Loss: 0.6936727166175842, D Loss: 1.385773777961731\n",
            "Epoch 5, Batch 250, G Loss: 0.693703293800354, D Loss: 1.38584566116333\n",
            "Epoch 5, Batch 251, G Loss: 0.6937558054924011, D Loss: 1.3862855434417725\n",
            "Epoch 5, Batch 252, G Loss: 0.6937968730926514, D Loss: 1.3859679698944092\n",
            "Epoch 5, Batch 253, G Loss: 0.6938163042068481, D Loss: 1.3864073753356934\n",
            "Epoch 5, Batch 254, G Loss: 0.6938402056694031, D Loss: 1.3866013288497925\n",
            "Epoch 5, Batch 255, G Loss: 0.6938318014144897, D Loss: 1.3858823776245117\n",
            "Epoch 5, Batch 256, G Loss: 0.6938470602035522, D Loss: 1.3855572938919067\n",
            "Epoch 5, Batch 257, G Loss: 0.6938542127609253, D Loss: 1.3857638835906982\n",
            "Epoch 5, Batch 258, G Loss: 0.6938549876213074, D Loss: 1.386319875717163\n",
            "Epoch 5, Batch 259, G Loss: 0.6938766241073608, D Loss: 1.3862979412078857\n",
            "Epoch 5, Batch 260, G Loss: 0.6938756108283997, D Loss: 1.3862143754959106\n",
            "Epoch 5, Batch 261, G Loss: 0.6938700079917908, D Loss: 1.3859390020370483\n",
            "Epoch 5, Batch 262, G Loss: 0.6938602924346924, D Loss: 1.385589361190796\n",
            "Epoch 5, Batch 263, G Loss: 0.693861722946167, D Loss: 1.3853304386138916\n",
            "Epoch 5, Batch 264, G Loss: 0.6938802003860474, D Loss: 1.3859819173812866\n",
            "Epoch 5, Batch 265, G Loss: 0.6938848495483398, D Loss: 1.3864175081253052\n",
            "Epoch 5, Batch 266, G Loss: 0.6938862800598145, D Loss: 1.3856608867645264\n",
            "Epoch 5, Batch 267, G Loss: 0.6938885450363159, D Loss: 1.385993242263794\n",
            "Epoch 5, Batch 268, G Loss: 0.6938862204551697, D Loss: 1.385688304901123\n",
            "Epoch 5, Batch 269, G Loss: 0.6938976645469666, D Loss: 1.3862330913543701\n",
            "Epoch 5, Batch 270, G Loss: 0.6938992142677307, D Loss: 1.3861689567565918\n",
            "Epoch 5, Batch 271, G Loss: 0.6938791275024414, D Loss: 1.3861290216445923\n",
            "Epoch 5, Batch 272, G Loss: 0.6938760876655579, D Loss: 1.3861231803894043\n",
            "Epoch 5, Batch 273, G Loss: 0.6938581466674805, D Loss: 1.3860726356506348\n",
            "Epoch 5, Batch 274, G Loss: 0.693858802318573, D Loss: 1.386277198791504\n",
            "Epoch 5, Batch 275, G Loss: 0.6938286423683167, D Loss: 1.385788917541504\n",
            "Epoch 5, Batch 276, G Loss: 0.6938217878341675, D Loss: 1.3856418132781982\n",
            "Epoch 5, Batch 277, G Loss: 0.6938089728355408, D Loss: 1.3862380981445312\n",
            "Epoch 5, Batch 278, G Loss: 0.6938132643699646, D Loss: 1.3861372470855713\n",
            "Epoch 5, Batch 279, G Loss: 0.6937864422798157, D Loss: 1.3859434127807617\n",
            "Epoch 5, Batch 280, G Loss: 0.693781316280365, D Loss: 1.385919451713562\n",
            "Epoch 5, Batch 281, G Loss: 0.6937639117240906, D Loss: 1.3854520320892334\n",
            "Epoch 5, Batch 282, G Loss: 0.693764328956604, D Loss: 1.385103464126587\n",
            "Epoch 5, Batch 283, G Loss: 0.6937733888626099, D Loss: 1.3867682218551636\n",
            "Epoch 5, Batch 284, G Loss: 0.6937807202339172, D Loss: 1.3867576122283936\n",
            "Epoch 5, Batch 285, G Loss: 0.6937670111656189, D Loss: 1.3858880996704102\n",
            "Epoch 5, Batch 286, G Loss: 0.6937386393547058, D Loss: 1.386470079421997\n",
            "Epoch 5, Batch 287, G Loss: 0.6937215328216553, D Loss: 1.3870893716812134\n",
            "Epoch 5, Batch 288, G Loss: 0.6936890482902527, D Loss: 1.3866539001464844\n",
            "Epoch 5, Batch 289, G Loss: 0.6936433911323547, D Loss: 1.3860825300216675\n",
            "Epoch 5, Batch 290, G Loss: 0.693599283695221, D Loss: 1.3862831592559814\n",
            "Epoch 5, Batch 291, G Loss: 0.6935633420944214, D Loss: 1.3866971731185913\n",
            "Epoch 5, Batch 292, G Loss: 0.6935117244720459, D Loss: 1.3864734172821045\n",
            "Epoch 5, Batch 293, G Loss: 0.6934570670127869, D Loss: 1.3859596252441406\n",
            "Epoch 5, Batch 294, G Loss: 0.6934369206428528, D Loss: 1.3855702877044678\n",
            "Epoch 5, Batch 295, G Loss: 0.6934008002281189, D Loss: 1.385911226272583\n",
            "Epoch 5, Batch 296, G Loss: 0.6933593153953552, D Loss: 1.3863310813903809\n",
            "Epoch 5, Batch 297, G Loss: 0.6933425664901733, D Loss: 1.3858627080917358\n",
            "Epoch 5, Batch 298, G Loss: 0.6933229565620422, D Loss: 1.3858312368392944\n",
            "Epoch 5, Batch 299, G Loss: 0.6932998299598694, D Loss: 1.3861582279205322\n",
            "Epoch 5, Batch 300, G Loss: 0.6932902932167053, D Loss: 1.3857715129852295\n",
            "Epoch 5, Batch 301, G Loss: 0.6932712197303772, D Loss: 1.3856546878814697\n",
            "Epoch 5, Batch 302, G Loss: 0.6932687759399414, D Loss: 1.3862991333007812\n",
            "Epoch 5, Batch 303, G Loss: 0.6932626962661743, D Loss: 1.3859202861785889\n",
            "Epoch 5, Batch 304, G Loss: 0.6932525634765625, D Loss: 1.3861117362976074\n",
            "Epoch 5, Batch 305, G Loss: 0.6932451128959656, D Loss: 1.3857147693634033\n",
            "Epoch 5, Batch 306, G Loss: 0.6932516098022461, D Loss: 1.3863657712936401\n",
            "Epoch 5, Batch 307, G Loss: 0.6932564377784729, D Loss: 1.3864264488220215\n",
            "Epoch 5, Batch 308, G Loss: 0.6932196617126465, D Loss: 1.3862706422805786\n",
            "Epoch 5, Batch 309, G Loss: 0.6932068467140198, D Loss: 1.3862636089324951\n",
            "Epoch 5, Batch 310, G Loss: 0.693193256855011, D Loss: 1.3862971067428589\n",
            "Epoch 5, Batch 311, G Loss: 0.6931691765785217, D Loss: 1.3854389190673828\n",
            "Epoch 5, Batch 312, G Loss: 0.6931760907173157, D Loss: 1.3855878114700317\n",
            "Epoch 5, Batch 313, G Loss: 0.6931833624839783, D Loss: 1.3860468864440918\n",
            "Epoch 5, Batch 314, G Loss: 0.6931837201118469, D Loss: 1.3861558437347412\n",
            "Epoch 5, Batch 315, G Loss: 0.693196177482605, D Loss: 1.3858147859573364\n",
            "Epoch 5, Batch 316, G Loss: 0.6931893229484558, D Loss: 1.3860667943954468\n",
            "Epoch 5, Batch 317, G Loss: 0.693211019039154, D Loss: 1.386542797088623\n",
            "Epoch 5, Batch 318, G Loss: 0.6932016611099243, D Loss: 1.385960340499878\n",
            "Epoch 5, Batch 319, G Loss: 0.6932017803192139, D Loss: 1.3856651782989502\n",
            "Epoch 5, Batch 320, G Loss: 0.6932164430618286, D Loss: 1.385803461074829\n",
            "Epoch 5, Batch 321, G Loss: 0.693217396736145, D Loss: 1.3857572078704834\n",
            "Epoch 5, Batch 322, G Loss: 0.693235456943512, D Loss: 1.385979413986206\n",
            "Epoch 5, Batch 323, G Loss: 0.6932463645935059, D Loss: 1.3859456777572632\n",
            "Epoch 5, Batch 324, G Loss: 0.6932587623596191, D Loss: 1.386115312576294\n",
            "Epoch 5, Batch 325, G Loss: 0.6932752728462219, D Loss: 1.3858551979064941\n",
            "Epoch 5, Batch 326, G Loss: 0.6932859420776367, D Loss: 1.3854782581329346\n",
            "Epoch 5, Batch 327, G Loss: 0.6933050155639648, D Loss: 1.385191559791565\n",
            "Epoch 5, Batch 328, G Loss: 0.6933392882347107, D Loss: 1.386214017868042\n",
            "Epoch 5, Batch 329, G Loss: 0.6933650374412537, D Loss: 1.386497974395752\n",
            "Epoch 5, Batch 330, G Loss: 0.6933860182762146, D Loss: 1.3862203359603882\n",
            "Epoch 5, Batch 331, G Loss: 0.69339519739151, D Loss: 1.3860645294189453\n",
            "Epoch 5, Batch 332, G Loss: 0.693414568901062, D Loss: 1.3858509063720703\n",
            "Epoch 5, Batch 333, G Loss: 0.6934241652488708, D Loss: 1.3861303329467773\n",
            "Epoch 5, Batch 334, G Loss: 0.6934314370155334, D Loss: 1.3865382671356201\n",
            "Epoch 5, Batch 335, G Loss: 0.6934399604797363, D Loss: 1.3863710165023804\n",
            "Epoch 5, Batch 336, G Loss: 0.6934345960617065, D Loss: 1.385644793510437\n",
            "Epoch 5, Batch 337, G Loss: 0.693428635597229, D Loss: 1.385521650314331\n",
            "Epoch 5, Batch 338, G Loss: 0.6934389472007751, D Loss: 1.386284351348877\n",
            "Epoch 5, Batch 339, G Loss: 0.6934483051300049, D Loss: 1.3859946727752686\n",
            "Epoch 5, Batch 340, G Loss: 0.6934521794319153, D Loss: 1.3861186504364014\n",
            "Epoch 5, Batch 341, G Loss: 0.6934419870376587, D Loss: 1.3859622478485107\n",
            "Epoch 5, Batch 342, G Loss: 0.6934380531311035, D Loss: 1.386035680770874\n",
            "Epoch 5, Batch 343, G Loss: 0.6934331655502319, D Loss: 1.385728359222412\n",
            "Epoch 5, Batch 344, G Loss: 0.693446695804596, D Loss: 1.385457992553711\n",
            "Epoch 5, Batch 345, G Loss: 0.6934573650360107, D Loss: 1.3858160972595215\n",
            "Epoch 5, Batch 346, G Loss: 0.6934747695922852, D Loss: 1.38592529296875\n",
            "Epoch 5, Batch 347, G Loss: 0.6934974193572998, D Loss: 1.3858617544174194\n",
            "Epoch 5, Batch 348, G Loss: 0.6935175061225891, D Loss: 1.385953664779663\n",
            "Epoch 5, Batch 349, G Loss: 0.6935186386108398, D Loss: 1.3858648538589478\n",
            "Epoch 5, Batch 350, G Loss: 0.6935519576072693, D Loss: 1.3857077360153198\n",
            "Epoch 5, Batch 351, G Loss: 0.6935648322105408, D Loss: 1.385706901550293\n",
            "Epoch 5, Batch 352, G Loss: 0.6936014890670776, D Loss: 1.3860512971878052\n",
            "Epoch 5, Batch 353, G Loss: 0.6936144232749939, D Loss: 1.3860656023025513\n",
            "Epoch 5, Batch 354, G Loss: 0.6936069130897522, D Loss: 1.3860492706298828\n",
            "Epoch 5, Batch 355, G Loss: 0.693638265132904, D Loss: 1.3857691287994385\n",
            "Epoch 5, Batch 356, G Loss: 0.6936478614807129, D Loss: 1.3856562376022339\n",
            "Epoch 5, Batch 357, G Loss: 0.6936517953872681, D Loss: 1.3857873678207397\n",
            "Epoch 5, Batch 358, G Loss: 0.693668782711029, D Loss: 1.3859248161315918\n",
            "Epoch 5, Batch 359, G Loss: 0.6936724781990051, D Loss: 1.385707139968872\n",
            "Epoch 5, Batch 360, G Loss: 0.6937009692192078, D Loss: 1.3851834535598755\n",
            "Epoch 5, Batch 361, G Loss: 0.6937187910079956, D Loss: 1.385441780090332\n",
            "Epoch 5, Batch 362, G Loss: 0.6937504410743713, D Loss: 1.385709524154663\n",
            "Epoch 5, Batch 363, G Loss: 0.6937965750694275, D Loss: 1.386028528213501\n",
            "Epoch 5, Batch 364, G Loss: 0.6938067078590393, D Loss: 1.3858869075775146\n",
            "Epoch 5, Batch 365, G Loss: 0.6938372254371643, D Loss: 1.3862831592559814\n",
            "Epoch 5, Batch 366, G Loss: 0.6938634514808655, D Loss: 1.3862688541412354\n",
            "Epoch 5, Batch 367, G Loss: 0.6938577890396118, D Loss: 1.3861908912658691\n",
            "Epoch 5, Batch 368, G Loss: 0.6938602924346924, D Loss: 1.386004090309143\n",
            "Epoch 5, Batch 369, G Loss: 0.6938610672950745, D Loss: 1.385751485824585\n",
            "Epoch 5, Batch 370, G Loss: 0.6938677430152893, D Loss: 1.3853099346160889\n",
            "Epoch 5, Batch 371, G Loss: 0.6938848495483398, D Loss: 1.3862342834472656\n",
            "Epoch 5, Batch 372, G Loss: 0.6938803791999817, D Loss: 1.3864185810089111\n",
            "Epoch 5, Batch 373, G Loss: 0.6938895583152771, D Loss: 1.3867409229278564\n",
            "Epoch 5, Batch 374, G Loss: 0.6938661932945251, D Loss: 1.3863186836242676\n",
            "Epoch 5, Batch 375, G Loss: 0.6938426494598389, D Loss: 1.386094331741333\n",
            "Epoch 5, Batch 376, G Loss: 0.6938199400901794, D Loss: 1.3865296840667725\n",
            "Epoch 5, Batch 377, G Loss: 0.6937968134880066, D Loss: 1.386716604232788\n",
            "Epoch 5, Batch 378, G Loss: 0.6937515735626221, D Loss: 1.3865737915039062\n",
            "Epoch 5, Batch 379, G Loss: 0.6937195062637329, D Loss: 1.3863744735717773\n",
            "Epoch 5, Batch 380, G Loss: 0.6936625242233276, D Loss: 1.386352777481079\n",
            "Epoch 5, Batch 381, G Loss: 0.6936127543449402, D Loss: 1.386381983757019\n",
            "Epoch 5, Batch 382, G Loss: 0.693565845489502, D Loss: 1.3862650394439697\n",
            "Epoch 5, Batch 383, G Loss: 0.6935177445411682, D Loss: 1.385201334953308\n",
            "Epoch 5, Batch 384, G Loss: 0.6934726238250732, D Loss: 1.3858977556228638\n",
            "Epoch 5, Batch 385, G Loss: 0.6934587359428406, D Loss: 1.3861820697784424\n",
            "Epoch 5, Batch 386, G Loss: 0.6934270858764648, D Loss: 1.3858842849731445\n",
            "Epoch 5, Batch 387, G Loss: 0.6934010982513428, D Loss: 1.3860507011413574\n",
            "Epoch 5, Batch 388, G Loss: 0.6933800578117371, D Loss: 1.3859095573425293\n",
            "Epoch 5, Batch 389, G Loss: 0.6933543682098389, D Loss: 1.385941505432129\n",
            "Epoch 5, Batch 390, G Loss: 0.6933428645133972, D Loss: 1.3856542110443115\n",
            "Epoch 5, Batch 391, G Loss: 0.693336009979248, D Loss: 1.3854827880859375\n",
            "Epoch 5, Batch 392, G Loss: 0.6933273077011108, D Loss: 1.3859089612960815\n",
            "Epoch 5, Batch 393, G Loss: 0.6933362483978271, D Loss: 1.385753870010376\n",
            "Epoch 5, Batch 394, G Loss: 0.6933470964431763, D Loss: 1.385789155960083\n",
            "Epoch 5, Batch 395, G Loss: 0.693351149559021, D Loss: 1.3859211206436157\n",
            "Epoch 5, Batch 396, G Loss: 0.6933600902557373, D Loss: 1.3856730461120605\n",
            "Epoch 5, Batch 397, G Loss: 0.6933627724647522, D Loss: 1.3861734867095947\n",
            "Epoch 5, Batch 398, G Loss: 0.6933736801147461, D Loss: 1.3860623836517334\n",
            "Epoch 5, Batch 399, G Loss: 0.6933888792991638, D Loss: 1.3857917785644531\n",
            "Epoch 5, Batch 400, G Loss: 0.6933881044387817, D Loss: 1.3863317966461182\n",
            "Epoch 5, Batch 401, G Loss: 0.6933960318565369, D Loss: 1.3865597248077393\n",
            "Epoch 5, Batch 402, G Loss: 0.6933838725090027, D Loss: 1.3861483335494995\n",
            "Epoch 5, Batch 403, G Loss: 0.6933916807174683, D Loss: 1.3860827684402466\n",
            "Epoch 5, Batch 404, G Loss: 0.6933833360671997, D Loss: 1.3859940767288208\n",
            "Epoch 5, Batch 405, G Loss: 0.6933817863464355, D Loss: 1.3857145309448242\n",
            "Epoch 5, Batch 406, G Loss: 0.6933881640434265, D Loss: 1.3857331275939941\n",
            "Epoch 5, Batch 407, G Loss: 0.6933870911598206, D Loss: 1.385435938835144\n",
            "Epoch 5, Batch 408, G Loss: 0.69342041015625, D Loss: 1.385521650314331\n",
            "Epoch 5, Batch 409, G Loss: 0.6934298276901245, D Loss: 1.3855328559875488\n",
            "Epoch 5, Batch 410, G Loss: 0.6934546828269958, D Loss: 1.3861465454101562\n",
            "Epoch 5, Batch 411, G Loss: 0.6934784650802612, D Loss: 1.3860610723495483\n",
            "Epoch 5, Batch 412, G Loss: 0.6934899091720581, D Loss: 1.385667085647583\n",
            "Epoch 5, Batch 413, G Loss: 0.6935056447982788, D Loss: 1.3856406211853027\n",
            "Epoch 5, Batch 414, G Loss: 0.6935335993766785, D Loss: 1.3861479759216309\n",
            "Epoch 5, Batch 415, G Loss: 0.6935505867004395, D Loss: 1.3864293098449707\n",
            "Epoch 5, Batch 416, G Loss: 0.6935440301895142, D Loss: 1.3859515190124512\n",
            "Epoch 5, Batch 417, G Loss: 0.6935598254203796, D Loss: 1.3857758045196533\n",
            "Epoch 5, Batch 418, G Loss: 0.6935610175132751, D Loss: 1.385780930519104\n",
            "Epoch 5, Batch 419, G Loss: 0.6935814023017883, D Loss: 1.385540246963501\n",
            "Epoch 5, Batch 420, G Loss: 0.6935821771621704, D Loss: 1.3857166767120361\n",
            "Epoch 5, Batch 421, G Loss: 0.693611741065979, D Loss: 1.3854596614837646\n",
            "Epoch 5, Batch 422, G Loss: 0.6936379075050354, D Loss: 1.3861511945724487\n",
            "Epoch 5, Batch 423, G Loss: 0.6936461925506592, D Loss: 1.3859562873840332\n",
            "Epoch 5, Batch 424, G Loss: 0.6936649680137634, D Loss: 1.3855441808700562\n",
            "Epoch 5, Batch 425, G Loss: 0.6936916708946228, D Loss: 1.3858321905136108\n",
            "Epoch 5, Batch 426, G Loss: 0.6937073469161987, D Loss: 1.38627290725708\n",
            "Epoch 5, Batch 427, G Loss: 0.6937110424041748, D Loss: 1.3863089084625244\n",
            "Epoch 5, Batch 428, G Loss: 0.693727433681488, D Loss: 1.3862122297286987\n",
            "Epoch 5, Batch 429, G Loss: 0.6937270164489746, D Loss: 1.3859295845031738\n",
            "Epoch 5, Batch 430, G Loss: 0.6937342882156372, D Loss: 1.3860912322998047\n",
            "Epoch 5, Batch 431, G Loss: 0.6937278509140015, D Loss: 1.386007308959961\n",
            "Epoch 5, Batch 432, G Loss: 0.6937210559844971, D Loss: 1.3860793113708496\n",
            "Epoch 5, Batch 433, G Loss: 0.6937098503112793, D Loss: 1.3857159614562988\n",
            "Epoch 5, Batch 434, G Loss: 0.6937109231948853, D Loss: 1.3861606121063232\n",
            "Epoch 5, Batch 435, G Loss: 0.6937129497528076, D Loss: 1.3866472244262695\n",
            "Epoch 5, Batch 436, G Loss: 0.6936876177787781, D Loss: 1.3868062496185303\n",
            "Epoch 5, Batch 437, G Loss: 0.6936498880386353, D Loss: 1.3860549926757812\n",
            "Epoch 5, Batch 438, G Loss: 0.6936167478561401, D Loss: 1.3860106468200684\n",
            "Epoch 5, Batch 439, G Loss: 0.6935875415802002, D Loss: 1.3859837055206299\n",
            "Epoch 5, Batch 440, G Loss: 0.6935603022575378, D Loss: 1.3858330249786377\n",
            "Epoch 5, Batch 441, G Loss: 0.6935360431671143, D Loss: 1.3854331970214844\n",
            "Epoch 5, Batch 442, G Loss: 0.6935322284698486, D Loss: 1.3858277797698975\n",
            "Epoch 5, Batch 443, G Loss: 0.6935326457023621, D Loss: 1.3857427835464478\n",
            "Epoch 5, Batch 444, G Loss: 0.6935330629348755, D Loss: 1.3864467144012451\n",
            "Epoch 5, Batch 445, G Loss: 0.6935197710990906, D Loss: 1.3860098123550415\n",
            "Epoch 5, Batch 446, G Loss: 0.6935075521469116, D Loss: 1.3852558135986328\n",
            "Epoch 5, Batch 447, G Loss: 0.6935078501701355, D Loss: 1.3856135606765747\n",
            "Epoch 5, Batch 448, G Loss: 0.6935244202613831, D Loss: 1.3859708309173584\n",
            "Epoch 5, Batch 449, G Loss: 0.6935290098190308, D Loss: 1.385885238647461\n",
            "Epoch 5, Batch 450, G Loss: 0.6935366988182068, D Loss: 1.386138916015625\n",
            "Epoch 5, Batch 451, G Loss: 0.6935361623764038, D Loss: 1.3855233192443848\n",
            "Epoch 5, Batch 452, G Loss: 0.6935417056083679, D Loss: 1.3853023052215576\n",
            "Epoch 5, Batch 453, G Loss: 0.6935635209083557, D Loss: 1.3855650424957275\n",
            "Epoch 5, Batch 454, G Loss: 0.6935739517211914, D Loss: 1.3856232166290283\n",
            "Epoch 5, Batch 455, G Loss: 0.6936107277870178, D Loss: 1.3855724334716797\n",
            "Epoch 5, Batch 456, G Loss: 0.6936398148536682, D Loss: 1.3861435651779175\n",
            "Epoch 5, Batch 457, G Loss: 0.6936588883399963, D Loss: 1.3865107297897339\n",
            "Epoch 5, Batch 458, G Loss: 0.6936650276184082, D Loss: 1.3852169513702393\n",
            "Epoch 5, Batch 459, G Loss: 0.6936797499656677, D Loss: 1.3848631381988525\n",
            "Epoch 5, Batch 460, G Loss: 0.6937240958213806, D Loss: 1.3857903480529785\n",
            "Epoch 5, Batch 461, G Loss: 0.6937593221664429, D Loss: 1.3860052824020386\n",
            "Epoch 5, Batch 462, G Loss: 0.6937807202339172, D Loss: 1.3857017755508423\n",
            "Epoch 5, Batch 463, G Loss: 0.6938072443008423, D Loss: 1.386003017425537\n",
            "Epoch 5, Batch 464, G Loss: 0.6938289999961853, D Loss: 1.3874626159667969\n",
            "Epoch 5, Batch 465, G Loss: 0.6938175559043884, D Loss: 1.3881645202636719\n",
            "Epoch 5, Batch 466, G Loss: 0.693773090839386, D Loss: 1.385632872581482\n",
            "Epoch 5, Batch 467, G Loss: 0.6937413215637207, D Loss: 1.386135458946228\n",
            "Epoch 5, Batch 468, G Loss: 0.6937004923820496, D Loss: 1.3865289688110352\n",
            "Epoch 5, Batch 469, G Loss: 0.693670392036438, D Loss: 1.3852643966674805\n",
            "Epoch 6, Batch 1, G Loss: 0.6936482787132263, D Loss: 1.3858612775802612\n",
            "Epoch 6, Batch 2, G Loss: 0.6936347484588623, D Loss: 1.3859236240386963\n",
            "Epoch 6, Batch 3, G Loss: 0.693615198135376, D Loss: 1.3857221603393555\n",
            "Epoch 6, Batch 4, G Loss: 0.6936032176017761, D Loss: 1.38565993309021\n",
            "Epoch 6, Batch 5, G Loss: 0.6935986876487732, D Loss: 1.3857002258300781\n",
            "Epoch 6, Batch 6, G Loss: 0.6935943961143494, D Loss: 1.3861870765686035\n",
            "Epoch 6, Batch 7, G Loss: 0.6935833692550659, D Loss: 1.3861294984817505\n",
            "Epoch 6, Batch 8, G Loss: 0.6935701966285706, D Loss: 1.3858392238616943\n",
            "Epoch 6, Batch 9, G Loss: 0.6935574412345886, D Loss: 1.3853776454925537\n",
            "Epoch 6, Batch 10, G Loss: 0.6935605406761169, D Loss: 1.3866801261901855\n",
            "Epoch 6, Batch 11, G Loss: 0.6935549974441528, D Loss: 1.3870539665222168\n",
            "Epoch 6, Batch 12, G Loss: 0.6935198307037354, D Loss: 1.3857910633087158\n",
            "Epoch 6, Batch 13, G Loss: 0.6934959888458252, D Loss: 1.3856873512268066\n",
            "Epoch 6, Batch 14, G Loss: 0.6934834718704224, D Loss: 1.3856816291809082\n",
            "Epoch 6, Batch 15, G Loss: 0.6934695243835449, D Loss: 1.385880470275879\n",
            "Epoch 6, Batch 16, G Loss: 0.6934659481048584, D Loss: 1.3865983486175537\n",
            "Epoch 6, Batch 17, G Loss: 0.6934512257575989, D Loss: 1.3861570358276367\n",
            "Epoch 6, Batch 18, G Loss: 0.6934306621551514, D Loss: 1.3860808610916138\n",
            "Epoch 6, Batch 19, G Loss: 0.6934133768081665, D Loss: 1.3858652114868164\n",
            "Epoch 6, Batch 20, G Loss: 0.6933983564376831, D Loss: 1.3860622644424438\n",
            "Epoch 6, Batch 21, G Loss: 0.6933771371841431, D Loss: 1.3862037658691406\n",
            "Epoch 6, Batch 22, G Loss: 0.6933654546737671, D Loss: 1.3859093189239502\n",
            "Epoch 6, Batch 23, G Loss: 0.6933506727218628, D Loss: 1.3862109184265137\n",
            "Epoch 6, Batch 24, G Loss: 0.6933414936065674, D Loss: 1.385660171508789\n",
            "Epoch 6, Batch 25, G Loss: 0.6933238506317139, D Loss: 1.385549783706665\n",
            "Epoch 6, Batch 26, G Loss: 0.6933338642120361, D Loss: 1.3855373859405518\n",
            "Epoch 6, Batch 27, G Loss: 0.6933375597000122, D Loss: 1.3857060670852661\n",
            "Epoch 6, Batch 28, G Loss: 0.6933621764183044, D Loss: 1.3856778144836426\n",
            "Epoch 6, Batch 29, G Loss: 0.6933680772781372, D Loss: 1.3864374160766602\n",
            "Epoch 6, Batch 30, G Loss: 0.693376362323761, D Loss: 1.3858211040496826\n",
            "Epoch 6, Batch 31, G Loss: 0.693385660648346, D Loss: 1.385735034942627\n",
            "Epoch 6, Batch 32, G Loss: 0.6933907270431519, D Loss: 1.3859951496124268\n",
            "Epoch 6, Batch 33, G Loss: 0.6934078931808472, D Loss: 1.3857192993164062\n",
            "Epoch 6, Batch 34, G Loss: 0.6934130787849426, D Loss: 1.3859403133392334\n",
            "Epoch 6, Batch 35, G Loss: 0.6934252381324768, D Loss: 1.385617971420288\n",
            "Epoch 6, Batch 36, G Loss: 0.6934389472007751, D Loss: 1.386317491531372\n",
            "Epoch 6, Batch 37, G Loss: 0.6934508681297302, D Loss: 1.3866674900054932\n",
            "Epoch 6, Batch 38, G Loss: 0.6934428215026855, D Loss: 1.386256456375122\n",
            "Epoch 6, Batch 39, G Loss: 0.6934223771095276, D Loss: 1.3859562873840332\n",
            "Epoch 6, Batch 40, G Loss: 0.6934189200401306, D Loss: 1.3861689567565918\n",
            "Epoch 6, Batch 41, G Loss: 0.6934041976928711, D Loss: 1.386244297027588\n",
            "Epoch 6, Batch 42, G Loss: 0.6933937668800354, D Loss: 1.3864209651947021\n",
            "Epoch 6, Batch 43, G Loss: 0.6933742165565491, D Loss: 1.3866350650787354\n",
            "Epoch 6, Batch 44, G Loss: 0.6933472156524658, D Loss: 1.3862576484680176\n",
            "Epoch 6, Batch 45, G Loss: 0.6933193802833557, D Loss: 1.3861480951309204\n",
            "Epoch 6, Batch 46, G Loss: 0.6932921409606934, D Loss: 1.3857650756835938\n",
            "Epoch 6, Batch 47, G Loss: 0.693271815776825, D Loss: 1.3864799737930298\n",
            "Epoch 6, Batch 48, G Loss: 0.6932505369186401, D Loss: 1.3870537281036377\n",
            "Epoch 6, Batch 49, G Loss: 0.6932094097137451, D Loss: 1.3865875005722046\n",
            "Epoch 6, Batch 50, G Loss: 0.6931608319282532, D Loss: 1.3864328861236572\n",
            "Epoch 6, Batch 51, G Loss: 0.6931114196777344, D Loss: 1.3860626220703125\n",
            "Epoch 6, Batch 52, G Loss: 0.6930631399154663, D Loss: 1.3858659267425537\n",
            "Epoch 6, Batch 53, G Loss: 0.69302898645401, D Loss: 1.3851433992385864\n",
            "Epoch 6, Batch 54, G Loss: 0.6930150985717773, D Loss: 1.385596752166748\n",
            "Epoch 6, Batch 55, G Loss: 0.6930055618286133, D Loss: 1.3862167596817017\n",
            "Epoch 6, Batch 56, G Loss: 0.6930107474327087, D Loss: 1.385462760925293\n",
            "Epoch 6, Batch 57, G Loss: 0.6930137276649475, D Loss: 1.3854169845581055\n",
            "Epoch 6, Batch 58, G Loss: 0.6930285692214966, D Loss: 1.3850078582763672\n",
            "Epoch 6, Batch 59, G Loss: 0.6930570006370544, D Loss: 1.3854522705078125\n",
            "Epoch 6, Batch 60, G Loss: 0.69310462474823, D Loss: 1.3856499195098877\n",
            "Epoch 6, Batch 61, G Loss: 0.6931459307670593, D Loss: 1.385800838470459\n",
            "Epoch 6, Batch 62, G Loss: 0.6931816339492798, D Loss: 1.3864209651947021\n",
            "Epoch 6, Batch 63, G Loss: 0.6932116150856018, D Loss: 1.386483907699585\n",
            "Epoch 6, Batch 64, G Loss: 0.6932238936424255, D Loss: 1.3857696056365967\n",
            "Epoch 6, Batch 65, G Loss: 0.6932483315467834, D Loss: 1.385906457901001\n",
            "Epoch 6, Batch 66, G Loss: 0.693259596824646, D Loss: 1.3859858512878418\n",
            "Epoch 6, Batch 67, G Loss: 0.6932782530784607, D Loss: 1.3870906829833984\n",
            "Epoch 6, Batch 68, G Loss: 0.693272054195404, D Loss: 1.386768102645874\n",
            "Epoch 6, Batch 69, G Loss: 0.6932569742202759, D Loss: 1.3860790729522705\n",
            "Epoch 6, Batch 70, G Loss: 0.6932357549667358, D Loss: 1.3858813047409058\n",
            "Epoch 6, Batch 71, G Loss: 0.6932291388511658, D Loss: 1.385857343673706\n",
            "Epoch 6, Batch 72, G Loss: 0.6932254433631897, D Loss: 1.3855184316635132\n",
            "Epoch 6, Batch 73, G Loss: 0.6932373046875, D Loss: 1.3855805397033691\n",
            "Epoch 6, Batch 74, G Loss: 0.6932512521743774, D Loss: 1.3862030506134033\n",
            "Epoch 6, Batch 75, G Loss: 0.6932551860809326, D Loss: 1.3860009908676147\n",
            "Epoch 6, Batch 76, G Loss: 0.6932693719863892, D Loss: 1.3860794305801392\n",
            "Epoch 6, Batch 77, G Loss: 0.69327312707901, D Loss: 1.3859786987304688\n",
            "Epoch 6, Batch 78, G Loss: 0.6932737231254578, D Loss: 1.3858115673065186\n",
            "Epoch 6, Batch 79, G Loss: 0.6932807564735413, D Loss: 1.3860502243041992\n",
            "Epoch 6, Batch 80, G Loss: 0.6932845115661621, D Loss: 1.3864014148712158\n",
            "Epoch 6, Batch 81, G Loss: 0.693280816078186, D Loss: 1.3871495723724365\n",
            "Epoch 6, Batch 82, G Loss: 0.6932536363601685, D Loss: 1.3865480422973633\n",
            "Epoch 6, Batch 83, G Loss: 0.693219006061554, D Loss: 1.3861727714538574\n",
            "Epoch 6, Batch 84, G Loss: 0.6931866407394409, D Loss: 1.3856430053710938\n",
            "Epoch 6, Batch 85, G Loss: 0.6931674480438232, D Loss: 1.386181116104126\n",
            "Epoch 6, Batch 86, G Loss: 0.693150520324707, D Loss: 1.386286735534668\n",
            "Epoch 6, Batch 87, G Loss: 0.6931331157684326, D Loss: 1.3862439393997192\n",
            "Epoch 6, Batch 88, G Loss: 0.6931120157241821, D Loss: 1.3860344886779785\n",
            "Epoch 6, Batch 89, G Loss: 0.6930901408195496, D Loss: 1.3859148025512695\n",
            "Epoch 6, Batch 90, G Loss: 0.6930812001228333, D Loss: 1.385864019393921\n",
            "Epoch 6, Batch 91, G Loss: 0.6930664777755737, D Loss: 1.3858561515808105\n",
            "Epoch 6, Batch 92, G Loss: 0.6930627822875977, D Loss: 1.3858667612075806\n",
            "Epoch 6, Batch 93, G Loss: 0.6930626630783081, D Loss: 1.3858927488327026\n",
            "Epoch 6, Batch 94, G Loss: 0.6930679678916931, D Loss: 1.3861433267593384\n",
            "Epoch 6, Batch 95, G Loss: 0.6930599808692932, D Loss: 1.3861595392227173\n",
            "Epoch 6, Batch 96, G Loss: 0.6930582523345947, D Loss: 1.3863385915756226\n",
            "Epoch 6, Batch 97, G Loss: 0.6930480599403381, D Loss: 1.3861608505249023\n",
            "Epoch 6, Batch 98, G Loss: 0.6930381059646606, D Loss: 1.386334776878357\n",
            "Epoch 6, Batch 99, G Loss: 0.6930140256881714, D Loss: 1.386319875717163\n",
            "Epoch 6, Batch 100, G Loss: 0.6929926872253418, D Loss: 1.3858706951141357\n",
            "Epoch 6, Batch 101, G Loss: 0.6929838061332703, D Loss: 1.385698676109314\n",
            "Epoch 6, Batch 102, G Loss: 0.6929870843887329, D Loss: 1.3866283893585205\n",
            "Epoch 6, Batch 103, G Loss: 0.6929787397384644, D Loss: 1.386658787727356\n",
            "Epoch 6, Batch 104, G Loss: 0.6929565668106079, D Loss: 1.3863201141357422\n",
            "Epoch 6, Batch 105, G Loss: 0.6929299235343933, D Loss: 1.3861987590789795\n",
            "Epoch 6, Batch 106, G Loss: 0.6929090619087219, D Loss: 1.3865312337875366\n",
            "Epoch 6, Batch 107, G Loss: 0.6928824782371521, D Loss: 1.3859162330627441\n",
            "Epoch 6, Batch 108, G Loss: 0.6928589344024658, D Loss: 1.3854566812515259\n",
            "Epoch 6, Batch 109, G Loss: 0.6928480267524719, D Loss: 1.3859310150146484\n",
            "Epoch 6, Batch 110, G Loss: 0.692847728729248, D Loss: 1.3861232995986938\n",
            "Epoch 6, Batch 111, G Loss: 0.6928443312644958, D Loss: 1.3866205215454102\n",
            "Epoch 6, Batch 112, G Loss: 0.692821741104126, D Loss: 1.386415719985962\n",
            "Epoch 6, Batch 113, G Loss: 0.6927988529205322, D Loss: 1.3860211372375488\n",
            "Epoch 6, Batch 114, G Loss: 0.6927871108055115, D Loss: 1.3856091499328613\n",
            "Epoch 6, Batch 115, G Loss: 0.6927886009216309, D Loss: 1.3859624862670898\n",
            "Epoch 6, Batch 116, G Loss: 0.6927866339683533, D Loss: 1.3858742713928223\n",
            "Epoch 6, Batch 117, G Loss: 0.6927869915962219, D Loss: 1.385854721069336\n",
            "Epoch 6, Batch 118, G Loss: 0.6928036212921143, D Loss: 1.3857324123382568\n",
            "Epoch 6, Batch 119, G Loss: 0.6928231120109558, D Loss: 1.3855924606323242\n",
            "Epoch 6, Batch 120, G Loss: 0.692846953868866, D Loss: 1.3856420516967773\n",
            "Epoch 6, Batch 121, G Loss: 0.6928960084915161, D Loss: 1.3855109214782715\n",
            "Epoch 6, Batch 122, G Loss: 0.6929408311843872, D Loss: 1.3857500553131104\n",
            "Epoch 6, Batch 123, G Loss: 0.6929720640182495, D Loss: 1.3858308792114258\n",
            "Epoch 6, Batch 124, G Loss: 0.6930183172225952, D Loss: 1.3857531547546387\n",
            "Epoch 6, Batch 125, G Loss: 0.6930608153343201, D Loss: 1.3859736919403076\n",
            "Epoch 6, Batch 126, G Loss: 0.6931058764457703, D Loss: 1.386216402053833\n",
            "Epoch 6, Batch 127, G Loss: 0.6931329965591431, D Loss: 1.3861920833587646\n",
            "Epoch 6, Batch 128, G Loss: 0.693157970905304, D Loss: 1.385939598083496\n",
            "Epoch 6, Batch 129, G Loss: 0.6931862831115723, D Loss: 1.385838270187378\n",
            "Epoch 6, Batch 130, G Loss: 0.6932219862937927, D Loss: 1.3857927322387695\n",
            "Epoch 6, Batch 131, G Loss: 0.6932453513145447, D Loss: 1.3857078552246094\n",
            "Epoch 6, Batch 132, G Loss: 0.693291962146759, D Loss: 1.385699987411499\n",
            "Epoch 6, Batch 133, G Loss: 0.6933266520500183, D Loss: 1.385772466659546\n",
            "Epoch 6, Batch 134, G Loss: 0.6933685541152954, D Loss: 1.3858811855316162\n",
            "Epoch 6, Batch 135, G Loss: 0.6934059858322144, D Loss: 1.3861846923828125\n",
            "Epoch 6, Batch 136, G Loss: 0.6934331059455872, D Loss: 1.386085033416748\n",
            "Epoch 6, Batch 137, G Loss: 0.693457841873169, D Loss: 1.3857475519180298\n",
            "Epoch 6, Batch 138, G Loss: 0.6934813857078552, D Loss: 1.3854581117630005\n",
            "Epoch 6, Batch 139, G Loss: 0.693507194519043, D Loss: 1.3855323791503906\n",
            "Epoch 6, Batch 140, G Loss: 0.6935455203056335, D Loss: 1.3854334354400635\n",
            "Epoch 6, Batch 141, G Loss: 0.6935935020446777, D Loss: 1.385908603668213\n",
            "Epoch 6, Batch 142, G Loss: 0.6936379075050354, D Loss: 1.385819911956787\n",
            "Epoch 6, Batch 143, G Loss: 0.6936734318733215, D Loss: 1.385939121246338\n",
            "Epoch 6, Batch 144, G Loss: 0.6937108039855957, D Loss: 1.3856773376464844\n",
            "Epoch 6, Batch 145, G Loss: 0.693749725818634, D Loss: 1.3857653141021729\n",
            "Epoch 6, Batch 146, G Loss: 0.6937896013259888, D Loss: 1.385901689529419\n",
            "Epoch 6, Batch 147, G Loss: 0.693818211555481, D Loss: 1.3857429027557373\n",
            "Epoch 6, Batch 148, G Loss: 0.6938519477844238, D Loss: 1.3858075141906738\n",
            "Epoch 6, Batch 149, G Loss: 0.6938705444335938, D Loss: 1.3855559825897217\n",
            "Epoch 6, Batch 150, G Loss: 0.6939051151275635, D Loss: 1.3855626583099365\n",
            "Epoch 6, Batch 151, G Loss: 0.6939428448677063, D Loss: 1.3864878416061401\n",
            "Epoch 6, Batch 152, G Loss: 0.6939504146575928, D Loss: 1.3866033554077148\n",
            "Epoch 6, Batch 153, G Loss: 0.6939493417739868, D Loss: 1.3865993022918701\n",
            "Epoch 6, Batch 154, G Loss: 0.6939271688461304, D Loss: 1.3864233493804932\n",
            "Epoch 6, Batch 155, G Loss: 0.6939027905464172, D Loss: 1.3864870071411133\n",
            "Epoch 6, Batch 156, G Loss: 0.6938675045967102, D Loss: 1.3863787651062012\n",
            "Epoch 6, Batch 157, G Loss: 0.6938275694847107, D Loss: 1.3861937522888184\n",
            "Epoch 6, Batch 158, G Loss: 0.6937920451164246, D Loss: 1.3861095905303955\n",
            "Epoch 6, Batch 159, G Loss: 0.6937505006790161, D Loss: 1.3857479095458984\n",
            "Epoch 6, Batch 160, G Loss: 0.6937199831008911, D Loss: 1.3859810829162598\n",
            "Epoch 6, Batch 161, G Loss: 0.6936952471733093, D Loss: 1.3861217498779297\n",
            "Epoch 6, Batch 162, G Loss: 0.6936640739440918, D Loss: 1.3862478733062744\n",
            "Epoch 6, Batch 163, G Loss: 0.6936302781105042, D Loss: 1.3858888149261475\n",
            "Epoch 6, Batch 164, G Loss: 0.6935903429985046, D Loss: 1.3861016035079956\n",
            "Epoch 6, Batch 165, G Loss: 0.6935544013977051, D Loss: 1.385604977607727\n",
            "Epoch 6, Batch 166, G Loss: 0.6935359239578247, D Loss: 1.3858166933059692\n",
            "Epoch 6, Batch 167, G Loss: 0.6935119032859802, D Loss: 1.3860218524932861\n",
            "Epoch 6, Batch 168, G Loss: 0.6934926509857178, D Loss: 1.38615882396698\n",
            "Epoch 6, Batch 169, G Loss: 0.6934621930122375, D Loss: 1.3858546018600464\n",
            "Epoch 6, Batch 170, G Loss: 0.6934425830841064, D Loss: 1.385376214981079\n",
            "Epoch 6, Batch 171, G Loss: 0.6934409141540527, D Loss: 1.3867480754852295\n",
            "Epoch 6, Batch 172, G Loss: 0.6934146285057068, D Loss: 1.386709451675415\n",
            "Epoch 6, Batch 173, G Loss: 0.693378746509552, D Loss: 1.3859275579452515\n",
            "Epoch 6, Batch 174, G Loss: 0.693352222442627, D Loss: 1.3859219551086426\n",
            "Epoch 6, Batch 175, G Loss: 0.6933316588401794, D Loss: 1.3858129978179932\n",
            "Epoch 6, Batch 176, G Loss: 0.6933174133300781, D Loss: 1.3858451843261719\n",
            "Epoch 6, Batch 177, G Loss: 0.6933123469352722, D Loss: 1.3861736059188843\n",
            "Epoch 6, Batch 178, G Loss: 0.6933003664016724, D Loss: 1.3867368698120117\n",
            "Epoch 6, Batch 179, G Loss: 0.6932722330093384, D Loss: 1.3863184452056885\n",
            "Epoch 6, Batch 180, G Loss: 0.6932339072227478, D Loss: 1.386452555656433\n",
            "Epoch 6, Batch 181, G Loss: 0.6931933760643005, D Loss: 1.3865107297897339\n",
            "Epoch 6, Batch 182, G Loss: 0.6931486129760742, D Loss: 1.3860583305358887\n",
            "Epoch 6, Batch 183, G Loss: 0.6931053400039673, D Loss: 1.3861762285232544\n",
            "Epoch 6, Batch 184, G Loss: 0.6930618286132812, D Loss: 1.3858743906021118\n",
            "Epoch 6, Batch 185, G Loss: 0.693034291267395, D Loss: 1.3856061697006226\n",
            "Epoch 6, Batch 186, G Loss: 0.6930088996887207, D Loss: 1.3858215808868408\n",
            "Epoch 6, Batch 187, G Loss: 0.6930081248283386, D Loss: 1.3859350681304932\n",
            "Epoch 6, Batch 188, G Loss: 0.6929943561553955, D Loss: 1.3860230445861816\n",
            "Epoch 6, Batch 189, G Loss: 0.6929827332496643, D Loss: 1.3859494924545288\n",
            "Epoch 6, Batch 190, G Loss: 0.6929794549942017, D Loss: 1.3859039545059204\n",
            "Epoch 6, Batch 191, G Loss: 0.6929726004600525, D Loss: 1.3859504461288452\n",
            "Epoch 6, Batch 192, G Loss: 0.6929799318313599, D Loss: 1.3859578371047974\n",
            "Epoch 6, Batch 193, G Loss: 0.6929827332496643, D Loss: 1.3862330913543701\n",
            "Epoch 6, Batch 194, G Loss: 0.6929814219474792, D Loss: 1.3859838247299194\n",
            "Epoch 6, Batch 195, G Loss: 0.6929827332496643, D Loss: 1.3854970932006836\n",
            "Epoch 6, Batch 196, G Loss: 0.6930050253868103, D Loss: 1.3857319355010986\n",
            "Epoch 6, Batch 197, G Loss: 0.6930178999900818, D Loss: 1.3858016729354858\n",
            "Epoch 6, Batch 198, G Loss: 0.6930421590805054, D Loss: 1.3863930702209473\n",
            "Epoch 6, Batch 199, G Loss: 0.6930564045906067, D Loss: 1.3865745067596436\n",
            "Epoch 6, Batch 200, G Loss: 0.6930508017539978, D Loss: 1.386029601097107\n",
            "Epoch 6, Batch 201, G Loss: 0.6930422186851501, D Loss: 1.3860477209091187\n",
            "Epoch 6, Batch 202, G Loss: 0.6930403709411621, D Loss: 1.3854620456695557\n",
            "Epoch 6, Batch 203, G Loss: 0.6930623054504395, D Loss: 1.3853062391281128\n",
            "Epoch 6, Batch 204, G Loss: 0.693091094493866, D Loss: 1.3861300945281982\n",
            "Epoch 6, Batch 205, G Loss: 0.693114697933197, D Loss: 1.3858904838562012\n",
            "Epoch 6, Batch 206, G Loss: 0.69314044713974, D Loss: 1.3856338262557983\n",
            "Epoch 6, Batch 207, G Loss: 0.693174421787262, D Loss: 1.3855781555175781\n",
            "Epoch 6, Batch 208, G Loss: 0.6932151913642883, D Loss: 1.3853740692138672\n",
            "Epoch 6, Batch 209, G Loss: 0.6932575702667236, D Loss: 1.3859813213348389\n",
            "Epoch 6, Batch 210, G Loss: 0.6932970285415649, D Loss: 1.3861448764801025\n",
            "Epoch 6, Batch 211, G Loss: 0.693324625492096, D Loss: 1.3856987953186035\n",
            "Epoch 6, Batch 212, G Loss: 0.6933579444885254, D Loss: 1.3861124515533447\n",
            "Epoch 6, Batch 213, G Loss: 0.6933887004852295, D Loss: 1.386252522468567\n",
            "Epoch 6, Batch 214, G Loss: 0.693403422832489, D Loss: 1.3860745429992676\n",
            "Epoch 6, Batch 215, G Loss: 0.6934314370155334, D Loss: 1.385860800743103\n",
            "Epoch 6, Batch 216, G Loss: 0.693442702293396, D Loss: 1.3857463598251343\n",
            "Epoch 6, Batch 217, G Loss: 0.6934705972671509, D Loss: 1.3855432271957397\n",
            "Epoch 6, Batch 218, G Loss: 0.6935004591941833, D Loss: 1.385573387145996\n",
            "Epoch 6, Batch 219, G Loss: 0.693535327911377, D Loss: 1.3863325119018555\n",
            "Epoch 6, Batch 220, G Loss: 0.6935539841651917, D Loss: 1.3863145112991333\n",
            "Epoch 6, Batch 221, G Loss: 0.6935635805130005, D Loss: 1.3864796161651611\n",
            "Epoch 6, Batch 222, G Loss: 0.6935569643974304, D Loss: 1.3864178657531738\n",
            "Epoch 6, Batch 223, G Loss: 0.6935366988182068, D Loss: 1.3863983154296875\n",
            "Epoch 6, Batch 224, G Loss: 0.6935069561004639, D Loss: 1.3861762285232544\n",
            "Epoch 6, Batch 225, G Loss: 0.6934783458709717, D Loss: 1.3859690427780151\n",
            "Epoch 6, Batch 226, G Loss: 0.6934563517570496, D Loss: 1.3861624002456665\n",
            "Epoch 6, Batch 227, G Loss: 0.6934326887130737, D Loss: 1.3861668109893799\n",
            "Epoch 6, Batch 228, G Loss: 0.6934029459953308, D Loss: 1.3854749202728271\n",
            "Epoch 6, Batch 229, G Loss: 0.6933911442756653, D Loss: 1.38612699508667\n",
            "Epoch 6, Batch 230, G Loss: 0.6933733224868774, D Loss: 1.3865764141082764\n",
            "Epoch 6, Batch 231, G Loss: 0.6933547258377075, D Loss: 1.3860652446746826\n",
            "Epoch 6, Batch 232, G Loss: 0.6933285593986511, D Loss: 1.385644793510437\n",
            "Epoch 6, Batch 233, G Loss: 0.6933140754699707, D Loss: 1.3854234218597412\n",
            "Epoch 6, Batch 234, G Loss: 0.6933093070983887, D Loss: 1.3854560852050781\n",
            "Epoch 6, Batch 235, G Loss: 0.693324625492096, D Loss: 1.385534644126892\n",
            "Epoch 6, Batch 236, G Loss: 0.6933466196060181, D Loss: 1.3857762813568115\n",
            "Epoch 6, Batch 237, G Loss: 0.693359911441803, D Loss: 1.3857300281524658\n",
            "Epoch 6, Batch 238, G Loss: 0.693387508392334, D Loss: 1.386130928993225\n",
            "Epoch 6, Batch 239, G Loss: 0.6934000253677368, D Loss: 1.38594651222229\n",
            "Epoch 6, Batch 240, G Loss: 0.6934082508087158, D Loss: 1.385104775428772\n",
            "Epoch 6, Batch 241, G Loss: 0.6934423446655273, D Loss: 1.3855921030044556\n",
            "Epoch 6, Batch 242, G Loss: 0.6934806108474731, D Loss: 1.3857239484786987\n",
            "Epoch 6, Batch 243, G Loss: 0.6935176253318787, D Loss: 1.3856744766235352\n",
            "Epoch 6, Batch 244, G Loss: 0.6935571432113647, D Loss: 1.3856205940246582\n",
            "Epoch 6, Batch 245, G Loss: 0.6935999393463135, D Loss: 1.3856468200683594\n",
            "Epoch 6, Batch 246, G Loss: 0.6936429142951965, D Loss: 1.3855928182601929\n",
            "Epoch 6, Batch 247, G Loss: 0.6936907172203064, D Loss: 1.3853294849395752\n",
            "Epoch 6, Batch 248, G Loss: 0.6937418580055237, D Loss: 1.3854035139083862\n",
            "Epoch 6, Batch 249, G Loss: 0.6937920451164246, D Loss: 1.3856945037841797\n",
            "Epoch 6, Batch 250, G Loss: 0.6938473582267761, D Loss: 1.3857457637786865\n",
            "Epoch 6, Batch 251, G Loss: 0.6939003467559814, D Loss: 1.3861727714538574\n",
            "Epoch 6, Batch 252, G Loss: 0.6939390301704407, D Loss: 1.38587486743927\n",
            "Epoch 6, Batch 253, G Loss: 0.6939751505851746, D Loss: 1.3863003253936768\n",
            "Epoch 6, Batch 254, G Loss: 0.6939898729324341, D Loss: 1.3865009546279907\n",
            "Epoch 6, Batch 255, G Loss: 0.6939958333969116, D Loss: 1.3857746124267578\n",
            "Epoch 6, Batch 256, G Loss: 0.6940004229545593, D Loss: 1.385463833808899\n",
            "Epoch 6, Batch 257, G Loss: 0.6940101385116577, D Loss: 1.3856682777404785\n",
            "Epoch 6, Batch 258, G Loss: 0.694023609161377, D Loss: 1.3861877918243408\n",
            "Epoch 6, Batch 259, G Loss: 0.6940256357192993, D Loss: 1.3861961364746094\n",
            "Epoch 6, Batch 260, G Loss: 0.6940203309059143, D Loss: 1.3861029148101807\n",
            "Epoch 6, Batch 261, G Loss: 0.6940045952796936, D Loss: 1.3858401775360107\n",
            "Epoch 6, Batch 262, G Loss: 0.6939961314201355, D Loss: 1.3854944705963135\n",
            "Epoch 6, Batch 263, G Loss: 0.6939927935600281, D Loss: 1.385237455368042\n",
            "Epoch 6, Batch 264, G Loss: 0.6939997673034668, D Loss: 1.3858917951583862\n",
            "Epoch 6, Batch 265, G Loss: 0.6940048336982727, D Loss: 1.3863143920898438\n",
            "Epoch 6, Batch 266, G Loss: 0.6940005421638489, D Loss: 1.3855646848678589\n",
            "Epoch 6, Batch 267, G Loss: 0.6940024495124817, D Loss: 1.3858771324157715\n",
            "Epoch 6, Batch 268, G Loss: 0.6939959526062012, D Loss: 1.3855870962142944\n",
            "Epoch 6, Batch 269, G Loss: 0.6939977407455444, D Loss: 1.3861274719238281\n",
            "Epoch 6, Batch 270, G Loss: 0.6939855813980103, D Loss: 1.386068344116211\n",
            "Epoch 6, Batch 271, G Loss: 0.6939753890037537, D Loss: 1.3860142230987549\n",
            "Epoch 6, Batch 272, G Loss: 0.6939606666564941, D Loss: 1.3860129117965698\n",
            "Epoch 6, Batch 273, G Loss: 0.6939380168914795, D Loss: 1.3859671354293823\n",
            "Epoch 6, Batch 274, G Loss: 0.693915069103241, D Loss: 1.3861808776855469\n",
            "Epoch 6, Batch 275, G Loss: 0.6938897967338562, D Loss: 1.3856794834136963\n",
            "Epoch 6, Batch 276, G Loss: 0.6938682794570923, D Loss: 1.3855459690093994\n",
            "Epoch 6, Batch 277, G Loss: 0.693858802318573, D Loss: 1.386124610900879\n",
            "Epoch 6, Batch 278, G Loss: 0.69384765625, D Loss: 1.3860249519348145\n",
            "Epoch 6, Batch 279, G Loss: 0.6938346028327942, D Loss: 1.385817289352417\n",
            "Epoch 6, Batch 280, G Loss: 0.6938075423240662, D Loss: 1.3858176469802856\n",
            "Epoch 6, Batch 281, G Loss: 0.6937944293022156, D Loss: 1.385345458984375\n",
            "Epoch 6, Batch 282, G Loss: 0.6937844157218933, D Loss: 1.385011911392212\n",
            "Epoch 6, Batch 283, G Loss: 0.6938012838363647, D Loss: 1.3866409063339233\n",
            "Epoch 6, Batch 284, G Loss: 0.6937893629074097, D Loss: 1.3866405487060547\n",
            "Epoch 6, Batch 285, G Loss: 0.6937612295150757, D Loss: 1.3857886791229248\n",
            "Epoch 6, Batch 286, G Loss: 0.693732738494873, D Loss: 1.3863521814346313\n",
            "Epoch 6, Batch 287, G Loss: 0.6937022805213928, D Loss: 1.3869564533233643\n",
            "Epoch 6, Batch 288, G Loss: 0.6936442255973816, D Loss: 1.386542558670044\n",
            "Epoch 6, Batch 289, G Loss: 0.6935837268829346, D Loss: 1.3859838247299194\n",
            "Epoch 6, Batch 290, G Loss: 0.6935343146324158, D Loss: 1.3861808776855469\n",
            "Epoch 6, Batch 291, G Loss: 0.6934884786605835, D Loss: 1.386588454246521\n",
            "Epoch 6, Batch 292, G Loss: 0.693437397480011, D Loss: 1.3863415718078613\n",
            "Epoch 6, Batch 293, G Loss: 0.6933768391609192, D Loss: 1.385845422744751\n",
            "Epoch 6, Batch 294, G Loss: 0.6933290958404541, D Loss: 1.3854870796203613\n",
            "Epoch 6, Batch 295, G Loss: 0.6932904124259949, D Loss: 1.3858098983764648\n",
            "Epoch 6, Batch 296, G Loss: 0.6932628154754639, D Loss: 1.3861900568008423\n",
            "Epoch 6, Batch 297, G Loss: 0.6932293772697449, D Loss: 1.3857612609863281\n",
            "Epoch 6, Batch 298, G Loss: 0.6932067275047302, D Loss: 1.3857276439666748\n",
            "Epoch 6, Batch 299, G Loss: 0.6931843161582947, D Loss: 1.3860355615615845\n",
            "Epoch 6, Batch 300, G Loss: 0.6931630969047546, D Loss: 1.3856792449951172\n",
            "Epoch 6, Batch 301, G Loss: 0.6931518316268921, D Loss: 1.385562777519226\n",
            "Epoch 6, Batch 302, G Loss: 0.6931498050689697, D Loss: 1.3861720561981201\n",
            "Epoch 6, Batch 303, G Loss: 0.6931437253952026, D Loss: 1.3858147859573364\n",
            "Epoch 6, Batch 304, G Loss: 0.6931366920471191, D Loss: 1.385998249053955\n",
            "Epoch 6, Batch 305, G Loss: 0.6931360363960266, D Loss: 1.3856171369552612\n",
            "Epoch 6, Batch 306, G Loss: 0.6931391358375549, D Loss: 1.3862541913986206\n",
            "Epoch 6, Batch 307, G Loss: 0.6931398510932922, D Loss: 1.3863131999969482\n",
            "Epoch 6, Batch 308, G Loss: 0.6931272149085999, D Loss: 1.3861445188522339\n",
            "Epoch 6, Batch 309, G Loss: 0.6931167840957642, D Loss: 1.3861503601074219\n",
            "Epoch 6, Batch 310, G Loss: 0.6930949687957764, D Loss: 1.3861868381500244\n",
            "Epoch 6, Batch 311, G Loss: 0.6930864453315735, D Loss: 1.3853402137756348\n",
            "Epoch 6, Batch 312, G Loss: 0.6930882930755615, D Loss: 1.3854873180389404\n",
            "Epoch 6, Batch 313, G Loss: 0.693109929561615, D Loss: 1.3859310150146484\n",
            "Epoch 6, Batch 314, G Loss: 0.6931174993515015, D Loss: 1.3860337734222412\n",
            "Epoch 6, Batch 315, G Loss: 0.6931315660476685, D Loss: 1.3857121467590332\n",
            "Epoch 6, Batch 316, G Loss: 0.6931487321853638, D Loss: 1.3859338760375977\n",
            "Epoch 6, Batch 317, G Loss: 0.6931604743003845, D Loss: 1.3864072561264038\n",
            "Epoch 6, Batch 318, G Loss: 0.6931613087654114, D Loss: 1.38584303855896\n",
            "Epoch 6, Batch 319, G Loss: 0.693168580532074, D Loss: 1.3855535984039307\n",
            "Epoch 6, Batch 320, G Loss: 0.6931750178337097, D Loss: 1.3857076168060303\n",
            "Epoch 6, Batch 321, G Loss: 0.6931910514831543, D Loss: 1.385655164718628\n",
            "Epoch 6, Batch 322, G Loss: 0.6932156682014465, D Loss: 1.3858592510223389\n",
            "Epoch 6, Batch 323, G Loss: 0.6932321786880493, D Loss: 1.3858301639556885\n",
            "Epoch 6, Batch 324, G Loss: 0.6932559609413147, D Loss: 1.385976791381836\n",
            "Epoch 6, Batch 325, G Loss: 0.6932740211486816, D Loss: 1.3857498168945312\n",
            "Epoch 6, Batch 326, G Loss: 0.693291187286377, D Loss: 1.3853886127471924\n",
            "Epoch 6, Batch 327, G Loss: 0.6933282613754272, D Loss: 1.38511323928833\n",
            "Epoch 6, Batch 328, G Loss: 0.693377673625946, D Loss: 1.3860939741134644\n",
            "Epoch 6, Batch 329, G Loss: 0.6934199333190918, D Loss: 1.3863539695739746\n",
            "Epoch 6, Batch 330, G Loss: 0.6934423446655273, D Loss: 1.3861111402511597\n",
            "Epoch 6, Batch 331, G Loss: 0.6934618949890137, D Loss: 1.3859472274780273\n",
            "Epoch 6, Batch 332, G Loss: 0.6934783458709717, D Loss: 1.3857371807098389\n",
            "Epoch 6, Batch 333, G Loss: 0.6934948563575745, D Loss: 1.3860063552856445\n",
            "Epoch 6, Batch 334, G Loss: 0.6935074329376221, D Loss: 1.3863942623138428\n",
            "Epoch 6, Batch 335, G Loss: 0.6935040354728699, D Loss: 1.38624107837677\n",
            "Epoch 6, Batch 336, G Loss: 0.6934977173805237, D Loss: 1.3855395317077637\n",
            "Epoch 6, Batch 337, G Loss: 0.6934933066368103, D Loss: 1.3854377269744873\n",
            "Epoch 6, Batch 338, G Loss: 0.6935052871704102, D Loss: 1.386147141456604\n",
            "Epoch 6, Batch 339, G Loss: 0.6935105323791504, D Loss: 1.385884404182434\n",
            "Epoch 6, Batch 340, G Loss: 0.6935056447982788, D Loss: 1.3860031366348267\n",
            "Epoch 6, Batch 341, G Loss: 0.6935126781463623, D Loss: 1.3858381509780884\n",
            "Epoch 6, Batch 342, G Loss: 0.6935076117515564, D Loss: 1.3859179019927979\n",
            "Epoch 6, Batch 343, G Loss: 0.6935110092163086, D Loss: 1.3856065273284912\n",
            "Epoch 6, Batch 344, G Loss: 0.6935153603553772, D Loss: 1.385366439819336\n",
            "Epoch 6, Batch 345, G Loss: 0.6935410499572754, D Loss: 1.3856861591339111\n",
            "Epoch 6, Batch 346, G Loss: 0.6935561895370483, D Loss: 1.385797142982483\n",
            "Epoch 6, Batch 347, G Loss: 0.6935791373252869, D Loss: 1.385749101638794\n",
            "Epoch 6, Batch 348, G Loss: 0.6935956478118896, D Loss: 1.3858431577682495\n",
            "Epoch 6, Batch 349, G Loss: 0.693610668182373, D Loss: 1.3857471942901611\n",
            "Epoch 6, Batch 350, G Loss: 0.6936266422271729, D Loss: 1.385602355003357\n",
            "Epoch 6, Batch 351, G Loss: 0.6936476826667786, D Loss: 1.3855997323989868\n",
            "Epoch 6, Batch 352, G Loss: 0.6936768293380737, D Loss: 1.3859450817108154\n",
            "Epoch 6, Batch 353, G Loss: 0.693696141242981, D Loss: 1.3859511613845825\n",
            "Epoch 6, Batch 354, G Loss: 0.693712592124939, D Loss: 1.3859070539474487\n",
            "Epoch 6, Batch 355, G Loss: 0.693725049495697, D Loss: 1.3856585025787354\n",
            "Epoch 6, Batch 356, G Loss: 0.693737268447876, D Loss: 1.385547161102295\n",
            "Epoch 6, Batch 357, G Loss: 0.6937574148178101, D Loss: 1.3856687545776367\n",
            "Epoch 6, Batch 358, G Loss: 0.6937769055366516, D Loss: 1.3857922554016113\n",
            "Epoch 6, Batch 359, G Loss: 0.6937926411628723, D Loss: 1.3855726718902588\n",
            "Epoch 6, Batch 360, G Loss: 0.6938168406486511, D Loss: 1.3850734233856201\n",
            "Epoch 6, Batch 361, G Loss: 0.6938424110412598, D Loss: 1.385336995124817\n",
            "Epoch 6, Batch 362, G Loss: 0.693885862827301, D Loss: 1.3855854272842407\n",
            "Epoch 6, Batch 363, G Loss: 0.6939198970794678, D Loss: 1.3859102725982666\n",
            "Epoch 6, Batch 364, G Loss: 0.693949818611145, D Loss: 1.3857581615447998\n",
            "Epoch 6, Batch 365, G Loss: 0.6939743757247925, D Loss: 1.386145830154419\n",
            "Epoch 6, Batch 366, G Loss: 0.6939865350723267, D Loss: 1.3861457109451294\n",
            "Epoch 6, Batch 367, G Loss: 0.6939902901649475, D Loss: 1.3860559463500977\n",
            "Epoch 6, Batch 368, G Loss: 0.6939910650253296, D Loss: 1.385874629020691\n",
            "Epoch 6, Batch 369, G Loss: 0.6939918398857117, D Loss: 1.3856284618377686\n",
            "Epoch 6, Batch 370, G Loss: 0.6939964890480042, D Loss: 1.3852016925811768\n",
            "Epoch 6, Batch 371, G Loss: 0.6940078735351562, D Loss: 1.3861079216003418\n",
            "Epoch 6, Batch 372, G Loss: 0.6940139532089233, D Loss: 1.3862782716751099\n",
            "Epoch 6, Batch 373, G Loss: 0.694000780582428, D Loss: 1.3866076469421387\n",
            "Epoch 6, Batch 374, G Loss: 0.6939681768417358, D Loss: 1.3861913681030273\n",
            "Epoch 6, Batch 375, G Loss: 0.6939260363578796, D Loss: 1.385974645614624\n",
            "Epoch 6, Batch 376, G Loss: 0.6938868761062622, D Loss: 1.3864058256149292\n",
            "Epoch 6, Batch 377, G Loss: 0.6938479542732239, D Loss: 1.3865858316421509\n",
            "Epoch 6, Batch 378, G Loss: 0.6938008666038513, D Loss: 1.3864343166351318\n",
            "Epoch 6, Batch 379, G Loss: 0.6937416791915894, D Loss: 1.3862576484680176\n",
            "Epoch 6, Batch 380, G Loss: 0.6936812400817871, D Loss: 1.38621187210083\n",
            "Epoch 6, Batch 381, G Loss: 0.6936101913452148, D Loss: 1.3862395286560059\n",
            "Epoch 6, Batch 382, G Loss: 0.693538248538971, D Loss: 1.3861362934112549\n",
            "Epoch 6, Batch 383, G Loss: 0.6934645771980286, D Loss: 1.3851207494735718\n",
            "Epoch 6, Batch 384, G Loss: 0.693419337272644, D Loss: 1.3857611417770386\n",
            "Epoch 6, Batch 385, G Loss: 0.6933761835098267, D Loss: 1.3860487937927246\n",
            "Epoch 6, Batch 386, G Loss: 0.6933434009552002, D Loss: 1.3857630491256714\n",
            "Epoch 6, Batch 387, G Loss: 0.6933125853538513, D Loss: 1.385913372039795\n",
            "Epoch 6, Batch 388, G Loss: 0.6932840943336487, D Loss: 1.3857839107513428\n",
            "Epoch 6, Batch 389, G Loss: 0.6932652592658997, D Loss: 1.3858013153076172\n",
            "Epoch 6, Batch 390, G Loss: 0.6932449340820312, D Loss: 1.3855477571487427\n",
            "Epoch 6, Batch 391, G Loss: 0.6932418346405029, D Loss: 1.3853850364685059\n",
            "Epoch 6, Batch 392, G Loss: 0.6932486891746521, D Loss: 1.3857829570770264\n",
            "Epoch 6, Batch 393, G Loss: 0.6932535171508789, D Loss: 1.3856323957443237\n",
            "Epoch 6, Batch 394, G Loss: 0.6932682394981384, D Loss: 1.3856663703918457\n",
            "Epoch 6, Batch 395, G Loss: 0.6932830214500427, D Loss: 1.385788083076477\n",
            "Epoch 6, Batch 396, G Loss: 0.6932978630065918, D Loss: 1.38555908203125\n",
            "Epoch 6, Batch 397, G Loss: 0.6933161616325378, D Loss: 1.3860265016555786\n",
            "Epoch 6, Batch 398, G Loss: 0.6933350563049316, D Loss: 1.3859353065490723\n",
            "Epoch 6, Batch 399, G Loss: 0.6933422684669495, D Loss: 1.3856780529022217\n",
            "Epoch 6, Batch 400, G Loss: 0.6933624744415283, D Loss: 1.3861808776855469\n",
            "Epoch 6, Batch 401, G Loss: 0.6933687925338745, D Loss: 1.3864061832427979\n",
            "Epoch 6, Batch 402, G Loss: 0.6933606266975403, D Loss: 1.3860013484954834\n",
            "Epoch 6, Batch 403, G Loss: 0.6933521628379822, D Loss: 1.3859500885009766\n",
            "Epoch 6, Batch 404, G Loss: 0.693341851234436, D Loss: 1.3858513832092285\n",
            "Epoch 6, Batch 405, G Loss: 0.6933346390724182, D Loss: 1.3855977058410645\n",
            "Epoch 6, Batch 406, G Loss: 0.6933373212814331, D Loss: 1.3856168985366821\n",
            "Epoch 6, Batch 407, G Loss: 0.6933455467224121, D Loss: 1.385326623916626\n",
            "Epoch 6, Batch 408, G Loss: 0.6933692097663879, D Loss: 1.3854265213012695\n",
            "Epoch 6, Batch 409, G Loss: 0.6933998465538025, D Loss: 1.385427713394165\n",
            "Epoch 6, Batch 410, G Loss: 0.6934386491775513, D Loss: 1.3860044479370117\n",
            "Epoch 6, Batch 411, G Loss: 0.6934661865234375, D Loss: 1.3859269618988037\n",
            "Epoch 6, Batch 412, G Loss: 0.6934892535209656, D Loss: 1.3855516910552979\n",
            "Epoch 6, Batch 413, G Loss: 0.6935167908668518, D Loss: 1.3855252265930176\n",
            "Epoch 6, Batch 414, G Loss: 0.6935508251190186, D Loss: 1.3860061168670654\n",
            "Epoch 6, Batch 415, G Loss: 0.693574845790863, D Loss: 1.3862779140472412\n",
            "Epoch 6, Batch 416, G Loss: 0.6935839056968689, D Loss: 1.385817527770996\n",
            "Epoch 6, Batch 417, G Loss: 0.6935924887657166, D Loss: 1.3856570720672607\n",
            "Epoch 6, Batch 418, G Loss: 0.6935994625091553, D Loss: 1.385663390159607\n",
            "Epoch 6, Batch 419, G Loss: 0.6936185359954834, D Loss: 1.3854358196258545\n",
            "Epoch 6, Batch 420, G Loss: 0.6936399340629578, D Loss: 1.3855748176574707\n",
            "Epoch 6, Batch 421, G Loss: 0.6936643719673157, D Loss: 1.3853520154953003\n",
            "Epoch 6, Batch 422, G Loss: 0.6936988830566406, D Loss: 1.3860031366348267\n",
            "Epoch 6, Batch 423, G Loss: 0.6937220692634583, D Loss: 1.3858171701431274\n",
            "Epoch 6, Batch 424, G Loss: 0.6937403082847595, D Loss: 1.3854305744171143\n",
            "Epoch 6, Batch 425, G Loss: 0.6937634944915771, D Loss: 1.385711431503296\n",
            "Epoch 6, Batch 426, G Loss: 0.6937914490699768, D Loss: 1.3861236572265625\n",
            "Epoch 6, Batch 427, G Loss: 0.6938024163246155, D Loss: 1.3861565589904785\n",
            "Epoch 6, Batch 428, G Loss: 0.6938043832778931, D Loss: 1.3860864639282227\n",
            "Epoch 6, Batch 429, G Loss: 0.6938014030456543, D Loss: 1.385803461074829\n",
            "Epoch 6, Batch 430, G Loss: 0.693793773651123, D Loss: 1.385960340499878\n",
            "Epoch 6, Batch 431, G Loss: 0.6937832832336426, D Loss: 1.385880947113037\n",
            "Epoch 6, Batch 432, G Loss: 0.6937741637229919, D Loss: 1.3859460353851318\n",
            "Epoch 6, Batch 433, G Loss: 0.6937631964683533, D Loss: 1.3855996131896973\n",
            "Epoch 6, Batch 434, G Loss: 0.6937645077705383, D Loss: 1.386018991470337\n",
            "Epoch 6, Batch 435, G Loss: 0.6937530636787415, D Loss: 1.3864976167678833\n",
            "Epoch 6, Batch 436, G Loss: 0.6937198042869568, D Loss: 1.3866441249847412\n",
            "Epoch 6, Batch 437, G Loss: 0.6936793327331543, D Loss: 1.3859117031097412\n",
            "Epoch 6, Batch 438, G Loss: 0.6936306357383728, D Loss: 1.3858821392059326\n",
            "Epoch 6, Batch 439, G Loss: 0.6935935616493225, D Loss: 1.3858517408370972\n",
            "Epoch 6, Batch 440, G Loss: 0.6935615539550781, D Loss: 1.3857040405273438\n",
            "Epoch 6, Batch 441, G Loss: 0.6935386061668396, D Loss: 1.3853278160095215\n",
            "Epoch 6, Batch 442, G Loss: 0.6935237050056458, D Loss: 1.3857007026672363\n",
            "Epoch 6, Batch 443, G Loss: 0.6935175657272339, D Loss: 1.3856303691864014\n",
            "Epoch 6, Batch 444, G Loss: 0.6935117244720459, D Loss: 1.386305332183838\n",
            "Epoch 6, Batch 445, G Loss: 0.693499743938446, D Loss: 1.3858799934387207\n",
            "Epoch 6, Batch 446, G Loss: 0.693482518196106, D Loss: 1.3851544857025146\n",
            "Epoch 6, Batch 447, G Loss: 0.693477213382721, D Loss: 1.385491132736206\n",
            "Epoch 6, Batch 448, G Loss: 0.6934853196144104, D Loss: 1.385831594467163\n",
            "Epoch 6, Batch 449, G Loss: 0.6934913992881775, D Loss: 1.3857561349868774\n",
            "Epoch 6, Batch 450, G Loss: 0.6934993267059326, D Loss: 1.385986089706421\n",
            "Epoch 6, Batch 451, G Loss: 0.6935026049613953, D Loss: 1.3854126930236816\n",
            "Epoch 6, Batch 452, G Loss: 0.693513810634613, D Loss: 1.3852052688598633\n",
            "Epoch 6, Batch 453, G Loss: 0.6935388445854187, D Loss: 1.3854551315307617\n",
            "Epoch 6, Batch 454, G Loss: 0.6935706734657288, D Loss: 1.3854987621307373\n",
            "Epoch 6, Batch 455, G Loss: 0.693604588508606, D Loss: 1.385452389717102\n",
            "Epoch 6, Batch 456, G Loss: 0.6936402320861816, D Loss: 1.386001706123352\n",
            "Epoch 6, Batch 457, G Loss: 0.693665623664856, D Loss: 1.3863517045974731\n",
            "Epoch 6, Batch 458, G Loss: 0.6936706900596619, D Loss: 1.3851261138916016\n",
            "Epoch 6, Batch 459, G Loss: 0.6936935186386108, D Loss: 1.384779930114746\n",
            "Epoch 6, Batch 460, G Loss: 0.6937392950057983, D Loss: 1.3856450319290161\n",
            "Epoch 6, Batch 461, G Loss: 0.6937814950942993, D Loss: 1.3858749866485596\n",
            "Epoch 6, Batch 462, G Loss: 0.6938163638114929, D Loss: 1.3855690956115723\n",
            "Epoch 6, Batch 463, G Loss: 0.6938532590866089, D Loss: 1.3858740329742432\n",
            "Epoch 6, Batch 464, G Loss: 0.693874180316925, D Loss: 1.3872650861740112\n",
            "Epoch 6, Batch 465, G Loss: 0.693861722946167, D Loss: 1.387953758239746\n",
            "Epoch 6, Batch 466, G Loss: 0.6937964558601379, D Loss: 1.3855119943618774\n",
            "Epoch 6, Batch 467, G Loss: 0.6937509775161743, D Loss: 1.3860082626342773\n",
            "Epoch 6, Batch 468, G Loss: 0.693713903427124, D Loss: 1.3863767385482788\n",
            "Epoch 6, Batch 469, G Loss: 0.6936604976654053, D Loss: 1.385159969329834\n",
            "Epoch 7, Batch 1, G Loss: 0.6936355829238892, D Loss: 1.3857349157333374\n",
            "Epoch 7, Batch 2, G Loss: 0.693609893321991, D Loss: 1.3857945203781128\n",
            "Epoch 7, Batch 3, G Loss: 0.693580150604248, D Loss: 1.3856136798858643\n",
            "Epoch 7, Batch 4, G Loss: 0.6935619711875916, D Loss: 1.3855483531951904\n",
            "Epoch 7, Batch 5, G Loss: 0.6935495138168335, D Loss: 1.3855831623077393\n",
            "Epoch 7, Batch 6, G Loss: 0.6935417056083679, D Loss: 1.386041522026062\n",
            "Epoch 7, Batch 7, G Loss: 0.6935262680053711, D Loss: 1.3859919309616089\n",
            "Epoch 7, Batch 8, G Loss: 0.6935082077980042, D Loss: 1.3857197761535645\n",
            "Epoch 7, Batch 9, G Loss: 0.6934968829154968, D Loss: 1.3852713108062744\n",
            "Epoch 7, Batch 10, G Loss: 0.6934987306594849, D Loss: 1.386519432067871\n",
            "Epoch 7, Batch 11, G Loss: 0.6934848427772522, D Loss: 1.38688325881958\n",
            "Epoch 7, Batch 12, G Loss: 0.6934450268745422, D Loss: 1.3856761455535889\n",
            "Epoch 7, Batch 13, G Loss: 0.6934142112731934, D Loss: 1.3855745792388916\n",
            "Epoch 7, Batch 14, G Loss: 0.6933955550193787, D Loss: 1.3855702877044678\n",
            "Epoch 7, Batch 15, G Loss: 0.6933867931365967, D Loss: 1.3857650756835938\n",
            "Epoch 7, Batch 16, G Loss: 0.6933811902999878, D Loss: 1.3864445686340332\n",
            "Epoch 7, Batch 17, G Loss: 0.6933634877204895, D Loss: 1.3860208988189697\n",
            "Epoch 7, Batch 18, G Loss: 0.6933385133743286, D Loss: 1.3859362602233887\n",
            "Epoch 7, Batch 19, G Loss: 0.6933140754699707, D Loss: 1.3857414722442627\n",
            "Epoch 7, Batch 20, G Loss: 0.6932960748672485, D Loss: 1.3859214782714844\n",
            "Epoch 7, Batch 21, G Loss: 0.6932766437530518, D Loss: 1.386044979095459\n",
            "Epoch 7, Batch 22, G Loss: 0.6932588815689087, D Loss: 1.385786771774292\n",
            "Epoch 7, Batch 23, G Loss: 0.6932380795478821, D Loss: 1.386056661605835\n",
            "Epoch 7, Batch 24, G Loss: 0.6932167410850525, D Loss: 1.3855551481246948\n",
            "Epoch 7, Batch 25, G Loss: 0.693204939365387, D Loss: 1.3854434490203857\n",
            "Epoch 7, Batch 26, G Loss: 0.6932054758071899, D Loss: 1.3854598999023438\n",
            "Epoch 7, Batch 27, G Loss: 0.6932212114334106, D Loss: 1.3856008052825928\n",
            "Epoch 7, Batch 28, G Loss: 0.6932432055473328, D Loss: 1.3855817317962646\n",
            "Epoch 7, Batch 29, G Loss: 0.6932682991027832, D Loss: 1.38625168800354\n",
            "Epoch 7, Batch 30, G Loss: 0.6932793259620667, D Loss: 1.3856947422027588\n",
            "Epoch 7, Batch 31, G Loss: 0.6932907104492188, D Loss: 1.3856284618377686\n",
            "Epoch 7, Batch 32, G Loss: 0.6933061480522156, D Loss: 1.3858609199523926\n",
            "Epoch 7, Batch 33, G Loss: 0.6933193802833557, D Loss: 1.3856288194656372\n",
            "Epoch 7, Batch 34, G Loss: 0.6933286786079407, D Loss: 1.3858169317245483\n",
            "Epoch 7, Batch 35, G Loss: 0.6933450698852539, D Loss: 1.38551926612854\n",
            "Epoch 7, Batch 36, G Loss: 0.6933671832084656, D Loss: 1.3861665725708008\n",
            "Epoch 7, Batch 37, G Loss: 0.6933730840682983, D Loss: 1.3864920139312744\n",
            "Epoch 7, Batch 38, G Loss: 0.6933629512786865, D Loss: 1.3861043453216553\n",
            "Epoch 7, Batch 39, G Loss: 0.6933506727218628, D Loss: 1.3858323097229004\n",
            "Epoch 7, Batch 40, G Loss: 0.6933391094207764, D Loss: 1.386025071144104\n",
            "Epoch 7, Batch 41, G Loss: 0.6933314204216003, D Loss: 1.3860841989517212\n",
            "Epoch 7, Batch 42, G Loss: 0.6933183670043945, D Loss: 1.3862403631210327\n",
            "Epoch 7, Batch 43, G Loss: 0.6932949423789978, D Loss: 1.3864494562149048\n",
            "Epoch 7, Batch 44, G Loss: 0.6932592391967773, D Loss: 1.3860998153686523\n",
            "Epoch 7, Batch 45, G Loss: 0.6932216286659241, D Loss: 1.3860046863555908\n",
            "Epoch 7, Batch 46, G Loss: 0.6931909322738647, D Loss: 1.3856611251831055\n",
            "Epoch 7, Batch 47, G Loss: 0.6931673884391785, D Loss: 1.3863003253936768\n",
            "Epoch 7, Batch 48, G Loss: 0.6931343674659729, D Loss: 1.3868145942687988\n",
            "Epoch 7, Batch 49, G Loss: 0.6930786371231079, D Loss: 1.386403203010559\n",
            "Epoch 7, Batch 50, G Loss: 0.6930175423622131, D Loss: 1.3862576484680176\n",
            "Epoch 7, Batch 51, G Loss: 0.6929466724395752, D Loss: 1.3859279155731201\n",
            "Epoch 7, Batch 52, G Loss: 0.692893385887146, D Loss: 1.385758399963379\n",
            "Epoch 7, Batch 53, G Loss: 0.6928555369377136, D Loss: 1.3851287364959717\n",
            "Epoch 7, Batch 54, G Loss: 0.6928418278694153, D Loss: 1.3855249881744385\n",
            "Epoch 7, Batch 55, G Loss: 0.6928439736366272, D Loss: 1.3860547542572021\n",
            "Epoch 7, Batch 56, G Loss: 0.6928417682647705, D Loss: 1.3854080438613892\n",
            "Epoch 7, Batch 57, G Loss: 0.6928514838218689, D Loss: 1.3853614330291748\n",
            "Epoch 7, Batch 58, G Loss: 0.6928772926330566, D Loss: 1.3850078582763672\n",
            "Epoch 7, Batch 59, G Loss: 0.6929163932800293, D Loss: 1.3853967189788818\n",
            "Epoch 7, Batch 60, G Loss: 0.6929728984832764, D Loss: 1.3855724334716797\n",
            "Epoch 7, Batch 61, G Loss: 0.6930277347564697, D Loss: 1.3856993913650513\n",
            "Epoch 7, Batch 62, G Loss: 0.6930804252624512, D Loss: 1.3862346410751343\n",
            "Epoch 7, Batch 63, G Loss: 0.6931169629096985, D Loss: 1.3863005638122559\n",
            "Epoch 7, Batch 64, G Loss: 0.6931396722793579, D Loss: 1.385676383972168\n",
            "Epoch 7, Batch 65, G Loss: 0.6931663751602173, D Loss: 1.3857910633087158\n",
            "Epoch 7, Batch 66, G Loss: 0.6931921243667603, D Loss: 1.3858627080917358\n",
            "Epoch 7, Batch 67, G Loss: 0.6932192444801331, D Loss: 1.3868441581726074\n",
            "Epoch 7, Batch 68, G Loss: 0.6932170391082764, D Loss: 1.3865453004837036\n",
            "Epoch 7, Batch 69, G Loss: 0.6931943297386169, D Loss: 1.3859440088272095\n",
            "Epoch 7, Batch 70, G Loss: 0.6931703686714172, D Loss: 1.385770320892334\n",
            "Epoch 7, Batch 71, G Loss: 0.6931605339050293, D Loss: 1.385744333267212\n",
            "Epoch 7, Batch 72, G Loss: 0.6931538581848145, D Loss: 1.3854506015777588\n",
            "Epoch 7, Batch 73, G Loss: 0.6931686401367188, D Loss: 1.3855173587799072\n",
            "Epoch 7, Batch 74, G Loss: 0.6931876540184021, D Loss: 1.386035442352295\n",
            "Epoch 7, Batch 75, G Loss: 0.6931968927383423, D Loss: 1.385871171951294\n",
            "Epoch 7, Batch 76, G Loss: 0.6932098865509033, D Loss: 1.3859407901763916\n",
            "Epoch 7, Batch 77, G Loss: 0.6932092905044556, D Loss: 1.3858418464660645\n",
            "Epoch 7, Batch 78, G Loss: 0.6932133436203003, D Loss: 1.3856985569000244\n",
            "Epoch 7, Batch 79, G Loss: 0.6932228207588196, D Loss: 1.3859115839004517\n",
            "Epoch 7, Batch 80, G Loss: 0.6932299733161926, D Loss: 1.3862190246582031\n",
            "Epoch 7, Batch 81, G Loss: 0.6932278275489807, D Loss: 1.3868539333343506\n",
            "Epoch 7, Batch 82, G Loss: 0.6931957602500916, D Loss: 1.3863203525543213\n",
            "Epoch 7, Batch 83, G Loss: 0.6931552290916443, D Loss: 1.386009693145752\n",
            "Epoch 7, Batch 84, G Loss: 0.6931115984916687, D Loss: 1.3855667114257812\n",
            "Epoch 7, Batch 85, G Loss: 0.693091869354248, D Loss: 1.3860199451446533\n",
            "Epoch 7, Batch 86, G Loss: 0.6930767893791199, D Loss: 1.386117696762085\n",
            "Epoch 7, Batch 87, G Loss: 0.6930544376373291, D Loss: 1.386070966720581\n",
            "Epoch 7, Batch 88, G Loss: 0.6930242776870728, D Loss: 1.3858983516693115\n",
            "Epoch 7, Batch 89, G Loss: 0.693000316619873, D Loss: 1.3857996463775635\n",
            "Epoch 7, Batch 90, G Loss: 0.6929799318313599, D Loss: 1.3857636451721191\n",
            "Epoch 7, Batch 91, G Loss: 0.6929628252983093, D Loss: 1.385758399963379\n",
            "Epoch 7, Batch 92, G Loss: 0.6929514408111572, D Loss: 1.3857741355895996\n",
            "Epoch 7, Batch 93, G Loss: 0.6929445862770081, D Loss: 1.3857909440994263\n",
            "Epoch 7, Batch 94, G Loss: 0.6929439902305603, D Loss: 1.3859941959381104\n",
            "Epoch 7, Batch 95, G Loss: 0.6929459571838379, D Loss: 1.3859989643096924\n",
            "Epoch 7, Batch 96, G Loss: 0.6929454207420349, D Loss: 1.3861284255981445\n",
            "Epoch 7, Batch 97, G Loss: 0.6929343938827515, D Loss: 1.3859944343566895\n",
            "Epoch 7, Batch 98, G Loss: 0.6929301619529724, D Loss: 1.3861157894134521\n",
            "Epoch 7, Batch 99, G Loss: 0.6929100155830383, D Loss: 1.386094093322754\n",
            "Epoch 7, Batch 100, G Loss: 0.6928897500038147, D Loss: 1.3857519626617432\n",
            "Epoch 7, Batch 101, G Loss: 0.6928802132606506, D Loss: 1.3856189250946045\n",
            "Epoch 7, Batch 102, G Loss: 0.6928756833076477, D Loss: 1.3863677978515625\n",
            "Epoch 7, Batch 103, G Loss: 0.6928616166114807, D Loss: 1.3863909244537354\n",
            "Epoch 7, Batch 104, G Loss: 0.6928313374519348, D Loss: 1.3860983848571777\n",
            "Epoch 7, Batch 105, G Loss: 0.6927998065948486, D Loss: 1.38600492477417\n",
            "Epoch 7, Batch 106, G Loss: 0.6927732229232788, D Loss: 1.3862669467926025\n",
            "Epoch 7, Batch 107, G Loss: 0.6927427053451538, D Loss: 1.3858001232147217\n",
            "Epoch 7, Batch 108, G Loss: 0.6927186846733093, D Loss: 1.38545560836792\n",
            "Epoch 7, Batch 109, G Loss: 0.6927177309989929, D Loss: 1.3857959508895874\n",
            "Epoch 7, Batch 110, G Loss: 0.6927188038825989, D Loss: 1.3859573602676392\n",
            "Epoch 7, Batch 111, G Loss: 0.6927136778831482, D Loss: 1.3863379955291748\n",
            "Epoch 7, Batch 112, G Loss: 0.6926957964897156, D Loss: 1.3861784934997559\n",
            "Epoch 7, Batch 113, G Loss: 0.692668080329895, D Loss: 1.3858685493469238\n",
            "Epoch 7, Batch 114, G Loss: 0.6926507949829102, D Loss: 1.3855667114257812\n",
            "Epoch 7, Batch 115, G Loss: 0.6926538944244385, D Loss: 1.3858380317687988\n",
            "Epoch 7, Batch 116, G Loss: 0.6926599740982056, D Loss: 1.3857866525650024\n",
            "Epoch 7, Batch 117, G Loss: 0.6926767826080322, D Loss: 1.3857614994049072\n",
            "Epoch 7, Batch 118, G Loss: 0.692695677280426, D Loss: 1.3856759071350098\n",
            "Epoch 7, Batch 119, G Loss: 0.692720353603363, D Loss: 1.3855654001235962\n",
            "Epoch 7, Batch 120, G Loss: 0.6927623152732849, D Loss: 1.385589361190796\n",
            "Epoch 7, Batch 121, G Loss: 0.6928103566169739, D Loss: 1.3855066299438477\n",
            "Epoch 7, Batch 122, G Loss: 0.6928655505180359, D Loss: 1.3856825828552246\n",
            "Epoch 7, Batch 123, G Loss: 0.6929234266281128, D Loss: 1.385719895362854\n",
            "Epoch 7, Batch 124, G Loss: 0.6929835081100464, D Loss: 1.385660171508789\n",
            "Epoch 7, Batch 125, G Loss: 0.6930420398712158, D Loss: 1.3858542442321777\n",
            "Epoch 7, Batch 126, G Loss: 0.6930981278419495, D Loss: 1.3860273361206055\n",
            "Epoch 7, Batch 127, G Loss: 0.6931375861167908, D Loss: 1.3860080242156982\n",
            "Epoch 7, Batch 128, G Loss: 0.6931711435317993, D Loss: 1.3858177661895752\n",
            "Epoch 7, Batch 129, G Loss: 0.6932057738304138, D Loss: 1.3857355117797852\n",
            "Epoch 7, Batch 130, G Loss: 0.6932370066642761, D Loss: 1.385699987411499\n",
            "Epoch 7, Batch 131, G Loss: 0.6932739019393921, D Loss: 1.3856501579284668\n",
            "Epoch 7, Batch 132, G Loss: 0.6933144330978394, D Loss: 1.3856146335601807\n",
            "Epoch 7, Batch 133, G Loss: 0.6933615803718567, D Loss: 1.3857064247131348\n",
            "Epoch 7, Batch 134, G Loss: 0.6934062242507935, D Loss: 1.3857783079147339\n",
            "Epoch 7, Batch 135, G Loss: 0.6934494972229004, D Loss: 1.3859808444976807\n",
            "Epoch 7, Batch 136, G Loss: 0.6934804916381836, D Loss: 1.3859087228775024\n",
            "Epoch 7, Batch 137, G Loss: 0.6935079097747803, D Loss: 1.3856675624847412\n",
            "Epoch 7, Batch 138, G Loss: 0.6935370564460754, D Loss: 1.3854402303695679\n",
            "Epoch 7, Batch 139, G Loss: 0.6935815811157227, D Loss: 1.385509729385376\n",
            "Epoch 7, Batch 140, G Loss: 0.6936267018318176, D Loss: 1.3854477405548096\n",
            "Epoch 7, Batch 141, G Loss: 0.6936787366867065, D Loss: 1.385797142982483\n",
            "Epoch 7, Batch 142, G Loss: 0.6937263607978821, D Loss: 1.3857401609420776\n",
            "Epoch 7, Batch 143, G Loss: 0.6937670707702637, D Loss: 1.3858294486999512\n",
            "Epoch 7, Batch 144, G Loss: 0.6938026547431946, D Loss: 1.3856234550476074\n",
            "Epoch 7, Batch 145, G Loss: 0.6938308477401733, D Loss: 1.3856940269470215\n",
            "Epoch 7, Batch 146, G Loss: 0.6938650012016296, D Loss: 1.3857674598693848\n",
            "Epoch 7, Batch 147, G Loss: 0.693897008895874, D Loss: 1.3856624364852905\n",
            "Epoch 7, Batch 148, G Loss: 0.6939244270324707, D Loss: 1.3857157230377197\n",
            "Epoch 7, Batch 149, G Loss: 0.6939521431922913, D Loss: 1.385515809059143\n",
            "Epoch 7, Batch 150, G Loss: 0.6939778923988342, D Loss: 1.3855220079421997\n",
            "Epoch 7, Batch 151, G Loss: 0.6940096020698547, D Loss: 1.3862390518188477\n",
            "Epoch 7, Batch 152, G Loss: 0.6940215826034546, D Loss: 1.3863234519958496\n",
            "Epoch 7, Batch 153, G Loss: 0.6940038204193115, D Loss: 1.3863368034362793\n",
            "Epoch 7, Batch 154, G Loss: 0.6939711570739746, D Loss: 1.3861937522888184\n",
            "Epoch 7, Batch 155, G Loss: 0.6939354538917542, D Loss: 1.386230230331421\n",
            "Epoch 7, Batch 156, G Loss: 0.6938863396644592, D Loss: 1.3861483335494995\n",
            "Epoch 7, Batch 157, G Loss: 0.6938300728797913, D Loss: 1.3860008716583252\n",
            "Epoch 7, Batch 158, G Loss: 0.6937701106071472, D Loss: 1.3859403133392334\n",
            "Epoch 7, Batch 159, G Loss: 0.6937121748924255, D Loss: 1.3856661319732666\n",
            "Epoch 7, Batch 160, G Loss: 0.6936643719673157, D Loss: 1.3858420848846436\n",
            "Epoch 7, Batch 161, G Loss: 0.6936175227165222, D Loss: 1.385939121246338\n",
            "Epoch 7, Batch 162, G Loss: 0.693567156791687, D Loss: 1.3860536813735962\n",
            "Epoch 7, Batch 163, G Loss: 0.6935166120529175, D Loss: 1.385786533355713\n",
            "Epoch 7, Batch 164, G Loss: 0.6934699416160583, D Loss: 1.3859264850616455\n",
            "Epoch 7, Batch 165, G Loss: 0.6934238076210022, D Loss: 1.3855631351470947\n",
            "Epoch 7, Batch 166, G Loss: 0.6933883428573608, D Loss: 1.3857240676879883\n",
            "Epoch 7, Batch 167, G Loss: 0.6933637857437134, D Loss: 1.3858473300933838\n",
            "Epoch 7, Batch 168, G Loss: 0.6933382749557495, D Loss: 1.3859680891036987\n",
            "Epoch 7, Batch 169, G Loss: 0.6933134198188782, D Loss: 1.3857275247573853\n",
            "Epoch 7, Batch 170, G Loss: 0.6932873129844666, D Loss: 1.3854012489318848\n",
            "Epoch 7, Batch 171, G Loss: 0.6932860612869263, D Loss: 1.3864003419876099\n",
            "Epoch 7, Batch 172, G Loss: 0.6932585835456848, D Loss: 1.38637113571167\n",
            "Epoch 7, Batch 173, G Loss: 0.6932166218757629, D Loss: 1.3857791423797607\n",
            "Epoch 7, Batch 174, G Loss: 0.6931807994842529, D Loss: 1.3857829570770264\n",
            "Epoch 7, Batch 175, G Loss: 0.6931546330451965, D Loss: 1.3857147693634033\n",
            "Epoch 7, Batch 176, G Loss: 0.6931347250938416, D Loss: 1.385742425918579\n",
            "Epoch 7, Batch 177, G Loss: 0.6931225657463074, D Loss: 1.3859772682189941\n",
            "Epoch 7, Batch 178, G Loss: 0.6931029558181763, D Loss: 1.386370062828064\n",
            "Epoch 7, Batch 179, G Loss: 0.6930645704269409, D Loss: 1.386082649230957\n",
            "Epoch 7, Batch 180, G Loss: 0.693016529083252, D Loss: 1.3861584663391113\n",
            "Epoch 7, Batch 181, G Loss: 0.6929656267166138, D Loss: 1.3861961364746094\n",
            "Epoch 7, Batch 182, G Loss: 0.6929108500480652, D Loss: 1.3859076499938965\n",
            "Epoch 7, Batch 183, G Loss: 0.6928632259368896, D Loss: 1.385972261428833\n",
            "Epoch 7, Batch 184, G Loss: 0.6928215622901917, D Loss: 1.3857700824737549\n",
            "Epoch 7, Batch 185, G Loss: 0.6927874684333801, D Loss: 1.3855721950531006\n",
            "Epoch 7, Batch 186, G Loss: 0.6927778124809265, D Loss: 1.385698676109314\n",
            "Epoch 7, Batch 187, G Loss: 0.692768931388855, D Loss: 1.3857886791229248\n",
            "Epoch 7, Batch 188, G Loss: 0.6927708983421326, D Loss: 1.3858637809753418\n",
            "Epoch 7, Batch 189, G Loss: 0.6927692890167236, D Loss: 1.3858245611190796\n",
            "Epoch 7, Batch 190, G Loss: 0.6927710771560669, D Loss: 1.3857886791229248\n",
            "Epoch 7, Batch 191, G Loss: 0.6927823424339294, D Loss: 1.3858096599578857\n",
            "Epoch 7, Batch 192, G Loss: 0.6927914023399353, D Loss: 1.3858191967010498\n",
            "Epoch 7, Batch 193, G Loss: 0.6928009986877441, D Loss: 1.3859950304031372\n",
            "Epoch 7, Batch 194, G Loss: 0.6928077340126038, D Loss: 1.385850191116333\n",
            "Epoch 7, Batch 195, G Loss: 0.692811906337738, D Loss: 1.3855338096618652\n",
            "Epoch 7, Batch 196, G Loss: 0.6928345561027527, D Loss: 1.3856863975524902\n",
            "Epoch 7, Batch 197, G Loss: 0.6928650736808777, D Loss: 1.3856902122497559\n",
            "Epoch 7, Batch 198, G Loss: 0.6928986310958862, D Loss: 1.38608717918396\n",
            "Epoch 7, Batch 199, G Loss: 0.6929234266281128, D Loss: 1.3862223625183105\n",
            "Epoch 7, Batch 200, G Loss: 0.6929242014884949, D Loss: 1.385871171951294\n",
            "Epoch 7, Batch 201, G Loss: 0.6929290294647217, D Loss: 1.3858592510223389\n",
            "Epoch 7, Batch 202, G Loss: 0.692925214767456, D Loss: 1.3855161666870117\n",
            "Epoch 7, Batch 203, G Loss: 0.6929472088813782, D Loss: 1.3854172229766846\n",
            "Epoch 7, Batch 204, G Loss: 0.6929888129234314, D Loss: 1.3859254121780396\n",
            "Epoch 7, Batch 205, G Loss: 0.6930219531059265, D Loss: 1.3857746124267578\n",
            "Epoch 7, Batch 206, G Loss: 0.6930574178695679, D Loss: 1.3856003284454346\n",
            "Epoch 7, Batch 207, G Loss: 0.6930934190750122, D Loss: 1.3855782747268677\n",
            "Epoch 7, Batch 208, G Loss: 0.693141520023346, D Loss: 1.385440707206726\n",
            "Epoch 7, Batch 209, G Loss: 0.6932013630867004, D Loss: 1.385822057723999\n",
            "Epoch 7, Batch 210, G Loss: 0.6932520270347595, D Loss: 1.3859379291534424\n",
            "Epoch 7, Batch 211, G Loss: 0.693295955657959, D Loss: 1.3856476545333862\n",
            "Epoch 7, Batch 212, G Loss: 0.6933428645133972, D Loss: 1.3858914375305176\n",
            "Epoch 7, Batch 213, G Loss: 0.6933809518814087, D Loss: 1.3859853744506836\n",
            "Epoch 7, Batch 214, G Loss: 0.6934020519256592, D Loss: 1.3858749866485596\n",
            "Epoch 7, Batch 215, G Loss: 0.6934199929237366, D Loss: 1.3857436180114746\n",
            "Epoch 7, Batch 216, G Loss: 0.6934415698051453, D Loss: 1.3856773376464844\n",
            "Epoch 7, Batch 217, G Loss: 0.6934695839881897, D Loss: 1.3855345249176025\n",
            "Epoch 7, Batch 218, G Loss: 0.6935040354728699, D Loss: 1.3855456113815308\n",
            "Epoch 7, Batch 219, G Loss: 0.693547248840332, D Loss: 1.3860584497451782\n",
            "Epoch 7, Batch 220, G Loss: 0.6935672163963318, D Loss: 1.3860598802566528\n",
            "Epoch 7, Batch 221, G Loss: 0.693574845790863, D Loss: 1.3861403465270996\n",
            "Epoch 7, Batch 222, G Loss: 0.6935614943504333, D Loss: 1.3861159086227417\n",
            "Epoch 7, Batch 223, G Loss: 0.6935345530509949, D Loss: 1.3861045837402344\n",
            "Epoch 7, Batch 224, G Loss: 0.6934987306594849, D Loss: 1.3859549760818481\n",
            "Epoch 7, Batch 225, G Loss: 0.6934608817100525, D Loss: 1.3858060836791992\n",
            "Epoch 7, Batch 226, G Loss: 0.6934326887130737, D Loss: 1.3859238624572754\n",
            "Epoch 7, Batch 227, G Loss: 0.6933969855308533, D Loss: 1.3859188556671143\n",
            "Epoch 7, Batch 228, G Loss: 0.6933563351631165, D Loss: 1.3854799270629883\n",
            "Epoch 7, Batch 229, G Loss: 0.6933372616767883, D Loss: 1.385901689529419\n",
            "Epoch 7, Batch 230, G Loss: 0.693317174911499, D Loss: 1.3861801624298096\n",
            "Epoch 7, Batch 231, G Loss: 0.6932754516601562, D Loss: 1.3858864307403564\n",
            "Epoch 7, Batch 232, G Loss: 0.6932411193847656, D Loss: 1.3856197595596313\n",
            "Epoch 7, Batch 233, G Loss: 0.6932195425033569, D Loss: 1.3855035305023193\n",
            "Epoch 7, Batch 234, G Loss: 0.6932229995727539, D Loss: 1.3855029344558716\n",
            "Epoch 7, Batch 235, G Loss: 0.6932364702224731, D Loss: 1.385530710220337\n",
            "Epoch 7, Batch 236, G Loss: 0.6932561993598938, D Loss: 1.385698914527893\n",
            "Epoch 7, Batch 237, G Loss: 0.693286657333374, D Loss: 1.3856613636016846\n",
            "Epoch 7, Batch 238, G Loss: 0.6933107376098633, D Loss: 1.3859057426452637\n",
            "Epoch 7, Batch 239, G Loss: 0.6933255791664124, D Loss: 1.3857901096343994\n",
            "Epoch 7, Batch 240, G Loss: 0.6933372616767883, D Loss: 1.385270118713379\n",
            "Epoch 7, Batch 241, G Loss: 0.6933739185333252, D Loss: 1.3855769634246826\n",
            "Epoch 7, Batch 242, G Loss: 0.6934188604354858, D Loss: 1.385669231414795\n",
            "Epoch 7, Batch 243, G Loss: 0.6934624314308167, D Loss: 1.3856310844421387\n",
            "Epoch 7, Batch 244, G Loss: 0.693510890007019, D Loss: 1.3855782747268677\n",
            "Epoch 7, Batch 245, G Loss: 0.6935579776763916, D Loss: 1.3856111764907837\n",
            "Epoch 7, Batch 246, G Loss: 0.6936073899269104, D Loss: 1.3855674266815186\n",
            "Epoch 7, Batch 247, G Loss: 0.6936556696891785, D Loss: 1.3854008913040161\n",
            "Epoch 7, Batch 248, G Loss: 0.6937159895896912, D Loss: 1.3854399919509888\n",
            "Epoch 7, Batch 249, G Loss: 0.693780243396759, D Loss: 1.3856453895568848\n",
            "Epoch 7, Batch 250, G Loss: 0.6938461065292358, D Loss: 1.3856710195541382\n",
            "Epoch 7, Batch 251, G Loss: 0.693906843662262, D Loss: 1.3859376907348633\n",
            "Epoch 7, Batch 252, G Loss: 0.6939442157745361, D Loss: 1.3857448101043701\n",
            "Epoch 7, Batch 253, G Loss: 0.6939770579338074, D Loss: 1.386047601699829\n",
            "Epoch 7, Batch 254, G Loss: 0.6939898729324341, D Loss: 1.3861768245697021\n",
            "Epoch 7, Batch 255, G Loss: 0.6939775347709656, D Loss: 1.385682225227356\n",
            "Epoch 7, Batch 256, G Loss: 0.6939736008644104, D Loss: 1.385467767715454\n",
            "Epoch 7, Batch 257, G Loss: 0.6939799785614014, D Loss: 1.3856130838394165\n",
            "Epoch 7, Batch 258, G Loss: 0.693993091583252, D Loss: 1.3859405517578125\n",
            "Epoch 7, Batch 259, G Loss: 0.6939936876296997, D Loss: 1.3859708309173584\n",
            "Epoch 7, Batch 260, G Loss: 0.6939829587936401, D Loss: 1.3858894109725952\n",
            "Epoch 7, Batch 261, G Loss: 0.693956196308136, D Loss: 1.3857187032699585\n",
            "Epoch 7, Batch 262, G Loss: 0.6939371228218079, D Loss: 1.3854976892471313\n",
            "Epoch 7, Batch 263, G Loss: 0.6939272284507751, D Loss: 1.385318398475647\n",
            "Epoch 7, Batch 264, G Loss: 0.6939247846603394, D Loss: 1.385777235031128\n",
            "Epoch 7, Batch 265, G Loss: 0.6939259767532349, D Loss: 1.386059045791626\n",
            "Epoch 7, Batch 266, G Loss: 0.6939082741737366, D Loss: 1.3855466842651367\n",
            "Epoch 7, Batch 267, G Loss: 0.6938982605934143, D Loss: 1.3857464790344238\n",
            "Epoch 7, Batch 268, G Loss: 0.6938817501068115, D Loss: 1.3855698108673096\n",
            "Epoch 7, Batch 269, G Loss: 0.6938797235488892, D Loss: 1.3859273195266724\n",
            "Epoch 7, Batch 270, G Loss: 0.693867564201355, D Loss: 1.3858678340911865\n",
            "Epoch 7, Batch 271, G Loss: 0.6938460469245911, D Loss: 1.3858404159545898\n",
            "Epoch 7, Batch 272, G Loss: 0.6938212513923645, D Loss: 1.3858433961868286\n",
            "Epoch 7, Batch 273, G Loss: 0.6937987804412842, D Loss: 1.3858219385147095\n",
            "Epoch 7, Batch 274, G Loss: 0.6937736868858337, D Loss: 1.3859570026397705\n",
            "Epoch 7, Batch 275, G Loss: 0.693733274936676, D Loss: 1.3856148719787598\n",
            "Epoch 7, Batch 276, G Loss: 0.6937041878700256, D Loss: 1.3855316638946533\n",
            "Epoch 7, Batch 277, G Loss: 0.6936803460121155, D Loss: 1.3859243392944336\n",
            "Epoch 7, Batch 278, G Loss: 0.6936551332473755, D Loss: 1.385838508605957\n",
            "Epoch 7, Batch 279, G Loss: 0.6936220526695251, D Loss: 1.3857126235961914\n",
            "Epoch 7, Batch 280, G Loss: 0.693591296672821, D Loss: 1.385718584060669\n",
            "Epoch 7, Batch 281, G Loss: 0.6935665607452393, D Loss: 1.385401964187622\n",
            "Epoch 7, Batch 282, G Loss: 0.6935632824897766, D Loss: 1.385176420211792\n",
            "Epoch 7, Batch 283, G Loss: 0.6935786008834839, D Loss: 1.3862637281417847\n",
            "Epoch 7, Batch 284, G Loss: 0.6935716867446899, D Loss: 1.3862520456314087\n",
            "Epoch 7, Batch 285, G Loss: 0.6935402750968933, D Loss: 1.3856844902038574\n",
            "Epoch 7, Batch 286, G Loss: 0.6935142874717712, D Loss: 1.386059045791626\n",
            "Epoch 7, Batch 287, G Loss: 0.6934763193130493, D Loss: 1.3864582777023315\n",
            "Epoch 7, Batch 288, G Loss: 0.6934140920639038, D Loss: 1.3861900568008423\n",
            "Epoch 7, Batch 289, G Loss: 0.6933431625366211, D Loss: 1.3858251571655273\n",
            "Epoch 7, Batch 290, G Loss: 0.6932845115661621, D Loss: 1.38596773147583\n",
            "Epoch 7, Batch 291, G Loss: 0.6932217478752136, D Loss: 1.3862369060516357\n",
            "Epoch 7, Batch 292, G Loss: 0.6931474804878235, D Loss: 1.3860411643981934\n",
            "Epoch 7, Batch 293, G Loss: 0.6930676102638245, D Loss: 1.3857414722442627\n",
            "Epoch 7, Batch 294, G Loss: 0.6929953694343567, D Loss: 1.3855230808258057\n",
            "Epoch 7, Batch 295, G Loss: 0.6929541826248169, D Loss: 1.3857085704803467\n",
            "Epoch 7, Batch 296, G Loss: 0.6929174661636353, D Loss: 1.385932207107544\n",
            "Epoch 7, Batch 297, G Loss: 0.6928810477256775, D Loss: 1.3856950998306274\n",
            "Epoch 7, Batch 298, G Loss: 0.6928569078445435, D Loss: 1.3856755495071411\n",
            "Epoch 7, Batch 299, G Loss: 0.6928431987762451, D Loss: 1.3858393430709839\n",
            "Epoch 7, Batch 300, G Loss: 0.6928243637084961, D Loss: 1.3856499195098877\n",
            "Epoch 7, Batch 301, G Loss: 0.6928178668022156, D Loss: 1.3855903148651123\n",
            "Epoch 7, Batch 302, G Loss: 0.6928237080574036, D Loss: 1.3859186172485352\n",
            "Epoch 7, Batch 303, G Loss: 0.6928244829177856, D Loss: 1.3857221603393555\n",
            "Epoch 7, Batch 304, G Loss: 0.6928315162658691, D Loss: 1.3858213424682617\n",
            "Epoch 7, Batch 305, G Loss: 0.6928351521492004, D Loss: 1.3856112957000732\n",
            "Epoch 7, Batch 306, G Loss: 0.6928457021713257, D Loss: 1.385986089706421\n",
            "Epoch 7, Batch 307, G Loss: 0.6928514838218689, D Loss: 1.386012077331543\n",
            "Epoch 7, Batch 308, G Loss: 0.6928449273109436, D Loss: 1.3859241008758545\n",
            "Epoch 7, Batch 309, G Loss: 0.6928390860557556, D Loss: 1.3859517574310303\n",
            "Epoch 7, Batch 310, G Loss: 0.6928254961967468, D Loss: 1.385951280593872\n",
            "Epoch 7, Batch 311, G Loss: 0.6928113102912903, D Loss: 1.3854589462280273\n",
            "Epoch 7, Batch 312, G Loss: 0.6928222179412842, D Loss: 1.3855254650115967\n",
            "Epoch 7, Batch 313, G Loss: 0.6928420066833496, D Loss: 1.3857978582382202\n",
            "Epoch 7, Batch 314, G Loss: 0.6928602457046509, D Loss: 1.3858431577682495\n",
            "Epoch 7, Batch 315, G Loss: 0.6928746700286865, D Loss: 1.3856730461120605\n",
            "Epoch 7, Batch 316, G Loss: 0.6928958296775818, D Loss: 1.385786533355713\n",
            "Epoch 7, Batch 317, G Loss: 0.6929150223731995, D Loss: 1.3860409259796143\n",
            "Epoch 7, Batch 318, G Loss: 0.6929201483726501, D Loss: 1.3857412338256836\n",
            "Epoch 7, Batch 319, G Loss: 0.6929271221160889, D Loss: 1.3855891227722168\n",
            "Epoch 7, Batch 320, G Loss: 0.6929478645324707, D Loss: 1.3856778144836426\n",
            "Epoch 7, Batch 321, G Loss: 0.692969799041748, D Loss: 1.3856523036956787\n",
            "Epoch 7, Batch 322, G Loss: 0.693001389503479, D Loss: 1.3857390880584717\n",
            "Epoch 7, Batch 323, G Loss: 0.6930288076400757, D Loss: 1.3857293128967285\n",
            "Epoch 7, Batch 324, G Loss: 0.6930612325668335, D Loss: 1.3857872486114502\n",
            "Epoch 7, Batch 325, G Loss: 0.6930866837501526, D Loss: 1.3857074975967407\n",
            "Epoch 7, Batch 326, G Loss: 0.6931130290031433, D Loss: 1.3855104446411133\n",
            "Epoch 7, Batch 327, G Loss: 0.6931582689285278, D Loss: 1.3853836059570312\n",
            "Epoch 7, Batch 328, G Loss: 0.6932204961776733, D Loss: 1.3858951330184937\n",
            "Epoch 7, Batch 329, G Loss: 0.6932701468467712, D Loss: 1.3860069513320923\n",
            "Epoch 7, Batch 330, G Loss: 0.69329434633255, D Loss: 1.3859121799468994\n",
            "Epoch 7, Batch 331, G Loss: 0.6933081746101379, D Loss: 1.3858110904693604\n",
            "Epoch 7, Batch 332, G Loss: 0.6933218240737915, D Loss: 1.385679006576538\n",
            "Epoch 7, Batch 333, G Loss: 0.6933348774909973, D Loss: 1.3858321905136108\n",
            "Epoch 7, Batch 334, G Loss: 0.6933449506759644, D Loss: 1.3860321044921875\n",
            "Epoch 7, Batch 335, G Loss: 0.6933391094207764, D Loss: 1.3859450817108154\n",
            "Epoch 7, Batch 336, G Loss: 0.6933249831199646, D Loss: 1.3855904340744019\n",
            "Epoch 7, Batch 337, G Loss: 0.6933265924453735, D Loss: 1.385576844215393\n",
            "Epoch 7, Batch 338, G Loss: 0.6933385133743286, D Loss: 1.3858954906463623\n",
            "Epoch 7, Batch 339, G Loss: 0.693341076374054, D Loss: 1.3857812881469727\n",
            "Epoch 7, Batch 340, G Loss: 0.6933472156524658, D Loss: 1.3858039379119873\n",
            "Epoch 7, Batch 341, G Loss: 0.6933375000953674, D Loss: 1.3857421875\n",
            "Epoch 7, Batch 342, G Loss: 0.6933331489562988, D Loss: 1.385798454284668\n",
            "Epoch 7, Batch 343, G Loss: 0.6933276653289795, D Loss: 1.385624885559082\n",
            "Epoch 7, Batch 344, G Loss: 0.6933311223983765, D Loss: 1.385517954826355\n",
            "Epoch 7, Batch 345, G Loss: 0.6933501958847046, D Loss: 1.3856472969055176\n",
            "Epoch 7, Batch 346, G Loss: 0.6933704018592834, D Loss: 1.3856945037841797\n",
            "Epoch 7, Batch 347, G Loss: 0.6933867931365967, D Loss: 1.385694980621338\n",
            "Epoch 7, Batch 348, G Loss: 0.6934050917625427, D Loss: 1.3857364654541016\n",
            "Epoch 7, Batch 349, G Loss: 0.6934208869934082, D Loss: 1.3857033252716064\n",
            "Epoch 7, Batch 350, G Loss: 0.6934381723403931, D Loss: 1.385614275932312\n",
            "Epoch 7, Batch 351, G Loss: 0.6934569478034973, D Loss: 1.3856258392333984\n",
            "Epoch 7, Batch 352, G Loss: 0.6934820413589478, D Loss: 1.3858083486557007\n",
            "Epoch 7, Batch 353, G Loss: 0.6935030817985535, D Loss: 1.3857998847961426\n",
            "Epoch 7, Batch 354, G Loss: 0.6935185194015503, D Loss: 1.3857612609863281\n",
            "Epoch 7, Batch 355, G Loss: 0.6935276389122009, D Loss: 1.3856397867202759\n",
            "Epoch 7, Batch 356, G Loss: 0.6935346722602844, D Loss: 1.3855817317962646\n",
            "Epoch 7, Batch 357, G Loss: 0.6935474872589111, D Loss: 1.385666847229004\n",
            "Epoch 7, Batch 358, G Loss: 0.6935608386993408, D Loss: 1.3857059478759766\n",
            "Epoch 7, Batch 359, G Loss: 0.6935742497444153, D Loss: 1.3855935335159302\n",
            "Epoch 7, Batch 360, G Loss: 0.6935913562774658, D Loss: 1.385347604751587\n",
            "Epoch 7, Batch 361, G Loss: 0.6936264038085938, D Loss: 1.385506510734558\n",
            "Epoch 7, Batch 362, G Loss: 0.6936681866645813, D Loss: 1.3856167793273926\n",
            "Epoch 7, Batch 363, G Loss: 0.6937101483345032, D Loss: 1.385768175125122\n",
            "Epoch 7, Batch 364, G Loss: 0.693738579750061, D Loss: 1.3857060670852661\n",
            "Epoch 7, Batch 365, G Loss: 0.6937599778175354, D Loss: 1.3858941793441772\n",
            "Epoch 7, Batch 366, G Loss: 0.693769633769989, D Loss: 1.3858973979949951\n",
            "Epoch 7, Batch 367, G Loss: 0.6937612295150757, D Loss: 1.3858534097671509\n",
            "Epoch 7, Batch 368, G Loss: 0.6937490701675415, D Loss: 1.3857669830322266\n",
            "Epoch 7, Batch 369, G Loss: 0.6937355399131775, D Loss: 1.3856474161148071\n",
            "Epoch 7, Batch 370, G Loss: 0.693737268447876, D Loss: 1.3854327201843262\n",
            "Epoch 7, Batch 371, G Loss: 0.6937530040740967, D Loss: 1.385871410369873\n",
            "Epoch 7, Batch 372, G Loss: 0.693755030632019, D Loss: 1.3859663009643555\n",
            "Epoch 7, Batch 373, G Loss: 0.6937372088432312, D Loss: 1.3861310482025146\n",
            "Epoch 7, Batch 374, G Loss: 0.6936970949172974, D Loss: 1.3859341144561768\n",
            "Epoch 7, Batch 375, G Loss: 0.6936486959457397, D Loss: 1.3858277797698975\n",
            "Epoch 7, Batch 376, G Loss: 0.6935949325561523, D Loss: 1.38604736328125\n",
            "Epoch 7, Batch 377, G Loss: 0.6935360431671143, D Loss: 1.3861275911331177\n",
            "Epoch 7, Batch 378, G Loss: 0.6934638619422913, D Loss: 1.386038899421692\n",
            "Epoch 7, Batch 379, G Loss: 0.6933860778808594, D Loss: 1.3859565258026123\n",
            "Epoch 7, Batch 380, G Loss: 0.6933084726333618, D Loss: 1.385911226272583\n",
            "Epoch 7, Batch 381, G Loss: 0.6932250261306763, D Loss: 1.3859140872955322\n",
            "Epoch 7, Batch 382, G Loss: 0.6931483745574951, D Loss: 1.3858835697174072\n",
            "Epoch 7, Batch 383, G Loss: 0.6930712461471558, D Loss: 1.3854498863220215\n",
            "Epoch 7, Batch 384, G Loss: 0.6930238604545593, D Loss: 1.3856942653656006\n",
            "Epoch 7, Batch 385, G Loss: 0.692982017993927, D Loss: 1.3858107328414917\n",
            "Epoch 7, Batch 386, G Loss: 0.6929416060447693, D Loss: 1.3857108354568481\n",
            "Epoch 7, Batch 387, G Loss: 0.6929080486297607, D Loss: 1.3857548236846924\n",
            "Epoch 7, Batch 388, G Loss: 0.6928748488426208, D Loss: 1.385706901550293\n",
            "Epoch 7, Batch 389, G Loss: 0.6928510665893555, D Loss: 1.3857052326202393\n",
            "Epoch 7, Batch 390, G Loss: 0.6928324699401855, D Loss: 1.3856322765350342\n",
            "Epoch 7, Batch 391, G Loss: 0.6928300857543945, D Loss: 1.385577917098999\n",
            "Epoch 7, Batch 392, G Loss: 0.6928415298461914, D Loss: 1.3857383728027344\n",
            "Epoch 7, Batch 393, G Loss: 0.6928545832633972, D Loss: 1.385648250579834\n",
            "Epoch 7, Batch 394, G Loss: 0.6928725242614746, D Loss: 1.385657548904419\n",
            "Epoch 7, Batch 395, G Loss: 0.692893385887146, D Loss: 1.385709285736084\n",
            "Epoch 7, Batch 396, G Loss: 0.6929138898849487, D Loss: 1.3856452703475952\n",
            "Epoch 7, Batch 397, G Loss: 0.6929414868354797, D Loss: 1.3858153820037842\n",
            "Epoch 7, Batch 398, G Loss: 0.6929664611816406, D Loss: 1.3858160972595215\n",
            "Epoch 7, Batch 399, G Loss: 0.6929877400398254, D Loss: 1.3856785297393799\n",
            "Epoch 7, Batch 400, G Loss: 0.693011999130249, D Loss: 1.3858784437179565\n",
            "Epoch 7, Batch 401, G Loss: 0.6930283308029175, D Loss: 1.3859710693359375\n",
            "Epoch 7, Batch 402, G Loss: 0.6930241584777832, D Loss: 1.3858039379119873\n",
            "Epoch 7, Batch 403, G Loss: 0.6930162310600281, D Loss: 1.3857823610305786\n",
            "Epoch 7, Batch 404, G Loss: 0.6930121779441833, D Loss: 1.3857172727584839\n",
            "Epoch 7, Batch 405, G Loss: 0.6930135488510132, D Loss: 1.3856464624404907\n",
            "Epoch 7, Batch 406, G Loss: 0.6930185556411743, D Loss: 1.3856532573699951\n",
            "Epoch 7, Batch 407, G Loss: 0.6930343508720398, D Loss: 1.3855481147766113\n",
            "Epoch 7, Batch 408, G Loss: 0.693060576915741, D Loss: 1.3855960369110107\n",
            "Epoch 7, Batch 409, G Loss: 0.6930966973304749, D Loss: 1.3856089115142822\n",
            "Epoch 7, Batch 410, G Loss: 0.6931411027908325, D Loss: 1.3858098983764648\n",
            "Epoch 7, Batch 411, G Loss: 0.6931756734848022, D Loss: 1.385777473449707\n",
            "Epoch 7, Batch 412, G Loss: 0.6932019591331482, D Loss: 1.385650873184204\n",
            "Epoch 7, Batch 413, G Loss: 0.6932302713394165, D Loss: 1.385641098022461\n",
            "Epoch 7, Batch 414, G Loss: 0.6932668685913086, D Loss: 1.3857882022857666\n",
            "Epoch 7, Batch 415, G Loss: 0.6932926774024963, D Loss: 1.385888695716858\n",
            "Epoch 7, Batch 416, G Loss: 0.6933046579360962, D Loss: 1.3857438564300537\n",
            "Epoch 7, Batch 417, G Loss: 0.6933174729347229, D Loss: 1.3856747150421143\n",
            "Epoch 7, Batch 418, G Loss: 0.6933357119560242, D Loss: 1.3856881856918335\n",
            "Epoch 7, Batch 419, G Loss: 0.6933560967445374, D Loss: 1.3856086730957031\n",
            "Epoch 7, Batch 420, G Loss: 0.6933833360671997, D Loss: 1.3856174945831299\n",
            "Epoch 7, Batch 421, G Loss: 0.693412184715271, D Loss: 1.3855640888214111\n",
            "Epoch 7, Batch 422, G Loss: 0.6934487223625183, D Loss: 1.3857656717300415\n",
            "Epoch 7, Batch 423, G Loss: 0.6934723854064941, D Loss: 1.3857293128967285\n",
            "Epoch 7, Batch 424, G Loss: 0.6934890747070312, D Loss: 1.3856074810028076\n",
            "Epoch 7, Batch 425, G Loss: 0.6935122609138489, D Loss: 1.3857005834579468\n",
            "Epoch 7, Batch 426, G Loss: 0.6935375928878784, D Loss: 1.3858528137207031\n",
            "Epoch 7, Batch 427, G Loss: 0.6935529708862305, D Loss: 1.3858715295791626\n",
            "Epoch 7, Batch 428, G Loss: 0.6935514211654663, D Loss: 1.3858633041381836\n",
            "Epoch 7, Batch 429, G Loss: 0.6935414671897888, D Loss: 1.3857415914535522\n",
            "Epoch 7, Batch 430, G Loss: 0.693530797958374, D Loss: 1.3857755661010742\n",
            "Epoch 7, Batch 431, G Loss: 0.693513810634613, D Loss: 1.3857662677764893\n",
            "Epoch 7, Batch 432, G Loss: 0.6934999227523804, D Loss: 1.3857848644256592\n",
            "Epoch 7, Batch 433, G Loss: 0.693489670753479, D Loss: 1.3856868743896484\n",
            "Epoch 7, Batch 434, G Loss: 0.6934812664985657, D Loss: 1.38582444190979\n",
            "Epoch 7, Batch 435, G Loss: 0.6934616565704346, D Loss: 1.3859890699386597\n",
            "Epoch 7, Batch 436, G Loss: 0.6934263110160828, D Loss: 1.38601815700531\n",
            "Epoch 7, Batch 437, G Loss: 0.6933693885803223, D Loss: 1.385765790939331\n",
            "Epoch 7, Batch 438, G Loss: 0.6933133602142334, D Loss: 1.3857753276824951\n",
            "Epoch 7, Batch 439, G Loss: 0.6932704448699951, D Loss: 1.3857585191726685\n",
            "Epoch 7, Batch 440, G Loss: 0.6932312250137329, D Loss: 1.3857004642486572\n",
            "Epoch 7, Batch 441, G Loss: 0.6932002305984497, D Loss: 1.3856172561645508\n",
            "Epoch 7, Batch 442, G Loss: 0.6931913495063782, D Loss: 1.3856942653656006\n",
            "Epoch 7, Batch 443, G Loss: 0.6931848526000977, D Loss: 1.3856871128082275\n",
            "Epoch 7, Batch 444, G Loss: 0.6931834816932678, D Loss: 1.385891318321228\n",
            "Epoch 7, Batch 445, G Loss: 0.6931622624397278, D Loss: 1.385772705078125\n",
            "Epoch 7, Batch 446, G Loss: 0.6931411027908325, D Loss: 1.3855465650558472\n",
            "Epoch 7, Batch 447, G Loss: 0.6931379437446594, D Loss: 1.3856348991394043\n",
            "Epoch 7, Batch 448, G Loss: 0.6931431293487549, D Loss: 1.3857231140136719\n",
            "Epoch 7, Batch 449, G Loss: 0.6931511759757996, D Loss: 1.3857226371765137\n",
            "Epoch 7, Batch 450, G Loss: 0.6931607127189636, D Loss: 1.385751724243164\n",
            "Epoch 7, Batch 451, G Loss: 0.693164587020874, D Loss: 1.3856160640716553\n",
            "Epoch 7, Batch 452, G Loss: 0.693178117275238, D Loss: 1.385568380355835\n",
            "Epoch 7, Batch 453, G Loss: 0.6932085156440735, D Loss: 1.3856265544891357\n",
            "Epoch 7, Batch 454, G Loss: 0.6932427287101746, D Loss: 1.3856439590454102\n",
            "Epoch 7, Batch 455, G Loss: 0.6932788491249084, D Loss: 1.3856061697006226\n",
            "Epoch 7, Batch 456, G Loss: 0.6933179497718811, D Loss: 1.385778784751892\n",
            "Epoch 7, Batch 457, G Loss: 0.6933456659317017, D Loss: 1.385890245437622\n",
            "Epoch 7, Batch 458, G Loss: 0.6933560371398926, D Loss: 1.3855445384979248\n",
            "Epoch 7, Batch 459, G Loss: 0.6933777332305908, D Loss: 1.3854382038116455\n",
            "Epoch 7, Batch 460, G Loss: 0.6934278011322021, D Loss: 1.3856335878372192\n",
            "Epoch 7, Batch 461, G Loss: 0.6934710144996643, D Loss: 1.38575279712677\n",
            "Epoch 7, Batch 462, G Loss: 0.693504810333252, D Loss: 1.385634183883667\n",
            "Epoch 7, Batch 463, G Loss: 0.693535327911377, D Loss: 1.3857841491699219\n",
            "Epoch 7, Batch 464, G Loss: 0.6935613751411438, D Loss: 1.3861825466156006\n",
            "Epoch 7, Batch 465, G Loss: 0.6935429573059082, D Loss: 1.3864409923553467\n",
            "Epoch 7, Batch 466, G Loss: 0.6934720873832703, D Loss: 1.3856451511383057\n",
            "Epoch 7, Batch 467, G Loss: 0.6934283375740051, D Loss: 1.3858156204223633\n",
            "Epoch 7, Batch 468, G Loss: 0.6933866143226624, D Loss: 1.385934829711914\n",
            "Epoch 7, Batch 469, G Loss: 0.693330705165863, D Loss: 1.385532259941101\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "key = train_GAN( 128 , 7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ryCMeRf0Bx5"
      },
      "outputs": [],
      "source": [
        "show_result(x_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SjSCIKYPXcY6",
        "outputId": "6aaa6de9-b240-4c0f-f233-31a32051e847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 1, Batch 258, G Loss: 0.6871870160102844, D Loss: 1.386091709136963\n",
            "Epoch 1, Batch 259, G Loss: 0.6869339942932129, D Loss: 1.3864376544952393\n",
            "Epoch 1, Batch 260, G Loss: 0.6869618892669678, D Loss: 1.3861730098724365\n",
            "Epoch 1, Batch 261, G Loss: 0.6874763369560242, D Loss: 1.385347843170166\n",
            "Epoch 1, Batch 262, G Loss: 0.6872107982635498, D Loss: 1.3858940601348877\n",
            "Epoch 1, Batch 263, G Loss: 0.6875807046890259, D Loss: 1.3856987953186035\n",
            "Epoch 1, Batch 264, G Loss: 0.6869651675224304, D Loss: 1.3858742713928223\n",
            "Epoch 1, Batch 265, G Loss: 0.6874104738235474, D Loss: 1.3853614330291748\n",
            "Epoch 1, Batch 266, G Loss: 0.6872320175170898, D Loss: 1.3857641220092773\n",
            "Epoch 1, Batch 267, G Loss: 0.6875940561294556, D Loss: 1.3850884437561035\n",
            "Epoch 1, Batch 268, G Loss: 0.6874316334724426, D Loss: 1.3858890533447266\n",
            "Epoch 1, Batch 269, G Loss: 0.6872687935829163, D Loss: 1.3862589597702026\n",
            "Epoch 1, Batch 270, G Loss: 0.6872411966323853, D Loss: 1.3859792947769165\n",
            "Epoch 1, Batch 271, G Loss: 0.6873552203178406, D Loss: 1.3862686157226562\n",
            "Epoch 1, Batch 272, G Loss: 0.6868873238563538, D Loss: 1.3858048915863037\n",
            "Epoch 1, Batch 273, G Loss: 0.687443196773529, D Loss: 1.3851618766784668\n",
            "Epoch 1, Batch 274, G Loss: 0.687161922454834, D Loss: 1.3855609893798828\n",
            "Epoch 1, Batch 275, G Loss: 0.687139630317688, D Loss: 1.3852527141571045\n",
            "Epoch 1, Batch 276, G Loss: 0.6873161792755127, D Loss: 1.3850889205932617\n",
            "Epoch 1, Batch 277, G Loss: 0.6879555583000183, D Loss: 1.3844130039215088\n",
            "Epoch 1, Batch 278, G Loss: 0.6873174905776978, D Loss: 1.3849400281906128\n",
            "Epoch 1, Batch 279, G Loss: 0.6872668862342834, D Loss: 1.3848507404327393\n",
            "Epoch 1, Batch 280, G Loss: 0.687439501285553, D Loss: 1.384705901145935\n",
            "Epoch 1, Batch 281, G Loss: 0.6879026293754578, D Loss: 1.3847098350524902\n",
            "Epoch 1, Batch 282, G Loss: 0.6877011060714722, D Loss: 1.3846982717514038\n",
            "Epoch 1, Batch 283, G Loss: 0.6878086924552917, D Loss: 1.3844373226165771\n",
            "Epoch 1, Batch 284, G Loss: 0.6875239610671997, D Loss: 1.3848977088928223\n",
            "Epoch 1, Batch 285, G Loss: 0.6875108480453491, D Loss: 1.384885549545288\n",
            "Epoch 1, Batch 286, G Loss: 0.6880089044570923, D Loss: 1.3842412233352661\n",
            "Epoch 1, Batch 287, G Loss: 0.6877583265304565, D Loss: 1.3843016624450684\n",
            "Epoch 1, Batch 288, G Loss: 0.6878615617752075, D Loss: 1.384340524673462\n",
            "Epoch 1, Batch 289, G Loss: 0.6877848505973816, D Loss: 1.3842686414718628\n",
            "Epoch 1, Batch 290, G Loss: 0.688130259513855, D Loss: 1.3840787410736084\n",
            "Epoch 1, Batch 291, G Loss: 0.6880767345428467, D Loss: 1.3841769695281982\n",
            "Epoch 1, Batch 292, G Loss: 0.6880266070365906, D Loss: 1.3842064142227173\n",
            "Epoch 1, Batch 293, G Loss: 0.6879705190658569, D Loss: 1.3841159343719482\n",
            "Epoch 1, Batch 294, G Loss: 0.6880165934562683, D Loss: 1.3839645385742188\n",
            "Epoch 1, Batch 295, G Loss: 0.6887468695640564, D Loss: 1.3832013607025146\n",
            "Epoch 1, Batch 296, G Loss: 0.6881330609321594, D Loss: 1.3838943243026733\n",
            "Epoch 1, Batch 297, G Loss: 0.6883979439735413, D Loss: 1.3835115432739258\n",
            "Epoch 1, Batch 298, G Loss: 0.6883208751678467, D Loss: 1.383448600769043\n",
            "Epoch 1, Batch 299, G Loss: 0.6884697675704956, D Loss: 1.3833401203155518\n",
            "Epoch 1, Batch 300, G Loss: 0.688409149646759, D Loss: 1.3833489418029785\n",
            "Epoch 1, Batch 301, G Loss: 0.6887949705123901, D Loss: 1.3831913471221924\n",
            "Epoch 1, Batch 302, G Loss: 0.688890278339386, D Loss: 1.3828835487365723\n",
            "Epoch 1, Batch 303, G Loss: 0.6884058713912964, D Loss: 1.3834588527679443\n",
            "Epoch 1, Batch 304, G Loss: 0.6888160109519958, D Loss: 1.3828359842300415\n",
            "Epoch 1, Batch 305, G Loss: 0.6888809204101562, D Loss: 1.3827273845672607\n",
            "Epoch 1, Batch 306, G Loss: 0.6890550851821899, D Loss: 1.382493495941162\n",
            "Epoch 1, Batch 307, G Loss: 0.6886578798294067, D Loss: 1.3827736377716064\n",
            "Epoch 1, Batch 308, G Loss: 0.6888407468795776, D Loss: 1.3825879096984863\n",
            "Epoch 1, Batch 309, G Loss: 0.688898503780365, D Loss: 1.3824608325958252\n",
            "Epoch 1, Batch 310, G Loss: 0.6890841722488403, D Loss: 1.382209300994873\n",
            "Epoch 1, Batch 311, G Loss: 0.6895047426223755, D Loss: 1.3817260265350342\n",
            "Epoch 1, Batch 312, G Loss: 0.6888847351074219, D Loss: 1.3822427988052368\n",
            "Epoch 1, Batch 313, G Loss: 0.6891888976097107, D Loss: 1.3819444179534912\n",
            "Epoch 1, Batch 314, G Loss: 0.6890737414360046, D Loss: 1.3819559812545776\n",
            "Epoch 1, Batch 315, G Loss: 0.6892049312591553, D Loss: 1.3817651271820068\n",
            "Epoch 1, Batch 316, G Loss: 0.6891776919364929, D Loss: 1.3817989826202393\n",
            "Epoch 1, Batch 317, G Loss: 0.6892364025115967, D Loss: 1.3816874027252197\n",
            "Epoch 1, Batch 318, G Loss: 0.6888552904129028, D Loss: 1.3821114301681519\n",
            "Epoch 1, Batch 319, G Loss: 0.6894901990890503, D Loss: 1.3812150955200195\n",
            "Epoch 1, Batch 320, G Loss: 0.6894610524177551, D Loss: 1.3813014030456543\n",
            "Epoch 1, Batch 321, G Loss: 0.6896039247512817, D Loss: 1.381004810333252\n",
            "Epoch 1, Batch 322, G Loss: 0.6897379755973816, D Loss: 1.3808403015136719\n",
            "Epoch 1, Batch 323, G Loss: 0.6898484230041504, D Loss: 1.3804906606674194\n",
            "Epoch 1, Batch 324, G Loss: 0.6896818280220032, D Loss: 1.3806982040405273\n",
            "Epoch 1, Batch 325, G Loss: 0.6894752979278564, D Loss: 1.3810820579528809\n",
            "Epoch 1, Batch 326, G Loss: 0.6902263164520264, D Loss: 1.3801072835922241\n",
            "Epoch 1, Batch 327, G Loss: 0.6899192333221436, D Loss: 1.3804861307144165\n",
            "Epoch 1, Batch 328, G Loss: 0.6897178292274475, D Loss: 1.3803434371948242\n",
            "Epoch 1, Batch 329, G Loss: 0.690185546875, D Loss: 1.3801233768463135\n",
            "Epoch 1, Batch 330, G Loss: 0.6899998188018799, D Loss: 1.3805729150772095\n",
            "Epoch 1, Batch 331, G Loss: 0.6904131770133972, D Loss: 1.3798487186431885\n",
            "Epoch 1, Batch 332, G Loss: 0.6903534531593323, D Loss: 1.3798034191131592\n",
            "Epoch 1, Batch 333, G Loss: 0.6901754140853882, D Loss: 1.3797004222869873\n",
            "Epoch 1, Batch 334, G Loss: 0.6901023387908936, D Loss: 1.3799911737442017\n",
            "Epoch 1, Batch 335, G Loss: 0.6903133988380432, D Loss: 1.379455804824829\n",
            "Epoch 1, Batch 336, G Loss: 0.6904754638671875, D Loss: 1.3791699409484863\n",
            "Epoch 1, Batch 337, G Loss: 0.6906968951225281, D Loss: 1.3786956071853638\n",
            "Epoch 1, Batch 338, G Loss: 0.690586507320404, D Loss: 1.3798389434814453\n",
            "Epoch 1, Batch 339, G Loss: 0.6904709339141846, D Loss: 1.379607915878296\n",
            "Epoch 1, Batch 340, G Loss: 0.6907549500465393, D Loss: 1.3797414302825928\n",
            "Epoch 1, Batch 341, G Loss: 0.6905214786529541, D Loss: 1.3781020641326904\n",
            "Epoch 1, Batch 342, G Loss: 0.690919041633606, D Loss: 1.3778667449951172\n",
            "Epoch 1, Batch 343, G Loss: 0.6911224722862244, D Loss: 1.3774057626724243\n",
            "Epoch 1, Batch 344, G Loss: 0.691188395023346, D Loss: 1.3774834871292114\n",
            "Epoch 1, Batch 345, G Loss: 0.6912646293640137, D Loss: 1.378040075302124\n",
            "Epoch 1, Batch 346, G Loss: 0.6911633610725403, D Loss: 1.3783645629882812\n",
            "Epoch 1, Batch 347, G Loss: 0.6912211179733276, D Loss: 1.37819242477417\n",
            "Epoch 1, Batch 348, G Loss: 0.6913018226623535, D Loss: 1.3778767585754395\n",
            "Epoch 1, Batch 349, G Loss: 0.6913865804672241, D Loss: 1.3781533241271973\n",
            "Epoch 1, Batch 350, G Loss: 0.6912099123001099, D Loss: 1.3779079914093018\n",
            "Epoch 1, Batch 351, G Loss: 0.6912422180175781, D Loss: 1.3778706789016724\n",
            "Epoch 1, Batch 352, G Loss: 0.6916642189025879, D Loss: 1.3775570392608643\n",
            "Epoch 1, Batch 353, G Loss: 0.6914259195327759, D Loss: 1.3772791624069214\n",
            "Epoch 1, Batch 354, G Loss: 0.6917444467544556, D Loss: 1.376677393913269\n",
            "Epoch 1, Batch 355, G Loss: 0.6917116641998291, D Loss: 1.3757750988006592\n",
            "Epoch 1, Batch 356, G Loss: 0.6915109753608704, D Loss: 1.37614107131958\n",
            "Epoch 1, Batch 357, G Loss: 0.6918026804924011, D Loss: 1.3764727115631104\n",
            "Epoch 1, Batch 358, G Loss: 0.6916061043739319, D Loss: 1.3762460947036743\n",
            "Epoch 1, Batch 359, G Loss: 0.6919926404953003, D Loss: 1.3756184577941895\n",
            "Epoch 1, Batch 360, G Loss: 0.692193865776062, D Loss: 1.3756136894226074\n",
            "Epoch 1, Batch 361, G Loss: 0.6919891834259033, D Loss: 1.3750618696212769\n",
            "Epoch 1, Batch 362, G Loss: 0.6919593214988708, D Loss: 1.3758752346038818\n",
            "Epoch 1, Batch 363, G Loss: 0.6923918724060059, D Loss: 1.3758132457733154\n",
            "Epoch 1, Batch 364, G Loss: 0.6921728253364563, D Loss: 1.3758001327514648\n",
            "Epoch 1, Batch 365, G Loss: 0.6919459700584412, D Loss: 1.3758280277252197\n",
            "Epoch 1, Batch 366, G Loss: 0.6925861835479736, D Loss: 1.375157117843628\n",
            "Epoch 1, Batch 367, G Loss: 0.6925057172775269, D Loss: 1.3756327629089355\n",
            "Epoch 1, Batch 368, G Loss: 0.6925461292266846, D Loss: 1.375794529914856\n",
            "Epoch 1, Batch 369, G Loss: 0.692632794380188, D Loss: 1.375838041305542\n",
            "Epoch 1, Batch 370, G Loss: 0.6926820874214172, D Loss: 1.3763096332550049\n",
            "Epoch 1, Batch 371, G Loss: 0.6928360462188721, D Loss: 1.375990867614746\n",
            "Epoch 1, Batch 372, G Loss: 0.6927010416984558, D Loss: 1.374834656715393\n",
            "Epoch 1, Batch 373, G Loss: 0.692943811416626, D Loss: 1.3746132850646973\n",
            "Epoch 1, Batch 374, G Loss: 0.6930616497993469, D Loss: 1.3749737739562988\n",
            "Epoch 1, Batch 375, G Loss: 0.6928138732910156, D Loss: 1.3747962713241577\n",
            "Epoch 1, Batch 376, G Loss: 0.6933038830757141, D Loss: 1.3740516901016235\n",
            "Epoch 1, Batch 377, G Loss: 0.6932098865509033, D Loss: 1.3741388320922852\n",
            "Epoch 1, Batch 378, G Loss: 0.6933146715164185, D Loss: 1.3744860887527466\n",
            "Epoch 1, Batch 379, G Loss: 0.6933022141456604, D Loss: 1.3743880987167358\n",
            "Epoch 1, Batch 380, G Loss: 0.6935089826583862, D Loss: 1.374006748199463\n",
            "Epoch 1, Batch 381, G Loss: 0.69345623254776, D Loss: 1.3749003410339355\n",
            "Epoch 1, Batch 382, G Loss: 0.6935331225395203, D Loss: 1.3728814125061035\n",
            "Epoch 1, Batch 383, G Loss: 0.6934999823570251, D Loss: 1.3734058141708374\n",
            "Epoch 1, Batch 384, G Loss: 0.6937108039855957, D Loss: 1.37388277053833\n",
            "Epoch 1, Batch 385, G Loss: 0.6937804222106934, D Loss: 1.3729472160339355\n",
            "Epoch 1, Batch 386, G Loss: 0.6938831806182861, D Loss: 1.3721914291381836\n",
            "Epoch 1, Batch 387, G Loss: 0.6939412355422974, D Loss: 1.3729143142700195\n",
            "Epoch 1, Batch 388, G Loss: 0.6939333081245422, D Loss: 1.373155117034912\n",
            "Epoch 1, Batch 389, G Loss: 0.6941121816635132, D Loss: 1.3732950687408447\n",
            "Epoch 1, Batch 390, G Loss: 0.6941349506378174, D Loss: 1.3749895095825195\n",
            "Epoch 1, Batch 391, G Loss: 0.6942356824874878, D Loss: 1.373169183731079\n",
            "Epoch 1, Batch 392, G Loss: 0.6944494247436523, D Loss: 1.3731439113616943\n",
            "Epoch 1, Batch 393, G Loss: 0.6940442323684692, D Loss: 1.373331904411316\n",
            "Epoch 1, Batch 394, G Loss: 0.694474995136261, D Loss: 1.3725730180740356\n",
            "Epoch 1, Batch 395, G Loss: 0.6946492195129395, D Loss: 1.3727589845657349\n",
            "Epoch 1, Batch 396, G Loss: 0.6946194767951965, D Loss: 1.3685734272003174\n",
            "Epoch 1, Batch 397, G Loss: 0.6944161057472229, D Loss: 1.3696482181549072\n",
            "Epoch 1, Batch 398, G Loss: 0.6947289109230042, D Loss: 1.3704326152801514\n",
            "Epoch 1, Batch 399, G Loss: 0.694683313369751, D Loss: 1.3703311681747437\n",
            "Epoch 1, Batch 400, G Loss: 0.6949241757392883, D Loss: 1.372450351715088\n",
            "Epoch 1, Batch 401, G Loss: 0.6948814392089844, D Loss: 1.3714120388031006\n",
            "Epoch 1, Batch 402, G Loss: 0.694885790348053, D Loss: 1.370867133140564\n",
            "Epoch 1, Batch 403, G Loss: 0.6950080990791321, D Loss: 1.3723318576812744\n",
            "Epoch 1, Batch 404, G Loss: 0.6951387524604797, D Loss: 1.3733179569244385\n",
            "Epoch 1, Batch 405, G Loss: 0.6951994895935059, D Loss: 1.3732472658157349\n",
            "Epoch 1, Batch 406, G Loss: 0.6952821612358093, D Loss: 1.3729474544525146\n",
            "Epoch 1, Batch 407, G Loss: 0.6954253911972046, D Loss: 1.3695775270462036\n",
            "Epoch 1, Batch 408, G Loss: 0.695384681224823, D Loss: 1.3704662322998047\n",
            "Epoch 1, Batch 409, G Loss: 0.6955466270446777, D Loss: 1.3709267377853394\n",
            "Epoch 1, Batch 410, G Loss: 0.6955302357673645, D Loss: 1.3703579902648926\n",
            "Epoch 1, Batch 411, G Loss: 0.6956583261489868, D Loss: 1.371380090713501\n",
            "Epoch 1, Batch 412, G Loss: 0.6957604289054871, D Loss: 1.3712217807769775\n",
            "Epoch 1, Batch 413, G Loss: 0.695838451385498, D Loss: 1.3710143566131592\n",
            "Epoch 1, Batch 414, G Loss: 0.6957696676254272, D Loss: 1.3716235160827637\n",
            "Epoch 1, Batch 415, G Loss: 0.6958289742469788, D Loss: 1.371863842010498\n",
            "Epoch 1, Batch 416, G Loss: 0.696118950843811, D Loss: 1.3717126846313477\n",
            "Epoch 1, Batch 417, G Loss: 0.6960330605506897, D Loss: 1.3694286346435547\n",
            "Epoch 1, Batch 418, G Loss: 0.6960390210151672, D Loss: 1.3693456649780273\n",
            "Epoch 1, Batch 419, G Loss: 0.6961809992790222, D Loss: 1.3683936595916748\n",
            "Epoch 1, Batch 420, G Loss: 0.6961194276809692, D Loss: 1.3687758445739746\n",
            "Epoch 1, Batch 421, G Loss: 0.6963348984718323, D Loss: 1.3695998191833496\n",
            "Epoch 1, Batch 422, G Loss: 0.6964480876922607, D Loss: 1.3702945709228516\n",
            "Epoch 1, Batch 423, G Loss: 0.6963828802108765, D Loss: 1.3696815967559814\n",
            "Epoch 1, Batch 424, G Loss: 0.696635901927948, D Loss: 1.3665437698364258\n",
            "Epoch 1, Batch 425, G Loss: 0.6966471076011658, D Loss: 1.366992473602295\n",
            "Epoch 1, Batch 426, G Loss: 0.6966199278831482, D Loss: 1.3676748275756836\n",
            "Epoch 1, Batch 427, G Loss: 0.6967948079109192, D Loss: 1.3672713041305542\n",
            "Epoch 1, Batch 428, G Loss: 0.6968013048171997, D Loss: 1.368293046951294\n",
            "Epoch 1, Batch 429, G Loss: 0.6968961954116821, D Loss: 1.367353916168213\n",
            "Epoch 1, Batch 430, G Loss: 0.6971542835235596, D Loss: 1.369166374206543\n",
            "Epoch 1, Batch 431, G Loss: 0.697089672088623, D Loss: 1.3691471815109253\n",
            "Epoch 1, Batch 432, G Loss: 0.6971650123596191, D Loss: 1.3680367469787598\n",
            "Epoch 1, Batch 433, G Loss: 0.697149932384491, D Loss: 1.3693736791610718\n",
            "Epoch 1, Batch 434, G Loss: 0.6972681283950806, D Loss: 1.3690845966339111\n",
            "Epoch 1, Batch 435, G Loss: 0.6974120736122131, D Loss: 1.369692087173462\n",
            "Epoch 1, Batch 436, G Loss: 0.6974403262138367, D Loss: 1.3679313659667969\n",
            "Epoch 1, Batch 437, G Loss: 0.697527289390564, D Loss: 1.3651268482208252\n",
            "Epoch 1, Batch 438, G Loss: 0.6974742412567139, D Loss: 1.3658993244171143\n",
            "Epoch 1, Batch 439, G Loss: 0.6975862383842468, D Loss: 1.365494966506958\n",
            "Epoch 1, Batch 440, G Loss: 0.6977287530899048, D Loss: 1.365211844444275\n",
            "Epoch 1, Batch 441, G Loss: 0.6978851556777954, D Loss: 1.3640655279159546\n",
            "Epoch 1, Batch 442, G Loss: 0.6979160904884338, D Loss: 1.364405632019043\n",
            "Epoch 1, Batch 443, G Loss: 0.6979639530181885, D Loss: 1.365088701248169\n",
            "Epoch 1, Batch 444, G Loss: 0.6980386972427368, D Loss: 1.3634710311889648\n",
            "Epoch 1, Batch 445, G Loss: 0.6980881690979004, D Loss: 1.3637040853500366\n",
            "Epoch 1, Batch 446, G Loss: 0.6982665061950684, D Loss: 1.3644508123397827\n",
            "Epoch 1, Batch 447, G Loss: 0.6983181834220886, D Loss: 1.3641029596328735\n",
            "Epoch 1, Batch 448, G Loss: 0.6983866691589355, D Loss: 1.365488886833191\n",
            "Epoch 1, Batch 449, G Loss: 0.6984832286834717, D Loss: 1.365722417831421\n",
            "Epoch 1, Batch 450, G Loss: 0.6985595226287842, D Loss: 1.3651847839355469\n",
            "Epoch 1, Batch 451, G Loss: 0.6985657215118408, D Loss: 1.3656829595565796\n",
            "Epoch 1, Batch 452, G Loss: 0.6986852884292603, D Loss: 1.3629841804504395\n",
            "Epoch 1, Batch 453, G Loss: 0.6987980604171753, D Loss: 1.3640692234039307\n",
            "Epoch 1, Batch 454, G Loss: 0.6987678408622742, D Loss: 1.3639919757843018\n",
            "Epoch 1, Batch 455, G Loss: 0.698937714099884, D Loss: 1.3674373626708984\n",
            "Epoch 1, Batch 456, G Loss: 0.6989854574203491, D Loss: 1.3667411804199219\n",
            "Epoch 1, Batch 457, G Loss: 0.6990821957588196, D Loss: 1.366101861000061\n",
            "Epoch 1, Batch 458, G Loss: 0.6991828083992004, D Loss: 1.3612725734710693\n",
            "Epoch 1, Batch 459, G Loss: 0.6992377638816833, D Loss: 1.3614583015441895\n",
            "Epoch 1, Batch 460, G Loss: 0.6993593573570251, D Loss: 1.3609485626220703\n",
            "Epoch 1, Batch 461, G Loss: 0.6994414329528809, D Loss: 1.3615994453430176\n",
            "Epoch 1, Batch 462, G Loss: 0.6994290351867676, D Loss: 1.3653876781463623\n",
            "Epoch 1, Batch 463, G Loss: 0.6995301246643066, D Loss: 1.3659662008285522\n",
            "Epoch 1, Batch 464, G Loss: 0.6995757818222046, D Loss: 1.364885687828064\n",
            "Epoch 1, Batch 465, G Loss: 0.6996735334396362, D Loss: 1.3664491176605225\n",
            "Epoch 1, Batch 466, G Loss: 0.6997309327125549, D Loss: 1.3664085865020752\n",
            "Epoch 1, Batch 467, G Loss: 0.6997804641723633, D Loss: 1.3662896156311035\n",
            "Epoch 1, Batch 468, G Loss: 0.6999253034591675, D Loss: 1.3658043146133423\n",
            "Epoch 1, Batch 469, G Loss: 0.6999786496162415, D Loss: 1.3673908710479736\n",
            "Epoch 1, Batch 470, G Loss: 0.7001137137413025, D Loss: 1.3632240295410156\n",
            "Epoch 1, Batch 471, G Loss: 0.7001062631607056, D Loss: 1.363893985748291\n",
            "Epoch 1, Batch 472, G Loss: 0.7001892328262329, D Loss: 1.3638181686401367\n",
            "Epoch 1, Batch 473, G Loss: 0.7002519965171814, D Loss: 1.3639922142028809\n",
            "Epoch 1, Batch 474, G Loss: 0.7002637982368469, D Loss: 1.3639163970947266\n",
            "Epoch 1, Batch 475, G Loss: 0.7002672553062439, D Loss: 1.3618288040161133\n",
            "Epoch 1, Batch 476, G Loss: 0.7003872990608215, D Loss: 1.36129891872406\n",
            "Epoch 1, Batch 477, G Loss: 0.7005465626716614, D Loss: 1.3615198135375977\n",
            "Epoch 1, Batch 478, G Loss: 0.7006451487541199, D Loss: 1.3629261255264282\n",
            "Epoch 1, Batch 479, G Loss: 0.7007268071174622, D Loss: 1.3673651218414307\n",
            "Epoch 1, Batch 480, G Loss: 0.7006902694702148, D Loss: 1.366049885749817\n",
            "Epoch 1, Batch 481, G Loss: 0.7008064985275269, D Loss: 1.366087555885315\n",
            "Epoch 1, Batch 482, G Loss: 0.700775682926178, D Loss: 1.361556887626648\n",
            "Epoch 1, Batch 483, G Loss: 0.700890839099884, D Loss: 1.3622223138809204\n",
            "Epoch 1, Batch 484, G Loss: 0.7010248899459839, D Loss: 1.3635237216949463\n",
            "Epoch 1, Batch 485, G Loss: 0.7011637687683105, D Loss: 1.3630468845367432\n",
            "Epoch 1, Batch 486, G Loss: 0.7010902762413025, D Loss: 1.3627541065216064\n",
            "Epoch 1, Batch 487, G Loss: 0.7011660933494568, D Loss: 1.3630833625793457\n",
            "Epoch 1, Batch 488, G Loss: 0.7012996077537537, D Loss: 1.3628995418548584\n",
            "Epoch 1, Batch 489, G Loss: 0.7013466358184814, D Loss: 1.3627002239227295\n",
            "Epoch 1, Batch 490, G Loss: 0.701368510723114, D Loss: 1.3626879453659058\n",
            "Epoch 1, Batch 491, G Loss: 0.7014829516410828, D Loss: 1.3633146286010742\n",
            "Epoch 1, Batch 492, G Loss: 0.7016103267669678, D Loss: 1.3621330261230469\n",
            "Epoch 1, Batch 493, G Loss: 0.7015902400016785, D Loss: 1.3641915321350098\n",
            "Epoch 1, Batch 494, G Loss: 0.7016262412071228, D Loss: 1.3639318943023682\n",
            "Epoch 1, Batch 495, G Loss: 0.7018924355506897, D Loss: 1.3635351657867432\n",
            "Epoch 1, Batch 496, G Loss: 0.7017806768417358, D Loss: 1.3631305694580078\n",
            "Epoch 1, Batch 497, G Loss: 0.7018788456916809, D Loss: 1.361818552017212\n",
            "Epoch 1, Batch 498, G Loss: 0.7020411491394043, D Loss: 1.36134934425354\n",
            "Epoch 1, Batch 499, G Loss: 0.7020958065986633, D Loss: 1.362212896347046\n",
            "Epoch 1, Batch 500, G Loss: 0.7021933794021606, D Loss: 1.3598551750183105\n",
            "Epoch 1, Batch 501, G Loss: 0.7021545171737671, D Loss: 1.3584651947021484\n",
            "Epoch 1, Batch 502, G Loss: 0.7023380994796753, D Loss: 1.3583563566207886\n",
            "Epoch 1, Batch 503, G Loss: 0.7023931741714478, D Loss: 1.359405279159546\n",
            "Epoch 1, Batch 504, G Loss: 0.7024948000907898, D Loss: 1.3602702617645264\n",
            "Epoch 1, Batch 505, G Loss: 0.7025607228279114, D Loss: 1.3571951389312744\n",
            "Epoch 1, Batch 506, G Loss: 0.7026145458221436, D Loss: 1.3573498725891113\n",
            "Epoch 1, Batch 507, G Loss: 0.7026886940002441, D Loss: 1.3550069332122803\n",
            "Epoch 1, Batch 508, G Loss: 0.7028417587280273, D Loss: 1.3568471670150757\n",
            "Epoch 1, Batch 509, G Loss: 0.7028670310974121, D Loss: 1.3601295948028564\n",
            "Epoch 1, Batch 510, G Loss: 0.7029694318771362, D Loss: 1.3593742847442627\n",
            "Epoch 1, Batch 511, G Loss: 0.7030362486839294, D Loss: 1.360368013381958\n",
            "Epoch 1, Batch 512, G Loss: 0.703217625617981, D Loss: 1.3621011972427368\n",
            "Epoch 1, Batch 513, G Loss: 0.7031944394111633, D Loss: 1.359879493713379\n",
            "Epoch 1, Batch 514, G Loss: 0.7033336162567139, D Loss: 1.3599882125854492\n",
            "Epoch 1, Batch 515, G Loss: 0.7033775448799133, D Loss: 1.3575177192687988\n",
            "Epoch 1, Batch 516, G Loss: 0.7034351825714111, D Loss: 1.3558180332183838\n",
            "Epoch 1, Batch 517, G Loss: 0.7035647630691528, D Loss: 1.3552783727645874\n",
            "Epoch 1, Batch 518, G Loss: 0.7036329507827759, D Loss: 1.3576059341430664\n",
            "Epoch 1, Batch 519, G Loss: 0.7037327885627747, D Loss: 1.3565418720245361\n",
            "Epoch 1, Batch 520, G Loss: 0.703681468963623, D Loss: 1.3569209575653076\n",
            "Epoch 1, Batch 521, G Loss: 0.7039353251457214, D Loss: 1.3569109439849854\n",
            "Epoch 1, Batch 522, G Loss: 0.7039446234703064, D Loss: 1.3591370582580566\n",
            "Epoch 1, Batch 523, G Loss: 0.7040252685546875, D Loss: 1.3601582050323486\n",
            "Epoch 1, Batch 524, G Loss: 0.7040783762931824, D Loss: 1.3596701622009277\n",
            "Epoch 1, Batch 525, G Loss: 0.7040993571281433, D Loss: 1.3613567352294922\n",
            "Epoch 1, Batch 526, G Loss: 0.7042704820632935, D Loss: 1.3611390590667725\n",
            "Epoch 1, Batch 527, G Loss: 0.7043790221214294, D Loss: 1.358642578125\n",
            "Epoch 1, Batch 528, G Loss: 0.7044830322265625, D Loss: 1.3556129932403564\n",
            "Epoch 1, Batch 529, G Loss: 0.7044883370399475, D Loss: 1.3531367778778076\n",
            "Epoch 1, Batch 530, G Loss: 0.7047174572944641, D Loss: 1.3556663990020752\n",
            "Epoch 1, Batch 531, G Loss: 0.7047374248504639, D Loss: 1.3566291332244873\n",
            "Epoch 1, Batch 532, G Loss: 0.7048057913780212, D Loss: 1.3606104850769043\n",
            "Epoch 1, Batch 533, G Loss: 0.7048383355140686, D Loss: 1.3569221496582031\n",
            "Epoch 1, Batch 534, G Loss: 0.7049023509025574, D Loss: 1.356091022491455\n",
            "Epoch 1, Batch 535, G Loss: 0.7049691081047058, D Loss: 1.3576574325561523\n",
            "Epoch 1, Batch 536, G Loss: 0.7050392031669617, D Loss: 1.358652949333191\n",
            "Epoch 1, Batch 537, G Loss: 0.7052780985832214, D Loss: 1.3550829887390137\n",
            "Epoch 1, Batch 538, G Loss: 0.7051720023155212, D Loss: 1.3540759086608887\n",
            "Epoch 1, Batch 539, G Loss: 0.7053323984146118, D Loss: 1.3548698425292969\n",
            "Epoch 1, Batch 540, G Loss: 0.7053322196006775, D Loss: 1.3546264171600342\n",
            "Epoch 1, Batch 541, G Loss: 0.7055477499961853, D Loss: 1.354557752609253\n",
            "Epoch 1, Batch 542, G Loss: 0.7056039571762085, D Loss: 1.354975700378418\n",
            "Epoch 1, Batch 543, G Loss: 0.7055960893630981, D Loss: 1.354712724685669\n",
            "Epoch 1, Batch 544, G Loss: 0.705757737159729, D Loss: 1.3544477224349976\n",
            "Epoch 1, Batch 545, G Loss: 0.7057065367698669, D Loss: 1.3546967506408691\n",
            "Epoch 1, Batch 546, G Loss: 0.7059981822967529, D Loss: 1.3547286987304688\n",
            "Epoch 1, Batch 547, G Loss: 0.7060340046882629, D Loss: 1.3546496629714966\n",
            "Epoch 1, Batch 548, G Loss: 0.7060781121253967, D Loss: 1.3515443801879883\n",
            "Epoch 1, Batch 549, G Loss: 0.7059602737426758, D Loss: 1.3548550605773926\n",
            "Epoch 1, Batch 550, G Loss: 0.7062066793441772, D Loss: 1.3574330806732178\n",
            "Epoch 1, Batch 551, G Loss: 0.7060807943344116, D Loss: 1.3569586277008057\n",
            "Epoch 1, Batch 552, G Loss: 0.7062854170799255, D Loss: 1.356858730316162\n",
            "Epoch 1, Batch 553, G Loss: 0.7065081000328064, D Loss: 1.3532501459121704\n",
            "Epoch 1, Batch 554, G Loss: 0.706571638584137, D Loss: 1.352386236190796\n",
            "Epoch 1, Batch 555, G Loss: 0.7066949605941772, D Loss: 1.351271629333496\n",
            "Epoch 1, Batch 556, G Loss: 0.7068644165992737, D Loss: 1.3548252582550049\n",
            "Epoch 1, Batch 557, G Loss: 0.7069188356399536, D Loss: 1.355646014213562\n",
            "Epoch 1, Batch 558, G Loss: 0.7068284749984741, D Loss: 1.3528121709823608\n",
            "Epoch 1, Batch 559, G Loss: 0.7069747447967529, D Loss: 1.3543561697006226\n",
            "Epoch 1, Batch 560, G Loss: 0.7070580124855042, D Loss: 1.3539040088653564\n",
            "Epoch 1, Batch 561, G Loss: 0.7071284651756287, D Loss: 1.3562030792236328\n",
            "Epoch 1, Batch 562, G Loss: 0.7068102955818176, D Loss: 1.3581591844558716\n",
            "Epoch 1, Batch 563, G Loss: 0.7073989510536194, D Loss: 1.357800006866455\n",
            "Epoch 1, Batch 564, G Loss: 0.7074325084686279, D Loss: 1.36009681224823\n",
            "Epoch 1, Batch 565, G Loss: 0.7073320746421814, D Loss: 1.3516300916671753\n",
            "Epoch 1, Batch 566, G Loss: 0.7075713276863098, D Loss: 1.3443418741226196\n",
            "Epoch 1, Batch 567, G Loss: 0.7075945734977722, D Loss: 1.3482422828674316\n",
            "Epoch 1, Batch 568, G Loss: 0.7075421214103699, D Loss: 1.3473303318023682\n",
            "Epoch 1, Batch 569, G Loss: 0.7076948285102844, D Loss: 1.3539636135101318\n",
            "Epoch 1, Batch 570, G Loss: 0.7077366709709167, D Loss: 1.3525276184082031\n",
            "Epoch 1, Batch 571, G Loss: 0.7077593207359314, D Loss: 1.3541830778121948\n",
            "Epoch 1, Batch 572, G Loss: 0.7080159187316895, D Loss: 1.344299077987671\n",
            "Epoch 1, Batch 573, G Loss: 0.7080657482147217, D Loss: 1.3462066650390625\n",
            "Epoch 1, Batch 574, G Loss: 0.7082102298736572, D Loss: 1.3432338237762451\n",
            "Epoch 1, Batch 575, G Loss: 0.7081314921379089, D Loss: 1.3449132442474365\n",
            "Epoch 1, Batch 576, G Loss: 0.7083055377006531, D Loss: 1.3499928712844849\n",
            "Epoch 1, Batch 577, G Loss: 0.7083290815353394, D Loss: 1.3498635292053223\n",
            "Epoch 1, Batch 578, G Loss: 0.708391010761261, D Loss: 1.3524779081344604\n",
            "Epoch 1, Batch 579, G Loss: 0.7085263133049011, D Loss: 1.353515386581421\n",
            "Epoch 1, Batch 580, G Loss: 0.7086126208305359, D Loss: 1.345654010772705\n",
            "Epoch 1, Batch 581, G Loss: 0.7084266543388367, D Loss: 1.3471031188964844\n",
            "Epoch 1, Batch 582, G Loss: 0.7088595628738403, D Loss: 1.3459630012512207\n",
            "Epoch 1, Batch 583, G Loss: 0.7089077830314636, D Loss: 1.3457224369049072\n",
            "Epoch 1, Batch 584, G Loss: 0.7091708779335022, D Loss: 1.3495831489562988\n",
            "Epoch 1, Batch 585, G Loss: 0.7091576457023621, D Loss: 1.3504652976989746\n",
            "Epoch 1, Batch 586, G Loss: 0.7093217968940735, D Loss: 1.3518224954605103\n",
            "Epoch 1, Batch 587, G Loss: 0.7092553973197937, D Loss: 1.3536529541015625\n",
            "Epoch 1, Batch 588, G Loss: 0.709382951259613, D Loss: 1.3537402153015137\n",
            "Epoch 1, Batch 589, G Loss: 0.7095591425895691, D Loss: 1.3544080257415771\n",
            "Epoch 1, Batch 590, G Loss: 0.7096283435821533, D Loss: 1.347485065460205\n",
            "Epoch 1, Batch 591, G Loss: 0.7097073197364807, D Loss: 1.347036361694336\n",
            "Epoch 1, Batch 592, G Loss: 0.7097883224487305, D Loss: 1.348517894744873\n",
            "Epoch 1, Batch 593, G Loss: 0.7097312808036804, D Loss: 1.3507943153381348\n",
            "Epoch 1, Batch 594, G Loss: 0.7099011540412903, D Loss: 1.3513615131378174\n",
            "Epoch 1, Batch 595, G Loss: 0.7097214460372925, D Loss: 1.3546404838562012\n",
            "Epoch 1, Batch 596, G Loss: 0.7100855708122253, D Loss: 1.3476743698120117\n",
            "Epoch 1, Batch 597, G Loss: 0.7103500962257385, D Loss: 1.3472669124603271\n",
            "Epoch 1, Batch 598, G Loss: 0.7102863788604736, D Loss: 1.3492159843444824\n",
            "Epoch 1, Batch 599, G Loss: 0.7104189991950989, D Loss: 1.3501102924346924\n",
            "Epoch 1, Batch 600, G Loss: 0.7098971009254456, D Loss: 1.3524384498596191\n",
            "Epoch 1, Batch 601, G Loss: 0.7100033164024353, D Loss: 1.3515706062316895\n",
            "Epoch 1, Batch 602, G Loss: 0.7104819416999817, D Loss: 1.3526742458343506\n",
            "Epoch 1, Batch 603, G Loss: 0.7105811238288879, D Loss: 1.3492870330810547\n",
            "Epoch 1, Batch 604, G Loss: 0.7107276916503906, D Loss: 1.3439286947250366\n",
            "Epoch 1, Batch 605, G Loss: 0.7107691168785095, D Loss: 1.349811315536499\n",
            "Epoch 1, Batch 606, G Loss: 0.7106921672821045, D Loss: 1.3490554094314575\n",
            "Epoch 1, Batch 607, G Loss: 0.710804283618927, D Loss: 1.3486896753311157\n",
            "Epoch 1, Batch 608, G Loss: 0.7107065916061401, D Loss: 1.3470361232757568\n",
            "Epoch 1, Batch 609, G Loss: 0.7108860611915588, D Loss: 1.350001335144043\n",
            "Epoch 1, Batch 610, G Loss: 0.7109598517417908, D Loss: 1.3516175746917725\n",
            "Epoch 1, Batch 611, G Loss: 0.7110639810562134, D Loss: 1.3469630479812622\n",
            "Epoch 1, Batch 612, G Loss: 0.7112360000610352, D Loss: 1.3435653448104858\n",
            "Epoch 1, Batch 613, G Loss: 0.7113761305809021, D Loss: 1.3436640501022339\n",
            "Epoch 1, Batch 614, G Loss: 0.711113452911377, D Loss: 1.3453775644302368\n",
            "Epoch 1, Batch 615, G Loss: 0.7112835645675659, D Loss: 1.3461860418319702\n",
            "Epoch 1, Batch 616, G Loss: 0.7114416360855103, D Loss: 1.3454993963241577\n",
            "Epoch 1, Batch 617, G Loss: 0.7111536264419556, D Loss: 1.344506025314331\n",
            "Epoch 1, Batch 618, G Loss: 0.7115016579627991, D Loss: 1.3472659587860107\n",
            "Epoch 1, Batch 619, G Loss: 0.711580753326416, D Loss: 1.3452928066253662\n",
            "Epoch 1, Batch 620, G Loss: 0.7116231918334961, D Loss: 1.3452184200286865\n",
            "Epoch 1, Batch 621, G Loss: 0.7117972373962402, D Loss: 1.3492164611816406\n",
            "Epoch 1, Batch 622, G Loss: 0.7114309072494507, D Loss: 1.3554906845092773\n",
            "Epoch 1, Batch 623, G Loss: 0.711918294429779, D Loss: 1.3495008945465088\n",
            "Epoch 1, Batch 624, G Loss: 0.7119582891464233, D Loss: 1.351870059967041\n",
            "Epoch 1, Batch 625, G Loss: 0.712078332901001, D Loss: 1.3460173606872559\n",
            "Epoch 1, Batch 626, G Loss: 0.7122007012367249, D Loss: 1.3471250534057617\n",
            "Epoch 1, Batch 627, G Loss: 0.7119972705841064, D Loss: 1.3458738327026367\n",
            "Epoch 1, Batch 628, G Loss: 0.7120219469070435, D Loss: 1.3456676006317139\n",
            "Epoch 1, Batch 629, G Loss: 0.712067723274231, D Loss: 1.347058892250061\n",
            "Epoch 1, Batch 630, G Loss: 0.7124407887458801, D Loss: 1.349625825881958\n",
            "Epoch 1, Batch 631, G Loss: 0.7123630046844482, D Loss: 1.349731206893921\n",
            "Epoch 1, Batch 632, G Loss: 0.7118657827377319, D Loss: 1.343058466911316\n",
            "Epoch 1, Batch 633, G Loss: 0.7122681140899658, D Loss: 1.3414257764816284\n",
            "Epoch 1, Batch 634, G Loss: 0.7124776244163513, D Loss: 1.3422949314117432\n",
            "Epoch 1, Batch 635, G Loss: 0.712518036365509, D Loss: 1.3449468612670898\n",
            "Epoch 1, Batch 636, G Loss: 0.7124028205871582, D Loss: 1.3487274646759033\n",
            "Epoch 1, Batch 637, G Loss: 0.7125864028930664, D Loss: 1.3479005098342896\n",
            "Epoch 1, Batch 638, G Loss: 0.7125346064567566, D Loss: 1.3505667448043823\n",
            "Epoch 1, Batch 639, G Loss: 0.7126815915107727, D Loss: 1.346224308013916\n",
            "Epoch 1, Batch 640, G Loss: 0.7126282453536987, D Loss: 1.3494391441345215\n",
            "Epoch 1, Batch 641, G Loss: 0.7126249074935913, D Loss: 1.3484163284301758\n",
            "Epoch 1, Batch 642, G Loss: 0.7126310467720032, D Loss: 1.3482115268707275\n",
            "Epoch 1, Batch 643, G Loss: 0.7124326229095459, D Loss: 1.3478951454162598\n",
            "Epoch 1, Batch 644, G Loss: 0.7126517295837402, D Loss: 1.3447335958480835\n",
            "Epoch 1, Batch 645, G Loss: 0.7127760052680969, D Loss: 1.348227858543396\n",
            "Epoch 1, Batch 646, G Loss: 0.7124350666999817, D Loss: 1.3449327945709229\n",
            "Epoch 1, Batch 647, G Loss: 0.7130414843559265, D Loss: 1.3439381122589111\n",
            "Epoch 1, Batch 648, G Loss: 0.7128587961196899, D Loss: 1.3455383777618408\n",
            "Epoch 1, Batch 649, G Loss: 0.7126737236976624, D Loss: 1.3499982357025146\n",
            "Epoch 1, Batch 650, G Loss: 0.7125457525253296, D Loss: 1.3447312116622925\n",
            "Epoch 1, Batch 651, G Loss: 0.7126322984695435, D Loss: 1.3476271629333496\n",
            "Epoch 1, Batch 652, G Loss: 0.712770402431488, D Loss: 1.353596806526184\n",
            "Epoch 1, Batch 653, G Loss: 0.7130142450332642, D Loss: 1.3552062511444092\n",
            "Epoch 1, Batch 654, G Loss: 0.7127649784088135, D Loss: 1.3508243560791016\n",
            "Epoch 1, Batch 655, G Loss: 0.7126286029815674, D Loss: 1.3443602323532104\n",
            "Epoch 1, Batch 656, G Loss: 0.712733268737793, D Loss: 1.3437628746032715\n",
            "Epoch 1, Batch 657, G Loss: 0.7129316329956055, D Loss: 1.3411400318145752\n",
            "Epoch 1, Batch 658, G Loss: 0.7129030823707581, D Loss: 1.341300368309021\n",
            "Epoch 1, Batch 659, G Loss: 0.7126123309135437, D Loss: 1.343733787536621\n",
            "Epoch 1, Batch 660, G Loss: 0.712524950504303, D Loss: 1.3443790674209595\n",
            "Epoch 1, Batch 661, G Loss: 0.7125893235206604, D Loss: 1.3436272144317627\n",
            "Epoch 1, Batch 662, G Loss: 0.7125635743141174, D Loss: 1.3472049236297607\n",
            "Epoch 1, Batch 663, G Loss: 0.7126259803771973, D Loss: 1.3457705974578857\n",
            "Epoch 1, Batch 664, G Loss: 0.7125605940818787, D Loss: 1.3486942052841187\n",
            "Epoch 1, Batch 665, G Loss: 0.7126724720001221, D Loss: 1.3497657775878906\n",
            "Epoch 1, Batch 666, G Loss: 0.7122654914855957, D Loss: 1.340100646018982\n",
            "Epoch 1, Batch 667, G Loss: 0.7128725647926331, D Loss: 1.3381357192993164\n",
            "Epoch 1, Batch 668, G Loss: 0.7123331427574158, D Loss: 1.3442573547363281\n",
            "Epoch 1, Batch 669, G Loss: 0.7125988602638245, D Loss: 1.3417973518371582\n",
            "Epoch 1, Batch 670, G Loss: 0.7126584649085999, D Loss: 1.3435006141662598\n",
            "Epoch 1, Batch 671, G Loss: 0.7129177451133728, D Loss: 1.3466888666152954\n",
            "Epoch 1, Batch 672, G Loss: 0.7125145792961121, D Loss: 1.351884365081787\n",
            "Epoch 1, Batch 673, G Loss: 0.7121009826660156, D Loss: 1.3508682250976562\n",
            "Epoch 1, Batch 674, G Loss: 0.712410032749176, D Loss: 1.3512589931488037\n",
            "Epoch 1, Batch 675, G Loss: 0.7123405337333679, D Loss: 1.3434250354766846\n",
            "Epoch 1, Batch 676, G Loss: 0.7125048041343689, D Loss: 1.3443281650543213\n",
            "Epoch 1, Batch 677, G Loss: 0.7121516466140747, D Loss: 1.3469829559326172\n",
            "Epoch 1, Batch 678, G Loss: 0.7125393748283386, D Loss: 1.3462755680084229\n",
            "Epoch 1, Batch 679, G Loss: 0.7119001150131226, D Loss: 1.3462733030319214\n",
            "Epoch 1, Batch 680, G Loss: 0.7120785117149353, D Loss: 1.3454474210739136\n",
            "Epoch 1, Batch 681, G Loss: 0.7122296094894409, D Loss: 1.348854660987854\n",
            "Epoch 1, Batch 682, G Loss: 0.7122411727905273, D Loss: 1.3457257747650146\n",
            "Epoch 1, Batch 683, G Loss: 0.7122045755386353, D Loss: 1.346696138381958\n",
            "Epoch 1, Batch 684, G Loss: 0.7115747332572937, D Loss: 1.347525954246521\n",
            "Epoch 1, Batch 685, G Loss: 0.7119779586791992, D Loss: 1.349448561668396\n",
            "Epoch 1, Batch 686, G Loss: 0.711519718170166, D Loss: 1.3510162830352783\n",
            "Epoch 1, Batch 687, G Loss: 0.7120480537414551, D Loss: 1.351707935333252\n",
            "Epoch 1, Batch 688, G Loss: 0.7114791870117188, D Loss: 1.3539834022521973\n",
            "Epoch 1, Batch 689, G Loss: 0.7113354206085205, D Loss: 1.3482112884521484\n",
            "Epoch 1, Batch 690, G Loss: 0.7113634943962097, D Loss: 1.3517519235610962\n",
            "Epoch 1, Batch 691, G Loss: 0.7113261222839355, D Loss: 1.349993109703064\n",
            "Epoch 1, Batch 692, G Loss: 0.7106201648712158, D Loss: 1.3488740921020508\n",
            "Epoch 1, Batch 693, G Loss: 0.710483968257904, D Loss: 1.3487606048583984\n",
            "Epoch 1, Batch 694, G Loss: 0.7100021839141846, D Loss: 1.3529884815216064\n",
            "Epoch 1, Batch 695, G Loss: 0.71053546667099, D Loss: 1.3485950231552124\n",
            "Epoch 1, Batch 696, G Loss: 0.7100921869277954, D Loss: 1.351572036743164\n",
            "Epoch 1, Batch 697, G Loss: 0.7097446918487549, D Loss: 1.351567268371582\n",
            "Epoch 1, Batch 698, G Loss: 0.7102938294410706, D Loss: 1.3516080379486084\n",
            "Epoch 1, Batch 699, G Loss: 0.7098034620285034, D Loss: 1.3535406589508057\n",
            "Epoch 1, Batch 700, G Loss: 0.7101133465766907, D Loss: 1.3527512550354004\n",
            "Epoch 1, Batch 701, G Loss: 0.7095146775245667, D Loss: 1.3554716110229492\n",
            "Epoch 1, Batch 702, G Loss: 0.7093965411186218, D Loss: 1.352475643157959\n",
            "Epoch 1, Batch 703, G Loss: 0.7094120383262634, D Loss: 1.349942684173584\n",
            "Epoch 1, Batch 704, G Loss: 0.7089343667030334, D Loss: 1.3522441387176514\n",
            "Epoch 1, Batch 705, G Loss: 0.708217203617096, D Loss: 1.3514599800109863\n",
            "Epoch 1, Batch 706, G Loss: 0.7089471817016602, D Loss: 1.3522396087646484\n",
            "Epoch 1, Batch 707, G Loss: 0.7087332010269165, D Loss: 1.3527953624725342\n",
            "Epoch 1, Batch 708, G Loss: 0.7083216905593872, D Loss: 1.35227370262146\n",
            "Epoch 1, Batch 709, G Loss: 0.7079028487205505, D Loss: 1.3553714752197266\n",
            "Epoch 1, Batch 710, G Loss: 0.7079282999038696, D Loss: 1.3563439846038818\n",
            "Epoch 1, Batch 711, G Loss: 0.7075528502464294, D Loss: 1.3603153228759766\n",
            "Epoch 1, Batch 712, G Loss: 0.707166850566864, D Loss: 1.3552896976470947\n",
            "Epoch 1, Batch 713, G Loss: 0.707133948802948, D Loss: 1.3574928045272827\n",
            "Epoch 1, Batch 714, G Loss: 0.7071240544319153, D Loss: 1.3570377826690674\n",
            "Epoch 1, Batch 715, G Loss: 0.7066231369972229, D Loss: 1.3561580181121826\n",
            "Epoch 1, Batch 716, G Loss: 0.7062263488769531, D Loss: 1.3577654361724854\n",
            "Epoch 1, Batch 717, G Loss: 0.7064797282218933, D Loss: 1.3569471836090088\n",
            "Epoch 1, Batch 718, G Loss: 0.7055094838142395, D Loss: 1.3629345893859863\n",
            "Epoch 1, Batch 719, G Loss: 0.7059721946716309, D Loss: 1.3662900924682617\n",
            "Epoch 1, Batch 720, G Loss: 0.7050524353981018, D Loss: 1.3647488355636597\n",
            "Epoch 1, Batch 721, G Loss: 0.7053601145744324, D Loss: 1.3651022911071777\n",
            "Epoch 1, Batch 722, G Loss: 0.7046501636505127, D Loss: 1.3631389141082764\n",
            "Epoch 1, Batch 723, G Loss: 0.704419732093811, D Loss: 1.3621879816055298\n",
            "Epoch 1, Batch 724, G Loss: 0.703948974609375, D Loss: 1.3635344505310059\n",
            "Epoch 1, Batch 725, G Loss: 0.7039446830749512, D Loss: 1.3588348627090454\n",
            "Epoch 1, Batch 726, G Loss: 0.7041865587234497, D Loss: 1.3618786334991455\n",
            "Epoch 1, Batch 727, G Loss: 0.7038942575454712, D Loss: 1.362779140472412\n",
            "Epoch 1, Batch 728, G Loss: 0.7036296129226685, D Loss: 1.3624343872070312\n",
            "Epoch 1, Batch 729, G Loss: 0.7031732797622681, D Loss: 1.3617584705352783\n",
            "Epoch 1, Batch 730, G Loss: 0.7030698657035828, D Loss: 1.358167290687561\n",
            "Epoch 1, Batch 731, G Loss: 0.7024518847465515, D Loss: 1.3620071411132812\n",
            "Epoch 1, Batch 732, G Loss: 0.7017940878868103, D Loss: 1.3610920906066895\n",
            "Epoch 1, Batch 733, G Loss: 0.7020450234413147, D Loss: 1.362046718597412\n",
            "Epoch 1, Batch 734, G Loss: 0.7016786932945251, D Loss: 1.3644100427627563\n",
            "Epoch 1, Batch 735, G Loss: 0.7014420032501221, D Loss: 1.3660314083099365\n",
            "Epoch 1, Batch 736, G Loss: 0.7012420296669006, D Loss: 1.3661694526672363\n",
            "Epoch 1, Batch 737, G Loss: 0.700843334197998, D Loss: 1.366565227508545\n",
            "Epoch 1, Batch 738, G Loss: 0.7005218267440796, D Loss: 1.3727407455444336\n",
            "Epoch 1, Batch 739, G Loss: 0.7005335092544556, D Loss: 1.3738510608673096\n",
            "Epoch 1, Batch 740, G Loss: 0.6999710202217102, D Loss: 1.3754539489746094\n",
            "Epoch 1, Batch 741, G Loss: 0.700036883354187, D Loss: 1.3667548894882202\n",
            "Epoch 1, Batch 742, G Loss: 0.6995363235473633, D Loss: 1.3679981231689453\n",
            "Epoch 1, Batch 743, G Loss: 0.6989219784736633, D Loss: 1.370471477508545\n",
            "Epoch 1, Batch 744, G Loss: 0.6988875865936279, D Loss: 1.3642792701721191\n",
            "Epoch 1, Batch 745, G Loss: 0.6983153820037842, D Loss: 1.3636631965637207\n",
            "Epoch 1, Batch 746, G Loss: 0.698235034942627, D Loss: 1.3679296970367432\n",
            "Epoch 1, Batch 747, G Loss: 0.6981369853019714, D Loss: 1.3704501390457153\n",
            "Epoch 1, Batch 748, G Loss: 0.697572648525238, D Loss: 1.3710235357284546\n",
            "Epoch 1, Batch 749, G Loss: 0.6975275278091431, D Loss: 1.3715864419937134\n",
            "Epoch 1, Batch 750, G Loss: 0.6973424553871155, D Loss: 1.3764114379882812\n",
            "Epoch 1, Batch 751, G Loss: 0.6968819499015808, D Loss: 1.3685197830200195\n",
            "Epoch 1, Batch 752, G Loss: 0.6968867778778076, D Loss: 1.3742306232452393\n",
            "Epoch 1, Batch 753, G Loss: 0.6965197920799255, D Loss: 1.3701238632202148\n",
            "Epoch 1, Batch 754, G Loss: 0.6965810060501099, D Loss: 1.3713808059692383\n",
            "Epoch 1, Batch 755, G Loss: 0.6961435675621033, D Loss: 1.3724620342254639\n",
            "Epoch 1, Batch 756, G Loss: 0.695720911026001, D Loss: 1.374678611755371\n",
            "Epoch 1, Batch 757, G Loss: 0.695806086063385, D Loss: 1.374760627746582\n",
            "Epoch 1, Batch 758, G Loss: 0.6953054666519165, D Loss: 1.3778877258300781\n",
            "Epoch 1, Batch 759, G Loss: 0.6955488324165344, D Loss: 1.3754465579986572\n",
            "Epoch 1, Batch 760, G Loss: 0.6951637864112854, D Loss: 1.380053997039795\n",
            "Epoch 1, Batch 761, G Loss: 0.6949264407157898, D Loss: 1.3792641162872314\n",
            "Epoch 1, Batch 762, G Loss: 0.6946778297424316, D Loss: 1.378859281539917\n",
            "Epoch 1, Batch 763, G Loss: 0.6948198080062866, D Loss: 1.3795294761657715\n",
            "Epoch 1, Batch 764, G Loss: 0.6945269703865051, D Loss: 1.382347583770752\n",
            "Epoch 1, Batch 765, G Loss: 0.6944142580032349, D Loss: 1.392521619796753\n",
            "Epoch 1, Batch 766, G Loss: 0.6942023634910583, D Loss: 1.3916206359863281\n",
            "Epoch 1, Batch 767, G Loss: 0.6939144730567932, D Loss: 1.3892018795013428\n",
            "Epoch 1, Batch 768, G Loss: 0.693620502948761, D Loss: 1.385248064994812\n",
            "Epoch 1, Batch 769, G Loss: 0.6935762763023376, D Loss: 1.3841181993484497\n",
            "Epoch 1, Batch 770, G Loss: 0.69355309009552, D Loss: 1.3863492012023926\n",
            "Epoch 1, Batch 771, G Loss: 0.6934760212898254, D Loss: 1.388768196105957\n",
            "Epoch 1, Batch 772, G Loss: 0.6930026412010193, D Loss: 1.3898824453353882\n",
            "Epoch 1, Batch 773, G Loss: 0.6929922103881836, D Loss: 1.3878154754638672\n",
            "Epoch 1, Batch 774, G Loss: 0.6929964423179626, D Loss: 1.3898673057556152\n",
            "Epoch 1, Batch 775, G Loss: 0.6927515864372253, D Loss: 1.3911457061767578\n",
            "Epoch 1, Batch 776, G Loss: 0.692542552947998, D Loss: 1.3916335105895996\n",
            "Epoch 1, Batch 777, G Loss: 0.6926780343055725, D Loss: 1.3956624269485474\n",
            "Epoch 1, Batch 778, G Loss: 0.692252516746521, D Loss: 1.3885504007339478\n",
            "Epoch 1, Batch 779, G Loss: 0.6919006109237671, D Loss: 1.3957011699676514\n",
            "Epoch 1, Batch 780, G Loss: 0.6921969056129456, D Loss: 1.3962321281433105\n",
            "Epoch 1, Batch 781, G Loss: 0.6919731497764587, D Loss: 1.399343729019165\n",
            "Epoch 1, Batch 782, G Loss: 0.6921246647834778, D Loss: 1.3968641757965088\n",
            "Epoch 1, Batch 783, G Loss: 0.6919353008270264, D Loss: 1.3958910703659058\n",
            "Epoch 1, Batch 784, G Loss: 0.6917134523391724, D Loss: 1.3947583436965942\n",
            "Epoch 1, Batch 785, G Loss: 0.691532552242279, D Loss: 1.3978488445281982\n",
            "Epoch 1, Batch 786, G Loss: 0.6913525462150574, D Loss: 1.398120641708374\n",
            "Epoch 1, Batch 787, G Loss: 0.6913399696350098, D Loss: 1.3975476026535034\n",
            "Epoch 1, Batch 788, G Loss: 0.6915240287780762, D Loss: 1.3990402221679688\n",
            "Epoch 1, Batch 789, G Loss: 0.691007673740387, D Loss: 1.3992688655853271\n",
            "Epoch 1, Batch 790, G Loss: 0.6908620595932007, D Loss: 1.3978824615478516\n",
            "Epoch 1, Batch 791, G Loss: 0.6907867789268494, D Loss: 1.401199460029602\n",
            "Epoch 1, Batch 792, G Loss: 0.6914271116256714, D Loss: 1.4008651971817017\n",
            "Epoch 1, Batch 793, G Loss: 0.6910902261734009, D Loss: 1.399101734161377\n",
            "Epoch 1, Batch 794, G Loss: 0.690574049949646, D Loss: 1.3964325189590454\n",
            "Epoch 1, Batch 795, G Loss: 0.6910918951034546, D Loss: 1.3997159004211426\n",
            "Epoch 1, Batch 796, G Loss: 0.6904646158218384, D Loss: 1.3991127014160156\n",
            "Epoch 1, Batch 797, G Loss: 0.6907651424407959, D Loss: 1.401882290840149\n",
            "Epoch 1, Batch 798, G Loss: 0.6908805966377258, D Loss: 1.4027107954025269\n",
            "Epoch 1, Batch 799, G Loss: 0.6910339593887329, D Loss: 1.3992319107055664\n",
            "Epoch 1, Batch 800, G Loss: 0.6908258199691772, D Loss: 1.3972804546356201\n",
            "Epoch 1, Batch 801, G Loss: 0.6903420090675354, D Loss: 1.3961899280548096\n",
            "Epoch 1, Batch 802, G Loss: 0.690528154373169, D Loss: 1.3992300033569336\n",
            "Epoch 1, Batch 803, G Loss: 0.6906136274337769, D Loss: 1.4006595611572266\n",
            "Epoch 1, Batch 804, G Loss: 0.6911203861236572, D Loss: 1.402012825012207\n",
            "Epoch 1, Batch 805, G Loss: 0.6906319260597229, D Loss: 1.4000861644744873\n",
            "Epoch 1, Batch 806, G Loss: 0.6907297968864441, D Loss: 1.4053802490234375\n",
            "Epoch 1, Batch 807, G Loss: 0.6910901665687561, D Loss: 1.4013621807098389\n",
            "Epoch 1, Batch 808, G Loss: 0.690842866897583, D Loss: 1.4062747955322266\n",
            "Epoch 1, Batch 809, G Loss: 0.6911963820457458, D Loss: 1.4072277545928955\n",
            "Epoch 1, Batch 810, G Loss: 0.6914820671081543, D Loss: 1.405216932296753\n",
            "Epoch 1, Batch 811, G Loss: 0.6913873553276062, D Loss: 1.408186435699463\n",
            "Epoch 1, Batch 812, G Loss: 0.6907740831375122, D Loss: 1.4057369232177734\n",
            "Epoch 1, Batch 813, G Loss: 0.6910143494606018, D Loss: 1.4080650806427002\n",
            "Epoch 1, Batch 814, G Loss: 0.6915019154548645, D Loss: 1.4115662574768066\n",
            "Epoch 1, Batch 815, G Loss: 0.6910974383354187, D Loss: 1.4087765216827393\n",
            "Epoch 1, Batch 816, G Loss: 0.6911339163780212, D Loss: 1.4106929302215576\n",
            "Epoch 1, Batch 817, G Loss: 0.6918990612030029, D Loss: 1.4108961820602417\n",
            "Epoch 1, Batch 818, G Loss: 0.690767228603363, D Loss: 1.4088079929351807\n",
            "Epoch 1, Batch 819, G Loss: 0.6914604902267456, D Loss: 1.4094690084457397\n",
            "Epoch 1, Batch 820, G Loss: 0.6918932795524597, D Loss: 1.4005963802337646\n",
            "Epoch 1, Batch 821, G Loss: 0.6916929483413696, D Loss: 1.4059585332870483\n",
            "Epoch 1, Batch 822, G Loss: 0.6917600631713867, D Loss: 1.4062302112579346\n",
            "Epoch 1, Batch 823, G Loss: 0.6919875144958496, D Loss: 1.4084354639053345\n",
            "Epoch 1, Batch 824, G Loss: 0.6916999220848083, D Loss: 1.410506010055542\n",
            "Epoch 1, Batch 825, G Loss: 0.692234218120575, D Loss: 1.4111335277557373\n",
            "Epoch 1, Batch 826, G Loss: 0.6919055581092834, D Loss: 1.4084601402282715\n",
            "Epoch 1, Batch 827, G Loss: 0.6922312378883362, D Loss: 1.406782865524292\n",
            "Epoch 1, Batch 828, G Loss: 0.6923981308937073, D Loss: 1.405397891998291\n",
            "Epoch 1, Batch 829, G Loss: 0.6926210522651672, D Loss: 1.4040316343307495\n",
            "Epoch 1, Batch 830, G Loss: 0.6929715275764465, D Loss: 1.4036000967025757\n",
            "Epoch 1, Batch 831, G Loss: 0.6926045417785645, D Loss: 1.4094576835632324\n",
            "Epoch 1, Batch 832, G Loss: 0.6923828125, D Loss: 1.4071035385131836\n",
            "Epoch 1, Batch 833, G Loss: 0.693084716796875, D Loss: 1.4081342220306396\n",
            "Epoch 1, Batch 834, G Loss: 0.6933074593544006, D Loss: 1.4098211526870728\n",
            "Epoch 1, Batch 835, G Loss: 0.6929401755332947, D Loss: 1.4086833000183105\n",
            "Epoch 1, Batch 836, G Loss: 0.6935661435127258, D Loss: 1.4097023010253906\n",
            "Epoch 1, Batch 837, G Loss: 0.6931612491607666, D Loss: 1.409916639328003\n",
            "Epoch 1, Batch 838, G Loss: 0.6933947801589966, D Loss: 1.4124921560287476\n",
            "Epoch 1, Batch 839, G Loss: 0.6930155754089355, D Loss: 1.4103631973266602\n",
            "Epoch 1, Batch 840, G Loss: 0.6934244632720947, D Loss: 1.4108283519744873\n",
            "Epoch 1, Batch 841, G Loss: 0.6936816573143005, D Loss: 1.4132585525512695\n",
            "Epoch 1, Batch 842, G Loss: 0.6936488747596741, D Loss: 1.4105827808380127\n",
            "Epoch 1, Batch 843, G Loss: 0.6940941214561462, D Loss: 1.407021403312683\n",
            "Epoch 1, Batch 844, G Loss: 0.6938535571098328, D Loss: 1.4073429107666016\n",
            "Epoch 1, Batch 845, G Loss: 0.693286657333374, D Loss: 1.4097262620925903\n",
            "Epoch 1, Batch 846, G Loss: 0.6942301392555237, D Loss: 1.408202886581421\n",
            "Epoch 1, Batch 847, G Loss: 0.6937341690063477, D Loss: 1.4104702472686768\n",
            "Epoch 1, Batch 848, G Loss: 0.6941649913787842, D Loss: 1.4126687049865723\n",
            "Epoch 1, Batch 849, G Loss: 0.6940974593162537, D Loss: 1.4129750728607178\n",
            "Epoch 1, Batch 850, G Loss: 0.6944729685783386, D Loss: 1.4061071872711182\n",
            "Epoch 1, Batch 851, G Loss: 0.6944663524627686, D Loss: 1.4079270362854004\n",
            "Epoch 1, Batch 852, G Loss: 0.6950705647468567, D Loss: 1.4048242568969727\n",
            "Epoch 1, Batch 853, G Loss: 0.6950967907905579, D Loss: 1.4059226512908936\n",
            "Epoch 1, Batch 854, G Loss: 0.6949371099472046, D Loss: 1.4063286781311035\n",
            "Epoch 1, Batch 855, G Loss: 0.6952072978019714, D Loss: 1.406407356262207\n",
            "Epoch 1, Batch 856, G Loss: 0.6951416730880737, D Loss: 1.4067802429199219\n",
            "Epoch 1, Batch 857, G Loss: 0.6948031783103943, D Loss: 1.4070475101470947\n",
            "Epoch 1, Batch 858, G Loss: 0.695336639881134, D Loss: 1.4103482961654663\n",
            "Epoch 1, Batch 859, G Loss: 0.6956558227539062, D Loss: 1.4081838130950928\n",
            "Epoch 1, Batch 860, G Loss: 0.6957572102546692, D Loss: 1.4062763452529907\n",
            "Epoch 1, Batch 861, G Loss: 0.695787787437439, D Loss: 1.408280372619629\n",
            "Epoch 1, Batch 862, G Loss: 0.6960484981536865, D Loss: 1.4068161249160767\n",
            "Epoch 1, Batch 863, G Loss: 0.6963167190551758, D Loss: 1.4069936275482178\n",
            "Epoch 1, Batch 864, G Loss: 0.6965187788009644, D Loss: 1.4064016342163086\n",
            "Epoch 1, Batch 865, G Loss: 0.6959266066551208, D Loss: 1.4094854593276978\n",
            "Epoch 1, Batch 866, G Loss: 0.6958249807357788, D Loss: 1.4096134901046753\n",
            "Epoch 1, Batch 867, G Loss: 0.696503221988678, D Loss: 1.408179759979248\n",
            "Epoch 1, Batch 868, G Loss: 0.6969847679138184, D Loss: 1.4039585590362549\n",
            "Epoch 1, Batch 869, G Loss: 0.6969350576400757, D Loss: 1.4053370952606201\n",
            "Epoch 1, Batch 870, G Loss: 0.6966817378997803, D Loss: 1.4012707471847534\n",
            "Epoch 1, Batch 871, G Loss: 0.6974321603775024, D Loss: 1.4017858505249023\n",
            "Epoch 1, Batch 872, G Loss: 0.6972473859786987, D Loss: 1.4022977352142334\n",
            "Epoch 1, Batch 873, G Loss: 0.6975430250167847, D Loss: 1.405778408050537\n",
            "Epoch 1, Batch 874, G Loss: 0.6979573369026184, D Loss: 1.4058434963226318\n",
            "Epoch 1, Batch 875, G Loss: 0.6973899602890015, D Loss: 1.406327724456787\n",
            "Epoch 1, Batch 876, G Loss: 0.6981364488601685, D Loss: 1.4058043956756592\n",
            "Epoch 1, Batch 877, G Loss: 0.6977171897888184, D Loss: 1.406803011894226\n",
            "Epoch 1, Batch 878, G Loss: 0.6981624960899353, D Loss: 1.4052741527557373\n",
            "Epoch 1, Batch 879, G Loss: 0.6974614858627319, D Loss: 1.4060659408569336\n",
            "Epoch 1, Batch 880, G Loss: 0.6986228823661804, D Loss: 1.4074956178665161\n",
            "Epoch 1, Batch 881, G Loss: 0.6977129578590393, D Loss: 1.4089422225952148\n",
            "Epoch 1, Batch 882, G Loss: 0.6981600522994995, D Loss: 1.4087053537368774\n",
            "Epoch 1, Batch 883, G Loss: 0.6986320614814758, D Loss: 1.4064847230911255\n",
            "Epoch 1, Batch 884, G Loss: 0.6979073286056519, D Loss: 1.4065803289413452\n",
            "Epoch 1, Batch 885, G Loss: 0.6992751955986023, D Loss: 1.4056826829910278\n",
            "Epoch 1, Batch 886, G Loss: 0.6987826824188232, D Loss: 1.4061789512634277\n",
            "Epoch 1, Batch 887, G Loss: 0.6986938714981079, D Loss: 1.401819109916687\n",
            "Epoch 1, Batch 888, G Loss: 0.6988431811332703, D Loss: 1.4034218788146973\n",
            "Epoch 1, Batch 889, G Loss: 0.6986967325210571, D Loss: 1.403829574584961\n",
            "Epoch 1, Batch 890, G Loss: 0.6990910172462463, D Loss: 1.4054310321807861\n",
            "Epoch 1, Batch 891, G Loss: 0.6990776658058167, D Loss: 1.4067164659500122\n",
            "Epoch 1, Batch 892, G Loss: 0.6991578936576843, D Loss: 1.4093314409255981\n",
            "Epoch 1, Batch 893, G Loss: 0.6989250779151917, D Loss: 1.4072701930999756\n",
            "Epoch 1, Batch 894, G Loss: 0.6992096900939941, D Loss: 1.4053584337234497\n",
            "Epoch 1, Batch 895, G Loss: 0.6994588971138, D Loss: 1.403845191001892\n",
            "Epoch 1, Batch 896, G Loss: 0.6992731094360352, D Loss: 1.404577612876892\n",
            "Epoch 1, Batch 897, G Loss: 0.7000396251678467, D Loss: 1.4032906293869019\n",
            "Epoch 1, Batch 898, G Loss: 0.6995606422424316, D Loss: 1.4044907093048096\n",
            "Epoch 1, Batch 899, G Loss: 0.699957013130188, D Loss: 1.4024600982666016\n",
            "Epoch 1, Batch 900, G Loss: 0.6999119520187378, D Loss: 1.4028913974761963\n",
            "Epoch 1, Batch 901, G Loss: 0.6995921730995178, D Loss: 1.4046615362167358\n",
            "Epoch 1, Batch 902, G Loss: 0.6999573707580566, D Loss: 1.4058644771575928\n",
            "Epoch 1, Batch 903, G Loss: 0.6999976634979248, D Loss: 1.4056092500686646\n",
            "Epoch 1, Batch 904, G Loss: 0.7000229358673096, D Loss: 1.4058024883270264\n",
            "Epoch 1, Batch 905, G Loss: 0.7000762224197388, D Loss: 1.404141902923584\n",
            "Epoch 1, Batch 906, G Loss: 0.6999161243438721, D Loss: 1.4047881364822388\n",
            "Epoch 1, Batch 907, G Loss: 0.7001619935035706, D Loss: 1.402407169342041\n",
            "Epoch 1, Batch 908, G Loss: 0.7008450627326965, D Loss: 1.4044815301895142\n",
            "Epoch 1, Batch 909, G Loss: 0.7006406188011169, D Loss: 1.4050086736679077\n",
            "Epoch 1, Batch 910, G Loss: 0.7007662653923035, D Loss: 1.401666283607483\n",
            "Epoch 1, Batch 911, G Loss: 0.7006174921989441, D Loss: 1.400862216949463\n",
            "Epoch 1, Batch 912, G Loss: 0.7004055976867676, D Loss: 1.4013253450393677\n",
            "Epoch 1, Batch 913, G Loss: 0.7009585499763489, D Loss: 1.3989694118499756\n",
            "Epoch 1, Batch 914, G Loss: 0.7008722424507141, D Loss: 1.3992197513580322\n",
            "Epoch 1, Batch 915, G Loss: 0.7003266215324402, D Loss: 1.4034746885299683\n",
            "Epoch 1, Batch 916, G Loss: 0.7008684873580933, D Loss: 1.4043638706207275\n",
            "Epoch 1, Batch 917, G Loss: 0.7006868720054626, D Loss: 1.404745101928711\n",
            "Epoch 1, Batch 918, G Loss: 0.7006532549858093, D Loss: 1.4051434993743896\n",
            "Epoch 1, Batch 919, G Loss: 0.7009028196334839, D Loss: 1.4035067558288574\n",
            "Epoch 1, Batch 920, G Loss: 0.7007570862770081, D Loss: 1.399529218673706\n",
            "Epoch 1, Batch 921, G Loss: 0.701244056224823, D Loss: 1.3999476432800293\n",
            "Epoch 1, Batch 922, G Loss: 0.701225221157074, D Loss: 1.399719476699829\n",
            "Epoch 1, Batch 923, G Loss: 0.7010297775268555, D Loss: 1.4009077548980713\n",
            "Epoch 1, Batch 924, G Loss: 0.7017043828964233, D Loss: 1.4004077911376953\n",
            "Epoch 1, Batch 925, G Loss: 0.701330840587616, D Loss: 1.399585247039795\n",
            "Epoch 1, Batch 926, G Loss: 0.7009475231170654, D Loss: 1.3992712497711182\n",
            "Epoch 1, Batch 927, G Loss: 0.7018359303474426, D Loss: 1.3968784809112549\n",
            "Epoch 1, Batch 928, G Loss: 0.7012054920196533, D Loss: 1.3922247886657715\n",
            "Epoch 1, Batch 929, G Loss: 0.7014973163604736, D Loss: 1.3880665302276611\n",
            "Epoch 1, Batch 930, G Loss: 0.7014638185501099, D Loss: 1.3960425853729248\n",
            "Epoch 1, Batch 931, G Loss: 0.7014098167419434, D Loss: 1.4005800485610962\n",
            "Epoch 1, Batch 932, G Loss: 0.7016187310218811, D Loss: 1.3987575769424438\n",
            "Epoch 1, Batch 933, G Loss: 0.701482355594635, D Loss: 1.3973207473754883\n",
            "Epoch 1, Batch 934, G Loss: 0.7018505930900574, D Loss: 1.3979851007461548\n",
            "Epoch 1, Batch 935, G Loss: 0.7015947699546814, D Loss: 1.3966772556304932\n",
            "Epoch 1, Batch 936, G Loss: 0.7015925049781799, D Loss: 1.3959442377090454\n",
            "Epoch 1, Batch 937, G Loss: 0.7015830278396606, D Loss: 1.4002296924591064\n",
            "Epoch 1, Batch 938, G Loss: 0.7014597654342651, D Loss: 1.3998711109161377\n",
            "Epoch 2, Batch 1, G Loss: 0.7019158005714417, D Loss: 1.397706389427185\n",
            "Epoch 2, Batch 2, G Loss: 0.7017081379890442, D Loss: 1.3976353406906128\n",
            "Epoch 2, Batch 3, G Loss: 0.7020483016967773, D Loss: 1.3975255489349365\n",
            "Epoch 2, Batch 4, G Loss: 0.7018396854400635, D Loss: 1.3967268466949463\n",
            "Epoch 2, Batch 5, G Loss: 0.7017775774002075, D Loss: 1.3978339433670044\n",
            "Epoch 2, Batch 6, G Loss: 0.7019321918487549, D Loss: 1.397089958190918\n",
            "Epoch 2, Batch 7, G Loss: 0.702153742313385, D Loss: 1.397210717201233\n",
            "Epoch 2, Batch 8, G Loss: 0.7017514109611511, D Loss: 1.3974932432174683\n",
            "Epoch 2, Batch 9, G Loss: 0.7016888856887817, D Loss: 1.396959900856018\n",
            "Epoch 2, Batch 10, G Loss: 0.702184796333313, D Loss: 1.397128701210022\n",
            "Epoch 2, Batch 11, G Loss: 0.7020425796508789, D Loss: 1.3960728645324707\n",
            "Epoch 2, Batch 12, G Loss: 0.7018199563026428, D Loss: 1.395232915878296\n",
            "Epoch 2, Batch 13, G Loss: 0.7021069526672363, D Loss: 1.3954994678497314\n",
            "Epoch 2, Batch 14, G Loss: 0.7016825675964355, D Loss: 1.3956060409545898\n",
            "Epoch 2, Batch 15, G Loss: 0.7019259333610535, D Loss: 1.3956345319747925\n",
            "Epoch 2, Batch 16, G Loss: 0.7023161053657532, D Loss: 1.3958410024642944\n",
            "Epoch 2, Batch 17, G Loss: 0.7018599510192871, D Loss: 1.3971654176712036\n",
            "Epoch 2, Batch 18, G Loss: 0.7021700143814087, D Loss: 1.3963284492492676\n",
            "Epoch 2, Batch 19, G Loss: 0.7023765444755554, D Loss: 1.3940871953964233\n",
            "Epoch 2, Batch 20, G Loss: 0.7022697329521179, D Loss: 1.3922255039215088\n",
            "Epoch 2, Batch 21, G Loss: 0.7025096416473389, D Loss: 1.392549991607666\n",
            "Epoch 2, Batch 22, G Loss: 0.7022082209587097, D Loss: 1.3916196823120117\n",
            "Epoch 2, Batch 23, G Loss: 0.7021920084953308, D Loss: 1.3947532176971436\n",
            "Epoch 2, Batch 24, G Loss: 0.7021676898002625, D Loss: 1.3948414325714111\n",
            "Epoch 2, Batch 25, G Loss: 0.702329158782959, D Loss: 1.394606113433838\n",
            "Epoch 2, Batch 26, G Loss: 0.7020337581634521, D Loss: 1.3950376510620117\n",
            "Epoch 2, Batch 27, G Loss: 0.7023277282714844, D Loss: 1.3945238590240479\n",
            "Epoch 2, Batch 28, G Loss: 0.7024211287498474, D Loss: 1.3942108154296875\n",
            "Epoch 2, Batch 29, G Loss: 0.7023691534996033, D Loss: 1.3941148519515991\n",
            "Epoch 2, Batch 30, G Loss: 0.7024406790733337, D Loss: 1.3930835723876953\n",
            "Epoch 2, Batch 31, G Loss: 0.7021324038505554, D Loss: 1.3925774097442627\n",
            "Epoch 2, Batch 32, G Loss: 0.702244222164154, D Loss: 1.3919355869293213\n",
            "Epoch 2, Batch 33, G Loss: 0.7025594711303711, D Loss: 1.3923923969268799\n",
            "Epoch 2, Batch 34, G Loss: 0.7019119262695312, D Loss: 1.3933264017105103\n",
            "Epoch 2, Batch 35, G Loss: 0.7022468447685242, D Loss: 1.3932297229766846\n",
            "Epoch 2, Batch 36, G Loss: 0.7022885680198669, D Loss: 1.3925044536590576\n",
            "Epoch 2, Batch 37, G Loss: 0.7019131779670715, D Loss: 1.3932480812072754\n",
            "Epoch 2, Batch 38, G Loss: 0.7024129629135132, D Loss: 1.392838478088379\n",
            "Epoch 2, Batch 39, G Loss: 0.7022798657417297, D Loss: 1.39263916015625\n",
            "Epoch 2, Batch 40, G Loss: 0.7022613286972046, D Loss: 1.3922126293182373\n",
            "Epoch 2, Batch 41, G Loss: 0.702318549156189, D Loss: 1.3921180963516235\n",
            "Epoch 2, Batch 42, G Loss: 0.7020761966705322, D Loss: 1.3920340538024902\n",
            "Epoch 2, Batch 43, G Loss: 0.7022072672843933, D Loss: 1.3922916650772095\n",
            "Epoch 2, Batch 44, G Loss: 0.7022415399551392, D Loss: 1.3918609619140625\n",
            "Epoch 2, Batch 45, G Loss: 0.7021400332450867, D Loss: 1.391512393951416\n",
            "Epoch 2, Batch 46, G Loss: 0.7021390795707703, D Loss: 1.3917922973632812\n",
            "Epoch 2, Batch 47, G Loss: 0.7018815875053406, D Loss: 1.3926364183425903\n",
            "Epoch 2, Batch 48, G Loss: 0.7018740773200989, D Loss: 1.392110824584961\n",
            "Epoch 2, Batch 49, G Loss: 0.7017937302589417, D Loss: 1.3924256563186646\n",
            "Epoch 2, Batch 50, G Loss: 0.7020503282546997, D Loss: 1.392004370689392\n",
            "Epoch 2, Batch 51, G Loss: 0.7017608880996704, D Loss: 1.3918853998184204\n",
            "Epoch 2, Batch 52, G Loss: 0.7018951773643494, D Loss: 1.3917207717895508\n",
            "Epoch 2, Batch 53, G Loss: 0.7018659710884094, D Loss: 1.391168236732483\n",
            "Epoch 2, Batch 54, G Loss: 0.7018646597862244, D Loss: 1.3916559219360352\n",
            "Epoch 2, Batch 55, G Loss: 0.7017183303833008, D Loss: 1.3915603160858154\n",
            "Epoch 2, Batch 56, G Loss: 0.7016819715499878, D Loss: 1.3912570476531982\n",
            "Epoch 2, Batch 57, G Loss: 0.7017008066177368, D Loss: 1.3907713890075684\n",
            "Epoch 2, Batch 58, G Loss: 0.7018122673034668, D Loss: 1.39042067527771\n",
            "Epoch 2, Batch 59, G Loss: 0.7016394734382629, D Loss: 1.3910084962844849\n",
            "Epoch 2, Batch 60, G Loss: 0.7017250061035156, D Loss: 1.3908556699752808\n",
            "Epoch 2, Batch 61, G Loss: 0.7015052437782288, D Loss: 1.3909425735473633\n",
            "Epoch 2, Batch 62, G Loss: 0.7015841603279114, D Loss: 1.3907086849212646\n",
            "Epoch 2, Batch 63, G Loss: 0.701915442943573, D Loss: 1.3903048038482666\n",
            "Epoch 2, Batch 64, G Loss: 0.7016515135765076, D Loss: 1.390048861503601\n",
            "Epoch 2, Batch 65, G Loss: 0.7016993165016174, D Loss: 1.390128493309021\n",
            "Epoch 2, Batch 66, G Loss: 0.7016454935073853, D Loss: 1.3901407718658447\n",
            "Epoch 2, Batch 67, G Loss: 0.7017406821250916, D Loss: 1.3898398876190186\n",
            "Epoch 2, Batch 68, G Loss: 0.7014722228050232, D Loss: 1.3901762962341309\n",
            "Epoch 2, Batch 69, G Loss: 0.7014351487159729, D Loss: 1.3903303146362305\n",
            "Epoch 2, Batch 70, G Loss: 0.7014697194099426, D Loss: 1.3899872303009033\n",
            "Epoch 2, Batch 71, G Loss: 0.7015037536621094, D Loss: 1.3891949653625488\n",
            "Epoch 2, Batch 72, G Loss: 0.7014211416244507, D Loss: 1.389533519744873\n",
            "Epoch 2, Batch 73, G Loss: 0.7014893293380737, D Loss: 1.3891535997390747\n",
            "Epoch 2, Batch 74, G Loss: 0.7013356685638428, D Loss: 1.389089584350586\n",
            "Epoch 2, Batch 75, G Loss: 0.7013676166534424, D Loss: 1.3894355297088623\n",
            "Epoch 2, Batch 76, G Loss: 0.7011664509773254, D Loss: 1.3893853425979614\n",
            "Epoch 2, Batch 77, G Loss: 0.7012125849723816, D Loss: 1.3891456127166748\n",
            "Epoch 2, Batch 78, G Loss: 0.7013517022132874, D Loss: 1.3891301155090332\n",
            "Epoch 2, Batch 79, G Loss: 0.7011037468910217, D Loss: 1.3892498016357422\n",
            "Epoch 2, Batch 80, G Loss: 0.7013996839523315, D Loss: 1.3887819051742554\n",
            "Epoch 2, Batch 81, G Loss: 0.701274037361145, D Loss: 1.3888728618621826\n",
            "Epoch 2, Batch 82, G Loss: 0.7011863589286804, D Loss: 1.3887925148010254\n",
            "Epoch 2, Batch 83, G Loss: 0.7011049389839172, D Loss: 1.3889307975769043\n",
            "Epoch 2, Batch 84, G Loss: 0.7011357545852661, D Loss: 1.388667345046997\n",
            "Epoch 2, Batch 85, G Loss: 0.7011367082595825, D Loss: 1.3884382247924805\n",
            "Epoch 2, Batch 86, G Loss: 0.7011359333992004, D Loss: 1.3882737159729004\n",
            "Epoch 2, Batch 87, G Loss: 0.700872540473938, D Loss: 1.3886699676513672\n",
            "Epoch 2, Batch 88, G Loss: 0.7011695504188538, D Loss: 1.3882780075073242\n",
            "Epoch 2, Batch 89, G Loss: 0.7010579109191895, D Loss: 1.388388991355896\n",
            "Epoch 2, Batch 90, G Loss: 0.7008485794067383, D Loss: 1.388384222984314\n",
            "Epoch 2, Batch 91, G Loss: 0.7009010910987854, D Loss: 1.3882131576538086\n",
            "Epoch 2, Batch 92, G Loss: 0.7009658813476562, D Loss: 1.3881497383117676\n",
            "Epoch 2, Batch 93, G Loss: 0.7007492184638977, D Loss: 1.388317346572876\n",
            "Epoch 2, Batch 94, G Loss: 0.7010833024978638, D Loss: 1.3875858783721924\n",
            "Epoch 2, Batch 95, G Loss: 0.7007774114608765, D Loss: 1.387960433959961\n",
            "Epoch 2, Batch 96, G Loss: 0.7008970379829407, D Loss: 1.3876808881759644\n",
            "Epoch 2, Batch 97, G Loss: 0.7008157968521118, D Loss: 1.387793779373169\n",
            "Epoch 2, Batch 98, G Loss: 0.7005640268325806, D Loss: 1.3877969980239868\n",
            "Epoch 2, Batch 99, G Loss: 0.700861394405365, D Loss: 1.3875527381896973\n",
            "Epoch 2, Batch 100, G Loss: 0.700783371925354, D Loss: 1.3874356746673584\n",
            "Epoch 2, Batch 101, G Loss: 0.7005924582481384, D Loss: 1.3876829147338867\n",
            "Epoch 2, Batch 102, G Loss: 0.7008357048034668, D Loss: 1.3874162435531616\n",
            "Epoch 2, Batch 103, G Loss: 0.7006639838218689, D Loss: 1.3871989250183105\n",
            "Epoch 2, Batch 104, G Loss: 0.700874924659729, D Loss: 1.387218713760376\n",
            "Epoch 2, Batch 105, G Loss: 0.7006499767303467, D Loss: 1.387019395828247\n",
            "Epoch 2, Batch 106, G Loss: 0.700623631477356, D Loss: 1.387138843536377\n",
            "Epoch 2, Batch 107, G Loss: 0.7004856467247009, D Loss: 1.387133002281189\n",
            "Epoch 2, Batch 108, G Loss: 0.700512170791626, D Loss: 1.3872179985046387\n",
            "Epoch 2, Batch 109, G Loss: 0.7006475329399109, D Loss: 1.387242317199707\n",
            "Epoch 2, Batch 110, G Loss: 0.7006092667579651, D Loss: 1.3869571685791016\n",
            "Epoch 2, Batch 111, G Loss: 0.7006914019584656, D Loss: 1.3866815567016602\n",
            "Epoch 2, Batch 112, G Loss: 0.7008638978004456, D Loss: 1.3865277767181396\n",
            "Epoch 2, Batch 113, G Loss: 0.7006328105926514, D Loss: 1.386631727218628\n",
            "Epoch 2, Batch 114, G Loss: 0.7006062269210815, D Loss: 1.3867337703704834\n",
            "Epoch 2, Batch 115, G Loss: 0.7005566358566284, D Loss: 1.3863615989685059\n",
            "Epoch 2, Batch 116, G Loss: 0.7008684873580933, D Loss: 1.3860790729522705\n",
            "Epoch 2, Batch 117, G Loss: 0.7007282972335815, D Loss: 1.3864140510559082\n",
            "Epoch 2, Batch 118, G Loss: 0.7008532881736755, D Loss: 1.3860297203063965\n",
            "Epoch 2, Batch 119, G Loss: 0.7007298469543457, D Loss: 1.386159896850586\n",
            "Epoch 2, Batch 120, G Loss: 0.700935959815979, D Loss: 1.386283278465271\n",
            "Epoch 2, Batch 121, G Loss: 0.7010871767997742, D Loss: 1.3860591650009155\n",
            "Epoch 2, Batch 122, G Loss: 0.7009574770927429, D Loss: 1.3862006664276123\n",
            "Epoch 2, Batch 123, G Loss: 0.7010749578475952, D Loss: 1.386298656463623\n",
            "Epoch 2, Batch 124, G Loss: 0.7012698650360107, D Loss: 1.3864625692367554\n",
            "Epoch 2, Batch 125, G Loss: 0.7011810541152954, D Loss: 1.3862383365631104\n",
            "Epoch 2, Batch 126, G Loss: 0.7012473940849304, D Loss: 1.386357069015503\n",
            "Epoch 2, Batch 127, G Loss: 0.7011754512786865, D Loss: 1.3860831260681152\n",
            "Epoch 2, Batch 128, G Loss: 0.7013435363769531, D Loss: 1.3852920532226562\n",
            "Epoch 2, Batch 129, G Loss: 0.7016820907592773, D Loss: 1.3851239681243896\n",
            "Epoch 2, Batch 130, G Loss: 0.7014353275299072, D Loss: 1.3861322402954102\n",
            "Epoch 2, Batch 131, G Loss: 0.7015627026557922, D Loss: 1.3857192993164062\n",
            "Epoch 2, Batch 132, G Loss: 0.7015295028686523, D Loss: 1.385511875152588\n",
            "Epoch 2, Batch 133, G Loss: 0.7017677426338196, D Loss: 1.3864367008209229\n",
            "Epoch 2, Batch 134, G Loss: 0.7019039988517761, D Loss: 1.385948657989502\n",
            "Epoch 2, Batch 135, G Loss: 0.7018617391586304, D Loss: 1.3859069347381592\n",
            "Epoch 2, Batch 136, G Loss: 0.7020883560180664, D Loss: 1.3860323429107666\n",
            "Epoch 2, Batch 137, G Loss: 0.7019137144088745, D Loss: 1.385509967803955\n",
            "Epoch 2, Batch 138, G Loss: 0.702018141746521, D Loss: 1.3852002620697021\n",
            "Epoch 2, Batch 139, G Loss: 0.7022175192832947, D Loss: 1.3845820426940918\n",
            "Epoch 2, Batch 140, G Loss: 0.7023623585700989, D Loss: 1.3851946592330933\n",
            "Epoch 2, Batch 141, G Loss: 0.7024096846580505, D Loss: 1.3847436904907227\n",
            "Epoch 2, Batch 142, G Loss: 0.702485203742981, D Loss: 1.3850858211517334\n",
            "Epoch 2, Batch 143, G Loss: 0.702690601348877, D Loss: 1.3842194080352783\n",
            "Epoch 2, Batch 144, G Loss: 0.7029452919960022, D Loss: 1.384648084640503\n",
            "Epoch 2, Batch 145, G Loss: 0.7029797434806824, D Loss: 1.3836965560913086\n",
            "Epoch 2, Batch 146, G Loss: 0.7032332420349121, D Loss: 1.3848164081573486\n",
            "Epoch 2, Batch 147, G Loss: 0.7032994031906128, D Loss: 1.3851115703582764\n",
            "Epoch 2, Batch 148, G Loss: 0.703313410282135, D Loss: 1.385152816772461\n",
            "Epoch 2, Batch 149, G Loss: 0.7035092711448669, D Loss: 1.3853963613510132\n",
            "Epoch 2, Batch 150, G Loss: 0.7034525871276855, D Loss: 1.3839068412780762\n",
            "Epoch 2, Batch 151, G Loss: 0.7036150097846985, D Loss: 1.3842459917068481\n",
            "Epoch 2, Batch 152, G Loss: 0.7036635875701904, D Loss: 1.3850306272506714\n",
            "Epoch 2, Batch 153, G Loss: 0.7037597298622131, D Loss: 1.3847359418869019\n",
            "Epoch 2, Batch 154, G Loss: 0.7036889791488647, D Loss: 1.3844578266143799\n",
            "Epoch 2, Batch 155, G Loss: 0.7038561701774597, D Loss: 1.384235143661499\n",
            "Epoch 2, Batch 156, G Loss: 0.7038342356681824, D Loss: 1.3841300010681152\n",
            "Epoch 2, Batch 157, G Loss: 0.7040826678276062, D Loss: 1.3846406936645508\n",
            "Epoch 2, Batch 158, G Loss: 0.7039591073989868, D Loss: 1.3837900161743164\n",
            "Epoch 2, Batch 159, G Loss: 0.7038571238517761, D Loss: 1.3832288980484009\n",
            "Epoch 2, Batch 160, G Loss: 0.7040733695030212, D Loss: 1.3861136436462402\n",
            "Epoch 2, Batch 161, G Loss: 0.7038944959640503, D Loss: 1.3864370584487915\n",
            "Epoch 2, Batch 162, G Loss: 0.7039438486099243, D Loss: 1.3851380348205566\n",
            "Epoch 2, Batch 163, G Loss: 0.7039204835891724, D Loss: 1.3848278522491455\n",
            "Epoch 2, Batch 164, G Loss: 0.7040011286735535, D Loss: 1.3849987983703613\n",
            "Epoch 2, Batch 165, G Loss: 0.703830897808075, D Loss: 1.3844449520111084\n",
            "Epoch 2, Batch 166, G Loss: 0.7039313912391663, D Loss: 1.3838226795196533\n",
            "Epoch 2, Batch 167, G Loss: 0.7037056684494019, D Loss: 1.3829247951507568\n",
            "Epoch 2, Batch 168, G Loss: 0.7038141489028931, D Loss: 1.3834354877471924\n",
            "Epoch 2, Batch 169, G Loss: 0.703648567199707, D Loss: 1.3840980529785156\n",
            "Epoch 2, Batch 170, G Loss: 0.7037981152534485, D Loss: 1.3838882446289062\n",
            "Epoch 2, Batch 171, G Loss: 0.7037497758865356, D Loss: 1.383589744567871\n",
            "Epoch 2, Batch 172, G Loss: 0.7036877274513245, D Loss: 1.3843576908111572\n",
            "Epoch 2, Batch 173, G Loss: 0.7037392258644104, D Loss: 1.384554386138916\n",
            "Epoch 2, Batch 174, G Loss: 0.7036344408988953, D Loss: 1.3833467960357666\n",
            "Epoch 2, Batch 175, G Loss: 0.7035651803016663, D Loss: 1.3837361335754395\n",
            "Epoch 2, Batch 176, G Loss: 0.7035924196243286, D Loss: 1.383453130722046\n",
            "Epoch 2, Batch 177, G Loss: 0.7037104964256287, D Loss: 1.3828699588775635\n",
            "Epoch 2, Batch 178, G Loss: 0.7035406231880188, D Loss: 1.3835740089416504\n",
            "Epoch 2, Batch 179, G Loss: 0.7036184072494507, D Loss: 1.383101224899292\n",
            "Epoch 2, Batch 180, G Loss: 0.7039457559585571, D Loss: 1.382781744003296\n",
            "Epoch 2, Batch 181, G Loss: 0.7037359476089478, D Loss: 1.3829305171966553\n",
            "Epoch 2, Batch 182, G Loss: 0.7038927674293518, D Loss: 1.3829106092453003\n",
            "Epoch 2, Batch 183, G Loss: 0.7040016055107117, D Loss: 1.382968544960022\n",
            "Epoch 2, Batch 184, G Loss: 0.703961968421936, D Loss: 1.3826496601104736\n",
            "Epoch 2, Batch 185, G Loss: 0.7041027545928955, D Loss: 1.3829611539840698\n",
            "Epoch 2, Batch 186, G Loss: 0.70435631275177, D Loss: 1.382704257965088\n",
            "Epoch 2, Batch 187, G Loss: 0.7043709754943848, D Loss: 1.3824697732925415\n",
            "Epoch 2, Batch 188, G Loss: 0.7046013474464417, D Loss: 1.384277582168579\n",
            "Epoch 2, Batch 189, G Loss: 0.704717218875885, D Loss: 1.3832671642303467\n",
            "Epoch 2, Batch 190, G Loss: 0.704717218875885, D Loss: 1.3833115100860596\n",
            "Epoch 2, Batch 191, G Loss: 0.705085277557373, D Loss: 1.3834307193756104\n",
            "Epoch 2, Batch 192, G Loss: 0.704951822757721, D Loss: 1.3839188814163208\n",
            "Epoch 2, Batch 193, G Loss: 0.7052006721496582, D Loss: 1.3831095695495605\n",
            "Epoch 2, Batch 194, G Loss: 0.7051617503166199, D Loss: 1.3830327987670898\n",
            "Epoch 2, Batch 195, G Loss: 0.7050526142120361, D Loss: 1.3839306831359863\n",
            "Epoch 2, Batch 196, G Loss: 0.7050826549530029, D Loss: 1.3835763931274414\n",
            "Epoch 2, Batch 197, G Loss: 0.7051626443862915, D Loss: 1.3837168216705322\n",
            "Epoch 2, Batch 198, G Loss: 0.7051752805709839, D Loss: 1.383647084236145\n",
            "Epoch 2, Batch 199, G Loss: 0.705084502696991, D Loss: 1.3830156326293945\n",
            "Epoch 2, Batch 200, G Loss: 0.7052503228187561, D Loss: 1.3816719055175781\n",
            "Epoch 2, Batch 201, G Loss: 0.7051556706428528, D Loss: 1.3808143138885498\n",
            "Epoch 2, Batch 202, G Loss: 0.7052179574966431, D Loss: 1.3827508687973022\n",
            "Epoch 2, Batch 203, G Loss: 0.7050827145576477, D Loss: 1.3843355178833008\n",
            "Epoch 2, Batch 204, G Loss: 0.7050819993019104, D Loss: 1.384103775024414\n",
            "Epoch 2, Batch 205, G Loss: 0.7049603462219238, D Loss: 1.3853589296340942\n",
            "Epoch 2, Batch 206, G Loss: 0.7050501108169556, D Loss: 1.3830541372299194\n",
            "Epoch 2, Batch 207, G Loss: 0.7050844430923462, D Loss: 1.383563756942749\n",
            "Epoch 2, Batch 208, G Loss: 0.7048704028129578, D Loss: 1.3831335306167603\n",
            "Epoch 2, Batch 209, G Loss: 0.7047131657600403, D Loss: 1.3832707405090332\n",
            "Epoch 2, Batch 210, G Loss: 0.7048051953315735, D Loss: 1.3826645612716675\n",
            "Epoch 2, Batch 211, G Loss: 0.7047337889671326, D Loss: 1.3832430839538574\n",
            "Epoch 2, Batch 212, G Loss: 0.7046667337417603, D Loss: 1.3842591047286987\n",
            "Epoch 2, Batch 213, G Loss: 0.7045300006866455, D Loss: 1.3831214904785156\n",
            "Epoch 2, Batch 214, G Loss: 0.7045109868049622, D Loss: 1.380843997001648\n",
            "Epoch 2, Batch 215, G Loss: 0.7045310139656067, D Loss: 1.379965901374817\n",
            "Epoch 2, Batch 216, G Loss: 0.7045379877090454, D Loss: 1.38100266456604\n",
            "Epoch 2, Batch 217, G Loss: 0.7045063972473145, D Loss: 1.3819507360458374\n",
            "Epoch 2, Batch 218, G Loss: 0.7045403122901917, D Loss: 1.3819479942321777\n",
            "Epoch 2, Batch 219, G Loss: 0.7045000195503235, D Loss: 1.3810020685195923\n",
            "Epoch 2, Batch 220, G Loss: 0.7044685482978821, D Loss: 1.38383150100708\n",
            "Epoch 2, Batch 221, G Loss: 0.7045162916183472, D Loss: 1.3835697174072266\n",
            "Epoch 2, Batch 222, G Loss: 0.7044744491577148, D Loss: 1.3842376470565796\n",
            "Epoch 2, Batch 223, G Loss: 0.704531192779541, D Loss: 1.383103847503662\n",
            "Epoch 2, Batch 224, G Loss: 0.7045440077781677, D Loss: 1.3832597732543945\n",
            "Epoch 2, Batch 225, G Loss: 0.7046735882759094, D Loss: 1.3824788331985474\n",
            "Epoch 2, Batch 226, G Loss: 0.7047715187072754, D Loss: 1.3813714981079102\n",
            "Epoch 2, Batch 227, G Loss: 0.7047826647758484, D Loss: 1.3807991743087769\n",
            "Epoch 2, Batch 228, G Loss: 0.704984188079834, D Loss: 1.3802070617675781\n",
            "Epoch 2, Batch 229, G Loss: 0.7048906683921814, D Loss: 1.3810855150222778\n",
            "Epoch 2, Batch 230, G Loss: 0.705023467540741, D Loss: 1.382200002670288\n",
            "Epoch 2, Batch 231, G Loss: 0.705091655254364, D Loss: 1.3810031414031982\n",
            "Epoch 2, Batch 232, G Loss: 0.7051185965538025, D Loss: 1.3814046382904053\n",
            "Epoch 2, Batch 233, G Loss: 0.7053287029266357, D Loss: 1.381095051765442\n",
            "Epoch 2, Batch 234, G Loss: 0.7054356336593628, D Loss: 1.3810186386108398\n",
            "Epoch 2, Batch 235, G Loss: 0.705602765083313, D Loss: 1.3796327114105225\n",
            "Epoch 2, Batch 236, G Loss: 0.7056788206100464, D Loss: 1.3814603090286255\n",
            "Epoch 2, Batch 237, G Loss: 0.7058111429214478, D Loss: 1.3804268836975098\n",
            "Epoch 2, Batch 238, G Loss: 0.7060244679450989, D Loss: 1.3795543909072876\n",
            "Epoch 2, Batch 239, G Loss: 0.7059532403945923, D Loss: 1.3804153203964233\n",
            "Epoch 2, Batch 240, G Loss: 0.7060274481773376, D Loss: 1.3799536228179932\n",
            "Epoch 2, Batch 241, G Loss: 0.706148087978363, D Loss: 1.3795814514160156\n",
            "Epoch 2, Batch 242, G Loss: 0.7061375379562378, D Loss: 1.3796377182006836\n",
            "Epoch 2, Batch 243, G Loss: 0.7063291668891907, D Loss: 1.3805023431777954\n",
            "Epoch 2, Batch 244, G Loss: 0.7062584757804871, D Loss: 1.3803205490112305\n",
            "Epoch 2, Batch 245, G Loss: 0.7064201831817627, D Loss: 1.380671739578247\n",
            "Epoch 2, Batch 246, G Loss: 0.7065004110336304, D Loss: 1.3805221319198608\n",
            "Epoch 2, Batch 247, G Loss: 0.7064012885093689, D Loss: 1.3807766437530518\n",
            "Epoch 2, Batch 248, G Loss: 0.7063949704170227, D Loss: 1.3799489736557007\n",
            "Epoch 2, Batch 249, G Loss: 0.7064639329910278, D Loss: 1.3816096782684326\n",
            "Epoch 2, Batch 250, G Loss: 0.7065381407737732, D Loss: 1.3798733949661255\n",
            "Epoch 2, Batch 251, G Loss: 0.706473708152771, D Loss: 1.3812315464019775\n",
            "Epoch 2, Batch 252, G Loss: 0.7064741253852844, D Loss: 1.382239818572998\n",
            "Epoch 2, Batch 253, G Loss: 0.7064319252967834, D Loss: 1.3820221424102783\n",
            "Epoch 2, Batch 254, G Loss: 0.7063930034637451, D Loss: 1.3812222480773926\n",
            "Epoch 2, Batch 255, G Loss: 0.7063478231430054, D Loss: 1.3805124759674072\n",
            "Epoch 2, Batch 256, G Loss: 0.7063536643981934, D Loss: 1.3806419372558594\n",
            "Epoch 2, Batch 257, G Loss: 0.7062522768974304, D Loss: 1.3802903890609741\n",
            "Epoch 2, Batch 258, G Loss: 0.7062423229217529, D Loss: 1.380119800567627\n",
            "Epoch 2, Batch 259, G Loss: 0.7062034010887146, D Loss: 1.3804153203964233\n",
            "Epoch 2, Batch 260, G Loss: 0.7062008380889893, D Loss: 1.3797788619995117\n",
            "Epoch 2, Batch 261, G Loss: 0.7062111496925354, D Loss: 1.3790230751037598\n",
            "Epoch 2, Batch 262, G Loss: 0.7062590718269348, D Loss: 1.3798234462738037\n",
            "Epoch 2, Batch 263, G Loss: 0.706233561038971, D Loss: 1.380375623703003\n",
            "Epoch 2, Batch 264, G Loss: 0.7062180042266846, D Loss: 1.379127860069275\n",
            "Epoch 2, Batch 265, G Loss: 0.7062972784042358, D Loss: 1.379106044769287\n",
            "Epoch 2, Batch 266, G Loss: 0.70628821849823, D Loss: 1.3799810409545898\n",
            "Epoch 2, Batch 267, G Loss: 0.7063962817192078, D Loss: 1.3788347244262695\n",
            "Epoch 2, Batch 268, G Loss: 0.706317126750946, D Loss: 1.3812083005905151\n",
            "Epoch 2, Batch 269, G Loss: 0.7063636779785156, D Loss: 1.3819290399551392\n",
            "Epoch 2, Batch 270, G Loss: 0.7064696550369263, D Loss: 1.3809614181518555\n",
            "Epoch 2, Batch 271, G Loss: 0.7065246105194092, D Loss: 1.3826889991760254\n",
            "Epoch 2, Batch 272, G Loss: 0.7064986824989319, D Loss: 1.3792734146118164\n",
            "Epoch 2, Batch 273, G Loss: 0.7066921591758728, D Loss: 1.3790397644042969\n",
            "Epoch 2, Batch 274, G Loss: 0.7067725658416748, D Loss: 1.379608392715454\n",
            "Epoch 2, Batch 275, G Loss: 0.7068946957588196, D Loss: 1.3780229091644287\n",
            "Epoch 2, Batch 276, G Loss: 0.7068862915039062, D Loss: 1.3782689571380615\n",
            "Epoch 2, Batch 277, G Loss: 0.7070043087005615, D Loss: 1.3782856464385986\n",
            "Epoch 2, Batch 278, G Loss: 0.7070563435554504, D Loss: 1.378051519393921\n",
            "Epoch 2, Batch 279, G Loss: 0.7071747779846191, D Loss: 1.3774940967559814\n",
            "Epoch 2, Batch 280, G Loss: 0.7073882818222046, D Loss: 1.377525806427002\n",
            "Epoch 2, Batch 281, G Loss: 0.7074271440505981, D Loss: 1.380254864692688\n",
            "Epoch 2, Batch 282, G Loss: 0.7075291275978088, D Loss: 1.3792082071304321\n",
            "Epoch 2, Batch 283, G Loss: 0.7077227830886841, D Loss: 1.3784770965576172\n",
            "Epoch 2, Batch 284, G Loss: 0.7078970074653625, D Loss: 1.3798551559448242\n",
            "Epoch 2, Batch 285, G Loss: 0.7079862356185913, D Loss: 1.3801610469818115\n",
            "Epoch 2, Batch 286, G Loss: 0.7080810070037842, D Loss: 1.3790373802185059\n",
            "Epoch 2, Batch 287, G Loss: 0.708214282989502, D Loss: 1.3777070045471191\n",
            "Epoch 2, Batch 288, G Loss: 0.7082105278968811, D Loss: 1.3791406154632568\n",
            "Epoch 2, Batch 289, G Loss: 0.7083188891410828, D Loss: 1.3780932426452637\n",
            "Epoch 2, Batch 290, G Loss: 0.7084470391273499, D Loss: 1.3794803619384766\n",
            "Epoch 2, Batch 291, G Loss: 0.708465576171875, D Loss: 1.3794660568237305\n",
            "Epoch 2, Batch 292, G Loss: 0.7085228562355042, D Loss: 1.379821538925171\n",
            "Epoch 2, Batch 293, G Loss: 0.7085250616073608, D Loss: 1.378791093826294\n",
            "Epoch 2, Batch 294, G Loss: 0.7085646986961365, D Loss: 1.378661870956421\n",
            "Epoch 2, Batch 295, G Loss: 0.7085677981376648, D Loss: 1.3784186840057373\n",
            "Epoch 2, Batch 296, G Loss: 0.7085471153259277, D Loss: 1.3796072006225586\n",
            "Epoch 2, Batch 297, G Loss: 0.7085530161857605, D Loss: 1.3786406517028809\n",
            "Epoch 2, Batch 298, G Loss: 0.708619236946106, D Loss: 1.3767362833023071\n",
            "Epoch 2, Batch 299, G Loss: 0.7085447311401367, D Loss: 1.377785563468933\n",
            "Epoch 2, Batch 300, G Loss: 0.7085643410682678, D Loss: 1.3776817321777344\n",
            "Epoch 2, Batch 301, G Loss: 0.7085646986961365, D Loss: 1.3836889266967773\n",
            "Epoch 2, Batch 302, G Loss: 0.7085106372833252, D Loss: 1.380215048789978\n",
            "Epoch 2, Batch 303, G Loss: 0.7085344195365906, D Loss: 1.3830680847167969\n",
            "Epoch 2, Batch 304, G Loss: 0.708446741104126, D Loss: 1.381653070449829\n",
            "Epoch 2, Batch 305, G Loss: 0.7084714770317078, D Loss: 1.3818981647491455\n",
            "Epoch 2, Batch 306, G Loss: 0.7083356380462646, D Loss: 1.3825123310089111\n",
            "Epoch 2, Batch 307, G Loss: 0.7081971168518066, D Loss: 1.3815152645111084\n",
            "Epoch 2, Batch 308, G Loss: 0.7081743478775024, D Loss: 1.3813350200653076\n",
            "Epoch 2, Batch 309, G Loss: 0.7080923318862915, D Loss: 1.3819267749786377\n",
            "Epoch 2, Batch 310, G Loss: 0.7080609798431396, D Loss: 1.3816710710525513\n",
            "Epoch 2, Batch 311, G Loss: 0.7079218626022339, D Loss: 1.3804339170455933\n",
            "Epoch 2, Batch 312, G Loss: 0.7079449892044067, D Loss: 1.3821043968200684\n",
            "Epoch 2, Batch 313, G Loss: 0.7078633904457092, D Loss: 1.3812787532806396\n",
            "Epoch 2, Batch 314, G Loss: 0.7078720331192017, D Loss: 1.3793768882751465\n",
            "Epoch 2, Batch 315, G Loss: 0.7077389359474182, D Loss: 1.3804452419281006\n",
            "Epoch 2, Batch 316, G Loss: 0.7076716423034668, D Loss: 1.3795127868652344\n",
            "Epoch 2, Batch 317, G Loss: 0.7076835036277771, D Loss: 1.3783457279205322\n",
            "Epoch 2, Batch 318, G Loss: 0.7077337503433228, D Loss: 1.3777902126312256\n",
            "Epoch 2, Batch 319, G Loss: 0.7077224850654602, D Loss: 1.3802473545074463\n",
            "Epoch 2, Batch 320, G Loss: 0.7077438831329346, D Loss: 1.377983808517456\n",
            "Epoch 2, Batch 321, G Loss: 0.7077685594558716, D Loss: 1.3805909156799316\n",
            "Epoch 2, Batch 322, G Loss: 0.7077817916870117, D Loss: 1.3793373107910156\n",
            "Epoch 2, Batch 323, G Loss: 0.7079140543937683, D Loss: 1.3810302019119263\n",
            "Epoch 2, Batch 324, G Loss: 0.7079211473464966, D Loss: 1.3796770572662354\n",
            "Epoch 2, Batch 325, G Loss: 0.7079176902770996, D Loss: 1.3778822422027588\n",
            "Epoch 2, Batch 326, G Loss: 0.7079389095306396, D Loss: 1.3793540000915527\n",
            "Epoch 2, Batch 327, G Loss: 0.7079963088035583, D Loss: 1.3782851696014404\n",
            "Epoch 2, Batch 328, G Loss: 0.7080569863319397, D Loss: 1.3810703754425049\n",
            "Epoch 2, Batch 329, G Loss: 0.7081067562103271, D Loss: 1.378173828125\n",
            "Epoch 2, Batch 330, G Loss: 0.7081790566444397, D Loss: 1.3759005069732666\n",
            "Epoch 2, Batch 331, G Loss: 0.7082202434539795, D Loss: 1.3779678344726562\n",
            "Epoch 2, Batch 332, G Loss: 0.7083290815353394, D Loss: 1.3784657716751099\n",
            "Epoch 2, Batch 333, G Loss: 0.7084159255027771, D Loss: 1.3801310062408447\n",
            "Epoch 2, Batch 334, G Loss: 0.7085312008857727, D Loss: 1.3786072731018066\n",
            "Epoch 2, Batch 335, G Loss: 0.708570659160614, D Loss: 1.379900574684143\n",
            "Epoch 2, Batch 336, G Loss: 0.7085934281349182, D Loss: 1.3799498081207275\n",
            "Epoch 2, Batch 337, G Loss: 0.7086454033851624, D Loss: 1.3815374374389648\n",
            "Epoch 2, Batch 338, G Loss: 0.7086616158485413, D Loss: 1.3750793933868408\n",
            "Epoch 2, Batch 339, G Loss: 0.7087457776069641, D Loss: 1.3767354488372803\n",
            "Epoch 2, Batch 340, G Loss: 0.7086753249168396, D Loss: 1.3744028806686401\n",
            "Epoch 2, Batch 341, G Loss: 0.7086822986602783, D Loss: 1.3836562633514404\n",
            "Epoch 2, Batch 342, G Loss: 0.7086799740791321, D Loss: 1.3823692798614502\n",
            "Epoch 2, Batch 343, G Loss: 0.7086600661277771, D Loss: 1.3832449913024902\n",
            "Epoch 2, Batch 344, G Loss: 0.708588182926178, D Loss: 1.3822996616363525\n",
            "Epoch 2, Batch 345, G Loss: 0.7085717916488647, D Loss: 1.3792494535446167\n",
            "Epoch 2, Batch 346, G Loss: 0.7084702253341675, D Loss: 1.3779559135437012\n",
            "Epoch 2, Batch 347, G Loss: 0.7083966135978699, D Loss: 1.3782994747161865\n",
            "Epoch 2, Batch 348, G Loss: 0.7083166241645813, D Loss: 1.3788710832595825\n",
            "Epoch 2, Batch 349, G Loss: 0.7083110213279724, D Loss: 1.3771361112594604\n",
            "Epoch 2, Batch 350, G Loss: 0.7082804441452026, D Loss: 1.3786373138427734\n",
            "Epoch 2, Batch 351, G Loss: 0.7083631753921509, D Loss: 1.3782709836959839\n",
            "Epoch 2, Batch 352, G Loss: 0.7083134651184082, D Loss: 1.3775520324707031\n",
            "Epoch 2, Batch 353, G Loss: 0.7082980275154114, D Loss: 1.3794423341751099\n",
            "Epoch 2, Batch 354, G Loss: 0.7082958817481995, D Loss: 1.3801343441009521\n",
            "Epoch 2, Batch 355, G Loss: 0.7082895040512085, D Loss: 1.3833975791931152\n",
            "Epoch 2, Batch 356, G Loss: 0.7082623243331909, D Loss: 1.3825507164001465\n",
            "Epoch 2, Batch 357, G Loss: 0.7081505656242371, D Loss: 1.3799182176589966\n",
            "Epoch 2, Batch 358, G Loss: 0.7081032395362854, D Loss: 1.3812769651412964\n",
            "Epoch 2, Batch 359, G Loss: 0.7080541849136353, D Loss: 1.3819032907485962\n",
            "Epoch 2, Batch 360, G Loss: 0.7080228924751282, D Loss: 1.3809202909469604\n",
            "Epoch 2, Batch 361, G Loss: 0.7079810500144958, D Loss: 1.3833324909210205\n",
            "Epoch 2, Batch 362, G Loss: 0.707933783531189, D Loss: 1.3802591562271118\n",
            "Epoch 2, Batch 363, G Loss: 0.707880437374115, D Loss: 1.3787662982940674\n",
            "Epoch 2, Batch 364, G Loss: 0.707878589630127, D Loss: 1.379201054573059\n",
            "Epoch 2, Batch 365, G Loss: 0.707804799079895, D Loss: 1.3798792362213135\n",
            "Epoch 2, Batch 366, G Loss: 0.7077837586402893, D Loss: 1.3797945976257324\n",
            "Epoch 2, Batch 367, G Loss: 0.7077493667602539, D Loss: 1.3782918453216553\n",
            "Epoch 2, Batch 368, G Loss: 0.7077168822288513, D Loss: 1.3775438070297241\n",
            "Epoch 2, Batch 369, G Loss: 0.7077063918113708, D Loss: 1.3772900104522705\n",
            "Epoch 2, Batch 370, G Loss: 0.7077288031578064, D Loss: 1.3757492303848267\n",
            "Epoch 2, Batch 371, G Loss: 0.7077563405036926, D Loss: 1.3760946989059448\n",
            "Epoch 2, Batch 372, G Loss: 0.7078191041946411, D Loss: 1.37957763671875\n",
            "Epoch 2, Batch 373, G Loss: 0.7079200148582458, D Loss: 1.3792349100112915\n",
            "Epoch 2, Batch 374, G Loss: 0.7079519033432007, D Loss: 1.3778204917907715\n",
            "Epoch 2, Batch 375, G Loss: 0.7080021500587463, D Loss: 1.378702163696289\n",
            "Epoch 2, Batch 376, G Loss: 0.7080357074737549, D Loss: 1.3790204524993896\n",
            "Epoch 2, Batch 377, G Loss: 0.7080835700035095, D Loss: 1.3789912462234497\n",
            "Epoch 2, Batch 378, G Loss: 0.7081332206726074, D Loss: 1.3775640726089478\n",
            "Epoch 2, Batch 379, G Loss: 0.7081658244132996, D Loss: 1.3778417110443115\n",
            "Epoch 2, Batch 380, G Loss: 0.7082382440567017, D Loss: 1.3782596588134766\n",
            "Epoch 2, Batch 381, G Loss: 0.70831298828125, D Loss: 1.3758915662765503\n",
            "Epoch 2, Batch 382, G Loss: 0.7083590626716614, D Loss: 1.3807945251464844\n",
            "Epoch 2, Batch 383, G Loss: 0.7084503173828125, D Loss: 1.3793041706085205\n",
            "Epoch 2, Batch 384, G Loss: 0.7085400223731995, D Loss: 1.377462387084961\n",
            "Epoch 2, Batch 385, G Loss: 0.708558976650238, D Loss: 1.3795133829116821\n",
            "Epoch 2, Batch 386, G Loss: 0.708605170249939, D Loss: 1.3808873891830444\n",
            "Epoch 2, Batch 387, G Loss: 0.7086303234100342, D Loss: 1.378751516342163\n",
            "Epoch 2, Batch 388, G Loss: 0.7086852192878723, D Loss: 1.3780617713928223\n",
            "Epoch 2, Batch 389, G Loss: 0.708720862865448, D Loss: 1.3773672580718994\n",
            "Epoch 2, Batch 390, G Loss: 0.7087811827659607, D Loss: 1.3730264902114868\n",
            "Epoch 2, Batch 391, G Loss: 0.708805501461029, D Loss: 1.3771352767944336\n",
            "Epoch 2, Batch 392, G Loss: 0.7088221311569214, D Loss: 1.3765023946762085\n",
            "Epoch 2, Batch 393, G Loss: 0.7089406251907349, D Loss: 1.3771014213562012\n",
            "Epoch 2, Batch 394, G Loss: 0.7089858651161194, D Loss: 1.3777751922607422\n",
            "Epoch 2, Batch 395, G Loss: 0.7090471386909485, D Loss: 1.3768436908721924\n",
            "Epoch 2, Batch 396, G Loss: 0.7091431617736816, D Loss: 1.3856548070907593\n",
            "Epoch 2, Batch 397, G Loss: 0.7090935707092285, D Loss: 1.383697748184204\n",
            "Epoch 2, Batch 398, G Loss: 0.7090920209884644, D Loss: 1.381028413772583\n",
            "Epoch 2, Batch 399, G Loss: 0.7090709209442139, D Loss: 1.3812658786773682\n",
            "Epoch 2, Batch 400, G Loss: 0.7090331315994263, D Loss: 1.3762450218200684\n",
            "Epoch 2, Batch 401, G Loss: 0.7090092897415161, D Loss: 1.3784542083740234\n",
            "Epoch 2, Batch 402, G Loss: 0.708980143070221, D Loss: 1.3794554471969604\n",
            "Epoch 2, Batch 403, G Loss: 0.7089641094207764, D Loss: 1.3759605884552002\n",
            "Epoch 2, Batch 404, G Loss: 0.7089001536369324, D Loss: 1.3736259937286377\n",
            "Epoch 2, Batch 405, G Loss: 0.7089136838912964, D Loss: 1.3735668659210205\n",
            "Epoch 2, Batch 406, G Loss: 0.7088925242424011, D Loss: 1.374032735824585\n",
            "Epoch 2, Batch 407, G Loss: 0.7089090347290039, D Loss: 1.3804255723953247\n",
            "Epoch 2, Batch 408, G Loss: 0.7089124917984009, D Loss: 1.3783414363861084\n",
            "Epoch 2, Batch 409, G Loss: 0.7088873982429504, D Loss: 1.3771822452545166\n",
            "Epoch 2, Batch 410, G Loss: 0.7088829874992371, D Loss: 1.3781943321228027\n",
            "Epoch 2, Batch 411, G Loss: 0.7088549137115479, D Loss: 1.375903606414795\n",
            "Epoch 2, Batch 412, G Loss: 0.7088755369186401, D Loss: 1.3761043548583984\n",
            "Epoch 2, Batch 413, G Loss: 0.708882212638855, D Loss: 1.3760570287704468\n",
            "Epoch 2, Batch 414, G Loss: 0.708881139755249, D Loss: 1.37508225440979\n",
            "Epoch 2, Batch 415, G Loss: 0.7089047431945801, D Loss: 1.3743715286254883\n",
            "Epoch 2, Batch 416, G Loss: 0.708903431892395, D Loss: 1.3741202354431152\n",
            "Epoch 2, Batch 417, G Loss: 0.7088926434516907, D Loss: 1.3784778118133545\n",
            "Epoch 2, Batch 418, G Loss: 0.7089217305183411, D Loss: 1.3784029483795166\n",
            "Epoch 2, Batch 419, G Loss: 0.7089105844497681, D Loss: 1.3797955513000488\n",
            "Epoch 2, Batch 420, G Loss: 0.7089230418205261, D Loss: 1.379159927368164\n",
            "Epoch 2, Batch 421, G Loss: 0.7088926434516907, D Loss: 1.377153992652893\n",
            "Epoch 2, Batch 422, G Loss: 0.7089081406593323, D Loss: 1.3756163120269775\n",
            "Epoch 2, Batch 423, G Loss: 0.7089315056800842, D Loss: 1.3767204284667969\n",
            "Epoch 2, Batch 424, G Loss: 0.70897376537323, D Loss: 1.3819595575332642\n",
            "Epoch 2, Batch 425, G Loss: 0.7089964151382446, D Loss: 1.3810490369796753\n",
            "Epoch 2, Batch 426, G Loss: 0.7089925408363342, D Loss: 1.379718542098999\n",
            "Epoch 2, Batch 427, G Loss: 0.7089681625366211, D Loss: 1.3800690174102783\n",
            "Epoch 2, Batch 428, G Loss: 0.708979070186615, D Loss: 1.378037929534912\n",
            "Epoch 2, Batch 429, G Loss: 0.7089444994926453, D Loss: 1.3795362710952759\n",
            "Epoch 2, Batch 430, G Loss: 0.7089572548866272, D Loss: 1.375715732574463\n",
            "Epoch 2, Batch 431, G Loss: 0.7089287638664246, D Loss: 1.3758132457733154\n",
            "Epoch 2, Batch 432, G Loss: 0.7088915705680847, D Loss: 1.3775701522827148\n",
            "Epoch 2, Batch 433, G Loss: 0.7088928818702698, D Loss: 1.375266194343567\n",
            "Epoch 2, Batch 434, G Loss: 0.7088600993156433, D Loss: 1.3754702806472778\n",
            "Epoch 2, Batch 435, G Loss: 0.7088775634765625, D Loss: 1.3741705417633057\n",
            "Epoch 2, Batch 436, G Loss: 0.7089340686798096, D Loss: 1.3770627975463867\n",
            "Epoch 2, Batch 437, G Loss: 0.7089809775352478, D Loss: 1.3813438415527344\n",
            "Epoch 2, Batch 438, G Loss: 0.7090011835098267, D Loss: 1.3800556659698486\n",
            "Epoch 2, Batch 439, G Loss: 0.7090480327606201, D Loss: 1.3804330825805664\n",
            "Epoch 2, Batch 440, G Loss: 0.7090650200843811, D Loss: 1.3805570602416992\n",
            "Epoch 2, Batch 441, G Loss: 0.7091562747955322, D Loss: 1.3822402954101562\n",
            "Epoch 2, Batch 442, G Loss: 0.7091829776763916, D Loss: 1.3814551830291748\n",
            "Epoch 2, Batch 443, G Loss: 0.7091682553291321, D Loss: 1.3802134990692139\n",
            "Epoch 2, Batch 444, G Loss: 0.7091418504714966, D Loss: 1.3824939727783203\n",
            "Epoch 2, Batch 445, G Loss: 0.7091531753540039, D Loss: 1.3819478750228882\n",
            "Epoch 2, Batch 446, G Loss: 0.7091659307479858, D Loss: 1.3803637027740479\n",
            "Epoch 2, Batch 447, G Loss: 0.7091715335845947, D Loss: 1.38079035282135\n",
            "Epoch 2, Batch 448, G Loss: 0.7092087268829346, D Loss: 1.3784775733947754\n",
            "Epoch 2, Batch 449, G Loss: 0.7091752886772156, D Loss: 1.3780394792556763\n",
            "Epoch 2, Batch 450, G Loss: 0.7091689109802246, D Loss: 1.3786070346832275\n",
            "Epoch 2, Batch 451, G Loss: 0.7091896533966064, D Loss: 1.3777786493301392\n",
            "Epoch 2, Batch 452, G Loss: 0.7091760635375977, D Loss: 1.3817052841186523\n",
            "Epoch 2, Batch 453, G Loss: 0.7091548442840576, D Loss: 1.3798469305038452\n",
            "Epoch 2, Batch 454, G Loss: 0.7091271281242371, D Loss: 1.379915714263916\n",
            "Epoch 2, Batch 455, G Loss: 0.7090845704078674, D Loss: 1.3744566440582275\n",
            "Epoch 2, Batch 456, G Loss: 0.7091514468193054, D Loss: 1.3754065036773682\n",
            "Epoch 2, Batch 457, G Loss: 0.7091545462608337, D Loss: 1.3761606216430664\n",
            "Epoch 2, Batch 458, G Loss: 0.7091458439826965, D Loss: 1.382857084274292\n",
            "Epoch 2, Batch 459, G Loss: 0.7091389298439026, D Loss: 1.3824704885482788\n",
            "Epoch 2, Batch 460, G Loss: 0.7090690732002258, D Loss: 1.3829563856124878\n",
            "Epoch 2, Batch 461, G Loss: 0.7090922594070435, D Loss: 1.3818151950836182\n",
            "Epoch 2, Batch 462, G Loss: 0.7090259194374084, D Loss: 1.3762633800506592\n",
            "Epoch 2, Batch 463, G Loss: 0.7090774774551392, D Loss: 1.3752714395523071\n",
            "Epoch 2, Batch 464, G Loss: 0.7089895009994507, D Loss: 1.37675142288208\n",
            "Epoch 2, Batch 465, G Loss: 0.7089813947677612, D Loss: 1.374277114868164\n",
            "Epoch 2, Batch 466, G Loss: 0.7089605331420898, D Loss: 1.374220609664917\n",
            "Epoch 2, Batch 467, G Loss: 0.7089626789093018, D Loss: 1.3743081092834473\n",
            "Epoch 2, Batch 468, G Loss: 0.7089714407920837, D Loss: 1.3747518062591553\n",
            "Epoch 2, Batch 469, G Loss: 0.7089558839797974, D Loss: 1.3724805116653442\n",
            "Epoch 2, Batch 470, G Loss: 0.708978533744812, D Loss: 1.3780701160430908\n",
            "Epoch 2, Batch 471, G Loss: 0.7090163230895996, D Loss: 1.3770105838775635\n",
            "Epoch 2, Batch 472, G Loss: 0.7089365720748901, D Loss: 1.3769640922546387\n",
            "Epoch 2, Batch 473, G Loss: 0.7089890241622925, D Loss: 1.376573085784912\n",
            "Epoch 2, Batch 474, G Loss: 0.7089685201644897, D Loss: 1.3766331672668457\n",
            "Epoch 2, Batch 475, G Loss: 0.7090082168579102, D Loss: 1.3794065713882446\n",
            "Epoch 2, Batch 476, G Loss: 0.7089434266090393, D Loss: 1.3799452781677246\n",
            "Epoch 2, Batch 477, G Loss: 0.7089184522628784, D Loss: 1.3794001340866089\n",
            "Epoch 2, Batch 478, G Loss: 0.7088671326637268, D Loss: 1.377321720123291\n",
            "Epoch 2, Batch 479, G Loss: 0.7087711691856384, D Loss: 1.371225118637085\n",
            "Epoch 2, Batch 480, G Loss: 0.7087702751159668, D Loss: 1.3730366230010986\n",
            "Epoch 2, Batch 481, G Loss: 0.7087727785110474, D Loss: 1.3727617263793945\n",
            "Epoch 2, Batch 482, G Loss: 0.7086865305900574, D Loss: 1.378810167312622\n",
            "Epoch 2, Batch 483, G Loss: 0.7087669968605042, D Loss: 1.377648115158081\n",
            "Epoch 2, Batch 484, G Loss: 0.7086820006370544, D Loss: 1.3757559061050415\n",
            "Epoch 2, Batch 485, G Loss: 0.7087229490280151, D Loss: 1.376169204711914\n",
            "Epoch 2, Batch 486, G Loss: 0.7086511850357056, D Loss: 1.3766486644744873\n",
            "Epoch 2, Batch 487, G Loss: 0.7087510824203491, D Loss: 1.376098871231079\n",
            "Epoch 2, Batch 488, G Loss: 0.7087236642837524, D Loss: 1.3761799335479736\n",
            "Epoch 2, Batch 489, G Loss: 0.7085935473442078, D Loss: 1.376273274421692\n",
            "Epoch 2, Batch 490, G Loss: 0.7087302803993225, D Loss: 1.3762309551239014\n",
            "Epoch 2, Batch 491, G Loss: 0.7086642384529114, D Loss: 1.3752450942993164\n",
            "Epoch 2, Batch 492, G Loss: 0.7086470723152161, D Loss: 1.3766570091247559\n",
            "Epoch 2, Batch 493, G Loss: 0.70862877368927, D Loss: 1.3739454746246338\n",
            "Epoch 2, Batch 494, G Loss: 0.7086571455001831, D Loss: 1.3742152452468872\n",
            "Epoch 2, Batch 495, G Loss: 0.708616316318512, D Loss: 1.3743963241577148\n",
            "Epoch 2, Batch 496, G Loss: 0.7086318731307983, D Loss: 1.375065803527832\n",
            "Epoch 2, Batch 497, G Loss: 0.7086530327796936, D Loss: 1.3765003681182861\n",
            "Epoch 2, Batch 498, G Loss: 0.708676815032959, D Loss: 1.3768863677978516\n",
            "Epoch 2, Batch 499, G Loss: 0.7086613178253174, D Loss: 1.3756868839263916\n",
            "Epoch 2, Batch 500, G Loss: 0.7087743282318115, D Loss: 1.3785715103149414\n",
            "Epoch 2, Batch 501, G Loss: 0.7086519002914429, D Loss: 1.3804388046264648\n",
            "Epoch 2, Batch 502, G Loss: 0.7087035179138184, D Loss: 1.3802692890167236\n",
            "Epoch 2, Batch 503, G Loss: 0.7087022662162781, D Loss: 1.378861427307129\n",
            "Epoch 2, Batch 504, G Loss: 0.7086523175239563, D Loss: 1.3776715993881226\n",
            "Epoch 2, Batch 505, G Loss: 0.7086551189422607, D Loss: 1.381394863128662\n",
            "Epoch 2, Batch 506, G Loss: 0.7085998058319092, D Loss: 1.3810062408447266\n",
            "Epoch 2, Batch 507, G Loss: 0.708626389503479, D Loss: 1.3838415145874023\n",
            "Epoch 2, Batch 508, G Loss: 0.7085044980049133, D Loss: 1.3813450336456299\n",
            "Epoch 2, Batch 509, G Loss: 0.7084380388259888, D Loss: 1.3772904872894287\n",
            "Epoch 2, Batch 510, G Loss: 0.7083501815795898, D Loss: 1.3781468868255615\n",
            "Epoch 2, Batch 511, G Loss: 0.7083135843276978, D Loss: 1.3768444061279297\n",
            "Epoch 2, Batch 512, G Loss: 0.7082145810127258, D Loss: 1.3744893074035645\n",
            "Epoch 2, Batch 513, G Loss: 0.7081455588340759, D Loss: 1.3772387504577637\n",
            "Epoch 2, Batch 514, G Loss: 0.7080886960029602, D Loss: 1.3768272399902344\n",
            "Epoch 2, Batch 515, G Loss: 0.7080724239349365, D Loss: 1.379791021347046\n",
            "Epoch 2, Batch 516, G Loss: 0.7079566121101379, D Loss: 1.381811499595642\n",
            "Epoch 2, Batch 517, G Loss: 0.7078098654747009, D Loss: 1.3823037147521973\n",
            "Epoch 2, Batch 518, G Loss: 0.7077177166938782, D Loss: 1.3793267011642456\n",
            "Epoch 2, Batch 519, G Loss: 0.7077102065086365, D Loss: 1.380560278892517\n",
            "Epoch 2, Batch 520, G Loss: 0.7076455950737, D Loss: 1.3801450729370117\n",
            "Epoch 2, Batch 521, G Loss: 0.7075353264808655, D Loss: 1.3798887729644775\n",
            "Epoch 2, Batch 522, G Loss: 0.707481861114502, D Loss: 1.3772265911102295\n",
            "Epoch 2, Batch 523, G Loss: 0.7074191570281982, D Loss: 1.3759502172470093\n",
            "Epoch 2, Batch 524, G Loss: 0.7075195908546448, D Loss: 1.3764674663543701\n",
            "Epoch 2, Batch 525, G Loss: 0.707429826259613, D Loss: 1.3745431900024414\n",
            "Epoch 2, Batch 526, G Loss: 0.7074317932128906, D Loss: 1.374613881111145\n",
            "Epoch 2, Batch 527, G Loss: 0.7074995040893555, D Loss: 1.3773574829101562\n",
            "Epoch 2, Batch 528, G Loss: 0.7075527906417847, D Loss: 1.3806346654891968\n",
            "Epoch 2, Batch 529, G Loss: 0.7074828147888184, D Loss: 1.3835649490356445\n",
            "Epoch 2, Batch 530, G Loss: 0.7075710892677307, D Loss: 1.3802876472473145\n",
            "Epoch 2, Batch 531, G Loss: 0.7074790596961975, D Loss: 1.3792986869812012\n",
            "Epoch 2, Batch 532, G Loss: 0.7075344920158386, D Loss: 1.3746778964996338\n",
            "Epoch 2, Batch 533, G Loss: 0.7075033187866211, D Loss: 1.3789141178131104\n",
            "Epoch 2, Batch 534, G Loss: 0.7075600028038025, D Loss: 1.3797670602798462\n",
            "Epoch 2, Batch 535, G Loss: 0.7076221108436584, D Loss: 1.3778951168060303\n",
            "Epoch 2, Batch 536, G Loss: 0.7075327634811401, D Loss: 1.3767271041870117\n",
            "Epoch 2, Batch 537, G Loss: 0.7076492309570312, D Loss: 1.380484938621521\n",
            "Epoch 2, Batch 538, G Loss: 0.7075218558311462, D Loss: 1.3817248344421387\n",
            "Epoch 2, Batch 539, G Loss: 0.7075843811035156, D Loss: 1.3806931972503662\n",
            "Epoch 2, Batch 540, G Loss: 0.707507848739624, D Loss: 1.3810606002807617\n",
            "Epoch 2, Batch 541, G Loss: 0.7074983716011047, D Loss: 1.3808071613311768\n",
            "Epoch 2, Batch 542, G Loss: 0.7073581218719482, D Loss: 1.3804152011871338\n",
            "Epoch 2, Batch 543, G Loss: 0.7072494626045227, D Loss: 1.3807263374328613\n",
            "Epoch 2, Batch 544, G Loss: 0.7073308229446411, D Loss: 1.3807708024978638\n",
            "Epoch 2, Batch 545, G Loss: 0.7072046399116516, D Loss: 1.3806064128875732\n",
            "Epoch 2, Batch 546, G Loss: 0.7070526480674744, D Loss: 1.3802680969238281\n",
            "Epoch 2, Batch 547, G Loss: 0.7069079875946045, D Loss: 1.3803329467773438\n",
            "Epoch 2, Batch 548, G Loss: 0.706828236579895, D Loss: 1.3837740421295166\n",
            "Epoch 2, Batch 549, G Loss: 0.7067009806632996, D Loss: 1.3804082870483398\n",
            "Epoch 2, Batch 550, G Loss: 0.7065309286117554, D Loss: 1.3773781061172485\n",
            "Epoch 2, Batch 551, G Loss: 0.7064733505249023, D Loss: 1.378061294555664\n",
            "Epoch 2, Batch 552, G Loss: 0.7064048647880554, D Loss: 1.377931833267212\n",
            "Epoch 2, Batch 553, G Loss: 0.7063287496566772, D Loss: 1.38148832321167\n",
            "Epoch 2, Batch 554, G Loss: 0.7061594724655151, D Loss: 1.382427453994751\n",
            "Epoch 2, Batch 555, G Loss: 0.7060733437538147, D Loss: 1.3835095167160034\n",
            "Epoch 2, Batch 556, G Loss: 0.7058360576629639, D Loss: 1.379755973815918\n",
            "Epoch 2, Batch 557, G Loss: 0.705818772315979, D Loss: 1.378795862197876\n",
            "Epoch 2, Batch 558, G Loss: 0.7055521011352539, D Loss: 1.3819403648376465\n",
            "Epoch 2, Batch 559, G Loss: 0.7055084705352783, D Loss: 1.380187749862671\n",
            "Epoch 2, Batch 560, G Loss: 0.7054744362831116, D Loss: 1.3805309534072876\n",
            "Epoch 2, Batch 561, G Loss: 0.705292284488678, D Loss: 1.378211259841919\n",
            "Epoch 2, Batch 562, G Loss: 0.7050284743309021, D Loss: 1.376665711402893\n",
            "Epoch 2, Batch 563, G Loss: 0.7050783038139343, D Loss: 1.3764503002166748\n",
            "Epoch 2, Batch 564, G Loss: 0.7048708200454712, D Loss: 1.3741090297698975\n",
            "Epoch 2, Batch 565, G Loss: 0.7048380374908447, D Loss: 1.3827099800109863\n",
            "Epoch 2, Batch 566, G Loss: 0.7047123908996582, D Loss: 1.3899033069610596\n",
            "Epoch 2, Batch 567, G Loss: 0.7044757008552551, D Loss: 1.386045217514038\n",
            "Epoch 2, Batch 568, G Loss: 0.7044380307197571, D Loss: 1.3870335817337036\n",
            "Epoch 2, Batch 569, G Loss: 0.7041202187538147, D Loss: 1.3803801536560059\n",
            "Epoch 2, Batch 570, G Loss: 0.7040123343467712, D Loss: 1.3817375898361206\n",
            "Epoch 2, Batch 571, G Loss: 0.7037783861160278, D Loss: 1.3802106380462646\n",
            "Epoch 2, Batch 572, G Loss: 0.70355224609375, D Loss: 1.3896572589874268\n",
            "Epoch 2, Batch 573, G Loss: 0.7034174203872681, D Loss: 1.3876962661743164\n",
            "Epoch 2, Batch 574, G Loss: 0.7031083106994629, D Loss: 1.390601396560669\n",
            "Epoch 2, Batch 575, G Loss: 0.7029045820236206, D Loss: 1.3889639377593994\n",
            "Epoch 2, Batch 576, G Loss: 0.7027136087417603, D Loss: 1.3839467763900757\n",
            "Epoch 2, Batch 577, G Loss: 0.7024721503257751, D Loss: 1.3841315507888794\n",
            "Epoch 2, Batch 578, G Loss: 0.7021580934524536, D Loss: 1.3815944194793701\n",
            "Epoch 2, Batch 579, G Loss: 0.7018775343894958, D Loss: 1.3805928230285645\n",
            "Epoch 2, Batch 580, G Loss: 0.7016105055809021, D Loss: 1.387974500656128\n",
            "Epoch 2, Batch 581, G Loss: 0.7013903856277466, D Loss: 1.3867731094360352\n",
            "Epoch 2, Batch 582, G Loss: 0.7011606693267822, D Loss: 1.3874640464782715\n",
            "Epoch 2, Batch 583, G Loss: 0.7008859515190125, D Loss: 1.3878644704818726\n",
            "Epoch 2, Batch 584, G Loss: 0.7006347179412842, D Loss: 1.3841001987457275\n",
            "Epoch 2, Batch 585, G Loss: 0.7004479169845581, D Loss: 1.3833056688308716\n",
            "Epoch 2, Batch 586, G Loss: 0.7001909017562866, D Loss: 1.3820281028747559\n",
            "Epoch 2, Batch 587, G Loss: 0.6999363303184509, D Loss: 1.3805257081985474\n",
            "Epoch 2, Batch 588, G Loss: 0.6997076272964478, D Loss: 1.3804655075073242\n",
            "Epoch 2, Batch 589, G Loss: 0.6994409561157227, D Loss: 1.3798028230667114\n",
            "Epoch 2, Batch 590, G Loss: 0.6993288397789001, D Loss: 1.3860161304473877\n",
            "Epoch 2, Batch 591, G Loss: 0.6991095542907715, D Loss: 1.3864619731903076\n",
            "Epoch 2, Batch 592, G Loss: 0.6989655494689941, D Loss: 1.3850626945495605\n",
            "Epoch 2, Batch 593, G Loss: 0.6987999677658081, D Loss: 1.3831596374511719\n",
            "Epoch 2, Batch 594, G Loss: 0.6985079646110535, D Loss: 1.382605791091919\n",
            "Epoch 2, Batch 595, G Loss: 0.698368489742279, D Loss: 1.3799751996994019\n",
            "Epoch 2, Batch 596, G Loss: 0.6981877088546753, D Loss: 1.3858439922332764\n",
            "Epoch 2, Batch 597, G Loss: 0.6981402039527893, D Loss: 1.3859522342681885\n",
            "Epoch 2, Batch 598, G Loss: 0.6979159712791443, D Loss: 1.3844645023345947\n",
            "Epoch 2, Batch 599, G Loss: 0.6977512836456299, D Loss: 1.3835985660552979\n",
            "Epoch 2, Batch 600, G Loss: 0.6975894570350647, D Loss: 1.3821437358856201\n",
            "Epoch 2, Batch 601, G Loss: 0.6974390745162964, D Loss: 1.382892370223999\n",
            "Epoch 2, Batch 602, G Loss: 0.6973685622215271, D Loss: 1.381587028503418\n",
            "Epoch 2, Batch 603, G Loss: 0.6972597241401672, D Loss: 1.384445071220398\n",
            "Epoch 2, Batch 604, G Loss: 0.697105348110199, D Loss: 1.3889845609664917\n",
            "Epoch 2, Batch 605, G Loss: 0.6969075202941895, D Loss: 1.3840954303741455\n",
            "Epoch 2, Batch 606, G Loss: 0.6968360543251038, D Loss: 1.384791612625122\n",
            "Epoch 2, Batch 607, G Loss: 0.6965904831886292, D Loss: 1.3851454257965088\n",
            "Epoch 2, Batch 608, G Loss: 0.696502685546875, D Loss: 1.3866640329360962\n",
            "Epoch 2, Batch 609, G Loss: 0.6964855790138245, D Loss: 1.3840162754058838\n",
            "Epoch 2, Batch 610, G Loss: 0.6962672472000122, D Loss: 1.3828116655349731\n",
            "Epoch 2, Batch 611, G Loss: 0.6960827708244324, D Loss: 1.386551856994629\n",
            "Epoch 2, Batch 612, G Loss: 0.6959578990936279, D Loss: 1.3892403841018677\n",
            "Epoch 2, Batch 613, G Loss: 0.695870041847229, D Loss: 1.3891541957855225\n",
            "Epoch 2, Batch 614, G Loss: 0.6957334280014038, D Loss: 1.3879449367523193\n",
            "Epoch 2, Batch 615, G Loss: 0.6955525279045105, D Loss: 1.3872835636138916\n",
            "Epoch 2, Batch 616, G Loss: 0.6953539252281189, D Loss: 1.3878204822540283\n",
            "Epoch 2, Batch 617, G Loss: 0.695243775844574, D Loss: 1.3887412548065186\n",
            "Epoch 2, Batch 618, G Loss: 0.6950104236602783, D Loss: 1.3864309787750244\n",
            "Epoch 2, Batch 619, G Loss: 0.6949028372764587, D Loss: 1.3879718780517578\n",
            "Epoch 2, Batch 620, G Loss: 0.694709300994873, D Loss: 1.3881447315216064\n",
            "Epoch 2, Batch 621, G Loss: 0.6945963501930237, D Loss: 1.3849821090698242\n",
            "Epoch 2, Batch 622, G Loss: 0.694366991519928, D Loss: 1.380770206451416\n",
            "Epoch 2, Batch 623, G Loss: 0.6942794919013977, D Loss: 1.3850278854370117\n",
            "Epoch 2, Batch 624, G Loss: 0.6940916180610657, D Loss: 1.3833699226379395\n",
            "Epoch 2, Batch 625, G Loss: 0.6939641237258911, D Loss: 1.3876413106918335\n",
            "Epoch 2, Batch 626, G Loss: 0.6939572691917419, D Loss: 1.3866357803344727\n",
            "Epoch 2, Batch 627, G Loss: 0.6937294602394104, D Loss: 1.3879194259643555\n",
            "Epoch 2, Batch 628, G Loss: 0.6936495900154114, D Loss: 1.3881330490112305\n",
            "Epoch 2, Batch 629, G Loss: 0.6935231685638428, D Loss: 1.387181043624878\n",
            "Epoch 2, Batch 630, G Loss: 0.6933779120445251, D Loss: 1.3850979804992676\n",
            "Epoch 2, Batch 631, G Loss: 0.6932486891746521, D Loss: 1.3852109909057617\n",
            "Epoch 2, Batch 632, G Loss: 0.6931325197219849, D Loss: 1.3904736042022705\n",
            "Epoch 2, Batch 633, G Loss: 0.6931158304214478, D Loss: 1.3911893367767334\n",
            "Epoch 2, Batch 634, G Loss: 0.6930475234985352, D Loss: 1.3904945850372314\n",
            "Epoch 2, Batch 635, G Loss: 0.6927065253257751, D Loss: 1.388866662979126\n",
            "Epoch 2, Batch 636, G Loss: 0.6925562620162964, D Loss: 1.3863890171051025\n",
            "Epoch 2, Batch 637, G Loss: 0.6924718618392944, D Loss: 1.3869256973266602\n",
            "Epoch 2, Batch 638, G Loss: 0.6923242807388306, D Loss: 1.3851103782653809\n",
            "Epoch 2, Batch 639, G Loss: 0.6923589706420898, D Loss: 1.3879659175872803\n",
            "Epoch 2, Batch 640, G Loss: 0.6921495795249939, D Loss: 1.3858966827392578\n",
            "Epoch 2, Batch 641, G Loss: 0.6920798420906067, D Loss: 1.3867003917694092\n",
            "Epoch 2, Batch 642, G Loss: 0.6918737292289734, D Loss: 1.3869991302490234\n",
            "Epoch 2, Batch 643, G Loss: 0.6917940974235535, D Loss: 1.387444257736206\n",
            "Epoch 2, Batch 644, G Loss: 0.6916710734367371, D Loss: 1.3895974159240723\n",
            "Epoch 2, Batch 645, G Loss: 0.6915865540504456, D Loss: 1.3871493339538574\n",
            "Epoch 2, Batch 646, G Loss: 0.6915910840034485, D Loss: 1.3895814418792725\n",
            "Epoch 2, Batch 647, G Loss: 0.6914148330688477, D Loss: 1.3901156187057495\n",
            "Epoch 2, Batch 648, G Loss: 0.6913276314735413, D Loss: 1.3891024589538574\n",
            "Epoch 2, Batch 649, G Loss: 0.69122314453125, D Loss: 1.3863091468811035\n",
            "Epoch 2, Batch 650, G Loss: 0.6911292672157288, D Loss: 1.3899104595184326\n",
            "Epoch 2, Batch 651, G Loss: 0.6910490989685059, D Loss: 1.388018250465393\n",
            "Epoch 2, Batch 652, G Loss: 0.6909294128417969, D Loss: 1.3842934370040894\n",
            "Epoch 2, Batch 653, G Loss: 0.6908396482467651, D Loss: 1.3830945491790771\n",
            "Epoch 2, Batch 654, G Loss: 0.690669059753418, D Loss: 1.3863074779510498\n",
            "Epoch 2, Batch 655, G Loss: 0.6906551718711853, D Loss: 1.3904087543487549\n",
            "Epoch 2, Batch 656, G Loss: 0.6906040906906128, D Loss: 1.3908101320266724\n",
            "Epoch 2, Batch 657, G Loss: 0.6905810236930847, D Loss: 1.3923685550689697\n",
            "Epoch 2, Batch 658, G Loss: 0.6905300617218018, D Loss: 1.392338752746582\n",
            "Epoch 2, Batch 659, G Loss: 0.6903266310691833, D Loss: 1.391007661819458\n",
            "Epoch 2, Batch 660, G Loss: 0.6902689933776855, D Loss: 1.3907341957092285\n",
            "Epoch 2, Batch 661, G Loss: 0.6902102828025818, D Loss: 1.3911702632904053\n",
            "Epoch 2, Batch 662, G Loss: 0.6899672746658325, D Loss: 1.3892827033996582\n",
            "Epoch 2, Batch 663, G Loss: 0.6898744702339172, D Loss: 1.390201210975647\n",
            "Epoch 2, Batch 664, G Loss: 0.6897638440132141, D Loss: 1.3886520862579346\n",
            "Epoch 2, Batch 665, G Loss: 0.6897833943367004, D Loss: 1.3878321647644043\n",
            "Epoch 2, Batch 666, G Loss: 0.6896530389785767, D Loss: 1.3938508033752441\n",
            "Epoch 2, Batch 667, G Loss: 0.689518928527832, D Loss: 1.3947757482528687\n",
            "Epoch 2, Batch 668, G Loss: 0.6894662976264954, D Loss: 1.3914473056793213\n",
            "Epoch 2, Batch 669, G Loss: 0.689214289188385, D Loss: 1.3929537534713745\n",
            "Epoch 2, Batch 670, G Loss: 0.6890996694564819, D Loss: 1.3920295238494873\n",
            "Epoch 2, Batch 671, G Loss: 0.6891818642616272, D Loss: 1.3899526596069336\n",
            "Epoch 2, Batch 672, G Loss: 0.6889625191688538, D Loss: 1.387354850769043\n",
            "Epoch 2, Batch 673, G Loss: 0.68896484375, D Loss: 1.387951135635376\n",
            "Epoch 2, Batch 674, G Loss: 0.6887367367744446, D Loss: 1.3879268169403076\n",
            "Epoch 2, Batch 675, G Loss: 0.6885947585105896, D Loss: 1.3924477100372314\n",
            "Epoch 2, Batch 676, G Loss: 0.6886541247367859, D Loss: 1.3918535709381104\n",
            "Epoch 2, Batch 677, G Loss: 0.6884404420852661, D Loss: 1.3906368017196655\n",
            "Epoch 2, Batch 678, G Loss: 0.68832927942276, D Loss: 1.3909988403320312\n",
            "Epoch 2, Batch 679, G Loss: 0.6883479356765747, D Loss: 1.3913614749908447\n",
            "Epoch 2, Batch 680, G Loss: 0.6883142590522766, D Loss: 1.3916552066802979\n",
            "Epoch 2, Batch 681, G Loss: 0.6881948709487915, D Loss: 1.3899555206298828\n",
            "Epoch 2, Batch 682, G Loss: 0.6879172921180725, D Loss: 1.3917160034179688\n",
            "Epoch 2, Batch 683, G Loss: 0.6878306269645691, D Loss: 1.3912534713745117\n",
            "Epoch 2, Batch 684, G Loss: 0.6878820657730103, D Loss: 1.3911116123199463\n",
            "Epoch 2, Batch 685, G Loss: 0.6877713799476624, D Loss: 1.3900320529937744\n",
            "Epoch 2, Batch 686, G Loss: 0.6877313852310181, D Loss: 1.3895981311798096\n",
            "Epoch 2, Batch 687, G Loss: 0.6876149773597717, D Loss: 1.3890529870986938\n",
            "Epoch 2, Batch 688, G Loss: 0.6876451969146729, D Loss: 1.3881866931915283\n",
            "Epoch 2, Batch 689, G Loss: 0.6874353885650635, D Loss: 1.3914597034454346\n",
            "Epoch 2, Batch 690, G Loss: 0.6874697804450989, D Loss: 1.38969087600708\n",
            "Epoch 2, Batch 691, G Loss: 0.6873946785926819, D Loss: 1.3907012939453125\n",
            "Epoch 2, Batch 692, G Loss: 0.6873491406440735, D Loss: 1.3916215896606445\n",
            "Epoch 2, Batch 693, G Loss: 0.6872846484184265, D Loss: 1.3917438983917236\n",
            "Epoch 2, Batch 694, G Loss: 0.6872951984405518, D Loss: 1.3899421691894531\n",
            "Epoch 2, Batch 695, G Loss: 0.687229573726654, D Loss: 1.391855001449585\n",
            "Epoch 2, Batch 696, G Loss: 0.6872395277023315, D Loss: 1.3907033205032349\n",
            "Epoch 2, Batch 697, G Loss: 0.6870856285095215, D Loss: 1.3909544944763184\n",
            "Epoch 2, Batch 698, G Loss: 0.6870513558387756, D Loss: 1.3907828330993652\n",
            "Epoch 2, Batch 699, G Loss: 0.68691486120224, D Loss: 1.3903124332427979\n",
            "Epoch 2, Batch 700, G Loss: 0.6869767904281616, D Loss: 1.390623927116394\n",
            "Epoch 2, Batch 701, G Loss: 0.6869461536407471, D Loss: 1.389624834060669\n",
            "Epoch 2, Batch 702, G Loss: 0.6869936585426331, D Loss: 1.3909952640533447\n",
            "Epoch 2, Batch 703, G Loss: 0.687004566192627, D Loss: 1.3920968770980835\n",
            "Epoch 2, Batch 704, G Loss: 0.6868039965629578, D Loss: 1.3914835453033447\n",
            "Epoch 2, Batch 705, G Loss: 0.6869553327560425, D Loss: 1.3920342922210693\n",
            "Epoch 2, Batch 706, G Loss: 0.6867092847824097, D Loss: 1.391739845275879\n",
            "Epoch 2, Batch 707, G Loss: 0.6868863701820374, D Loss: 1.391489028930664\n",
            "Epoch 2, Batch 708, G Loss: 0.6867285966873169, D Loss: 1.392111897468567\n",
            "Epoch 2, Batch 709, G Loss: 0.6866146922111511, D Loss: 1.3910444974899292\n",
            "Epoch 2, Batch 710, G Loss: 0.6865212321281433, D Loss: 1.3908631801605225\n",
            "Epoch 2, Batch 711, G Loss: 0.6865811944007874, D Loss: 1.3893060684204102\n",
            "Epoch 2, Batch 712, G Loss: 0.6867274045944214, D Loss: 1.3915009498596191\n",
            "Epoch 2, Batch 713, G Loss: 0.6863759756088257, D Loss: 1.3908817768096924\n",
            "Epoch 2, Batch 714, G Loss: 0.6865469217300415, D Loss: 1.3908915519714355\n",
            "Epoch 2, Batch 715, G Loss: 0.6866952776908875, D Loss: 1.3914806842803955\n",
            "Epoch 2, Batch 716, G Loss: 0.6863981485366821, D Loss: 1.3913042545318604\n",
            "Epoch 2, Batch 717, G Loss: 0.6864897012710571, D Loss: 1.3915942907333374\n",
            "Epoch 2, Batch 718, G Loss: 0.6864703297615051, D Loss: 1.3895840644836426\n",
            "Epoch 2, Batch 719, G Loss: 0.6864501237869263, D Loss: 1.388272762298584\n",
            "Epoch 2, Batch 720, G Loss: 0.6864001154899597, D Loss: 1.3893691301345825\n",
            "Epoch 2, Batch 721, G Loss: 0.6863881945610046, D Loss: 1.3890817165374756\n",
            "Epoch 2, Batch 722, G Loss: 0.6864021420478821, D Loss: 1.3900690078735352\n",
            "Epoch 2, Batch 723, G Loss: 0.6863342523574829, D Loss: 1.390800952911377\n",
            "Epoch 2, Batch 724, G Loss: 0.6862891316413879, D Loss: 1.3905866146087646\n",
            "Epoch 2, Batch 725, G Loss: 0.6862465739250183, D Loss: 1.3925491571426392\n",
            "Epoch 2, Batch 726, G Loss: 0.6864396929740906, D Loss: 1.3910964727401733\n",
            "Epoch 2, Batch 727, G Loss: 0.6863909959793091, D Loss: 1.391021966934204\n",
            "Epoch 2, Batch 728, G Loss: 0.6863541007041931, D Loss: 1.3912937641143799\n",
            "Epoch 2, Batch 729, G Loss: 0.6862869262695312, D Loss: 1.391944169998169\n",
            "Epoch 2, Batch 730, G Loss: 0.6863171458244324, D Loss: 1.3931593894958496\n",
            "Epoch 2, Batch 731, G Loss: 0.6862329840660095, D Loss: 1.3921699523925781\n",
            "Epoch 2, Batch 732, G Loss: 0.6862887740135193, D Loss: 1.3927479982376099\n",
            "Epoch 2, Batch 733, G Loss: 0.6861904859542847, D Loss: 1.3925516605377197\n",
            "Epoch 2, Batch 734, G Loss: 0.6861860156059265, D Loss: 1.3916904926300049\n",
            "Epoch 2, Batch 735, G Loss: 0.6861668825149536, D Loss: 1.3914084434509277\n",
            "Epoch 2, Batch 736, G Loss: 0.6862587332725525, D Loss: 1.3913776874542236\n",
            "Epoch 2, Batch 737, G Loss: 0.6862070560455322, D Loss: 1.3914821147918701\n",
            "Epoch 2, Batch 738, G Loss: 0.6860049366950989, D Loss: 1.3898303508758545\n",
            "Epoch 2, Batch 739, G Loss: 0.6859898567199707, D Loss: 1.3895652294158936\n",
            "Epoch 2, Batch 740, G Loss: 0.6860191226005554, D Loss: 1.389270305633545\n",
            "Epoch 2, Batch 741, G Loss: 0.6860560178756714, D Loss: 1.392041802406311\n",
            "Epoch 2, Batch 742, G Loss: 0.6861931681632996, D Loss: 1.391823649406433\n",
            "Epoch 2, Batch 743, G Loss: 0.6860180497169495, D Loss: 1.3913373947143555\n",
            "Epoch 2, Batch 744, G Loss: 0.6859714984893799, D Loss: 1.3933937549591064\n",
            "Epoch 2, Batch 745, G Loss: 0.686073362827301, D Loss: 1.393589973449707\n",
            "Epoch 2, Batch 746, G Loss: 0.6860228776931763, D Loss: 1.392538070678711\n",
            "Epoch 2, Batch 747, G Loss: 0.6859705448150635, D Loss: 1.3917126655578613\n",
            "Epoch 2, Batch 748, G Loss: 0.6857799887657166, D Loss: 1.391977310180664\n",
            "Epoch 2, Batch 749, G Loss: 0.6858341693878174, D Loss: 1.3917396068572998\n",
            "Epoch 2, Batch 750, G Loss: 0.685917317867279, D Loss: 1.3904271125793457\n",
            "Epoch 2, Batch 751, G Loss: 0.6857873797416687, D Loss: 1.3929028511047363\n",
            "Epoch 2, Batch 752, G Loss: 0.6858258843421936, D Loss: 1.3910987377166748\n",
            "Epoch 2, Batch 753, G Loss: 0.6857401132583618, D Loss: 1.3926002979278564\n",
            "Epoch 2, Batch 754, G Loss: 0.6857178211212158, D Loss: 1.392181396484375\n",
            "Epoch 2, Batch 755, G Loss: 0.6857513785362244, D Loss: 1.3920350074768066\n",
            "Epoch 2, Batch 756, G Loss: 0.6857259273529053, D Loss: 1.3917121887207031\n",
            "Epoch 2, Batch 757, G Loss: 0.6855465769767761, D Loss: 1.3918172121047974\n",
            "Epoch 2, Batch 758, G Loss: 0.685502827167511, D Loss: 1.3910071849822998\n",
            "Epoch 2, Batch 759, G Loss: 0.6855310201644897, D Loss: 1.391772985458374\n",
            "Epoch 2, Batch 760, G Loss: 0.6856023073196411, D Loss: 1.390625\n",
            "Epoch 2, Batch 761, G Loss: 0.6855213642120361, D Loss: 1.3910043239593506\n",
            "Epoch 2, Batch 762, G Loss: 0.6853902339935303, D Loss: 1.3913501501083374\n",
            "Epoch 2, Batch 763, G Loss: 0.6855745315551758, D Loss: 1.3908191919326782\n",
            "Epoch 2, Batch 764, G Loss: 0.6854214072227478, D Loss: 1.3903676271438599\n",
            "Epoch 2, Batch 765, G Loss: 0.6854174137115479, D Loss: 1.388134241104126\n",
            "Epoch 2, Batch 766, G Loss: 0.6853030323982239, D Loss: 1.3887056112289429\n",
            "Epoch 2, Batch 767, G Loss: 0.6853915452957153, D Loss: 1.3892428874969482\n",
            "Epoch 2, Batch 768, G Loss: 0.6852831244468689, D Loss: 1.3905017375946045\n",
            "Epoch 2, Batch 769, G Loss: 0.6852576732635498, D Loss: 1.3907462358474731\n",
            "Epoch 2, Batch 770, G Loss: 0.6852675676345825, D Loss: 1.390148401260376\n",
            "Epoch 2, Batch 771, G Loss: 0.6853342056274414, D Loss: 1.3897624015808105\n",
            "Epoch 2, Batch 772, G Loss: 0.6852667927742004, D Loss: 1.3895457983016968\n",
            "Epoch 2, Batch 773, G Loss: 0.6853573322296143, D Loss: 1.3901410102844238\n",
            "Epoch 2, Batch 774, G Loss: 0.6853477358818054, D Loss: 1.3896600008010864\n",
            "Epoch 2, Batch 775, G Loss: 0.6853486895561218, D Loss: 1.3895699977874756\n",
            "Epoch 2, Batch 776, G Loss: 0.685437023639679, D Loss: 1.3894715309143066\n",
            "Epoch 2, Batch 777, G Loss: 0.6854194402694702, D Loss: 1.388641119003296\n",
            "Epoch 2, Batch 778, G Loss: 0.6854486465454102, D Loss: 1.3903071880340576\n",
            "Epoch 2, Batch 779, G Loss: 0.6853576898574829, D Loss: 1.3890025615692139\n",
            "Epoch 2, Batch 780, G Loss: 0.6854347586631775, D Loss: 1.3887455463409424\n",
            "Epoch 2, Batch 781, G Loss: 0.6854366660118103, D Loss: 1.3882602453231812\n",
            "Epoch 2, Batch 782, G Loss: 0.6855028867721558, D Loss: 1.3886785507202148\n",
            "Epoch 2, Batch 783, G Loss: 0.6855618953704834, D Loss: 1.3887670040130615\n",
            "Epoch 2, Batch 784, G Loss: 0.6854562759399414, D Loss: 1.3892189264297485\n",
            "Epoch 2, Batch 785, G Loss: 0.6856010556221008, D Loss: 1.3888201713562012\n",
            "Epoch 2, Batch 786, G Loss: 0.6856032609939575, D Loss: 1.3888792991638184\n",
            "Epoch 2, Batch 787, G Loss: 0.685662567615509, D Loss: 1.3889174461364746\n",
            "Epoch 2, Batch 788, G Loss: 0.6854990720748901, D Loss: 1.3889578580856323\n",
            "Epoch 2, Batch 789, G Loss: 0.6856605410575867, D Loss: 1.3888195753097534\n",
            "Epoch 2, Batch 790, G Loss: 0.6858469247817993, D Loss: 1.3889265060424805\n",
            "Epoch 2, Batch 791, G Loss: 0.6857231259346008, D Loss: 1.3885009288787842\n",
            "Epoch 2, Batch 792, G Loss: 0.6859665513038635, D Loss: 1.3882153034210205\n",
            "Epoch 2, Batch 793, G Loss: 0.6859297752380371, D Loss: 1.3887267112731934\n",
            "Epoch 2, Batch 794, G Loss: 0.686011552810669, D Loss: 1.388871669769287\n",
            "Epoch 2, Batch 795, G Loss: 0.6858723163604736, D Loss: 1.3883521556854248\n",
            "Epoch 2, Batch 796, G Loss: 0.6860155463218689, D Loss: 1.3886338472366333\n",
            "Epoch 2, Batch 797, G Loss: 0.6860460638999939, D Loss: 1.3883967399597168\n",
            "Epoch 2, Batch 798, G Loss: 0.6860064268112183, D Loss: 1.388218879699707\n",
            "Epoch 2, Batch 799, G Loss: 0.6860482096672058, D Loss: 1.3886998891830444\n",
            "Epoch 2, Batch 800, G Loss: 0.6860098838806152, D Loss: 1.3887780904769897\n",
            "Epoch 2, Batch 801, G Loss: 0.6860445141792297, D Loss: 1.3890070915222168\n",
            "Epoch 2, Batch 802, G Loss: 0.6860979199409485, D Loss: 1.3885388374328613\n",
            "Epoch 2, Batch 803, G Loss: 0.686211109161377, D Loss: 1.3883864879608154\n",
            "Epoch 2, Batch 804, G Loss: 0.6861032247543335, D Loss: 1.3882687091827393\n",
            "Epoch 2, Batch 805, G Loss: 0.6862363219261169, D Loss: 1.3885447978973389\n",
            "Epoch 2, Batch 806, G Loss: 0.686281144618988, D Loss: 1.3877004384994507\n",
            "Epoch 2, Batch 807, G Loss: 0.686230480670929, D Loss: 1.3883671760559082\n",
            "Epoch 2, Batch 808, G Loss: 0.6862973570823669, D Loss: 1.3878743648529053\n",
            "Epoch 2, Batch 809, G Loss: 0.6862313151359558, D Loss: 1.3876962661743164\n",
            "Epoch 2, Batch 810, G Loss: 0.6862627863883972, D Loss: 1.3878681659698486\n",
            "Epoch 2, Batch 811, G Loss: 0.6862080693244934, D Loss: 1.387641191482544\n",
            "Epoch 2, Batch 812, G Loss: 0.6864486932754517, D Loss: 1.3877111673355103\n",
            "Epoch 2, Batch 813, G Loss: 0.6863828301429749, D Loss: 1.3876774311065674\n",
            "Epoch 2, Batch 814, G Loss: 0.6864864826202393, D Loss: 1.3871595859527588\n",
            "Epoch 2, Batch 815, G Loss: 0.6864473223686218, D Loss: 1.3875622749328613\n",
            "Epoch 2, Batch 816, G Loss: 0.6863553524017334, D Loss: 1.3873364925384521\n",
            "Epoch 2, Batch 817, G Loss: 0.6865214109420776, D Loss: 1.3870935440063477\n",
            "Epoch 2, Batch 818, G Loss: 0.686631441116333, D Loss: 1.3873181343078613\n",
            "Epoch 2, Batch 819, G Loss: 0.6864420771598816, D Loss: 1.3873419761657715\n",
            "Epoch 2, Batch 820, G Loss: 0.6866305470466614, D Loss: 1.387613296508789\n",
            "Epoch 2, Batch 821, G Loss: 0.6866621971130371, D Loss: 1.3873602151870728\n",
            "Epoch 2, Batch 822, G Loss: 0.6866976618766785, D Loss: 1.38728666305542\n",
            "Epoch 2, Batch 823, G Loss: 0.6866384744644165, D Loss: 1.3871972560882568\n",
            "Epoch 2, Batch 824, G Loss: 0.6867683529853821, D Loss: 1.3871166706085205\n",
            "Epoch 2, Batch 825, G Loss: 0.6868042945861816, D Loss: 1.387007474899292\n",
            "Epoch 2, Batch 826, G Loss: 0.6868265867233276, D Loss: 1.3870878219604492\n",
            "Epoch 2, Batch 827, G Loss: 0.6867976784706116, D Loss: 1.3872084617614746\n",
            "Epoch 2, Batch 828, G Loss: 0.6869768500328064, D Loss: 1.3871078491210938\n",
            "Epoch 2, Batch 829, G Loss: 0.6868935227394104, D Loss: 1.3871006965637207\n",
            "Epoch 2, Batch 830, G Loss: 0.6869603991508484, D Loss: 1.3871089220046997\n",
            "Epoch 2, Batch 831, G Loss: 0.6869196891784668, D Loss: 1.3868629932403564\n",
            "Epoch 2, Batch 832, G Loss: 0.687036395072937, D Loss: 1.3869022130966187\n",
            "Epoch 2, Batch 833, G Loss: 0.6870274543762207, D Loss: 1.3869996070861816\n",
            "Epoch 2, Batch 834, G Loss: 0.6871762871742249, D Loss: 1.386601209640503\n",
            "Epoch 2, Batch 835, G Loss: 0.6871014833450317, D Loss: 1.3867223262786865\n",
            "Epoch 2, Batch 836, G Loss: 0.6871464252471924, D Loss: 1.386642336845398\n",
            "Epoch 2, Batch 837, G Loss: 0.6871744990348816, D Loss: 1.3867062330245972\n",
            "Epoch 2, Batch 838, G Loss: 0.6872472167015076, D Loss: 1.3865857124328613\n",
            "Epoch 2, Batch 839, G Loss: 0.6871920228004456, D Loss: 1.3867673873901367\n",
            "Epoch 2, Batch 840, G Loss: 0.6872393488883972, D Loss: 1.3868235349655151\n",
            "Epoch 2, Batch 841, G Loss: 0.6872904300689697, D Loss: 1.3866629600524902\n",
            "Epoch 2, Batch 842, G Loss: 0.6873478889465332, D Loss: 1.386605143547058\n",
            "Epoch 2, Batch 843, G Loss: 0.6873740553855896, D Loss: 1.38646399974823\n",
            "Epoch 2, Batch 844, G Loss: 0.6874383687973022, D Loss: 1.386587142944336\n",
            "Epoch 2, Batch 845, G Loss: 0.6875134706497192, D Loss: 1.386340618133545\n",
            "Epoch 2, Batch 846, G Loss: 0.6874852180480957, D Loss: 1.3862946033477783\n",
            "Epoch 2, Batch 847, G Loss: 0.6874844431877136, D Loss: 1.386374831199646\n",
            "Epoch 2, Batch 848, G Loss: 0.687628984451294, D Loss: 1.3862637281417847\n",
            "Epoch 2, Batch 849, G Loss: 0.6875565052032471, D Loss: 1.3865313529968262\n",
            "Epoch 2, Batch 850, G Loss: 0.68763267993927, D Loss: 1.3858745098114014\n",
            "Epoch 2, Batch 851, G Loss: 0.6876316666603088, D Loss: 1.385958194732666\n",
            "Epoch 2, Batch 852, G Loss: 0.6875669360160828, D Loss: 1.3860175609588623\n",
            "Epoch 2, Batch 853, G Loss: 0.6876752972602844, D Loss: 1.3858559131622314\n",
            "Epoch 2, Batch 854, G Loss: 0.6877086162567139, D Loss: 1.385803461074829\n",
            "Epoch 2, Batch 855, G Loss: 0.6878501176834106, D Loss: 1.3855149745941162\n",
            "Epoch 2, Batch 856, G Loss: 0.6877299547195435, D Loss: 1.3857753276824951\n",
            "Epoch 2, Batch 857, G Loss: 0.6877105236053467, D Loss: 1.385690689086914\n",
            "Epoch 2, Batch 858, G Loss: 0.6878531575202942, D Loss: 1.3860502243041992\n",
            "Epoch 2, Batch 859, G Loss: 0.6877905130386353, D Loss: 1.3858833312988281\n",
            "Epoch 2, Batch 860, G Loss: 0.6878883242607117, D Loss: 1.3857157230377197\n",
            "Epoch 2, Batch 861, G Loss: 0.6879159808158875, D Loss: 1.3857219219207764\n",
            "Epoch 2, Batch 862, G Loss: 0.6878747940063477, D Loss: 1.3856292963027954\n",
            "Epoch 2, Batch 863, G Loss: 0.6880168318748474, D Loss: 1.3854899406433105\n",
            "Epoch 2, Batch 864, G Loss: 0.6879059076309204, D Loss: 1.385606288909912\n",
            "Epoch 2, Batch 865, G Loss: 0.688032329082489, D Loss: 1.3856282234191895\n",
            "Epoch 2, Batch 866, G Loss: 0.6879957318305969, D Loss: 1.3856797218322754\n",
            "Epoch 2, Batch 867, G Loss: 0.6880505681037903, D Loss: 1.3856176137924194\n",
            "Epoch 2, Batch 868, G Loss: 0.6881067752838135, D Loss: 1.3848462104797363\n",
            "Epoch 2, Batch 869, G Loss: 0.6880544424057007, D Loss: 1.385007619857788\n",
            "Epoch 2, Batch 870, G Loss: 0.688107967376709, D Loss: 1.3844609260559082\n",
            "Epoch 2, Batch 871, G Loss: 0.6881892681121826, D Loss: 1.3844935894012451\n",
            "Epoch 2, Batch 872, G Loss: 0.6881569623947144, D Loss: 1.3845150470733643\n",
            "Epoch 2, Batch 873, G Loss: 0.6883363723754883, D Loss: 1.3849753141403198\n",
            "Epoch 2, Batch 874, G Loss: 0.6882036328315735, D Loss: 1.3852431774139404\n",
            "Epoch 2, Batch 875, G Loss: 0.6883224248886108, D Loss: 1.3849997520446777\n",
            "Epoch 2, Batch 876, G Loss: 0.6882966756820679, D Loss: 1.3850164413452148\n",
            "Epoch 2, Batch 877, G Loss: 0.6882399916648865, D Loss: 1.385166883468628\n",
            "Epoch 2, Batch 878, G Loss: 0.6882477402687073, D Loss: 1.384992003440857\n",
            "Epoch 2, Batch 879, G Loss: 0.6883568167686462, D Loss: 1.3849337100982666\n",
            "Epoch 2, Batch 880, G Loss: 0.6883213520050049, D Loss: 1.385453224182129\n",
            "Epoch 2, Batch 881, G Loss: 0.6883617043495178, D Loss: 1.3854049444198608\n",
            "Epoch 2, Batch 882, G Loss: 0.6883812546730042, D Loss: 1.3856160640716553\n",
            "Epoch 2, Batch 883, G Loss: 0.6883811354637146, D Loss: 1.3851983547210693\n",
            "Epoch 2, Batch 884, G Loss: 0.6884642839431763, D Loss: 1.3850488662719727\n",
            "Epoch 2, Batch 885, G Loss: 0.6884889602661133, D Loss: 1.385129451751709\n",
            "Epoch 2, Batch 886, G Loss: 0.688452959060669, D Loss: 1.3851633071899414\n",
            "Epoch 2, Batch 887, G Loss: 0.6885510087013245, D Loss: 1.3838577270507812\n",
            "Epoch 2, Batch 888, G Loss: 0.6885635852813721, D Loss: 1.3842833042144775\n",
            "Epoch 2, Batch 889, G Loss: 0.6886307597160339, D Loss: 1.3843321800231934\n",
            "Epoch 2, Batch 890, G Loss: 0.6886650323867798, D Loss: 1.3848541975021362\n",
            "Epoch 2, Batch 891, G Loss: 0.688683271408081, D Loss: 1.3852779865264893\n",
            "Epoch 2, Batch 892, G Loss: 0.6887263059616089, D Loss: 1.3861374855041504\n",
            "Epoch 2, Batch 893, G Loss: 0.6886752247810364, D Loss: 1.385506272315979\n",
            "Epoch 2, Batch 894, G Loss: 0.6887650489807129, D Loss: 1.3849658966064453\n",
            "Epoch 2, Batch 895, G Loss: 0.6888200640678406, D Loss: 1.3845601081848145\n",
            "Epoch 2, Batch 896, G Loss: 0.6889083385467529, D Loss: 1.3846557140350342\n",
            "Epoch 2, Batch 897, G Loss: 0.6889427304267883, D Loss: 1.3845248222351074\n",
            "Epoch 2, Batch 898, G Loss: 0.68885338306427, D Loss: 1.384789228439331\n",
            "Epoch 2, Batch 899, G Loss: 0.6889352798461914, D Loss: 1.3843262195587158\n",
            "Epoch 2, Batch 900, G Loss: 0.6889829039573669, D Loss: 1.3843958377838135\n",
            "Epoch 2, Batch 901, G Loss: 0.6890596747398376, D Loss: 1.3848247528076172\n",
            "Epoch 2, Batch 902, G Loss: 0.6890593767166138, D Loss: 1.3855082988739014\n",
            "Epoch 2, Batch 903, G Loss: 0.6891130805015564, D Loss: 1.3853883743286133\n",
            "Epoch 2, Batch 904, G Loss: 0.6891018748283386, D Loss: 1.3855812549591064\n",
            "Epoch 2, Batch 905, G Loss: 0.6891160011291504, D Loss: 1.3850781917572021\n",
            "Epoch 2, Batch 906, G Loss: 0.6891951560974121, D Loss: 1.385108232498169\n",
            "Epoch 2, Batch 907, G Loss: 0.6892685890197754, D Loss: 1.3842742443084717\n",
            "Epoch 2, Batch 908, G Loss: 0.6892297863960266, D Loss: 1.3855738639831543\n",
            "Epoch 2, Batch 909, G Loss: 0.6892356872558594, D Loss: 1.3858288526535034\n",
            "Epoch 2, Batch 910, G Loss: 0.689311146736145, D Loss: 1.384413480758667\n",
            "Epoch 2, Batch 911, G Loss: 0.689460277557373, D Loss: 1.3838872909545898\n",
            "Epoch 2, Batch 912, G Loss: 0.689528226852417, D Loss: 1.3839082717895508\n",
            "Epoch 2, Batch 913, G Loss: 0.6894726157188416, D Loss: 1.3832104206085205\n",
            "Epoch 2, Batch 914, G Loss: 0.6895162463188171, D Loss: 1.3832625150680542\n",
            "Epoch 2, Batch 915, G Loss: 0.6895803213119507, D Loss: 1.38507080078125\n",
            "Epoch 2, Batch 916, G Loss: 0.6895840764045715, D Loss: 1.385866403579712\n",
            "Epoch 2, Batch 917, G Loss: 0.6896451711654663, D Loss: 1.3860127925872803\n",
            "Epoch 2, Batch 918, G Loss: 0.6895596981048584, D Loss: 1.3864376544952393\n",
            "Epoch 2, Batch 919, G Loss: 0.6897678971290588, D Loss: 1.3856372833251953\n",
            "Epoch 2, Batch 920, G Loss: 0.6897597312927246, D Loss: 1.3834517002105713\n",
            "Epoch 2, Batch 921, G Loss: 0.689772367477417, D Loss: 1.384016990661621\n",
            "Epoch 2, Batch 922, G Loss: 0.6898303031921387, D Loss: 1.383886694908142\n",
            "Epoch 2, Batch 923, G Loss: 0.6898932456970215, D Loss: 1.3844839334487915\n",
            "Epoch 2, Batch 924, G Loss: 0.6898145079612732, D Loss: 1.3848021030426025\n",
            "Epoch 2, Batch 925, G Loss: 0.689849317073822, D Loss: 1.384084701538086\n",
            "Epoch 2, Batch 926, G Loss: 0.6899691820144653, D Loss: 1.3835864067077637\n",
            "Epoch 2, Batch 927, G Loss: 0.6899660229682922, D Loss: 1.3827793598175049\n",
            "Epoch 2, Batch 928, G Loss: 0.6899731159210205, D Loss: 1.37935209274292\n",
            "Epoch 2, Batch 929, G Loss: 0.6901608109474182, D Loss: 1.3765740394592285\n",
            "Epoch 2, Batch 930, G Loss: 0.6899821758270264, D Loss: 1.3820642232894897\n",
            "Epoch 2, Batch 931, G Loss: 0.6901482343673706, D Loss: 1.3850080966949463\n",
            "Epoch 2, Batch 932, G Loss: 0.6901140213012695, D Loss: 1.3840183019638062\n",
            "Epoch 2, Batch 933, G Loss: 0.6901278495788574, D Loss: 1.3829314708709717\n",
            "Epoch 2, Batch 934, G Loss: 0.6901794672012329, D Loss: 1.3837236166000366\n",
            "Epoch 2, Batch 935, G Loss: 0.6902855038642883, D Loss: 1.3824986219406128\n",
            "Epoch 2, Batch 936, G Loss: 0.6901998519897461, D Loss: 1.3820888996124268\n",
            "Epoch 2, Batch 937, G Loss: 0.6903432607650757, D Loss: 1.3853247165679932\n",
            "Epoch 2, Batch 938, G Loss: 0.6902873516082764, D Loss: 1.385124683380127\n",
            "Epoch 3, Batch 1, G Loss: 0.6902589797973633, D Loss: 1.3839210271835327\n",
            "Epoch 3, Batch 2, G Loss: 0.6902204155921936, D Loss: 1.383831262588501\n",
            "Epoch 3, Batch 3, G Loss: 0.6903097033500671, D Loss: 1.384063482284546\n",
            "Epoch 3, Batch 4, G Loss: 0.6903579235076904, D Loss: 1.3832197189331055\n",
            "Epoch 3, Batch 5, G Loss: 0.6904605031013489, D Loss: 1.3841662406921387\n",
            "Epoch 3, Batch 6, G Loss: 0.6905226707458496, D Loss: 1.3837146759033203\n",
            "Epoch 3, Batch 7, G Loss: 0.6904751658439636, D Loss: 1.3842111825942993\n",
            "Epoch 3, Batch 8, G Loss: 0.6904656291007996, D Loss: 1.3842085599899292\n",
            "Epoch 3, Batch 9, G Loss: 0.6905708312988281, D Loss: 1.3836277723312378\n",
            "Epoch 3, Batch 10, G Loss: 0.6905977129936218, D Loss: 1.3844637870788574\n",
            "Epoch 3, Batch 11, G Loss: 0.6906143426895142, D Loss: 1.3833847045898438\n",
            "Epoch 3, Batch 12, G Loss: 0.6906391978263855, D Loss: 1.3823084831237793\n",
            "Epoch 3, Batch 13, G Loss: 0.6906819939613342, D Loss: 1.383056402206421\n",
            "Epoch 3, Batch 14, G Loss: 0.6907638311386108, D Loss: 1.382733941078186\n",
            "Epoch 3, Batch 15, G Loss: 0.6907870769500732, D Loss: 1.38315749168396\n",
            "Epoch 3, Batch 16, G Loss: 0.6908454298973083, D Loss: 1.3839337825775146\n",
            "Epoch 3, Batch 17, G Loss: 0.6908485293388367, D Loss: 1.385034203529358\n",
            "Epoch 3, Batch 18, G Loss: 0.6908623576164246, D Loss: 1.3845715522766113\n",
            "Epoch 3, Batch 19, G Loss: 0.6909271478652954, D Loss: 1.3823407888412476\n",
            "Epoch 3, Batch 20, G Loss: 0.6909306049346924, D Loss: 1.3801310062408447\n",
            "Epoch 3, Batch 21, G Loss: 0.6909757256507874, D Loss: 1.3808960914611816\n",
            "Epoch 3, Batch 22, G Loss: 0.6910216808319092, D Loss: 1.379345178604126\n",
            "Epoch 3, Batch 23, G Loss: 0.691066324710846, D Loss: 1.3833907842636108\n",
            "Epoch 3, Batch 24, G Loss: 0.6910967230796814, D Loss: 1.3836650848388672\n",
            "Epoch 3, Batch 25, G Loss: 0.6911443471908569, D Loss: 1.3836169242858887\n",
            "Epoch 3, Batch 26, G Loss: 0.6911357045173645, D Loss: 1.3840467929840088\n",
            "Epoch 3, Batch 27, G Loss: 0.6911933422088623, D Loss: 1.3839374780654907\n",
            "Epoch 3, Batch 28, G Loss: 0.6912069320678711, D Loss: 1.3836901187896729\n",
            "Epoch 3, Batch 29, G Loss: 0.6912049055099487, D Loss: 1.3838497400283813\n",
            "Epoch 3, Batch 30, G Loss: 0.691261887550354, D Loss: 1.3825500011444092\n",
            "Epoch 3, Batch 31, G Loss: 0.6912814378738403, D Loss: 1.381401538848877\n",
            "Epoch 3, Batch 32, G Loss: 0.6912865042686462, D Loss: 1.3807995319366455\n",
            "Epoch 3, Batch 33, G Loss: 0.6913223266601562, D Loss: 1.3820507526397705\n",
            "Epoch 3, Batch 34, G Loss: 0.6914535760879517, D Loss: 1.382615327835083\n",
            "Epoch 3, Batch 35, G Loss: 0.6913711428642273, D Loss: 1.3833163976669312\n",
            "Epoch 3, Batch 36, G Loss: 0.691471517086029, D Loss: 1.3819684982299805\n",
            "Epoch 3, Batch 37, G Loss: 0.6914911866188049, D Loss: 1.3830783367156982\n",
            "Epoch 3, Batch 38, G Loss: 0.6915401816368103, D Loss: 1.3833290338516235\n",
            "Epoch 3, Batch 39, G Loss: 0.6915230751037598, D Loss: 1.3830673694610596\n",
            "Epoch 3, Batch 40, G Loss: 0.6915260553359985, D Loss: 1.3822499513626099\n",
            "Epoch 3, Batch 41, G Loss: 0.6916072368621826, D Loss: 1.3825592994689941\n",
            "Epoch 3, Batch 42, G Loss: 0.6916897892951965, D Loss: 1.3817479610443115\n",
            "Epoch 3, Batch 43, G Loss: 0.6916791796684265, D Loss: 1.3833165168762207\n",
            "Epoch 3, Batch 44, G Loss: 0.6917453408241272, D Loss: 1.3825037479400635\n",
            "Epoch 3, Batch 45, G Loss: 0.6917768120765686, D Loss: 1.381610631942749\n",
            "Epoch 3, Batch 46, G Loss: 0.6917362809181213, D Loss: 1.3825074434280396\n",
            "Epoch 3, Batch 47, G Loss: 0.6918174624443054, D Loss: 1.384354829788208\n",
            "Epoch 3, Batch 48, G Loss: 0.6918062567710876, D Loss: 1.3831391334533691\n",
            "Epoch 3, Batch 49, G Loss: 0.6917975544929504, D Loss: 1.3842432498931885\n",
            "Epoch 3, Batch 50, G Loss: 0.6918647885322571, D Loss: 1.38411545753479\n",
            "Epoch 3, Batch 51, G Loss: 0.6919140219688416, D Loss: 1.3838093280792236\n",
            "Epoch 3, Batch 52, G Loss: 0.6920020580291748, D Loss: 1.3841233253479004\n",
            "Epoch 3, Batch 53, G Loss: 0.6920211911201477, D Loss: 1.3826197385787964\n",
            "Epoch 3, Batch 54, G Loss: 0.6919997930526733, D Loss: 1.3844059705734253\n",
            "Epoch 3, Batch 55, G Loss: 0.6920652389526367, D Loss: 1.3842003345489502\n",
            "Epoch 3, Batch 56, G Loss: 0.692119836807251, D Loss: 1.3829214572906494\n",
            "Epoch 3, Batch 57, G Loss: 0.6921448707580566, D Loss: 1.381716012954712\n",
            "Epoch 3, Batch 58, G Loss: 0.6921124458312988, D Loss: 1.3807740211486816\n",
            "Epoch 3, Batch 59, G Loss: 0.6922659873962402, D Loss: 1.3830738067626953\n",
            "Epoch 3, Batch 60, G Loss: 0.6922356486320496, D Loss: 1.3831700086593628\n",
            "Epoch 3, Batch 61, G Loss: 0.6922271251678467, D Loss: 1.3836534023284912\n",
            "Epoch 3, Batch 62, G Loss: 0.6922960877418518, D Loss: 1.3831682205200195\n",
            "Epoch 3, Batch 63, G Loss: 0.6923113465309143, D Loss: 1.3832721710205078\n",
            "Epoch 3, Batch 64, G Loss: 0.6922959089279175, D Loss: 1.3818758726119995\n",
            "Epoch 3, Batch 65, G Loss: 0.6923649311065674, D Loss: 1.3831409215927124\n",
            "Epoch 3, Batch 66, G Loss: 0.6924400925636292, D Loss: 1.3835623264312744\n",
            "Epoch 3, Batch 67, G Loss: 0.6924532055854797, D Loss: 1.3821837902069092\n",
            "Epoch 3, Batch 68, G Loss: 0.692486047744751, D Loss: 1.3832283020019531\n",
            "Epoch 3, Batch 69, G Loss: 0.6925346255302429, D Loss: 1.3843475580215454\n",
            "Epoch 3, Batch 70, G Loss: 0.6925584673881531, D Loss: 1.3831828832626343\n",
            "Epoch 3, Batch 71, G Loss: 0.6926048994064331, D Loss: 1.380239486694336\n",
            "Epoch 3, Batch 72, G Loss: 0.6926783323287964, D Loss: 1.3822880983352661\n",
            "Epoch 3, Batch 73, G Loss: 0.692604660987854, D Loss: 1.3807995319366455\n",
            "Epoch 3, Batch 74, G Loss: 0.6926696300506592, D Loss: 1.3794634342193604\n",
            "Epoch 3, Batch 75, G Loss: 0.692750096321106, D Loss: 1.3819693326950073\n",
            "Epoch 3, Batch 76, G Loss: 0.692704975605011, D Loss: 1.381138563156128\n",
            "Epoch 3, Batch 77, G Loss: 0.6928235292434692, D Loss: 1.381279468536377\n",
            "Epoch 3, Batch 78, G Loss: 0.6927971243858337, D Loss: 1.3837008476257324\n",
            "Epoch 3, Batch 79, G Loss: 0.6928229928016663, D Loss: 1.3823444843292236\n",
            "Epoch 3, Batch 80, G Loss: 0.6928476095199585, D Loss: 1.3812708854675293\n",
            "Epoch 3, Batch 81, G Loss: 0.6929013729095459, D Loss: 1.381425142288208\n",
            "Epoch 3, Batch 82, G Loss: 0.6929291486740112, D Loss: 1.3816158771514893\n",
            "Epoch 3, Batch 83, G Loss: 0.6929905414581299, D Loss: 1.3820542097091675\n",
            "Epoch 3, Batch 84, G Loss: 0.6930188536643982, D Loss: 1.379639744758606\n",
            "Epoch 3, Batch 85, G Loss: 0.6930789351463318, D Loss: 1.37989342212677\n",
            "Epoch 3, Batch 86, G Loss: 0.6930756568908691, D Loss: 1.3799588680267334\n",
            "Epoch 3, Batch 87, G Loss: 0.6931626200675964, D Loss: 1.381117343902588\n",
            "Epoch 3, Batch 88, G Loss: 0.6931043267250061, D Loss: 1.3814971446990967\n",
            "Epoch 3, Batch 89, G Loss: 0.6931909918785095, D Loss: 1.3820470571517944\n",
            "Epoch 3, Batch 90, G Loss: 0.6932411789894104, D Loss: 1.381298542022705\n",
            "Epoch 3, Batch 91, G Loss: 0.6932621598243713, D Loss: 1.3829277753829956\n",
            "Epoch 3, Batch 92, G Loss: 0.6932654976844788, D Loss: 1.383270263671875\n",
            "Epoch 3, Batch 93, G Loss: 0.6932997703552246, D Loss: 1.3826444149017334\n",
            "Epoch 3, Batch 94, G Loss: 0.6933342814445496, D Loss: 1.3780580759048462\n",
            "Epoch 3, Batch 95, G Loss: 0.6933483481407166, D Loss: 1.3780341148376465\n",
            "Epoch 3, Batch 96, G Loss: 0.6934045553207397, D Loss: 1.3780670166015625\n",
            "Epoch 3, Batch 97, G Loss: 0.6934384107589722, D Loss: 1.378937005996704\n",
            "Epoch 3, Batch 98, G Loss: 0.6934481859207153, D Loss: 1.3807053565979004\n",
            "Epoch 3, Batch 99, G Loss: 0.6935303211212158, D Loss: 1.3801945447921753\n",
            "Epoch 3, Batch 100, G Loss: 0.6935643553733826, D Loss: 1.3804612159729004\n",
            "Epoch 3, Batch 101, G Loss: 0.6935678720474243, D Loss: 1.381211519241333\n",
            "Epoch 3, Batch 102, G Loss: 0.693646252155304, D Loss: 1.3825044631958008\n",
            "Epoch 3, Batch 103, G Loss: 0.6936544179916382, D Loss: 1.3837864398956299\n",
            "Epoch 3, Batch 104, G Loss: 0.6936975717544556, D Loss: 1.3814622163772583\n",
            "Epoch 3, Batch 105, G Loss: 0.6937350630760193, D Loss: 1.3859566450119019\n",
            "Epoch 3, Batch 106, G Loss: 0.6937555074691772, D Loss: 1.38539457321167\n",
            "Epoch 3, Batch 107, G Loss: 0.6937808394432068, D Loss: 1.3864736557006836\n",
            "Epoch 3, Batch 108, G Loss: 0.6938139200210571, D Loss: 1.3811497688293457\n",
            "Epoch 3, Batch 109, G Loss: 0.6939140558242798, D Loss: 1.3801305294036865\n",
            "Epoch 3, Batch 110, G Loss: 0.6939261555671692, D Loss: 1.3821507692337036\n",
            "Epoch 3, Batch 111, G Loss: 0.6939797401428223, D Loss: 1.3843650817871094\n",
            "Epoch 3, Batch 112, G Loss: 0.6939646005630493, D Loss: 1.3843390941619873\n",
            "Epoch 3, Batch 113, G Loss: 0.6940183043479919, D Loss: 1.3849849700927734\n",
            "Epoch 3, Batch 114, G Loss: 0.6940540671348572, D Loss: 1.3842039108276367\n",
            "Epoch 3, Batch 115, G Loss: 0.6941858530044556, D Loss: 1.386924147605896\n",
            "Epoch 3, Batch 116, G Loss: 0.6941235661506653, D Loss: 1.3857042789459229\n",
            "Epoch 3, Batch 117, G Loss: 0.6941803693771362, D Loss: 1.3841502666473389\n",
            "Epoch 3, Batch 118, G Loss: 0.6942740678787231, D Loss: 1.3846440315246582\n",
            "Epoch 3, Batch 119, G Loss: 0.6942850947380066, D Loss: 1.3844873905181885\n",
            "Epoch 3, Batch 120, G Loss: 0.694312572479248, D Loss: 1.3825254440307617\n",
            "Epoch 3, Batch 121, G Loss: 0.6943454146385193, D Loss: 1.3829838037490845\n",
            "Epoch 3, Batch 122, G Loss: 0.6943947076797485, D Loss: 1.3827767372131348\n",
            "Epoch 3, Batch 123, G Loss: 0.6944036483764648, D Loss: 1.3814043998718262\n",
            "Epoch 3, Batch 124, G Loss: 0.6944499015808105, D Loss: 1.3790111541748047\n",
            "Epoch 3, Batch 125, G Loss: 0.6944636106491089, D Loss: 1.3798269033432007\n",
            "Epoch 3, Batch 126, G Loss: 0.69449383020401, D Loss: 1.3800039291381836\n",
            "Epoch 3, Batch 127, G Loss: 0.6945409774780273, D Loss: 1.3811581134796143\n",
            "Epoch 3, Batch 128, G Loss: 0.6946035027503967, D Loss: 1.3848598003387451\n",
            "Epoch 3, Batch 129, G Loss: 0.6945900917053223, D Loss: 1.3843317031860352\n",
            "Epoch 3, Batch 130, G Loss: 0.6946281790733337, D Loss: 1.3805725574493408\n",
            "Epoch 3, Batch 131, G Loss: 0.6947042346000671, D Loss: 1.3816195726394653\n",
            "Epoch 3, Batch 132, G Loss: 0.694708526134491, D Loss: 1.3825087547302246\n",
            "Epoch 3, Batch 133, G Loss: 0.694733738899231, D Loss: 1.3761351108551025\n",
            "Epoch 3, Batch 134, G Loss: 0.6947687864303589, D Loss: 1.3781673908233643\n",
            "Epoch 3, Batch 135, G Loss: 0.6947451829910278, D Loss: 1.3790833950042725\n",
            "Epoch 3, Batch 136, G Loss: 0.6948263049125671, D Loss: 1.3781547546386719\n",
            "Epoch 3, Batch 137, G Loss: 0.6948630809783936, D Loss: 1.3813010454177856\n",
            "Epoch 3, Batch 138, G Loss: 0.6949134469032288, D Loss: 1.3819258213043213\n",
            "Epoch 3, Batch 139, G Loss: 0.6949365735054016, D Loss: 1.3835893869400024\n",
            "Epoch 3, Batch 140, G Loss: 0.6949674487113953, D Loss: 1.3814588785171509\n",
            "Epoch 3, Batch 141, G Loss: 0.6949962973594666, D Loss: 1.383042812347412\n",
            "Epoch 3, Batch 142, G Loss: 0.6950189471244812, D Loss: 1.3822230100631714\n",
            "Epoch 3, Batch 143, G Loss: 0.6950607299804688, D Loss: 1.3849210739135742\n",
            "Epoch 3, Batch 144, G Loss: 0.6950854063034058, D Loss: 1.3834303617477417\n",
            "Epoch 3, Batch 145, G Loss: 0.6950989365577698, D Loss: 1.386094093322754\n",
            "Epoch 3, Batch 146, G Loss: 0.6951404213905334, D Loss: 1.381638526916504\n",
            "Epoch 3, Batch 147, G Loss: 0.6951546669006348, D Loss: 1.381089210510254\n",
            "Epoch 3, Batch 148, G Loss: 0.6952595710754395, D Loss: 1.3809607028961182\n",
            "Epoch 3, Batch 149, G Loss: 0.6952056884765625, D Loss: 1.379746437072754\n",
            "Epoch 3, Batch 150, G Loss: 0.6952961683273315, D Loss: 1.384127140045166\n",
            "Epoch 3, Batch 151, G Loss: 0.6952785849571228, D Loss: 1.3826229572296143\n",
            "Epoch 3, Batch 152, G Loss: 0.6953394412994385, D Loss: 1.3804550170898438\n",
            "Epoch 3, Batch 153, G Loss: 0.6953350901603699, D Loss: 1.381386160850525\n",
            "Epoch 3, Batch 154, G Loss: 0.6953545212745667, D Loss: 1.382786512374878\n",
            "Epoch 3, Batch 155, G Loss: 0.6953783631324768, D Loss: 1.3826322555541992\n",
            "Epoch 3, Batch 156, G Loss: 0.6954314708709717, D Loss: 1.3831077814102173\n",
            "Epoch 3, Batch 157, G Loss: 0.6954510807991028, D Loss: 1.380321979522705\n",
            "Epoch 3, Batch 158, G Loss: 0.6954531669616699, D Loss: 1.3831608295440674\n",
            "Epoch 3, Batch 159, G Loss: 0.6954932808876038, D Loss: 1.384751558303833\n",
            "Epoch 3, Batch 160, G Loss: 0.6955416202545166, D Loss: 1.3753938674926758\n",
            "Epoch 3, Batch 161, G Loss: 0.6955505609512329, D Loss: 1.3743906021118164\n",
            "Epoch 3, Batch 162, G Loss: 0.6955915093421936, D Loss: 1.378614068031311\n",
            "Epoch 3, Batch 163, G Loss: 0.6956368088722229, D Loss: 1.379992961883545\n",
            "Epoch 3, Batch 164, G Loss: 0.6956623792648315, D Loss: 1.3786683082580566\n",
            "Epoch 3, Batch 165, G Loss: 0.695684015750885, D Loss: 1.3803448677062988\n",
            "Epoch 3, Batch 166, G Loss: 0.6956969499588013, D Loss: 1.3819048404693604\n",
            "Epoch 3, Batch 167, G Loss: 0.6957215070724487, D Loss: 1.3846030235290527\n",
            "Epoch 3, Batch 168, G Loss: 0.6957578063011169, D Loss: 1.3828206062316895\n",
            "Epoch 3, Batch 169, G Loss: 0.6958158612251282, D Loss: 1.3811649084091187\n",
            "Epoch 3, Batch 170, G Loss: 0.6958221793174744, D Loss: 1.3808785676956177\n",
            "Epoch 3, Batch 171, G Loss: 0.695838451385498, D Loss: 1.381402611732483\n",
            "Epoch 3, Batch 172, G Loss: 0.6958404779434204, D Loss: 1.3796422481536865\n",
            "Epoch 3, Batch 173, G Loss: 0.6958624124526978, D Loss: 1.3792457580566406\n",
            "Epoch 3, Batch 174, G Loss: 0.6959001421928406, D Loss: 1.3822110891342163\n",
            "Epoch 3, Batch 175, G Loss: 0.6959553360939026, D Loss: 1.381487250328064\n",
            "Epoch 3, Batch 176, G Loss: 0.6959834098815918, D Loss: 1.3820019960403442\n",
            "Epoch 3, Batch 177, G Loss: 0.6960350275039673, D Loss: 1.383009672164917\n",
            "Epoch 3, Batch 178, G Loss: 0.6960656046867371, D Loss: 1.381638765335083\n",
            "Epoch 3, Batch 179, G Loss: 0.696025013923645, D Loss: 1.3826665878295898\n",
            "Epoch 3, Batch 180, G Loss: 0.6960847973823547, D Loss: 1.3826003074645996\n",
            "Epoch 3, Batch 181, G Loss: 0.6961374282836914, D Loss: 1.3827356100082397\n",
            "Epoch 3, Batch 182, G Loss: 0.6961426734924316, D Loss: 1.382605791091919\n",
            "Epoch 3, Batch 183, G Loss: 0.6961613893508911, D Loss: 1.3821666240692139\n",
            "Epoch 3, Batch 184, G Loss: 0.6962119936943054, D Loss: 1.3830342292785645\n",
            "Epoch 3, Batch 185, G Loss: 0.6962350606918335, D Loss: 1.3823800086975098\n",
            "Epoch 3, Batch 186, G Loss: 0.6962895393371582, D Loss: 1.3825597763061523\n",
            "Epoch 3, Batch 187, G Loss: 0.696307897567749, D Loss: 1.3833303451538086\n",
            "Epoch 3, Batch 188, G Loss: 0.6963410377502441, D Loss: 1.3788979053497314\n",
            "Epoch 3, Batch 189, G Loss: 0.6963323354721069, D Loss: 1.3810337781906128\n",
            "Epoch 3, Batch 190, G Loss: 0.6963765621185303, D Loss: 1.3811516761779785\n",
            "Epoch 3, Batch 191, G Loss: 0.6963725090026855, D Loss: 1.3803648948669434\n",
            "Epoch 3, Batch 192, G Loss: 0.6964117288589478, D Loss: 1.3799452781677246\n",
            "Epoch 3, Batch 193, G Loss: 0.6964797973632812, D Loss: 1.3811935186386108\n",
            "Epoch 3, Batch 194, G Loss: 0.6965158581733704, D Loss: 1.3808250427246094\n",
            "Epoch 3, Batch 195, G Loss: 0.6964909434318542, D Loss: 1.379796028137207\n",
            "Epoch 3, Batch 196, G Loss: 0.696563184261322, D Loss: 1.380425214767456\n",
            "Epoch 3, Batch 197, G Loss: 0.6965461373329163, D Loss: 1.3802974224090576\n",
            "Epoch 3, Batch 198, G Loss: 0.6965810060501099, D Loss: 1.3801770210266113\n",
            "Epoch 3, Batch 199, G Loss: 0.6966127157211304, D Loss: 1.3814936876296997\n",
            "Epoch 3, Batch 200, G Loss: 0.6966273784637451, D Loss: 1.3837863206863403\n",
            "Epoch 3, Batch 201, G Loss: 0.6966588497161865, D Loss: 1.385488510131836\n",
            "Epoch 3, Batch 202, G Loss: 0.6966640949249268, D Loss: 1.3816554546356201\n",
            "Epoch 3, Batch 203, G Loss: 0.6967092752456665, D Loss: 1.378157615661621\n",
            "Epoch 3, Batch 204, G Loss: 0.696735143661499, D Loss: 1.3786120414733887\n",
            "Epoch 3, Batch 205, G Loss: 0.6967535018920898, D Loss: 1.3760030269622803\n",
            "Epoch 3, Batch 206, G Loss: 0.6967552304267883, D Loss: 1.3803832530975342\n",
            "Epoch 3, Batch 207, G Loss: 0.6968168020248413, D Loss: 1.3795669078826904\n",
            "Epoch 3, Batch 208, G Loss: 0.6968137621879578, D Loss: 1.3805360794067383\n",
            "Epoch 3, Batch 209, G Loss: 0.6968640685081482, D Loss: 1.380373477935791\n",
            "Epoch 3, Batch 210, G Loss: 0.6968674063682556, D Loss: 1.3811358213424683\n",
            "Epoch 3, Batch 211, G Loss: 0.6968960165977478, D Loss: 1.37978196144104\n",
            "Epoch 3, Batch 212, G Loss: 0.6969162821769714, D Loss: 1.3778479099273682\n",
            "Epoch 3, Batch 213, G Loss: 0.6969448924064636, D Loss: 1.380305290222168\n",
            "Epoch 3, Batch 214, G Loss: 0.696967601776123, D Loss: 1.3843713998794556\n",
            "Epoch 3, Batch 215, G Loss: 0.6969985961914062, D Loss: 1.3860630989074707\n",
            "Epoch 3, Batch 216, G Loss: 0.6970031261444092, D Loss: 1.3840419054031372\n",
            "Epoch 3, Batch 217, G Loss: 0.6970383524894714, D Loss: 1.382279396057129\n",
            "Epoch 3, Batch 218, G Loss: 0.6970927715301514, D Loss: 1.3822215795516968\n",
            "Epoch 3, Batch 219, G Loss: 0.6970815658569336, D Loss: 1.3840469121932983\n",
            "Epoch 3, Batch 220, G Loss: 0.6971228718757629, D Loss: 1.378137230873108\n",
            "Epoch 3, Batch 221, G Loss: 0.6971404552459717, D Loss: 1.3785635232925415\n",
            "Epoch 3, Batch 222, G Loss: 0.6971638798713684, D Loss: 1.3775876760482788\n",
            "Epoch 3, Batch 223, G Loss: 0.6971868276596069, D Loss: 1.3792343139648438\n",
            "Epoch 3, Batch 224, G Loss: 0.6972075700759888, D Loss: 1.3794341087341309\n",
            "Epoch 3, Batch 225, G Loss: 0.6972376108169556, D Loss: 1.3807156085968018\n",
            "Epoch 3, Batch 226, G Loss: 0.6972597241401672, D Loss: 1.382735013961792\n",
            "Epoch 3, Batch 227, G Loss: 0.6972914934158325, D Loss: 1.3839361667633057\n",
            "Epoch 3, Batch 228, G Loss: 0.6972945928573608, D Loss: 1.3845367431640625\n",
            "Epoch 3, Batch 229, G Loss: 0.6973205804824829, D Loss: 1.383092999458313\n",
            "Epoch 3, Batch 230, G Loss: 0.6973363161087036, D Loss: 1.3809318542480469\n",
            "Epoch 3, Batch 231, G Loss: 0.6973718404769897, D Loss: 1.3828773498535156\n",
            "Epoch 3, Batch 232, G Loss: 0.6973744630813599, D Loss: 1.3823153972625732\n",
            "Epoch 3, Batch 233, G Loss: 0.6974121332168579, D Loss: 1.3827059268951416\n",
            "Epoch 3, Batch 234, G Loss: 0.6974561214447021, D Loss: 1.3827636241912842\n",
            "Epoch 3, Batch 235, G Loss: 0.6974412798881531, D Loss: 1.3848689794540405\n",
            "Epoch 3, Batch 236, G Loss: 0.6974703669548035, D Loss: 1.3821542263031006\n",
            "Epoch 3, Batch 237, G Loss: 0.6974871158599854, D Loss: 1.3839457035064697\n",
            "Epoch 3, Batch 238, G Loss: 0.697510838508606, D Loss: 1.384894609451294\n",
            "Epoch 3, Batch 239, G Loss: 0.6975169777870178, D Loss: 1.3837405443191528\n",
            "Epoch 3, Batch 240, G Loss: 0.6975641250610352, D Loss: 1.3845105171203613\n",
            "Epoch 3, Batch 241, G Loss: 0.6975690126419067, D Loss: 1.3850018978118896\n",
            "Epoch 3, Batch 242, G Loss: 0.6975824236869812, D Loss: 1.3846843242645264\n",
            "Epoch 3, Batch 243, G Loss: 0.6975980401039124, D Loss: 1.3830442428588867\n",
            "Epoch 3, Batch 244, G Loss: 0.6976149678230286, D Loss: 1.383563756942749\n",
            "Epoch 3, Batch 245, G Loss: 0.697631299495697, D Loss: 1.382676601409912\n",
            "Epoch 3, Batch 246, G Loss: 0.6976509690284729, D Loss: 1.3830904960632324\n",
            "Epoch 3, Batch 247, G Loss: 0.6976549029350281, D Loss: 1.3827524185180664\n",
            "Epoch 3, Batch 248, G Loss: 0.6976656317710876, D Loss: 1.383985161781311\n",
            "Epoch 3, Batch 249, G Loss: 0.6976742148399353, D Loss: 1.3808119297027588\n",
            "Epoch 3, Batch 250, G Loss: 0.6976932883262634, D Loss: 1.3832039833068848\n",
            "Epoch 3, Batch 251, G Loss: 0.6976982951164246, D Loss: 1.3812642097473145\n",
            "Epoch 3, Batch 252, G Loss: 0.6977348923683167, D Loss: 1.3796510696411133\n",
            "Epoch 3, Batch 253, G Loss: 0.6977553963661194, D Loss: 1.380126953125\n",
            "Epoch 3, Batch 254, G Loss: 0.6977331042289734, D Loss: 1.3811399936676025\n",
            "Epoch 3, Batch 255, G Loss: 0.6977509260177612, D Loss: 1.3823726177215576\n",
            "Epoch 3, Batch 256, G Loss: 0.6977600455284119, D Loss: 1.382117509841919\n",
            "Epoch 3, Batch 257, G Loss: 0.6978018879890442, D Loss: 1.3827990293502808\n",
            "Epoch 3, Batch 258, G Loss: 0.6978101134300232, D Loss: 1.3828215599060059\n",
            "Epoch 3, Batch 259, G Loss: 0.6978206038475037, D Loss: 1.3825331926345825\n",
            "Epoch 3, Batch 260, G Loss: 0.6978269815444946, D Loss: 1.3835744857788086\n",
            "Epoch 3, Batch 261, G Loss: 0.6978241205215454, D Loss: 1.384225606918335\n",
            "Epoch 3, Batch 262, G Loss: 0.697837769985199, D Loss: 1.3831583261489868\n",
            "Epoch 3, Batch 263, G Loss: 0.6978505849838257, D Loss: 1.382805585861206\n",
            "Epoch 3, Batch 264, G Loss: 0.6978610157966614, D Loss: 1.3844292163848877\n",
            "Epoch 3, Batch 265, G Loss: 0.6978737711906433, D Loss: 1.383893609046936\n",
            "Epoch 3, Batch 266, G Loss: 0.6978715658187866, D Loss: 1.3825608491897583\n",
            "Epoch 3, Batch 267, G Loss: 0.6978943347930908, D Loss: 1.3840571641921997\n",
            "Epoch 3, Batch 268, G Loss: 0.6979028582572937, D Loss: 1.381035566329956\n",
            "Epoch 3, Batch 269, G Loss: 0.697928786277771, D Loss: 1.3801474571228027\n",
            "Epoch 3, Batch 270, G Loss: 0.6979256272315979, D Loss: 1.3812464475631714\n",
            "Epoch 3, Batch 271, G Loss: 0.6979095935821533, D Loss: 1.3789362907409668\n",
            "Epoch 3, Batch 272, G Loss: 0.6979427933692932, D Loss: 1.3837189674377441\n",
            "Epoch 3, Batch 273, G Loss: 0.69792640209198, D Loss: 1.3836842775344849\n",
            "Epoch 3, Batch 274, G Loss: 0.6979517936706543, D Loss: 1.382993221282959\n",
            "Epoch 3, Batch 275, G Loss: 0.6979655623435974, D Loss: 1.38517165184021\n",
            "Epoch 3, Batch 276, G Loss: 0.6979454159736633, D Loss: 1.3849883079528809\n",
            "Epoch 3, Batch 277, G Loss: 0.6979530453681946, D Loss: 1.384708046913147\n",
            "Epoch 3, Batch 278, G Loss: 0.6979608535766602, D Loss: 1.3847274780273438\n",
            "Epoch 3, Batch 279, G Loss: 0.6979644298553467, D Loss: 1.385361909866333\n",
            "Epoch 3, Batch 280, G Loss: 0.6979548931121826, D Loss: 1.3852732181549072\n",
            "Epoch 3, Batch 281, G Loss: 0.6979650259017944, D Loss: 1.3818111419677734\n",
            "Epoch 3, Batch 282, G Loss: 0.6979491710662842, D Loss: 1.3831599950790405\n",
            "Epoch 3, Batch 283, G Loss: 0.6979804635047913, D Loss: 1.3839497566223145\n",
            "Epoch 3, Batch 284, G Loss: 0.6979796886444092, D Loss: 1.382000207901001\n",
            "Epoch 3, Batch 285, G Loss: 0.6979820132255554, D Loss: 1.3815057277679443\n",
            "Epoch 3, Batch 286, G Loss: 0.6979944705963135, D Loss: 1.383086085319519\n",
            "Epoch 3, Batch 287, G Loss: 0.6979899406433105, D Loss: 1.3847297430038452\n",
            "Epoch 3, Batch 288, G Loss: 0.6979958415031433, D Loss: 1.3829257488250732\n",
            "Epoch 3, Batch 289, G Loss: 0.6980087161064148, D Loss: 1.3841748237609863\n",
            "Epoch 3, Batch 290, G Loss: 0.6980186104774475, D Loss: 1.3824188709259033\n",
            "Epoch 3, Batch 291, G Loss: 0.6980117559432983, D Loss: 1.3827632665634155\n",
            "Epoch 3, Batch 292, G Loss: 0.6980155110359192, D Loss: 1.3821649551391602\n",
            "Epoch 3, Batch 293, G Loss: 0.6980018615722656, D Loss: 1.3834607601165771\n",
            "Epoch 3, Batch 294, G Loss: 0.6979987621307373, D Loss: 1.3834081888198853\n",
            "Epoch 3, Batch 295, G Loss: 0.6980162858963013, D Loss: 1.383741855621338\n",
            "Epoch 3, Batch 296, G Loss: 0.6980158090591431, D Loss: 1.3823504447937012\n",
            "Epoch 3, Batch 297, G Loss: 0.6980081796646118, D Loss: 1.3834643363952637\n",
            "Epoch 3, Batch 298, G Loss: 0.6980024576187134, D Loss: 1.3856772184371948\n",
            "Epoch 3, Batch 299, G Loss: 0.6980109810829163, D Loss: 1.384504795074463\n",
            "Epoch 3, Batch 300, G Loss: 0.6979840993881226, D Loss: 1.3845443725585938\n",
            "Epoch 3, Batch 301, G Loss: 0.6980277895927429, D Loss: 1.3772430419921875\n",
            "Epoch 3, Batch 302, G Loss: 0.6980028748512268, D Loss: 1.3814256191253662\n",
            "Epoch 3, Batch 303, G Loss: 0.6980148553848267, D Loss: 1.3780680894851685\n",
            "Epoch 3, Batch 304, G Loss: 0.6980128288269043, D Loss: 1.3795064687728882\n",
            "Epoch 3, Batch 305, G Loss: 0.6980105042457581, D Loss: 1.3791515827178955\n",
            "Epoch 3, Batch 306, G Loss: 0.6980323195457458, D Loss: 1.3784773349761963\n",
            "Epoch 3, Batch 307, G Loss: 0.6980308890342712, D Loss: 1.379636287689209\n",
            "Epoch 3, Batch 308, G Loss: 0.6980192065238953, D Loss: 1.3799903392791748\n",
            "Epoch 3, Batch 309, G Loss: 0.6980444192886353, D Loss: 1.3792898654937744\n",
            "Epoch 3, Batch 310, G Loss: 0.6980354189872742, D Loss: 1.3795833587646484\n",
            "Epoch 3, Batch 311, G Loss: 0.6980458498001099, D Loss: 1.3810863494873047\n",
            "Epoch 3, Batch 312, G Loss: 0.6980382204055786, D Loss: 1.3790462017059326\n",
            "Epoch 3, Batch 313, G Loss: 0.6980352401733398, D Loss: 1.3802099227905273\n",
            "Epoch 3, Batch 314, G Loss: 0.6980690360069275, D Loss: 1.3820195198059082\n",
            "Epoch 3, Batch 315, G Loss: 0.6980675458908081, D Loss: 1.3810060024261475\n",
            "Epoch 3, Batch 316, G Loss: 0.6980715394020081, D Loss: 1.3821184635162354\n",
            "Epoch 3, Batch 317, G Loss: 0.698066234588623, D Loss: 1.3833158016204834\n",
            "Epoch 3, Batch 318, G Loss: 0.6980822682380676, D Loss: 1.3840141296386719\n",
            "Epoch 3, Batch 319, G Loss: 0.6980737447738647, D Loss: 1.3811466693878174\n",
            "Epoch 3, Batch 320, G Loss: 0.6980783939361572, D Loss: 1.3836456537246704\n",
            "Epoch 3, Batch 321, G Loss: 0.6980883479118347, D Loss: 1.3809046745300293\n",
            "Epoch 3, Batch 322, G Loss: 0.6980950236320496, D Loss: 1.3822357654571533\n",
            "Epoch 3, Batch 323, G Loss: 0.6981155872344971, D Loss: 1.3801147937774658\n",
            "Epoch 3, Batch 324, G Loss: 0.6981152892112732, D Loss: 1.3816328048706055\n",
            "Epoch 3, Batch 325, G Loss: 0.6981285214424133, D Loss: 1.3837993144989014\n",
            "Epoch 3, Batch 326, G Loss: 0.6981384754180908, D Loss: 1.3821165561676025\n",
            "Epoch 3, Batch 327, G Loss: 0.6981337666511536, D Loss: 1.38331937789917\n",
            "Epoch 3, Batch 328, G Loss: 0.6981540322303772, D Loss: 1.3802409172058105\n",
            "Epoch 3, Batch 329, G Loss: 0.6981737613677979, D Loss: 1.3833906650543213\n",
            "Epoch 3, Batch 330, G Loss: 0.6981724500656128, D Loss: 1.3859739303588867\n",
            "Epoch 3, Batch 331, G Loss: 0.6981881856918335, D Loss: 1.3836891651153564\n",
            "Epoch 3, Batch 332, G Loss: 0.6981878280639648, D Loss: 1.3831758499145508\n",
            "Epoch 3, Batch 333, G Loss: 0.6982010006904602, D Loss: 1.381378173828125\n",
            "Epoch 3, Batch 334, G Loss: 0.6981873512268066, D Loss: 1.3831062316894531\n",
            "Epoch 3, Batch 335, G Loss: 0.6982114315032959, D Loss: 1.3815569877624512\n",
            "Epoch 3, Batch 336, G Loss: 0.6982265114784241, D Loss: 1.3814151287078857\n",
            "Epoch 3, Batch 337, G Loss: 0.6982268691062927, D Loss: 1.3798259496688843\n",
            "Epoch 3, Batch 338, G Loss: 0.6982566118240356, D Loss: 1.3868016004562378\n",
            "Epoch 3, Batch 339, G Loss: 0.6982516050338745, D Loss: 1.3848989009857178\n",
            "Epoch 3, Batch 340, G Loss: 0.6982828378677368, D Loss: 1.3875148296356201\n",
            "Epoch 3, Batch 341, G Loss: 0.6982866525650024, D Loss: 1.3773508071899414\n",
            "Epoch 3, Batch 342, G Loss: 0.6982918381690979, D Loss: 1.3787431716918945\n",
            "Epoch 3, Batch 343, G Loss: 0.6982808709144592, D Loss: 1.3778053522109985\n",
            "Epoch 3, Batch 344, G Loss: 0.6983047723770142, D Loss: 1.3788814544677734\n",
            "Epoch 3, Batch 345, G Loss: 0.6983133554458618, D Loss: 1.3822417259216309\n",
            "Epoch 3, Batch 346, G Loss: 0.6983178853988647, D Loss: 1.3836618661880493\n",
            "Epoch 3, Batch 347, G Loss: 0.6983523368835449, D Loss: 1.3833038806915283\n",
            "Epoch 3, Batch 348, G Loss: 0.6983332633972168, D Loss: 1.382673978805542\n",
            "Epoch 3, Batch 349, G Loss: 0.6983323097229004, D Loss: 1.3845137357711792\n",
            "Epoch 3, Batch 350, G Loss: 0.6983457207679749, D Loss: 1.3829073905944824\n",
            "Epoch 3, Batch 351, G Loss: 0.6983741521835327, D Loss: 1.3831390142440796\n",
            "Epoch 3, Batch 352, G Loss: 0.6983553767204285, D Loss: 1.383943796157837\n",
            "Epoch 3, Batch 353, G Loss: 0.6983837485313416, D Loss: 1.3819538354873657\n",
            "Epoch 3, Batch 354, G Loss: 0.6983851194381714, D Loss: 1.3811776638031006\n",
            "Epoch 3, Batch 355, G Loss: 0.6983945369720459, D Loss: 1.3776943683624268\n",
            "Epoch 3, Batch 356, G Loss: 0.6984021067619324, D Loss: 1.3786239624023438\n",
            "Epoch 3, Batch 357, G Loss: 0.6984102725982666, D Loss: 1.381413459777832\n",
            "Epoch 3, Batch 358, G Loss: 0.6984031796455383, D Loss: 1.3800570964813232\n",
            "Epoch 3, Batch 359, G Loss: 0.6984472274780273, D Loss: 1.379370927810669\n",
            "Epoch 3, Batch 360, G Loss: 0.6984243392944336, D Loss: 1.3804244995117188\n",
            "Epoch 3, Batch 361, G Loss: 0.6984697580337524, D Loss: 1.3779468536376953\n",
            "Epoch 3, Batch 362, G Loss: 0.698448896408081, D Loss: 1.3811243772506714\n",
            "Epoch 3, Batch 363, G Loss: 0.6984589099884033, D Loss: 1.3826560974121094\n",
            "Epoch 3, Batch 364, G Loss: 0.6984903216362, D Loss: 1.382105827331543\n",
            "Epoch 3, Batch 365, G Loss: 0.6985023617744446, D Loss: 1.3815604448318481\n",
            "Epoch 3, Batch 366, G Loss: 0.6985079050064087, D Loss: 1.3816728591918945\n",
            "Epoch 3, Batch 367, G Loss: 0.6985347270965576, D Loss: 1.3831769227981567\n",
            "Epoch 3, Batch 368, G Loss: 0.6985263228416443, D Loss: 1.3840014934539795\n",
            "Epoch 3, Batch 369, G Loss: 0.6985330581665039, D Loss: 1.3844255208969116\n",
            "Epoch 3, Batch 370, G Loss: 0.6985672116279602, D Loss: 1.3860650062561035\n",
            "Epoch 3, Batch 371, G Loss: 0.6985474228858948, D Loss: 1.3857510089874268\n",
            "Epoch 3, Batch 372, G Loss: 0.6985692977905273, D Loss: 1.3821003437042236\n",
            "Epoch 3, Batch 373, G Loss: 0.6985633969306946, D Loss: 1.3823838233947754\n",
            "Epoch 3, Batch 374, G Loss: 0.6985796689987183, D Loss: 1.3838939666748047\n",
            "Epoch 3, Batch 375, G Loss: 0.6985989212989807, D Loss: 1.3828988075256348\n",
            "Epoch 3, Batch 376, G Loss: 0.6985994577407837, D Loss: 1.3824772834777832\n",
            "Epoch 3, Batch 377, G Loss: 0.6985937356948853, D Loss: 1.3825712203979492\n",
            "Epoch 3, Batch 378, G Loss: 0.6986083388328552, D Loss: 1.383978247642517\n",
            "Epoch 3, Batch 379, G Loss: 0.6986205577850342, D Loss: 1.3837406635284424\n",
            "Epoch 3, Batch 380, G Loss: 0.6986503005027771, D Loss: 1.3833481073379517\n",
            "Epoch 3, Batch 381, G Loss: 0.6986414790153503, D Loss: 1.385697841644287\n",
            "Epoch 3, Batch 382, G Loss: 0.6986315846443176, D Loss: 1.3808770179748535\n",
            "Epoch 3, Batch 383, G Loss: 0.6986225843429565, D Loss: 1.3823158740997314\n",
            "Epoch 3, Batch 384, G Loss: 0.6986633539199829, D Loss: 1.3841187953948975\n",
            "Epoch 3, Batch 385, G Loss: 0.6986635327339172, D Loss: 1.3821394443511963\n",
            "Epoch 3, Batch 386, G Loss: 0.6986582279205322, D Loss: 1.3807435035705566\n",
            "Epoch 3, Batch 387, G Loss: 0.6986668109893799, D Loss: 1.3828296661376953\n",
            "Epoch 3, Batch 388, G Loss: 0.6986697316169739, D Loss: 1.383507490158081\n",
            "Epoch 3, Batch 389, G Loss: 0.6986750960350037, D Loss: 1.3843238353729248\n",
            "Epoch 3, Batch 390, G Loss: 0.6986677050590515, D Loss: 1.3884680271148682\n",
            "Epoch 3, Batch 391, G Loss: 0.6986773610115051, D Loss: 1.38456130027771\n",
            "Epoch 3, Batch 392, G Loss: 0.6987037062644958, D Loss: 1.3851109743118286\n",
            "Epoch 3, Batch 393, G Loss: 0.6987046003341675, D Loss: 1.3846325874328613\n",
            "Epoch 3, Batch 394, G Loss: 0.6986910700798035, D Loss: 1.3840402364730835\n",
            "Epoch 3, Batch 395, G Loss: 0.6987118124961853, D Loss: 1.3849250078201294\n",
            "Epoch 3, Batch 396, G Loss: 0.6986820697784424, D Loss: 1.3761720657348633\n",
            "Epoch 3, Batch 397, G Loss: 0.6986798048019409, D Loss: 1.3781828880310059\n",
            "Epoch 3, Batch 398, G Loss: 0.698724627494812, D Loss: 1.3806450366973877\n",
            "Epoch 3, Batch 399, G Loss: 0.6987085342407227, D Loss: 1.3805017471313477\n",
            "Epoch 3, Batch 400, G Loss: 0.6987037062644958, D Loss: 1.385430097579956\n",
            "Epoch 3, Batch 401, G Loss: 0.6987013816833496, D Loss: 1.383339524269104\n",
            "Epoch 3, Batch 402, G Loss: 0.6987130045890808, D Loss: 1.3823859691619873\n",
            "Epoch 3, Batch 403, G Loss: 0.6987329721450806, D Loss: 1.3856422901153564\n",
            "Epoch 3, Batch 404, G Loss: 0.6987080574035645, D Loss: 1.3879382610321045\n",
            "Epoch 3, Batch 405, G Loss: 0.698704719543457, D Loss: 1.3879932165145874\n",
            "Epoch 3, Batch 406, G Loss: 0.698723554611206, D Loss: 1.3875956535339355\n",
            "Epoch 3, Batch 407, G Loss: 0.6987137198448181, D Loss: 1.3815655708312988\n",
            "Epoch 3, Batch 408, G Loss: 0.6986615061759949, D Loss: 1.3833789825439453\n",
            "Epoch 3, Batch 409, G Loss: 0.698699951171875, D Loss: 1.384575605392456\n",
            "Epoch 3, Batch 410, G Loss: 0.698688805103302, D Loss: 1.3836138248443604\n",
            "Epoch 3, Batch 411, G Loss: 0.698674738407135, D Loss: 1.3858017921447754\n",
            "Epoch 3, Batch 412, G Loss: 0.6986591815948486, D Loss: 1.3857574462890625\n",
            "Epoch 3, Batch 413, G Loss: 0.6986846327781677, D Loss: 1.385606050491333\n",
            "Epoch 3, Batch 414, G Loss: 0.6986755728721619, D Loss: 1.3866212368011475\n",
            "Epoch 3, Batch 415, G Loss: 0.6986801028251648, D Loss: 1.3872053623199463\n",
            "Epoch 3, Batch 416, G Loss: 0.6986685991287231, D Loss: 1.3875133991241455\n",
            "Epoch 3, Batch 417, G Loss: 0.6986443400382996, D Loss: 1.3835361003875732\n",
            "Epoch 3, Batch 418, G Loss: 0.6986247301101685, D Loss: 1.383556604385376\n",
            "Epoch 3, Batch 419, G Loss: 0.698645830154419, D Loss: 1.3822412490844727\n",
            "Epoch 3, Batch 420, G Loss: 0.6986455917358398, D Loss: 1.3828721046447754\n",
            "Epoch 3, Batch 421, G Loss: 0.6986081004142761, D Loss: 1.3847227096557617\n",
            "Epoch 3, Batch 422, G Loss: 0.6986051201820374, D Loss: 1.3861167430877686\n",
            "Epoch 3, Batch 423, G Loss: 0.6985808610916138, D Loss: 1.3851125240325928\n",
            "Epoch 3, Batch 424, G Loss: 0.6985857486724854, D Loss: 1.3805413246154785\n",
            "Epoch 3, Batch 425, G Loss: 0.6985933184623718, D Loss: 1.3813740015029907\n",
            "Epoch 3, Batch 426, G Loss: 0.6985517144203186, D Loss: 1.3825739622116089\n",
            "Epoch 3, Batch 427, G Loss: 0.6985417604446411, D Loss: 1.3823060989379883\n",
            "Epoch 3, Batch 428, G Loss: 0.698529839515686, D Loss: 1.3840231895446777\n",
            "Epoch 3, Batch 429, G Loss: 0.6985349059104919, D Loss: 1.382779598236084\n",
            "Epoch 3, Batch 430, G Loss: 0.6985276937484741, D Loss: 1.3860468864440918\n",
            "Epoch 3, Batch 431, G Loss: 0.6984909176826477, D Loss: 1.3860125541687012\n",
            "Epoch 3, Batch 432, G Loss: 0.6985056400299072, D Loss: 1.384507417678833\n",
            "Epoch 3, Batch 433, G Loss: 0.6985092163085938, D Loss: 1.3865110874176025\n",
            "Epoch 3, Batch 434, G Loss: 0.6984634399414062, D Loss: 1.386368989944458\n",
            "Epoch 3, Batch 435, G Loss: 0.6984593868255615, D Loss: 1.3875219821929932\n",
            "Epoch 3, Batch 436, G Loss: 0.6984423995018005, D Loss: 1.3850669860839844\n",
            "Epoch 3, Batch 437, G Loss: 0.6984029412269592, D Loss: 1.3812814950942993\n",
            "Epoch 3, Batch 438, G Loss: 0.6984075307846069, D Loss: 1.3823994398117065\n",
            "Epoch 3, Batch 439, G Loss: 0.6984027028083801, D Loss: 1.3820667266845703\n",
            "Epoch 3, Batch 440, G Loss: 0.69837486743927, D Loss: 1.3819994926452637\n",
            "Epoch 3, Batch 441, G Loss: 0.6983768939971924, D Loss: 1.3807085752487183\n",
            "Epoch 3, Batch 442, G Loss: 0.6983376145362854, D Loss: 1.3813621997833252\n",
            "Epoch 3, Batch 443, G Loss: 0.6983358263969421, D Loss: 1.3824644088745117\n",
            "Epoch 3, Batch 444, G Loss: 0.6983491778373718, D Loss: 1.3804311752319336\n",
            "Epoch 3, Batch 445, G Loss: 0.6983160972595215, D Loss: 1.3809605836868286\n",
            "Epoch 3, Batch 446, G Loss: 0.6983321905136108, D Loss: 1.3822662830352783\n",
            "Epoch 3, Batch 447, G Loss: 0.698291540145874, D Loss: 1.3819947242736816\n",
            "Epoch 3, Batch 448, G Loss: 0.698297917842865, D Loss: 1.3839998245239258\n",
            "Epoch 3, Batch 449, G Loss: 0.6982970833778381, D Loss: 1.3844943046569824\n",
            "Epoch 3, Batch 450, G Loss: 0.698270857334137, D Loss: 1.3840038776397705\n",
            "Epoch 3, Batch 451, G Loss: 0.6982329487800598, D Loss: 1.3847618103027344\n",
            "Epoch 3, Batch 452, G Loss: 0.6982361078262329, D Loss: 1.3815068006515503\n",
            "Epoch 3, Batch 453, G Loss: 0.6982226967811584, D Loss: 1.3831324577331543\n",
            "Epoch 3, Batch 454, G Loss: 0.6982141733169556, D Loss: 1.3830832242965698\n",
            "Epoch 3, Batch 455, G Loss: 0.6981967091560364, D Loss: 1.387725591659546\n",
            "Epoch 3, Batch 456, G Loss: 0.6981669068336487, D Loss: 1.3869974613189697\n",
            "Epoch 3, Batch 457, G Loss: 0.6981775760650635, D Loss: 1.386364459991455\n",
            "Epoch 3, Batch 458, G Loss: 0.6981620192527771, D Loss: 1.3806121349334717\n",
            "Epoch 3, Batch 459, G Loss: 0.6981580853462219, D Loss: 1.381007432937622\n",
            "Epoch 3, Batch 460, G Loss: 0.6981503367424011, D Loss: 1.380631923675537\n",
            "Epoch 3, Batch 461, G Loss: 0.6981453895568848, D Loss: 1.3816184997558594\n",
            "Epoch 3, Batch 462, G Loss: 0.6981223225593567, D Loss: 1.3862526416778564\n",
            "Epoch 3, Batch 463, G Loss: 0.6981076598167419, D Loss: 1.3871352672576904\n",
            "Epoch 3, Batch 464, G Loss: 0.6981279253959656, D Loss: 1.3859434127807617\n",
            "Epoch 3, Batch 465, G Loss: 0.6980830430984497, D Loss: 1.3879982233047485\n",
            "Epoch 3, Batch 466, G Loss: 0.6980791687965393, D Loss: 1.388066291809082\n",
            "Epoch 3, Batch 467, G Loss: 0.6980478763580322, D Loss: 1.3880590200424194\n",
            "Epoch 3, Batch 468, G Loss: 0.6980552673339844, D Loss: 1.3877028226852417\n",
            "Epoch 3, Batch 469, G Loss: 0.6980681419372559, D Loss: 1.3896123170852661\n",
            "Epoch 3, Batch 470, G Loss: 0.6980584859848022, D Loss: 1.3850915431976318\n",
            "Epoch 3, Batch 471, G Loss: 0.6980099678039551, D Loss: 1.3859550952911377\n",
            "Epoch 3, Batch 472, G Loss: 0.6980093121528625, D Loss: 1.3860185146331787\n",
            "Epoch 3, Batch 473, G Loss: 0.6980102062225342, D Loss: 1.3863270282745361\n",
            "Epoch 3, Batch 474, G Loss: 0.6980438232421875, D Loss: 1.3862706422805786\n",
            "Epoch 3, Batch 475, G Loss: 0.6979660987854004, D Loss: 1.3841428756713867\n",
            "Epoch 3, Batch 476, G Loss: 0.6979751586914062, D Loss: 1.3837549686431885\n",
            "Epoch 3, Batch 477, G Loss: 0.697978138923645, D Loss: 1.3842318058013916\n",
            "Epoch 3, Batch 478, G Loss: 0.6979560256004333, D Loss: 1.3859206438064575\n",
            "Epoch 3, Batch 479, G Loss: 0.6979928016662598, D Loss: 1.390751600265503\n",
            "Epoch 3, Batch 480, G Loss: 0.6979538798332214, D Loss: 1.3893859386444092\n",
            "Epoch 3, Batch 481, G Loss: 0.6979320049285889, D Loss: 1.389596939086914\n",
            "Epoch 3, Batch 482, G Loss: 0.697892427444458, D Loss: 1.3849098682403564\n",
            "Epoch 3, Batch 483, G Loss: 0.6979239583015442, D Loss: 1.3857333660125732\n",
            "Epoch 3, Batch 484, G Loss: 0.6978866457939148, D Loss: 1.3872971534729004\n",
            "Epoch 3, Batch 485, G Loss: 0.6979021430015564, D Loss: 1.3869738578796387\n",
            "Epoch 3, Batch 486, G Loss: 0.6978691220283508, D Loss: 1.3866779804229736\n",
            "Epoch 3, Batch 487, G Loss: 0.6978110671043396, D Loss: 1.3872126340866089\n",
            "Epoch 3, Batch 488, G Loss: 0.6978242993354797, D Loss: 1.3871760368347168\n",
            "Epoch 3, Batch 489, G Loss: 0.6978297233581543, D Loss: 1.3870491981506348\n",
            "Epoch 3, Batch 490, G Loss: 0.6978806257247925, D Loss: 1.3870372772216797\n",
            "Epoch 3, Batch 491, G Loss: 0.6977922320365906, D Loss: 1.387890100479126\n",
            "Epoch 3, Batch 492, G Loss: 0.697810173034668, D Loss: 1.386884331703186\n",
            "Epoch 3, Batch 493, G Loss: 0.6977778077125549, D Loss: 1.3889203071594238\n",
            "Epoch 3, Batch 494, G Loss: 0.6977906227111816, D Loss: 1.3887085914611816\n",
            "Epoch 3, Batch 495, G Loss: 0.6977455019950867, D Loss: 1.3886626958847046\n",
            "Epoch 3, Batch 496, G Loss: 0.6977458000183105, D Loss: 1.3881936073303223\n",
            "Epoch 3, Batch 497, G Loss: 0.6977630257606506, D Loss: 1.3870375156402588\n",
            "Epoch 3, Batch 498, G Loss: 0.697754979133606, D Loss: 1.3867847919464111\n",
            "Epoch 3, Batch 499, G Loss: 0.6977239847183228, D Loss: 1.387703537940979\n",
            "Epoch 3, Batch 500, G Loss: 0.6976771354675293, D Loss: 1.385727882385254\n",
            "Epoch 3, Batch 501, G Loss: 0.6976935863494873, D Loss: 1.384458303451538\n",
            "Epoch 3, Batch 502, G Loss: 0.6977012157440186, D Loss: 1.384553074836731\n",
            "Epoch 3, Batch 503, G Loss: 0.6976938247680664, D Loss: 1.3856010437011719\n",
            "Epoch 3, Batch 504, G Loss: 0.6976340413093567, D Loss: 1.3865764141082764\n",
            "Epoch 3, Batch 505, G Loss: 0.6976689100265503, D Loss: 1.3838942050933838\n",
            "Epoch 3, Batch 506, G Loss: 0.697615385055542, D Loss: 1.3841536045074463\n",
            "Epoch 3, Batch 507, G Loss: 0.697624683380127, D Loss: 1.3822243213653564\n",
            "Epoch 3, Batch 508, G Loss: 0.6975961923599243, D Loss: 1.384035348892212\n",
            "Epoch 3, Batch 509, G Loss: 0.697577953338623, D Loss: 1.3869589567184448\n",
            "Epoch 3, Batch 510, G Loss: 0.6976039409637451, D Loss: 1.3864166736602783\n",
            "Epoch 3, Batch 511, G Loss: 0.697563886642456, D Loss: 1.3873871564865112\n",
            "Epoch 3, Batch 512, G Loss: 0.6975259780883789, D Loss: 1.3890639543533325\n",
            "Epoch 3, Batch 513, G Loss: 0.697515606880188, D Loss: 1.3872066736221313\n",
            "Epoch 3, Batch 514, G Loss: 0.6975142955780029, D Loss: 1.387408971786499\n",
            "Epoch 3, Batch 515, G Loss: 0.6974799633026123, D Loss: 1.3854840993881226\n",
            "Epoch 3, Batch 516, G Loss: 0.6975023746490479, D Loss: 1.3841577768325806\n",
            "Epoch 3, Batch 517, G Loss: 0.6974814534187317, D Loss: 1.3838703632354736\n",
            "Epoch 3, Batch 518, G Loss: 0.6974552869796753, D Loss: 1.385866403579712\n",
            "Epoch 3, Batch 519, G Loss: 0.6974380016326904, D Loss: 1.3851690292358398\n",
            "Epoch 3, Batch 520, G Loss: 0.6974367499351501, D Loss: 1.3854624032974243\n",
            "Epoch 3, Batch 521, G Loss: 0.697401225566864, D Loss: 1.3857340812683105\n",
            "Epoch 3, Batch 522, G Loss: 0.6973328590393066, D Loss: 1.3875970840454102\n",
            "Epoch 3, Batch 523, G Loss: 0.6973589658737183, D Loss: 1.3884440660476685\n",
            "Epoch 3, Batch 524, G Loss: 0.6973403692245483, D Loss: 1.388152003288269\n",
            "Epoch 3, Batch 525, G Loss: 0.6973515748977661, D Loss: 1.3894896507263184\n",
            "Epoch 3, Batch 526, G Loss: 0.6973333358764648, D Loss: 1.3894708156585693\n",
            "Epoch 3, Batch 527, G Loss: 0.6972908973693848, D Loss: 1.3876898288726807\n",
            "Epoch 3, Batch 528, G Loss: 0.6973171830177307, D Loss: 1.3854246139526367\n",
            "Epoch 3, Batch 529, G Loss: 0.6972889304161072, D Loss: 1.3836365938186646\n",
            "Epoch 3, Batch 530, G Loss: 0.697297990322113, D Loss: 1.38572359085083\n",
            "Epoch 3, Batch 531, G Loss: 0.6972959637641907, D Loss: 1.3865201473236084\n",
            "Epoch 3, Batch 532, G Loss: 0.6972494125366211, D Loss: 1.389620304107666\n",
            "Epoch 3, Batch 533, G Loss: 0.6972575187683105, D Loss: 1.386930227279663\n",
            "Epoch 3, Batch 534, G Loss: 0.6972622275352478, D Loss: 1.386375069618225\n",
            "Epoch 3, Batch 535, G Loss: 0.6972604990005493, D Loss: 1.387590765953064\n",
            "Epoch 3, Batch 536, G Loss: 0.6972726583480835, D Loss: 1.388360619544983\n",
            "Epoch 3, Batch 537, G Loss: 0.6972001791000366, D Loss: 1.3860831260681152\n",
            "Epoch 3, Batch 538, G Loss: 0.697224497795105, D Loss: 1.3852601051330566\n",
            "Epoch 3, Batch 539, G Loss: 0.6972534656524658, D Loss: 1.385960578918457\n",
            "Epoch 3, Batch 540, G Loss: 0.6971961259841919, D Loss: 1.3859084844589233\n",
            "Epoch 3, Batch 541, G Loss: 0.697160542011261, D Loss: 1.3860549926757812\n",
            "Epoch 3, Batch 542, G Loss: 0.6971821784973145, D Loss: 1.3864223957061768\n",
            "Epoch 3, Batch 543, G Loss: 0.6971778273582458, D Loss: 1.3862595558166504\n",
            "Epoch 3, Batch 544, G Loss: 0.6971703171730042, D Loss: 1.386202096939087\n",
            "Epoch 3, Batch 545, G Loss: 0.6971582174301147, D Loss: 1.3863881826400757\n",
            "Epoch 3, Batch 546, G Loss: 0.6971220970153809, D Loss: 1.3866503238677979\n",
            "Epoch 3, Batch 547, G Loss: 0.6971555352210999, D Loss: 1.3865875005722046\n",
            "Epoch 3, Batch 548, G Loss: 0.6971412301063538, D Loss: 1.3846182823181152\n",
            "Epoch 3, Batch 549, G Loss: 0.6971173882484436, D Loss: 1.3868297338485718\n",
            "Epoch 3, Batch 550, G Loss: 0.6970642805099487, D Loss: 1.3887619972229004\n",
            "Epoch 3, Batch 551, G Loss: 0.6970863938331604, D Loss: 1.3883607387542725\n",
            "Epoch 3, Batch 552, G Loss: 0.6970772743225098, D Loss: 1.3884389400482178\n",
            "Epoch 3, Batch 553, G Loss: 0.6970916986465454, D Loss: 1.386199951171875\n",
            "Epoch 3, Batch 554, G Loss: 0.6970718502998352, D Loss: 1.3857457637786865\n",
            "Epoch 3, Batch 555, G Loss: 0.6970484852790833, D Loss: 1.3851735591888428\n",
            "Epoch 3, Batch 556, G Loss: 0.6970183849334717, D Loss: 1.3876291513442993\n",
            "Epoch 3, Batch 557, G Loss: 0.6970314979553223, D Loss: 1.388185739517212\n",
            "Epoch 3, Batch 558, G Loss: 0.6970229148864746, D Loss: 1.3863571882247925\n",
            "Epoch 3, Batch 559, G Loss: 0.697035014629364, D Loss: 1.3874229192733765\n",
            "Epoch 3, Batch 560, G Loss: 0.6970093250274658, D Loss: 1.387224793434143\n",
            "Epoch 3, Batch 561, G Loss: 0.6969810128211975, D Loss: 1.3887653350830078\n",
            "Epoch 3, Batch 562, G Loss: 0.6970000267028809, D Loss: 1.3897868394851685\n",
            "Epoch 3, Batch 563, G Loss: 0.6970229148864746, D Loss: 1.3899176120758057\n",
            "Epoch 3, Batch 564, G Loss: 0.6969627737998962, D Loss: 1.3913753032684326\n",
            "Epoch 3, Batch 565, G Loss: 0.6969813704490662, D Loss: 1.3861594200134277\n",
            "Epoch 3, Batch 566, G Loss: 0.6969950199127197, D Loss: 1.3819451332092285\n",
            "Epoch 3, Batch 567, G Loss: 0.6969492435455322, D Loss: 1.3844068050384521\n",
            "Epoch 3, Batch 568, G Loss: 0.696973979473114, D Loss: 1.3838633298873901\n",
            "Epoch 3, Batch 569, G Loss: 0.696989893913269, D Loss: 1.3878777027130127\n",
            "Epoch 3, Batch 570, G Loss: 0.6969550848007202, D Loss: 1.3871158361434937\n",
            "Epoch 3, Batch 571, G Loss: 0.6969407796859741, D Loss: 1.388127088546753\n",
            "Epoch 3, Batch 572, G Loss: 0.6969537734985352, D Loss: 1.3825187683105469\n",
            "Epoch 3, Batch 573, G Loss: 0.696928083896637, D Loss: 1.3837003707885742\n",
            "Epoch 3, Batch 574, G Loss: 0.696891188621521, D Loss: 1.3821736574172974\n",
            "Epoch 3, Batch 575, G Loss: 0.6968560218811035, D Loss: 1.3831253051757812\n",
            "Epoch 3, Batch 576, G Loss: 0.6968970894813538, D Loss: 1.3860946893692017\n",
            "Epoch 3, Batch 577, G Loss: 0.6968483328819275, D Loss: 1.386117696762085\n",
            "Epoch 3, Batch 578, G Loss: 0.6968361735343933, D Loss: 1.3876030445098877\n",
            "Epoch 3, Batch 579, G Loss: 0.6968197226524353, D Loss: 1.3882808685302734\n",
            "Epoch 3, Batch 580, G Loss: 0.6967880129814148, D Loss: 1.3839869499206543\n",
            "Epoch 3, Batch 581, G Loss: 0.696771502494812, D Loss: 1.3847107887268066\n",
            "Epoch 3, Batch 582, G Loss: 0.6967681646347046, D Loss: 1.384329915046692\n",
            "Epoch 3, Batch 583, G Loss: 0.6967805624008179, D Loss: 1.3842706680297852\n",
            "Epoch 3, Batch 584, G Loss: 0.6967682838439941, D Loss: 1.386544942855835\n",
            "Epoch 3, Batch 585, G Loss: 0.6967198848724365, D Loss: 1.3870645761489868\n",
            "Epoch 3, Batch 586, G Loss: 0.6966968178749084, D Loss: 1.3879196643829346\n",
            "Epoch 3, Batch 587, G Loss: 0.6967049241065979, D Loss: 1.3888742923736572\n",
            "Epoch 3, Batch 588, G Loss: 0.6966677904129028, D Loss: 1.3890197277069092\n",
            "Epoch 3, Batch 589, G Loss: 0.6966823935508728, D Loss: 1.3894457817077637\n",
            "Epoch 3, Batch 590, G Loss: 0.6966550946235657, D Loss: 1.3858845233917236\n",
            "Epoch 3, Batch 591, G Loss: 0.6967025399208069, D Loss: 1.3856532573699951\n",
            "Epoch 3, Batch 592, G Loss: 0.6966466903686523, D Loss: 1.3865480422973633\n",
            "Epoch 3, Batch 593, G Loss: 0.696657121181488, D Loss: 1.3877027034759521\n",
            "Epoch 3, Batch 594, G Loss: 0.6966753602027893, D Loss: 1.3880470991134644\n",
            "Epoch 3, Batch 595, G Loss: 0.696672797203064, D Loss: 1.389657735824585\n",
            "Epoch 3, Batch 596, G Loss: 0.6966392397880554, D Loss: 1.3863548040390015\n",
            "Epoch 3, Batch 597, G Loss: 0.6966840028762817, D Loss: 1.386244535446167\n",
            "Epoch 3, Batch 598, G Loss: 0.6966372132301331, D Loss: 1.3872685432434082\n",
            "Epoch 3, Batch 599, G Loss: 0.6966460943222046, D Loss: 1.3877891302108765\n",
            "Epoch 3, Batch 600, G Loss: 0.6966652870178223, D Loss: 1.3886739015579224\n",
            "Epoch 3, Batch 601, G Loss: 0.696636438369751, D Loss: 1.3883442878723145\n",
            "Epoch 3, Batch 602, G Loss: 0.6966468691825867, D Loss: 1.3890949487686157\n",
            "Epoch 3, Batch 603, G Loss: 0.6966646313667297, D Loss: 1.3874843120574951\n",
            "Epoch 3, Batch 604, G Loss: 0.6966648697853088, D Loss: 1.385019063949585\n",
            "Epoch 3, Batch 605, G Loss: 0.696660578250885, D Loss: 1.3878815174102783\n",
            "Epoch 3, Batch 606, G Loss: 0.6966772079467773, D Loss: 1.3874750137329102\n",
            "Epoch 3, Batch 607, G Loss: 0.6966526508331299, D Loss: 1.3873939514160156\n",
            "Epoch 3, Batch 608, G Loss: 0.6966573596000671, D Loss: 1.3865876197814941\n",
            "Epoch 3, Batch 609, G Loss: 0.6966760754585266, D Loss: 1.388056993484497\n",
            "Epoch 3, Batch 610, G Loss: 0.6966825127601624, D Loss: 1.388848900794983\n",
            "Epoch 3, Batch 611, G Loss: 0.6966574788093567, D Loss: 1.386745810508728\n",
            "Epoch 3, Batch 612, G Loss: 0.6966719627380371, D Loss: 1.3852729797363281\n",
            "Epoch 3, Batch 613, G Loss: 0.696632981300354, D Loss: 1.3854634761810303\n",
            "Epoch 3, Batch 614, G Loss: 0.696643054485321, D Loss: 1.3861150741577148\n",
            "Epoch 3, Batch 615, G Loss: 0.6966619491577148, D Loss: 1.3865503072738647\n",
            "Epoch 3, Batch 616, G Loss: 0.6966397166252136, D Loss: 1.3863515853881836\n",
            "Epoch 3, Batch 617, G Loss: 0.6966260075569153, D Loss: 1.385775089263916\n",
            "Epoch 3, Batch 618, G Loss: 0.6966484189033508, D Loss: 1.3871240615844727\n",
            "Epoch 3, Batch 619, G Loss: 0.6966152787208557, D Loss: 1.386345624923706\n",
            "Epoch 3, Batch 620, G Loss: 0.6966003179550171, D Loss: 1.386361837387085\n",
            "Epoch 3, Batch 621, G Loss: 0.6965789794921875, D Loss: 1.3882426023483276\n",
            "Epoch 3, Batch 622, G Loss: 0.6965769529342651, D Loss: 1.390843152999878\n",
            "Epoch 3, Batch 623, G Loss: 0.6965906620025635, D Loss: 1.3884305953979492\n",
            "Epoch 3, Batch 624, G Loss: 0.6965909600257874, D Loss: 1.3894658088684082\n",
            "Epoch 3, Batch 625, G Loss: 0.6965628862380981, D Loss: 1.3870106935501099\n",
            "Epoch 3, Batch 626, G Loss: 0.6965561509132385, D Loss: 1.3875460624694824\n",
            "Epoch 3, Batch 627, G Loss: 0.6965735554695129, D Loss: 1.3869329690933228\n",
            "Epoch 3, Batch 628, G Loss: 0.6966128349304199, D Loss: 1.3868297338485718\n",
            "Epoch 3, Batch 629, G Loss: 0.6965663433074951, D Loss: 1.3874984979629517\n",
            "Epoch 3, Batch 630, G Loss: 0.696567714214325, D Loss: 1.3887008428573608\n",
            "Epoch 3, Batch 631, G Loss: 0.6965687274932861, D Loss: 1.388715386390686\n",
            "Epoch 3, Batch 632, G Loss: 0.6965634822845459, D Loss: 1.3858134746551514\n",
            "Epoch 3, Batch 633, G Loss: 0.6965654492378235, D Loss: 1.3853002786636353\n",
            "Epoch 3, Batch 634, G Loss: 0.6965560913085938, D Loss: 1.3857722282409668\n",
            "Epoch 3, Batch 635, G Loss: 0.6965556740760803, D Loss: 1.3868567943572998\n",
            "Epoch 3, Batch 636, G Loss: 0.6965285539627075, D Loss: 1.3883520364761353\n",
            "Epoch 3, Batch 637, G Loss: 0.6964917778968811, D Loss: 1.388149619102478\n",
            "Epoch 3, Batch 638, G Loss: 0.696516215801239, D Loss: 1.389103651046753\n",
            "Epoch 3, Batch 639, G Loss: 0.6965048909187317, D Loss: 1.3874902725219727\n",
            "Epoch 3, Batch 640, G Loss: 0.6965011358261108, D Loss: 1.3886890411376953\n",
            "Epoch 3, Batch 641, G Loss: 0.6964880228042603, D Loss: 1.3883187770843506\n",
            "Epoch 3, Batch 642, G Loss: 0.6964919567108154, D Loss: 1.388232946395874\n",
            "Epoch 3, Batch 643, G Loss: 0.6964810490608215, D Loss: 1.3880667686462402\n",
            "Epoch 3, Batch 644, G Loss: 0.6965212821960449, D Loss: 1.3869212865829468\n",
            "Epoch 3, Batch 645, G Loss: 0.6964775919914246, D Loss: 1.3883090019226074\n",
            "Epoch 3, Batch 646, G Loss: 0.6964719891548157, D Loss: 1.3869736194610596\n",
            "Epoch 3, Batch 647, G Loss: 0.6964649558067322, D Loss: 1.386871099472046\n",
            "Epoch 3, Batch 648, G Loss: 0.6964914202690125, D Loss: 1.3873255252838135\n",
            "Epoch 3, Batch 649, G Loss: 0.6964706182479858, D Loss: 1.3888769149780273\n",
            "Epoch 3, Batch 650, G Loss: 0.6964526772499084, D Loss: 1.3869348764419556\n",
            "Epoch 3, Batch 651, G Loss: 0.6964500546455383, D Loss: 1.3880021572113037\n",
            "Epoch 3, Batch 652, G Loss: 0.6964503526687622, D Loss: 1.39019775390625\n",
            "Epoch 3, Batch 653, G Loss: 0.6964179277420044, D Loss: 1.390832781791687\n",
            "Epoch 3, Batch 654, G Loss: 0.6964365839958191, D Loss: 1.3891836404800415\n",
            "Epoch 3, Batch 655, G Loss: 0.696441650390625, D Loss: 1.3868430852890015\n",
            "Epoch 3, Batch 656, G Loss: 0.6964495778083801, D Loss: 1.3866840600967407\n",
            "Epoch 3, Batch 657, G Loss: 0.6964706778526306, D Loss: 1.3858433961868286\n",
            "Epoch 3, Batch 658, G Loss: 0.6964553594589233, D Loss: 1.3859281539916992\n",
            "Epoch 3, Batch 659, G Loss: 0.6964051127433777, D Loss: 1.3866585493087769\n",
            "Epoch 3, Batch 660, G Loss: 0.696442186832428, D Loss: 1.386803388595581\n",
            "Epoch 3, Batch 661, G Loss: 0.6963862776756287, D Loss: 1.3866279125213623\n",
            "Epoch 3, Batch 662, G Loss: 0.6963872909545898, D Loss: 1.387799859046936\n",
            "Epoch 3, Batch 663, G Loss: 0.6963871717453003, D Loss: 1.3873348236083984\n",
            "Epoch 3, Batch 664, G Loss: 0.6963503956794739, D Loss: 1.3883106708526611\n",
            "Epoch 3, Batch 665, G Loss: 0.6963689923286438, D Loss: 1.388626217842102\n",
            "Epoch 3, Batch 666, G Loss: 0.6963724493980408, D Loss: 1.3853403329849243\n",
            "Epoch 3, Batch 667, G Loss: 0.6963732838630676, D Loss: 1.384897232055664\n",
            "Epoch 3, Batch 668, G Loss: 0.6963256597518921, D Loss: 1.3867008686065674\n",
            "Epoch 3, Batch 669, G Loss: 0.6963129639625549, D Loss: 1.385998249053955\n",
            "Epoch 3, Batch 670, G Loss: 0.6962801814079285, D Loss: 1.3865656852722168\n",
            "Epoch 3, Batch 671, G Loss: 0.6962313055992126, D Loss: 1.3876749277114868\n",
            "Epoch 3, Batch 672, G Loss: 0.6962275505065918, D Loss: 1.3891074657440186\n",
            "Epoch 3, Batch 673, G Loss: 0.6961965560913086, D Loss: 1.3886168003082275\n",
            "Epoch 3, Batch 674, G Loss: 0.6961881518363953, D Loss: 1.3888309001922607\n",
            "Epoch 3, Batch 675, G Loss: 0.6961667537689209, D Loss: 1.3864173889160156\n",
            "Epoch 3, Batch 676, G Loss: 0.6961904168128967, D Loss: 1.3867155313491821\n",
            "Epoch 3, Batch 677, G Loss: 0.6961604356765747, D Loss: 1.3873741626739502\n",
            "Epoch 3, Batch 678, G Loss: 0.6961389780044556, D Loss: 1.3873018026351929\n",
            "Epoch 3, Batch 679, G Loss: 0.6961609125137329, D Loss: 1.3870694637298584\n",
            "Epoch 3, Batch 680, G Loss: 0.6961166262626648, D Loss: 1.3868826627731323\n",
            "Epoch 3, Batch 681, G Loss: 0.6961031556129456, D Loss: 1.3879239559173584\n",
            "Epoch 3, Batch 682, G Loss: 0.6961074471473694, D Loss: 1.38694429397583\n",
            "Epoch 3, Batch 683, G Loss: 0.6960650086402893, D Loss: 1.3872301578521729\n",
            "Epoch 3, Batch 684, G Loss: 0.6960375308990479, D Loss: 1.3872933387756348\n",
            "Epoch 3, Batch 685, G Loss: 0.6960510015487671, D Loss: 1.3879034519195557\n",
            "Epoch 3, Batch 686, G Loss: 0.6960240602493286, D Loss: 1.3882339000701904\n",
            "Epoch 3, Batch 687, G Loss: 0.6960262656211853, D Loss: 1.3885279893875122\n",
            "Epoch 3, Batch 688, G Loss: 0.6960100531578064, D Loss: 1.38898503780365\n",
            "Epoch 3, Batch 689, G Loss: 0.6959793567657471, D Loss: 1.3873628377914429\n",
            "Epoch 3, Batch 690, G Loss: 0.696007251739502, D Loss: 1.38828706741333\n",
            "Epoch 3, Batch 691, G Loss: 0.6960304379463196, D Loss: 1.387739658355713\n",
            "Epoch 3, Batch 692, G Loss: 0.6960164904594421, D Loss: 1.387230396270752\n",
            "Epoch 3, Batch 693, G Loss: 0.696027398109436, D Loss: 1.3871073722839355\n",
            "Epoch 3, Batch 694, G Loss: 0.6960189342498779, D Loss: 1.3880873918533325\n",
            "Epoch 3, Batch 695, G Loss: 0.6960110068321228, D Loss: 1.3870294094085693\n",
            "Epoch 3, Batch 696, G Loss: 0.6959947943687439, D Loss: 1.3876861333847046\n",
            "Epoch 3, Batch 697, G Loss: 0.6959853172302246, D Loss: 1.3875563144683838\n",
            "Epoch 3, Batch 698, G Loss: 0.6959781050682068, D Loss: 1.3876811265945435\n",
            "Epoch 3, Batch 699, G Loss: 0.6959930658340454, D Loss: 1.3879952430725098\n",
            "Epoch 3, Batch 700, G Loss: 0.6959828734397888, D Loss: 1.3878594636917114\n",
            "Epoch 3, Batch 701, G Loss: 0.6959953904151917, D Loss: 1.3883261680603027\n",
            "Epoch 3, Batch 702, G Loss: 0.6960079669952393, D Loss: 1.3874790668487549\n",
            "Epoch 3, Batch 703, G Loss: 0.6960108280181885, D Loss: 1.3868131637573242\n",
            "Epoch 3, Batch 704, G Loss: 0.6959648132324219, D Loss: 1.3872570991516113\n",
            "Epoch 3, Batch 705, G Loss: 0.6959895491600037, D Loss: 1.3868391513824463\n",
            "Epoch 3, Batch 706, G Loss: 0.6959802508354187, D Loss: 1.3871729373931885\n",
            "Epoch 3, Batch 707, G Loss: 0.6959527730941772, D Loss: 1.387251853942871\n",
            "Epoch 3, Batch 708, G Loss: 0.6959690451622009, D Loss: 1.3869681358337402\n",
            "Epoch 3, Batch 709, G Loss: 0.6959360241889954, D Loss: 1.3875796794891357\n",
            "Epoch 3, Batch 710, G Loss: 0.6959521770477295, D Loss: 1.3877564668655396\n",
            "Epoch 3, Batch 711, G Loss: 0.6959425210952759, D Loss: 1.3885453939437866\n",
            "Epoch 3, Batch 712, G Loss: 0.6959323287010193, D Loss: 1.3872406482696533\n",
            "Epoch 3, Batch 713, G Loss: 0.6958967447280884, D Loss: 1.3877050876617432\n",
            "Epoch 3, Batch 714, G Loss: 0.6959119439125061, D Loss: 1.387526512145996\n",
            "Epoch 3, Batch 715, G Loss: 0.6958956122398376, D Loss: 1.3871946334838867\n",
            "Epoch 3, Batch 716, G Loss: 0.6958814263343811, D Loss: 1.3874213695526123\n",
            "Epoch 3, Batch 717, G Loss: 0.6959035992622375, D Loss: 1.3872218132019043\n",
            "Epoch 3, Batch 718, G Loss: 0.6958946585655212, D Loss: 1.3882668018341064\n",
            "Epoch 3, Batch 719, G Loss: 0.695900559425354, D Loss: 1.3890578746795654\n",
            "Epoch 3, Batch 720, G Loss: 0.6958910226821899, D Loss: 1.3884649276733398\n",
            "Epoch 3, Batch 721, G Loss: 0.6958832740783691, D Loss: 1.3885406255722046\n",
            "Epoch 3, Batch 722, G Loss: 0.6958941221237183, D Loss: 1.3878802061080933\n",
            "Epoch 3, Batch 723, G Loss: 0.6959106922149658, D Loss: 1.387570858001709\n",
            "Epoch 3, Batch 724, G Loss: 0.6959006786346436, D Loss: 1.3877203464508057\n",
            "Epoch 3, Batch 725, G Loss: 0.6958921551704407, D Loss: 1.3866941928863525\n",
            "Epoch 3, Batch 726, G Loss: 0.6959154605865479, D Loss: 1.387276291847229\n",
            "Epoch 3, Batch 727, G Loss: 0.6959347128868103, D Loss: 1.387317180633545\n",
            "Epoch 3, Batch 728, G Loss: 0.6959185600280762, D Loss: 1.387175440788269\n",
            "Epoch 3, Batch 729, G Loss: 0.6959285140037537, D Loss: 1.3868725299835205\n",
            "Epoch 3, Batch 730, G Loss: 0.6959094405174255, D Loss: 1.3860925436019897\n",
            "Epoch 3, Batch 731, G Loss: 0.695894718170166, D Loss: 1.3866722583770752\n",
            "Epoch 3, Batch 732, G Loss: 0.6958943605422974, D Loss: 1.3862860202789307\n",
            "Epoch 3, Batch 733, G Loss: 0.6958592534065247, D Loss: 1.3864924907684326\n",
            "Epoch 3, Batch 734, G Loss: 0.6958298683166504, D Loss: 1.386803150177002\n",
            "Epoch 3, Batch 735, G Loss: 0.6958158612251282, D Loss: 1.38700532913208\n",
            "Epoch 3, Batch 736, G Loss: 0.6957879066467285, D Loss: 1.3869346380233765\n",
            "Epoch 3, Batch 737, G Loss: 0.6957741379737854, D Loss: 1.3868474960327148\n",
            "Epoch 3, Batch 738, G Loss: 0.6957558393478394, D Loss: 1.3878957033157349\n",
            "Epoch 3, Batch 739, G Loss: 0.69571453332901, D Loss: 1.3880555629730225\n",
            "Epoch 3, Batch 740, G Loss: 0.6956919431686401, D Loss: 1.3881828784942627\n",
            "Epoch 3, Batch 741, G Loss: 0.6956721544265747, D Loss: 1.3865094184875488\n",
            "Epoch 3, Batch 742, G Loss: 0.6956790089607239, D Loss: 1.386584997177124\n",
            "Epoch 3, Batch 743, G Loss: 0.6956425905227661, D Loss: 1.3868606090545654\n",
            "Epoch 3, Batch 744, G Loss: 0.6956455707550049, D Loss: 1.3856561183929443\n",
            "Epoch 3, Batch 745, G Loss: 0.6956105828285217, D Loss: 1.3853652477264404\n",
            "Epoch 3, Batch 746, G Loss: 0.6955657005310059, D Loss: 1.3860727548599243\n",
            "Epoch 3, Batch 747, G Loss: 0.6955339312553406, D Loss: 1.386449933052063\n",
            "Epoch 3, Batch 748, G Loss: 0.6954818367958069, D Loss: 1.3863763809204102\n",
            "Epoch 3, Batch 749, G Loss: 0.6954734921455383, D Loss: 1.3863775730133057\n",
            "Epoch 3, Batch 750, G Loss: 0.6954084634780884, D Loss: 1.3871339559555054\n",
            "Epoch 3, Batch 751, G Loss: 0.6953989863395691, D Loss: 1.38557767868042\n",
            "Epoch 3, Batch 752, G Loss: 0.6953291893005371, D Loss: 1.3865182399749756\n",
            "Epoch 3, Batch 753, G Loss: 0.6953001022338867, D Loss: 1.3856470584869385\n",
            "Epoch 3, Batch 754, G Loss: 0.6952701210975647, D Loss: 1.3858249187469482\n",
            "Epoch 3, Batch 755, G Loss: 0.6952199935913086, D Loss: 1.3858587741851807\n",
            "Epoch 3, Batch 756, G Loss: 0.6951596140861511, D Loss: 1.386080026626587\n",
            "Epoch 3, Batch 757, G Loss: 0.6951041221618652, D Loss: 1.3860613107681274\n",
            "Epoch 3, Batch 758, G Loss: 0.6950591802597046, D Loss: 1.3864712715148926\n",
            "Epoch 3, Batch 759, G Loss: 0.6950207352638245, D Loss: 1.3860077857971191\n",
            "Epoch 3, Batch 760, G Loss: 0.6949824094772339, D Loss: 1.3866146802902222\n",
            "Epoch 3, Batch 761, G Loss: 0.6948947310447693, D Loss: 1.3864076137542725\n",
            "Epoch 3, Batch 762, G Loss: 0.694884181022644, D Loss: 1.3861651420593262\n",
            "Epoch 3, Batch 763, G Loss: 0.6948391795158386, D Loss: 1.3862684965133667\n",
            "Epoch 3, Batch 764, G Loss: 0.6947932243347168, D Loss: 1.3866310119628906\n",
            "Epoch 3, Batch 765, G Loss: 0.6947602033615112, D Loss: 1.388261079788208\n",
            "Epoch 3, Batch 766, G Loss: 0.6947339177131653, D Loss: 1.388000726699829\n",
            "Epoch 3, Batch 767, G Loss: 0.6947221755981445, D Loss: 1.3874695301055908\n",
            "Epoch 3, Batch 768, G Loss: 0.6947372555732727, D Loss: 1.386610984802246\n",
            "Epoch 3, Batch 769, G Loss: 0.6947135925292969, D Loss: 1.3864164352416992\n",
            "Epoch 3, Batch 770, G Loss: 0.6947245597839355, D Loss: 1.3867127895355225\n",
            "Epoch 3, Batch 771, G Loss: 0.6947144269943237, D Loss: 1.3870046138763428\n",
            "Epoch 3, Batch 772, G Loss: 0.6947102546691895, D Loss: 1.3870187997817993\n",
            "Epoch 3, Batch 773, G Loss: 0.6947099566459656, D Loss: 1.386622428894043\n",
            "Epoch 3, Batch 774, G Loss: 0.6947265267372131, D Loss: 1.3868762254714966\n",
            "Epoch 3, Batch 775, G Loss: 0.6947072148323059, D Loss: 1.3869894742965698\n",
            "Epoch 3, Batch 776, G Loss: 0.6947149038314819, D Loss: 1.386946439743042\n",
            "Epoch 3, Batch 777, G Loss: 0.6947235465049744, D Loss: 1.3875811100006104\n",
            "Epoch 3, Batch 778, G Loss: 0.6947505474090576, D Loss: 1.3862388134002686\n",
            "Epoch 3, Batch 779, G Loss: 0.6947328448295593, D Loss: 1.3872647285461426\n",
            "Epoch 3, Batch 780, G Loss: 0.6947556138038635, D Loss: 1.3873622417449951\n",
            "Epoch 3, Batch 781, G Loss: 0.6947786808013916, D Loss: 1.3877348899841309\n",
            "Epoch 3, Batch 782, G Loss: 0.694804847240448, D Loss: 1.3873133659362793\n",
            "Epoch 3, Batch 783, G Loss: 0.6948221325874329, D Loss: 1.3870664834976196\n",
            "Epoch 3, Batch 784, G Loss: 0.6948380470275879, D Loss: 1.3867769241333008\n",
            "Epoch 3, Batch 785, G Loss: 0.6948573589324951, D Loss: 1.3871536254882812\n",
            "Epoch 3, Batch 786, G Loss: 0.6948791742324829, D Loss: 1.3870971202850342\n",
            "Epoch 3, Batch 787, G Loss: 0.6949210166931152, D Loss: 1.3869246244430542\n",
            "Epoch 3, Batch 788, G Loss: 0.6949353814125061, D Loss: 1.3871524333953857\n",
            "Epoch 3, Batch 789, G Loss: 0.6949646472930908, D Loss: 1.3869946002960205\n",
            "Epoch 3, Batch 790, G Loss: 0.6949755549430847, D Loss: 1.3867077827453613\n",
            "Epoch 3, Batch 791, G Loss: 0.6949828267097473, D Loss: 1.3871409893035889\n",
            "Epoch 3, Batch 792, G Loss: 0.6950141787528992, D Loss: 1.387186050415039\n",
            "Epoch 3, Batch 793, G Loss: 0.6950365900993347, D Loss: 1.386789083480835\n",
            "Epoch 3, Batch 794, G Loss: 0.6950494647026062, D Loss: 1.386234998703003\n",
            "Epoch 3, Batch 795, G Loss: 0.6950726509094238, D Loss: 1.3867580890655518\n",
            "Epoch 3, Batch 796, G Loss: 0.6950693130493164, D Loss: 1.386500358581543\n",
            "Epoch 3, Batch 797, G Loss: 0.6950783133506775, D Loss: 1.3869023323059082\n",
            "Epoch 3, Batch 798, G Loss: 0.6950788497924805, D Loss: 1.3869918584823608\n",
            "Epoch 3, Batch 799, G Loss: 0.6950986385345459, D Loss: 1.3864696025848389\n",
            "Epoch 3, Batch 800, G Loss: 0.6950832009315491, D Loss: 1.3861286640167236\n",
            "Epoch 3, Batch 801, G Loss: 0.6950927376747131, D Loss: 1.3858208656311035\n",
            "Epoch 3, Batch 802, G Loss: 0.695066511631012, D Loss: 1.3862366676330566\n",
            "Epoch 3, Batch 803, G Loss: 0.6950616836547852, D Loss: 1.386387586593628\n",
            "Epoch 3, Batch 804, G Loss: 0.6950258612632751, D Loss: 1.3866384029388428\n",
            "Epoch 3, Batch 805, G Loss: 0.6950350403785706, D Loss: 1.386193037033081\n",
            "Epoch 3, Batch 806, G Loss: 0.694995641708374, D Loss: 1.3869085311889648\n",
            "Epoch 3, Batch 807, G Loss: 0.6949601173400879, D Loss: 1.3863918781280518\n",
            "Epoch 3, Batch 808, G Loss: 0.6949602961540222, D Loss: 1.3869457244873047\n",
            "Epoch 3, Batch 809, G Loss: 0.694922149181366, D Loss: 1.3871078491210938\n",
            "Epoch 3, Batch 810, G Loss: 0.6949164867401123, D Loss: 1.3868398666381836\n",
            "Epoch 3, Batch 811, G Loss: 0.6948880553245544, D Loss: 1.3871824741363525\n",
            "Epoch 3, Batch 812, G Loss: 0.6948972344398499, D Loss: 1.386685848236084\n",
            "Epoch 3, Batch 813, G Loss: 0.6948918104171753, D Loss: 1.3869818449020386\n",
            "Epoch 3, Batch 814, G Loss: 0.6949015259742737, D Loss: 1.3874855041503906\n",
            "Epoch 3, Batch 815, G Loss: 0.6948814392089844, D Loss: 1.3870126008987427\n",
            "Epoch 3, Batch 816, G Loss: 0.6948999166488647, D Loss: 1.3872225284576416\n",
            "Epoch 3, Batch 817, G Loss: 0.6949267983436584, D Loss: 1.3873379230499268\n",
            "Epoch 3, Batch 818, G Loss: 0.6949300765991211, D Loss: 1.3868329524993896\n",
            "Epoch 3, Batch 819, G Loss: 0.6949605941772461, D Loss: 1.3869929313659668\n",
            "Epoch 3, Batch 820, G Loss: 0.6949775815010071, D Loss: 1.385932445526123\n",
            "Epoch 3, Batch 821, G Loss: 0.6949909925460815, D Loss: 1.3865201473236084\n",
            "Epoch 3, Batch 822, G Loss: 0.6949871778488159, D Loss: 1.3865444660186768\n",
            "Epoch 3, Batch 823, G Loss: 0.6949922442436218, D Loss: 1.3868097066879272\n",
            "Epoch 3, Batch 824, G Loss: 0.6950037479400635, D Loss: 1.3869590759277344\n",
            "Epoch 3, Batch 825, G Loss: 0.6950063705444336, D Loss: 1.3870933055877686\n",
            "Epoch 3, Batch 826, G Loss: 0.6950048208236694, D Loss: 1.3867003917694092\n",
            "Epoch 3, Batch 827, G Loss: 0.6950335502624512, D Loss: 1.386502981185913\n",
            "Epoch 3, Batch 828, G Loss: 0.6950379014015198, D Loss: 1.3863372802734375\n",
            "Epoch 3, Batch 829, G Loss: 0.6950233578681946, D Loss: 1.386209487915039\n",
            "Epoch 3, Batch 830, G Loss: 0.6950260996818542, D Loss: 1.386175513267517\n",
            "Epoch 3, Batch 831, G Loss: 0.6950159072875977, D Loss: 1.3867435455322266\n",
            "Epoch 3, Batch 832, G Loss: 0.6949951648712158, D Loss: 1.3864185810089111\n",
            "Epoch 3, Batch 833, G Loss: 0.6949812173843384, D Loss: 1.3866006135940552\n",
            "Epoch 3, Batch 834, G Loss: 0.6949602961540222, D Loss: 1.3868123292922974\n",
            "Epoch 3, Batch 835, G Loss: 0.6949455738067627, D Loss: 1.3866090774536133\n",
            "Epoch 3, Batch 836, G Loss: 0.6949455142021179, D Loss: 1.3867723941802979\n",
            "Epoch 3, Batch 837, G Loss: 0.6949244737625122, D Loss: 1.3867172002792358\n",
            "Epoch 3, Batch 838, G Loss: 0.6949290037155151, D Loss: 1.3869876861572266\n",
            "Epoch 3, Batch 839, G Loss: 0.6949223279953003, D Loss: 1.3866817951202393\n",
            "Epoch 3, Batch 840, G Loss: 0.6949259638786316, D Loss: 1.386758804321289\n",
            "Epoch 3, Batch 841, G Loss: 0.6949229836463928, D Loss: 1.3870254755020142\n",
            "Epoch 3, Batch 842, G Loss: 0.6949172019958496, D Loss: 1.38673734664917\n",
            "Epoch 3, Batch 843, G Loss: 0.694952666759491, D Loss: 1.38639235496521\n",
            "Epoch 3, Batch 844, G Loss: 0.6949495673179626, D Loss: 1.3863718509674072\n",
            "Epoch 3, Batch 845, G Loss: 0.6949500441551208, D Loss: 1.3865264654159546\n",
            "Epoch 3, Batch 846, G Loss: 0.6949297189712524, D Loss: 1.386500597000122\n",
            "Epoch 3, Batch 847, G Loss: 0.6949272751808167, D Loss: 1.386629343032837\n",
            "Epoch 3, Batch 848, G Loss: 0.694945216178894, D Loss: 1.3868577480316162\n",
            "Epoch 3, Batch 849, G Loss: 0.6949383616447449, D Loss: 1.386852502822876\n",
            "Epoch 3, Batch 850, G Loss: 0.6949436664581299, D Loss: 1.3862600326538086\n",
            "Epoch 3, Batch 851, G Loss: 0.6949285268783569, D Loss: 1.3864219188690186\n",
            "Epoch 3, Batch 852, G Loss: 0.694940447807312, D Loss: 1.3861808776855469\n",
            "Epoch 3, Batch 853, G Loss: 0.6949284076690674, D Loss: 1.3862767219543457\n",
            "Epoch 3, Batch 854, G Loss: 0.6949099898338318, D Loss: 1.386281967163086\n",
            "Epoch 3, Batch 855, G Loss: 0.6948981285095215, D Loss: 1.3863035440444946\n",
            "Epoch 3, Batch 856, G Loss: 0.6948591470718384, D Loss: 1.3863275051116943\n",
            "Epoch 3, Batch 857, G Loss: 0.694844126701355, D Loss: 1.386290192604065\n",
            "Epoch 3, Batch 858, G Loss: 0.6947798728942871, D Loss: 1.386648178100586\n",
            "Epoch 3, Batch 859, G Loss: 0.6947992444038391, D Loss: 1.3864365816116333\n",
            "Epoch 3, Batch 860, G Loss: 0.6947646141052246, D Loss: 1.3862836360931396\n",
            "Epoch 3, Batch 861, G Loss: 0.6947294473648071, D Loss: 1.3864611387252808\n",
            "Epoch 3, Batch 862, G Loss: 0.6946814656257629, D Loss: 1.3863798379898071\n",
            "Epoch 3, Batch 863, G Loss: 0.6946940422058105, D Loss: 1.386376976966858\n",
            "Epoch 3, Batch 864, G Loss: 0.6946538686752319, D Loss: 1.3863496780395508\n",
            "Epoch 3, Batch 865, G Loss: 0.6946274042129517, D Loss: 1.3865382671356201\n",
            "Epoch 3, Batch 866, G Loss: 0.6946135759353638, D Loss: 1.3865211009979248\n",
            "Epoch 3, Batch 867, G Loss: 0.6945869326591492, D Loss: 1.3864707946777344\n",
            "Epoch 3, Batch 868, G Loss: 0.6945862770080566, D Loss: 1.3861652612686157\n",
            "Epoch 3, Batch 869, G Loss: 0.6945427060127258, D Loss: 1.3862929344177246\n",
            "Epoch 3, Batch 870, G Loss: 0.694517195224762, D Loss: 1.385932445526123\n",
            "Epoch 3, Batch 871, G Loss: 0.6944838762283325, D Loss: 1.3860419988632202\n",
            "Epoch 3, Batch 872, G Loss: 0.6944343447685242, D Loss: 1.3860652446746826\n",
            "Epoch 3, Batch 873, G Loss: 0.6943932175636292, D Loss: 1.3863471746444702\n",
            "Epoch 3, Batch 874, G Loss: 0.6943413615226746, D Loss: 1.386387586593628\n",
            "Epoch 3, Batch 875, G Loss: 0.6942898631095886, D Loss: 1.386376142501831\n",
            "Epoch 3, Batch 876, G Loss: 0.6942537426948547, D Loss: 1.386399745941162\n",
            "Epoch 3, Batch 877, G Loss: 0.6942232847213745, D Loss: 1.3864309787750244\n",
            "Epoch 3, Batch 878, G Loss: 0.694189727306366, D Loss: 1.3863484859466553\n",
            "Epoch 3, Batch 879, G Loss: 0.694159984588623, D Loss: 1.3863322734832764\n",
            "Epoch 3, Batch 880, G Loss: 0.6941424012184143, D Loss: 1.3865556716918945\n",
            "Epoch 3, Batch 881, G Loss: 0.6941203474998474, D Loss: 1.3865936994552612\n",
            "Epoch 3, Batch 882, G Loss: 0.6941084861755371, D Loss: 1.3866052627563477\n",
            "Epoch 3, Batch 883, G Loss: 0.6941074728965759, D Loss: 1.3864765167236328\n",
            "Epoch 3, Batch 884, G Loss: 0.6941192150115967, D Loss: 1.3864014148712158\n",
            "Epoch 3, Batch 885, G Loss: 0.694133996963501, D Loss: 1.3864531517028809\n",
            "Epoch 3, Batch 886, G Loss: 0.694139301776886, D Loss: 1.386449933052063\n",
            "Epoch 3, Batch 887, G Loss: 0.6941535472869873, D Loss: 1.3861277103424072\n",
            "Epoch 3, Batch 888, G Loss: 0.6941536068916321, D Loss: 1.3862648010253906\n",
            "Epoch 3, Batch 889, G Loss: 0.6941472291946411, D Loss: 1.3862860202789307\n",
            "Epoch 3, Batch 890, G Loss: 0.6941536068916321, D Loss: 1.3864223957061768\n",
            "Epoch 3, Batch 891, G Loss: 0.6941598653793335, D Loss: 1.3864974975585938\n",
            "Epoch 3, Batch 892, G Loss: 0.6941514015197754, D Loss: 1.3866904973983765\n",
            "Epoch 3, Batch 893, G Loss: 0.6941878795623779, D Loss: 1.3865137100219727\n",
            "Epoch 3, Batch 894, G Loss: 0.694208562374115, D Loss: 1.3864140510559082\n",
            "Epoch 3, Batch 895, G Loss: 0.6942338943481445, D Loss: 1.386336326599121\n",
            "Epoch 3, Batch 896, G Loss: 0.6942541003227234, D Loss: 1.3863682746887207\n",
            "Epoch 3, Batch 897, G Loss: 0.6942848563194275, D Loss: 1.3863327503204346\n",
            "Epoch 3, Batch 898, G Loss: 0.6942862868309021, D Loss: 1.3863928318023682\n",
            "Epoch 3, Batch 899, G Loss: 0.6943197250366211, D Loss: 1.3862775564193726\n",
            "Epoch 3, Batch 900, G Loss: 0.6943267583847046, D Loss: 1.3863065242767334\n",
            "Epoch 3, Batch 901, G Loss: 0.6943421363830566, D Loss: 1.3863744735717773\n",
            "Epoch 3, Batch 902, G Loss: 0.694335401058197, D Loss: 1.3864686489105225\n",
            "Epoch 3, Batch 903, G Loss: 0.6943580508232117, D Loss: 1.3864508867263794\n",
            "Epoch 3, Batch 904, G Loss: 0.694379448890686, D Loss: 1.3864524364471436\n",
            "Epoch 3, Batch 905, G Loss: 0.6944125294685364, D Loss: 1.386368751525879\n",
            "Epoch 3, Batch 906, G Loss: 0.694429337978363, D Loss: 1.386401891708374\n",
            "Epoch 3, Batch 907, G Loss: 0.6944605708122253, D Loss: 1.3863089084625244\n",
            "Epoch 3, Batch 908, G Loss: 0.6944943070411682, D Loss: 1.386406660079956\n",
            "Epoch 3, Batch 909, G Loss: 0.6945335865020752, D Loss: 1.3864004611968994\n",
            "Epoch 3, Batch 910, G Loss: 0.6945623159408569, D Loss: 1.3863039016723633\n",
            "Epoch 3, Batch 911, G Loss: 0.6945962905883789, D Loss: 1.3862733840942383\n",
            "Epoch 3, Batch 912, G Loss: 0.6946157217025757, D Loss: 1.3862838745117188\n",
            "Epoch 3, Batch 913, G Loss: 0.6946452260017395, D Loss: 1.3862292766571045\n",
            "Epoch 3, Batch 914, G Loss: 0.6946461796760559, D Loss: 1.3862364292144775\n",
            "Epoch 3, Batch 915, G Loss: 0.694624662399292, D Loss: 1.3863130807876587\n",
            "Epoch 3, Batch 916, G Loss: 0.6946136355400085, D Loss: 1.3863364458084106\n",
            "Epoch 3, Batch 917, G Loss: 0.6946292519569397, D Loss: 1.3863223791122437\n",
            "Epoch 3, Batch 918, G Loss: 0.6946396827697754, D Loss: 1.3863260746002197\n",
            "Epoch 3, Batch 919, G Loss: 0.6946823596954346, D Loss: 1.3862922191619873\n",
            "Epoch 3, Batch 920, G Loss: 0.6947146654129028, D Loss: 1.3862652778625488\n",
            "Epoch 3, Batch 921, G Loss: 0.694739580154419, D Loss: 1.386256456375122\n",
            "Epoch 3, Batch 922, G Loss: 0.6947396993637085, D Loss: 1.3862704038619995\n",
            "Epoch 3, Batch 923, G Loss: 0.6947550773620605, D Loss: 1.3862578868865967\n",
            "Epoch 3, Batch 924, G Loss: 0.6947624087333679, D Loss: 1.3862500190734863\n",
            "Epoch 3, Batch 925, G Loss: 0.6947808265686035, D Loss: 1.3862324953079224\n",
            "Epoch 3, Batch 926, G Loss: 0.6947898268699646, D Loss: 1.3862329721450806\n",
            "Epoch 3, Batch 927, G Loss: 0.6947726607322693, D Loss: 1.3862967491149902\n",
            "Epoch 3, Batch 928, G Loss: 0.6947445273399353, D Loss: 1.38641357421875\n",
            "Epoch 3, Batch 929, G Loss: 0.6947059035301208, D Loss: 1.3864734172821045\n",
            "Epoch 3, Batch 930, G Loss: 0.6946052312850952, D Loss: 1.3862922191619873\n",
            "Epoch 3, Batch 931, G Loss: 0.6944965720176697, D Loss: 1.386195421218872\n",
            "Epoch 3, Batch 932, G Loss: 0.6943890452384949, D Loss: 1.3862411975860596\n",
            "Epoch 3, Batch 933, G Loss: 0.6943116188049316, D Loss: 1.3862560987472534\n",
            "Epoch 3, Batch 934, G Loss: 0.6942373514175415, D Loss: 1.3862216472625732\n",
            "Epoch 3, Batch 935, G Loss: 0.6941467523574829, D Loss: 1.3862547874450684\n",
            "Epoch 3, Batch 936, G Loss: 0.6940666437149048, D Loss: 1.3862724304199219\n",
            "Epoch 3, Batch 937, G Loss: 0.6939954161643982, D Loss: 1.3862141370773315\n",
            "Epoch 3, Batch 938, G Loss: 0.6939589977264404, D Loss: 1.3861980438232422\n",
            "Epoch 4, Batch 1, G Loss: 0.6938916444778442, D Loss: 1.3862383365631104\n",
            "Epoch 4, Batch 2, G Loss: 0.6938607096672058, D Loss: 1.3862357139587402\n",
            "Epoch 4, Batch 3, G Loss: 0.6938378214836121, D Loss: 1.3862203359603882\n",
            "Epoch 4, Batch 4, G Loss: 0.6938266158103943, D Loss: 1.3862311840057373\n",
            "Epoch 4, Batch 5, G Loss: 0.6938033699989319, D Loss: 1.386204719543457\n",
            "Epoch 4, Batch 6, G Loss: 0.6937808394432068, D Loss: 1.3862217664718628\n",
            "Epoch 4, Batch 7, G Loss: 0.6937791705131531, D Loss: 1.3862063884735107\n",
            "Epoch 4, Batch 8, G Loss: 0.6937817335128784, D Loss: 1.3862046003341675\n",
            "Epoch 4, Batch 9, G Loss: 0.6937837600708008, D Loss: 1.3862242698669434\n",
            "Epoch 4, Batch 10, G Loss: 0.693798303604126, D Loss: 1.3861801624298096\n",
            "Epoch 4, Batch 11, G Loss: 0.6938069462776184, D Loss: 1.3862208127975464\n",
            "Epoch 4, Batch 12, G Loss: 0.693830668926239, D Loss: 1.3862617015838623\n",
            "Epoch 4, Batch 13, G Loss: 0.693840503692627, D Loss: 1.3862204551696777\n",
            "Epoch 4, Batch 14, G Loss: 0.6938193440437317, D Loss: 1.3862555027008057\n",
            "Epoch 4, Batch 15, G Loss: 0.6938343644142151, D Loss: 1.3862191438674927\n",
            "Epoch 4, Batch 16, G Loss: 0.6938414573669434, D Loss: 1.3861732482910156\n",
            "Epoch 4, Batch 17, G Loss: 0.6938400864601135, D Loss: 1.3861255645751953\n",
            "Epoch 4, Batch 18, G Loss: 0.6938613653182983, D Loss: 1.3861408233642578\n",
            "Epoch 4, Batch 19, G Loss: 0.6938743591308594, D Loss: 1.3862528800964355\n",
            "Epoch 4, Batch 20, G Loss: 0.6938954591751099, D Loss: 1.3863627910614014\n",
            "Epoch 4, Batch 21, G Loss: 0.6938842535018921, D Loss: 1.3863234519958496\n",
            "Epoch 4, Batch 22, G Loss: 0.6938680410385132, D Loss: 1.3864190578460693\n",
            "Epoch 4, Batch 23, G Loss: 0.6938280463218689, D Loss: 1.3861925601959229\n",
            "Epoch 4, Batch 24, G Loss: 0.6938024759292603, D Loss: 1.386167049407959\n",
            "Epoch 4, Batch 25, G Loss: 0.6937851309776306, D Loss: 1.3861671686172485\n",
            "Epoch 4, Batch 26, G Loss: 0.6937631368637085, D Loss: 1.3861446380615234\n",
            "Epoch 4, Batch 27, G Loss: 0.6937576532363892, D Loss: 1.3861414194107056\n",
            "Epoch 4, Batch 28, G Loss: 0.6937441229820251, D Loss: 1.3861727714538574\n",
            "Epoch 4, Batch 29, G Loss: 0.6937382221221924, D Loss: 1.3861587047576904\n",
            "Epoch 4, Batch 30, G Loss: 0.6937520503997803, D Loss: 1.3862245082855225\n",
            "Epoch 4, Batch 31, G Loss: 0.6937646865844727, D Loss: 1.3862898349761963\n",
            "Epoch 4, Batch 32, G Loss: 0.6937598586082458, D Loss: 1.3863253593444824\n",
            "Epoch 4, Batch 33, G Loss: 0.6937505602836609, D Loss: 1.3862481117248535\n",
            "Epoch 4, Batch 34, G Loss: 0.6937301754951477, D Loss: 1.3862073421478271\n",
            "Epoch 4, Batch 35, G Loss: 0.6937128305435181, D Loss: 1.3861711025238037\n",
            "Epoch 4, Batch 36, G Loss: 0.6937054991722107, D Loss: 1.3862643241882324\n",
            "Epoch 4, Batch 37, G Loss: 0.6936795711517334, D Loss: 1.38619065284729\n",
            "Epoch 4, Batch 38, G Loss: 0.6936699151992798, D Loss: 1.3861744403839111\n",
            "Epoch 4, Batch 39, G Loss: 0.6936619877815247, D Loss: 1.386187195777893\n",
            "Epoch 4, Batch 40, G Loss: 0.6936485767364502, D Loss: 1.3862571716308594\n",
            "Epoch 4, Batch 41, G Loss: 0.6936470866203308, D Loss: 1.3862128257751465\n",
            "Epoch 4, Batch 42, G Loss: 0.6936380863189697, D Loss: 1.3862758874893188\n",
            "Epoch 4, Batch 43, G Loss: 0.6936286091804504, D Loss: 1.3861452341079712\n",
            "Epoch 4, Batch 44, G Loss: 0.693608820438385, D Loss: 1.3862144947052002\n",
            "Epoch 4, Batch 45, G Loss: 0.6935915350914001, D Loss: 1.3862895965576172\n",
            "Epoch 4, Batch 46, G Loss: 0.6936010122299194, D Loss: 1.38621187210083\n",
            "Epoch 4, Batch 47, G Loss: 0.6935811638832092, D Loss: 1.3860795497894287\n",
            "Epoch 4, Batch 48, G Loss: 0.6935697793960571, D Loss: 1.3861844539642334\n",
            "Epoch 4, Batch 49, G Loss: 0.6935790777206421, D Loss: 1.3860946893692017\n",
            "Epoch 4, Batch 50, G Loss: 0.693579375743866, D Loss: 1.3861057758331299\n",
            "Epoch 4, Batch 51, G Loss: 0.6935979127883911, D Loss: 1.3861029148101807\n",
            "Epoch 4, Batch 52, G Loss: 0.6936185956001282, D Loss: 1.3860678672790527\n",
            "Epoch 4, Batch 53, G Loss: 0.6936508417129517, D Loss: 1.3861843347549438\n",
            "Epoch 4, Batch 54, G Loss: 0.6936686635017395, D Loss: 1.3860559463500977\n",
            "Epoch 4, Batch 55, G Loss: 0.693702757358551, D Loss: 1.3860571384429932\n",
            "Epoch 4, Batch 56, G Loss: 0.6937413215637207, D Loss: 1.3861701488494873\n",
            "Epoch 4, Batch 57, G Loss: 0.693780243396759, D Loss: 1.386268138885498\n",
            "Epoch 4, Batch 58, G Loss: 0.6938103437423706, D Loss: 1.3863685131072998\n",
            "Epoch 4, Batch 59, G Loss: 0.6938146352767944, D Loss: 1.3861424922943115\n",
            "Epoch 4, Batch 60, G Loss: 0.6938272714614868, D Loss: 1.3861439228057861\n",
            "Epoch 4, Batch 61, G Loss: 0.6938425898551941, D Loss: 1.3860881328582764\n",
            "Epoch 4, Batch 62, G Loss: 0.6938639283180237, D Loss: 1.3861281871795654\n",
            "Epoch 4, Batch 63, G Loss: 0.6938695907592773, D Loss: 1.38612699508667\n",
            "Epoch 4, Batch 64, G Loss: 0.69389808177948, D Loss: 1.3862464427947998\n",
            "Epoch 4, Batch 65, G Loss: 0.6938932538032532, D Loss: 1.3861263990402222\n",
            "Epoch 4, Batch 66, G Loss: 0.6939191818237305, D Loss: 1.386059284210205\n",
            "Epoch 4, Batch 67, G Loss: 0.6939173936843872, D Loss: 1.3862285614013672\n",
            "Epoch 4, Batch 68, G Loss: 0.6939281821250916, D Loss: 1.3861172199249268\n",
            "Epoch 4, Batch 69, G Loss: 0.6939420104026794, D Loss: 1.3859915733337402\n",
            "Epoch 4, Batch 70, G Loss: 0.6939495801925659, D Loss: 1.3861204385757446\n",
            "Epoch 4, Batch 71, G Loss: 0.6939677000045776, D Loss: 1.386407732963562\n",
            "Epoch 4, Batch 72, G Loss: 0.6939767599105835, D Loss: 1.3861775398254395\n",
            "Epoch 4, Batch 73, G Loss: 0.6939694881439209, D Loss: 1.3863565921783447\n",
            "Epoch 4, Batch 74, G Loss: 0.6939603686332703, D Loss: 1.3864989280700684\n",
            "Epoch 4, Batch 75, G Loss: 0.6939252018928528, D Loss: 1.3862357139587402\n",
            "Epoch 4, Batch 76, G Loss: 0.6938968300819397, D Loss: 1.3863321542739868\n",
            "Epoch 4, Batch 77, G Loss: 0.6938603520393372, D Loss: 1.3862897157669067\n",
            "Epoch 4, Batch 78, G Loss: 0.6938273906707764, D Loss: 1.386031985282898\n",
            "Epoch 4, Batch 79, G Loss: 0.6937896013259888, D Loss: 1.3861947059631348\n",
            "Epoch 4, Batch 80, G Loss: 0.6937685012817383, D Loss: 1.3863050937652588\n",
            "Epoch 4, Batch 81, G Loss: 0.6937317848205566, D Loss: 1.3862907886505127\n",
            "Epoch 4, Batch 82, G Loss: 0.6937004327774048, D Loss: 1.3862614631652832\n",
            "Epoch 4, Batch 83, G Loss: 0.6936724781990051, D Loss: 1.3862106800079346\n",
            "Epoch 4, Batch 84, G Loss: 0.6936256289482117, D Loss: 1.3864747285842896\n",
            "Epoch 4, Batch 85, G Loss: 0.6935839056968689, D Loss: 1.3864247798919678\n",
            "Epoch 4, Batch 86, G Loss: 0.6935378313064575, D Loss: 1.3864107131958008\n",
            "Epoch 4, Batch 87, G Loss: 0.6934752464294434, D Loss: 1.3863011598587036\n",
            "Epoch 4, Batch 88, G Loss: 0.6934202313423157, D Loss: 1.3862669467926025\n",
            "Epoch 4, Batch 89, G Loss: 0.6933716535568237, D Loss: 1.3862087726593018\n",
            "Epoch 4, Batch 90, G Loss: 0.6933234333992004, D Loss: 1.386270523071289\n",
            "Epoch 4, Batch 91, G Loss: 0.6932796835899353, D Loss: 1.3861134052276611\n",
            "Epoch 4, Batch 92, G Loss: 0.6932360529899597, D Loss: 1.3860983848571777\n",
            "Epoch 4, Batch 93, G Loss: 0.6932147145271301, D Loss: 1.386155605316162\n",
            "Epoch 4, Batch 94, G Loss: 0.6931940317153931, D Loss: 1.3865519762039185\n",
            "Epoch 4, Batch 95, G Loss: 0.6931616067886353, D Loss: 1.3865582942962646\n",
            "Epoch 4, Batch 96, G Loss: 0.6931210160255432, D Loss: 1.3865355253219604\n",
            "Epoch 4, Batch 97, G Loss: 0.6930679678916931, D Loss: 1.3864562511444092\n",
            "Epoch 4, Batch 98, G Loss: 0.6930098533630371, D Loss: 1.3862974643707275\n",
            "Epoch 4, Batch 99, G Loss: 0.6929389834403992, D Loss: 1.3863482475280762\n",
            "Epoch 4, Batch 100, G Loss: 0.6928868293762207, D Loss: 1.3863139152526855\n",
            "Epoch 4, Batch 101, G Loss: 0.692834198474884, D Loss: 1.386263132095337\n",
            "Epoch 4, Batch 102, G Loss: 0.6927846670150757, D Loss: 1.3861684799194336\n",
            "Epoch 4, Batch 103, G Loss: 0.6927460432052612, D Loss: 1.3860710859298706\n",
            "Epoch 4, Batch 104, G Loss: 0.6927415728569031, D Loss: 1.3862285614013672\n",
            "Epoch 4, Batch 105, G Loss: 0.6927180290222168, D Loss: 1.3859200477600098\n",
            "Epoch 4, Batch 106, G Loss: 0.6927210092544556, D Loss: 1.385970115661621\n",
            "Epoch 4, Batch 107, G Loss: 0.6927456259727478, D Loss: 1.3858970403671265\n",
            "Epoch 4, Batch 108, G Loss: 0.6927863955497742, D Loss: 1.3862667083740234\n",
            "Epoch 4, Batch 109, G Loss: 0.6928355097770691, D Loss: 1.3863316774368286\n",
            "Epoch 4, Batch 110, G Loss: 0.6928675174713135, D Loss: 1.386183738708496\n",
            "Epoch 4, Batch 111, G Loss: 0.692897379398346, D Loss: 1.386026382446289\n",
            "Epoch 4, Batch 112, G Loss: 0.6929447650909424, D Loss: 1.38602876663208\n",
            "Epoch 4, Batch 113, G Loss: 0.6929948925971985, D Loss: 1.3859758377075195\n",
            "Epoch 4, Batch 114, G Loss: 0.6930632591247559, D Loss: 1.386023998260498\n",
            "Epoch 4, Batch 115, G Loss: 0.6931284070014954, D Loss: 1.3857958316802979\n",
            "Epoch 4, Batch 116, G Loss: 0.6931934952735901, D Loss: 1.3859009742736816\n",
            "Epoch 4, Batch 117, G Loss: 0.693291187286377, D Loss: 1.386014699935913\n",
            "Epoch 4, Batch 118, G Loss: 0.6933748722076416, D Loss: 1.38595712184906\n",
            "Epoch 4, Batch 119, G Loss: 0.6934725642204285, D Loss: 1.385960578918457\n",
            "Epoch 4, Batch 120, G Loss: 0.6935712695121765, D Loss: 1.3861351013183594\n",
            "Epoch 4, Batch 121, G Loss: 0.6936424374580383, D Loss: 1.3861057758331299\n",
            "Epoch 4, Batch 122, G Loss: 0.6937224864959717, D Loss: 1.38611900806427\n",
            "Epoch 4, Batch 123, G Loss: 0.693791925907135, D Loss: 1.3862555027008057\n",
            "Epoch 4, Batch 124, G Loss: 0.6938672661781311, D Loss: 1.3864834308624268\n",
            "Epoch 4, Batch 125, G Loss: 0.693902313709259, D Loss: 1.3863916397094727\n",
            "Epoch 4, Batch 126, G Loss: 0.6939072608947754, D Loss: 1.3864089250564575\n",
            "Epoch 4, Batch 127, G Loss: 0.6939206719398499, D Loss: 1.386268138885498\n",
            "Epoch 4, Batch 128, G Loss: 0.6939125657081604, D Loss: 1.3858845233917236\n",
            "Epoch 4, Batch 129, G Loss: 0.6939314007759094, D Loss: 1.3859398365020752\n",
            "Epoch 4, Batch 130, G Loss: 0.6939387321472168, D Loss: 1.3863513469696045\n",
            "Epoch 4, Batch 131, G Loss: 0.6939364671707153, D Loss: 1.3862287998199463\n",
            "Epoch 4, Batch 132, G Loss: 0.693934977054596, D Loss: 1.3861303329467773\n",
            "Epoch 4, Batch 133, G Loss: 0.693928599357605, D Loss: 1.3868165016174316\n",
            "Epoch 4, Batch 134, G Loss: 0.693913996219635, D Loss: 1.3865749835968018\n",
            "Epoch 4, Batch 135, G Loss: 0.6938512921333313, D Loss: 1.3864984512329102\n",
            "Epoch 4, Batch 136, G Loss: 0.6938008666038513, D Loss: 1.3865902423858643\n",
            "Epoch 4, Batch 137, G Loss: 0.6937313675880432, D Loss: 1.3862500190734863\n",
            "Epoch 4, Batch 138, G Loss: 0.6936601400375366, D Loss: 1.3861877918243408\n",
            "Epoch 4, Batch 139, G Loss: 0.6935943365097046, D Loss: 1.386018991470337\n",
            "Epoch 4, Batch 140, G Loss: 0.6935461163520813, D Loss: 1.3862388134002686\n",
            "Epoch 4, Batch 141, G Loss: 0.693493127822876, D Loss: 1.3860809803009033\n",
            "Epoch 4, Batch 142, G Loss: 0.693450391292572, D Loss: 1.3861744403839111\n",
            "Epoch 4, Batch 143, G Loss: 0.6934263110160828, D Loss: 1.3858908414840698\n",
            "Epoch 4, Batch 144, G Loss: 0.6934067606925964, D Loss: 1.3860496282577515\n",
            "Epoch 4, Batch 145, G Loss: 0.6933942437171936, D Loss: 1.3857783079147339\n",
            "Epoch 4, Batch 146, G Loss: 0.6934025287628174, D Loss: 1.3862190246582031\n",
            "Epoch 4, Batch 147, G Loss: 0.69342041015625, D Loss: 1.3862707614898682\n",
            "Epoch 4, Batch 148, G Loss: 0.6934135556221008, D Loss: 1.3862807750701904\n",
            "Epoch 4, Batch 149, G Loss: 0.6934075355529785, D Loss: 1.386413335800171\n",
            "Epoch 4, Batch 150, G Loss: 0.6934013366699219, D Loss: 1.3859469890594482\n",
            "Epoch 4, Batch 151, G Loss: 0.6933928728103638, D Loss: 1.3861134052276611\n",
            "Epoch 4, Batch 152, G Loss: 0.6933846473693848, D Loss: 1.3863451480865479\n",
            "Epoch 4, Batch 153, G Loss: 0.6933901906013489, D Loss: 1.38624906539917\n",
            "Epoch 4, Batch 154, G Loss: 0.6933855414390564, D Loss: 1.3861135244369507\n",
            "Epoch 4, Batch 155, G Loss: 0.6933809518814087, D Loss: 1.3861243724822998\n",
            "Epoch 4, Batch 156, G Loss: 0.6933796405792236, D Loss: 1.3860759735107422\n",
            "Epoch 4, Batch 157, G Loss: 0.6933740377426147, D Loss: 1.3863605260849\n",
            "Epoch 4, Batch 158, G Loss: 0.6933720707893372, D Loss: 1.3860728740692139\n",
            "Epoch 4, Batch 159, G Loss: 0.6933825612068176, D Loss: 1.3858976364135742\n",
            "Epoch 4, Batch 160, G Loss: 0.6933931112289429, D Loss: 1.3868491649627686\n",
            "Epoch 4, Batch 161, G Loss: 0.6933667063713074, D Loss: 1.3869519233703613\n",
            "Epoch 4, Batch 162, G Loss: 0.6933254599571228, D Loss: 1.386523962020874\n",
            "Epoch 4, Batch 163, G Loss: 0.6932812333106995, D Loss: 1.3863787651062012\n",
            "Epoch 4, Batch 164, G Loss: 0.6932162046432495, D Loss: 1.3865056037902832\n",
            "Epoch 4, Batch 165, G Loss: 0.6931548118591309, D Loss: 1.386337161064148\n",
            "Epoch 4, Batch 166, G Loss: 0.693089485168457, D Loss: 1.3861994743347168\n",
            "Epoch 4, Batch 167, G Loss: 0.6930288076400757, D Loss: 1.3859587907791138\n",
            "Epoch 4, Batch 168, G Loss: 0.6929901838302612, D Loss: 1.3861225843429565\n",
            "Epoch 4, Batch 169, G Loss: 0.6929690837860107, D Loss: 1.3862560987472534\n",
            "Epoch 4, Batch 170, G Loss: 0.6929501891136169, D Loss: 1.3862724304199219\n",
            "Epoch 4, Batch 171, G Loss: 0.6929092407226562, D Loss: 1.386240839958191\n",
            "Epoch 4, Batch 172, G Loss: 0.6928939819335938, D Loss: 1.3863904476165771\n",
            "Epoch 4, Batch 173, G Loss: 0.6928654909133911, D Loss: 1.3864293098449707\n",
            "Epoch 4, Batch 174, G Loss: 0.6928372979164124, D Loss: 1.3861706256866455\n",
            "Epoch 4, Batch 175, G Loss: 0.6928110122680664, D Loss: 1.3862360715866089\n",
            "Epoch 4, Batch 176, G Loss: 0.6928064823150635, D Loss: 1.386178970336914\n",
            "Epoch 4, Batch 177, G Loss: 0.6927813291549683, D Loss: 1.386107325553894\n",
            "Epoch 4, Batch 178, G Loss: 0.6927940249443054, D Loss: 1.3861982822418213\n",
            "Epoch 4, Batch 179, G Loss: 0.6927787661552429, D Loss: 1.386139988899231\n",
            "Epoch 4, Batch 180, G Loss: 0.692792534828186, D Loss: 1.3861334323883057\n",
            "Epoch 4, Batch 181, G Loss: 0.6927918195724487, D Loss: 1.3861324787139893\n",
            "Epoch 4, Batch 182, G Loss: 0.6928125619888306, D Loss: 1.386141061782837\n",
            "Epoch 4, Batch 183, G Loss: 0.6928291916847229, D Loss: 1.3861768245697021\n",
            "Epoch 4, Batch 184, G Loss: 0.692865788936615, D Loss: 1.3860929012298584\n",
            "Epoch 4, Batch 185, G Loss: 0.6928818225860596, D Loss: 1.386162519454956\n",
            "Epoch 4, Batch 186, G Loss: 0.6929201483726501, D Loss: 1.3861360549926758\n",
            "Epoch 4, Batch 187, G Loss: 0.6929484009742737, D Loss: 1.386077880859375\n",
            "Epoch 4, Batch 188, G Loss: 0.6929853558540344, D Loss: 1.3864333629608154\n",
            "Epoch 4, Batch 189, G Loss: 0.6930059194564819, D Loss: 1.386263370513916\n",
            "Epoch 4, Batch 190, G Loss: 0.693026065826416, D Loss: 1.386254072189331\n",
            "Epoch 4, Batch 191, G Loss: 0.6930411458015442, D Loss: 1.3863214254379272\n",
            "Epoch 4, Batch 192, G Loss: 0.6930463910102844, D Loss: 1.3863627910614014\n",
            "Epoch 4, Batch 193, G Loss: 0.6930500864982605, D Loss: 1.386254906654358\n",
            "Epoch 4, Batch 194, G Loss: 0.6930530071258545, D Loss: 1.3862688541412354\n",
            "Epoch 4, Batch 195, G Loss: 0.6930466294288635, D Loss: 1.3863701820373535\n",
            "Epoch 4, Batch 196, G Loss: 0.6930376291275024, D Loss: 1.3863115310668945\n",
            "Epoch 4, Batch 197, G Loss: 0.6930205821990967, D Loss: 1.3863340616226196\n",
            "Epoch 4, Batch 198, G Loss: 0.6930016279220581, D Loss: 1.3863356113433838\n",
            "Epoch 4, Batch 199, G Loss: 0.6929776668548584, D Loss: 1.3862348794937134\n",
            "Epoch 4, Batch 200, G Loss: 0.6929610967636108, D Loss: 1.3860557079315186\n",
            "Epoch 4, Batch 201, G Loss: 0.6929486393928528, D Loss: 1.385934829711914\n",
            "Epoch 4, Batch 202, G Loss: 0.6929657459259033, D Loss: 1.3862123489379883\n",
            "Epoch 4, Batch 203, G Loss: 0.6929721236228943, D Loss: 1.3864610195159912\n",
            "Epoch 4, Batch 204, G Loss: 0.6929575204849243, D Loss: 1.3864367008209229\n",
            "Epoch 4, Batch 205, G Loss: 0.6929407119750977, D Loss: 1.3866195678710938\n",
            "Epoch 4, Batch 206, G Loss: 0.6929125785827637, D Loss: 1.3863041400909424\n",
            "Epoch 4, Batch 207, G Loss: 0.6928718686103821, D Loss: 1.3863654136657715\n",
            "Epoch 4, Batch 208, G Loss: 0.6928369998931885, D Loss: 1.3862965106964111\n",
            "Epoch 4, Batch 209, G Loss: 0.692803680896759, D Loss: 1.3863013982772827\n",
            "Epoch 4, Batch 210, G Loss: 0.6927685141563416, D Loss: 1.38625168800354\n",
            "Epoch 4, Batch 211, G Loss: 0.6927444934844971, D Loss: 1.3863263130187988\n",
            "Epoch 4, Batch 212, G Loss: 0.6927076578140259, D Loss: 1.386446475982666\n",
            "Epoch 4, Batch 213, G Loss: 0.6926727294921875, D Loss: 1.3862922191619873\n",
            "Epoch 4, Batch 214, G Loss: 0.6926320791244507, D Loss: 1.386061429977417\n",
            "Epoch 4, Batch 215, G Loss: 0.6926121115684509, D Loss: 1.3859734535217285\n",
            "Epoch 4, Batch 216, G Loss: 0.6926197409629822, D Loss: 1.3860855102539062\n",
            "Epoch 4, Batch 217, G Loss: 0.6926444172859192, D Loss: 1.3861749172210693\n",
            "Epoch 4, Batch 218, G Loss: 0.6926481127738953, D Loss: 1.386191725730896\n",
            "Epoch 4, Batch 219, G Loss: 0.6926769614219666, D Loss: 1.3860902786254883\n",
            "Epoch 4, Batch 220, G Loss: 0.6927091479301453, D Loss: 1.3864071369171143\n",
            "Epoch 4, Batch 221, G Loss: 0.6927314400672913, D Loss: 1.3863813877105713\n",
            "Epoch 4, Batch 222, G Loss: 0.6927396059036255, D Loss: 1.386438250541687\n",
            "Epoch 4, Batch 223, G Loss: 0.6927431225776672, D Loss: 1.386335849761963\n",
            "Epoch 4, Batch 224, G Loss: 0.6927279233932495, D Loss: 1.3863348960876465\n",
            "Epoch 4, Batch 225, G Loss: 0.692719042301178, D Loss: 1.3862617015838623\n",
            "Epoch 4, Batch 226, G Loss: 0.6927040815353394, D Loss: 1.3861620426177979\n",
            "Epoch 4, Batch 227, G Loss: 0.6927035450935364, D Loss: 1.3861013650894165\n",
            "Epoch 4, Batch 228, G Loss: 0.6927149295806885, D Loss: 1.3860695362091064\n",
            "Epoch 4, Batch 229, G Loss: 0.6927258372306824, D Loss: 1.3861479759216309\n",
            "Epoch 4, Batch 230, G Loss: 0.6927551627159119, D Loss: 1.3862485885620117\n",
            "Epoch 4, Batch 231, G Loss: 0.6927812695503235, D Loss: 1.3861491680145264\n",
            "Epoch 4, Batch 232, G Loss: 0.692802906036377, D Loss: 1.3861829042434692\n",
            "Epoch 4, Batch 233, G Loss: 0.6928368210792542, D Loss: 1.3861587047576904\n",
            "Epoch 4, Batch 234, G Loss: 0.6928690671920776, D Loss: 1.3861544132232666\n",
            "Epoch 4, Batch 235, G Loss: 0.6929030418395996, D Loss: 1.3860524892807007\n",
            "Epoch 4, Batch 236, G Loss: 0.6929601430892944, D Loss: 1.3861783742904663\n",
            "Epoch 4, Batch 237, G Loss: 0.6930020451545715, D Loss: 1.3860939741134644\n",
            "Epoch 4, Batch 238, G Loss: 0.6930506825447083, D Loss: 1.3860397338867188\n",
            "Epoch 4, Batch 239, G Loss: 0.6931032538414001, D Loss: 1.3861050605773926\n",
            "Epoch 4, Batch 240, G Loss: 0.693152666091919, D Loss: 1.3860671520233154\n",
            "Epoch 4, Batch 241, G Loss: 0.6932291984558105, D Loss: 1.3860208988189697\n",
            "Epoch 4, Batch 242, G Loss: 0.6932846307754517, D Loss: 1.3860447406768799\n",
            "Epoch 4, Batch 243, G Loss: 0.6933623552322388, D Loss: 1.3861286640167236\n",
            "Epoch 4, Batch 244, G Loss: 0.6934263110160828, D Loss: 1.3861005306243896\n",
            "Epoch 4, Batch 245, G Loss: 0.6934961080551147, D Loss: 1.386148452758789\n",
            "Epoch 4, Batch 246, G Loss: 0.6935588121414185, D Loss: 1.3861229419708252\n",
            "Epoch 4, Batch 247, G Loss: 0.6936162114143372, D Loss: 1.3861418962478638\n",
            "Epoch 4, Batch 248, G Loss: 0.6936638355255127, D Loss: 1.3860578536987305\n",
            "Epoch 4, Batch 249, G Loss: 0.6937111616134644, D Loss: 1.3862696886062622\n",
            "Epoch 4, Batch 250, G Loss: 0.6937627792358398, D Loss: 1.3860878944396973\n",
            "Epoch 4, Batch 251, G Loss: 0.6937839984893799, D Loss: 1.3862446546554565\n",
            "Epoch 4, Batch 252, G Loss: 0.6938132643699646, D Loss: 1.3863625526428223\n",
            "Epoch 4, Batch 253, G Loss: 0.6938266158103943, D Loss: 1.386330485343933\n",
            "Epoch 4, Batch 254, G Loss: 0.6938402056694031, D Loss: 1.3862420320510864\n",
            "Epoch 4, Batch 255, G Loss: 0.6938230395317078, D Loss: 1.3861706256866455\n",
            "Epoch 4, Batch 256, G Loss: 0.6938314437866211, D Loss: 1.386176586151123\n",
            "Epoch 4, Batch 257, G Loss: 0.6938163638114929, D Loss: 1.3861315250396729\n",
            "Epoch 4, Batch 258, G Loss: 0.6938051581382751, D Loss: 1.3861303329467773\n",
            "Epoch 4, Batch 259, G Loss: 0.6938003301620483, D Loss: 1.3861548900604248\n",
            "Epoch 4, Batch 260, G Loss: 0.693793773651123, D Loss: 1.3860763311386108\n",
            "Epoch 4, Batch 261, G Loss: 0.6937906742095947, D Loss: 1.3860183954238892\n",
            "Epoch 4, Batch 262, G Loss: 0.6938012838363647, D Loss: 1.3860974311828613\n",
            "Epoch 4, Batch 263, G Loss: 0.6937926411628723, D Loss: 1.3861432075500488\n",
            "Epoch 4, Batch 264, G Loss: 0.693800151348114, D Loss: 1.386002540588379\n",
            "Epoch 4, Batch 265, G Loss: 0.6938067674636841, D Loss: 1.3860340118408203\n",
            "Epoch 4, Batch 266, G Loss: 0.6938076615333557, D Loss: 1.3861521482467651\n",
            "Epoch 4, Batch 267, G Loss: 0.6938210129737854, D Loss: 1.3860243558883667\n",
            "Epoch 4, Batch 268, G Loss: 0.6938353776931763, D Loss: 1.3862779140472412\n",
            "Epoch 4, Batch 269, G Loss: 0.693846583366394, D Loss: 1.3863515853881836\n",
            "Epoch 4, Batch 270, G Loss: 0.6938323974609375, D Loss: 1.3862636089324951\n",
            "Epoch 4, Batch 271, G Loss: 0.6938233375549316, D Loss: 1.3864624500274658\n",
            "Epoch 4, Batch 272, G Loss: 0.6937957406044006, D Loss: 1.3860489130020142\n",
            "Epoch 4, Batch 273, G Loss: 0.6937772631645203, D Loss: 1.386047601699829\n",
            "Epoch 4, Batch 274, G Loss: 0.6937627792358398, D Loss: 1.3861037492752075\n",
            "Epoch 4, Batch 275, G Loss: 0.6937472820281982, D Loss: 1.385921835899353\n",
            "Epoch 4, Batch 276, G Loss: 0.6937365531921387, D Loss: 1.3859443664550781\n",
            "Epoch 4, Batch 277, G Loss: 0.6937452554702759, D Loss: 1.3859612941741943\n",
            "Epoch 4, Batch 278, G Loss: 0.6937593817710876, D Loss: 1.3859508037567139\n",
            "Epoch 4, Batch 279, G Loss: 0.6937755942344666, D Loss: 1.3858895301818848\n",
            "Epoch 4, Batch 280, G Loss: 0.6938029527664185, D Loss: 1.3858932256698608\n",
            "Epoch 4, Batch 281, G Loss: 0.693830132484436, D Loss: 1.386221170425415\n",
            "Epoch 4, Batch 282, G Loss: 0.6938631534576416, D Loss: 1.3860878944396973\n",
            "Epoch 4, Batch 283, G Loss: 0.6938837766647339, D Loss: 1.386007308959961\n",
            "Epoch 4, Batch 284, G Loss: 0.6939025521278381, D Loss: 1.3862011432647705\n",
            "Epoch 4, Batch 285, G Loss: 0.6939186453819275, D Loss: 1.386247158050537\n",
            "Epoch 4, Batch 286, G Loss: 0.693938136100769, D Loss: 1.3860853910446167\n",
            "Epoch 4, Batch 287, G Loss: 0.6939427852630615, D Loss: 1.3859224319458008\n",
            "Epoch 4, Batch 288, G Loss: 0.6939564943313599, D Loss: 1.386106252670288\n",
            "Epoch 4, Batch 289, G Loss: 0.6939628720283508, D Loss: 1.385979413986206\n",
            "Epoch 4, Batch 290, G Loss: 0.6939799189567566, D Loss: 1.3861606121063232\n",
            "Epoch 4, Batch 291, G Loss: 0.6939801573753357, D Loss: 1.3861420154571533\n",
            "Epoch 4, Batch 292, G Loss: 0.6939870119094849, D Loss: 1.3862028121948242\n",
            "Epoch 4, Batch 293, G Loss: 0.6939839124679565, D Loss: 1.386063575744629\n",
            "Epoch 4, Batch 294, G Loss: 0.6939887404441833, D Loss: 1.3860540390014648\n",
            "Epoch 4, Batch 295, G Loss: 0.693973958492279, D Loss: 1.3860299587249756\n",
            "Epoch 4, Batch 296, G Loss: 0.6939737200737, D Loss: 1.386186122894287\n",
            "Epoch 4, Batch 297, G Loss: 0.6939619183540344, D Loss: 1.3860676288604736\n",
            "Epoch 4, Batch 298, G Loss: 0.6939579844474792, D Loss: 1.3858137130737305\n",
            "Epoch 4, Batch 299, G Loss: 0.6939622759819031, D Loss: 1.3859425783157349\n",
            "Epoch 4, Batch 300, G Loss: 0.6939597725868225, D Loss: 1.3859457969665527\n",
            "Epoch 4, Batch 301, G Loss: 0.6939755082130432, D Loss: 1.3867871761322021\n",
            "Epoch 4, Batch 302, G Loss: 0.6939490437507629, D Loss: 1.3863089084625244\n",
            "Epoch 4, Batch 303, G Loss: 0.6939295530319214, D Loss: 1.3867056369781494\n",
            "Epoch 4, Batch 304, G Loss: 0.6938830018043518, D Loss: 1.3865259885787964\n",
            "Epoch 4, Batch 305, G Loss: 0.6938258409500122, D Loss: 1.3865694999694824\n",
            "Epoch 4, Batch 306, G Loss: 0.693758487701416, D Loss: 1.3866456747055054\n",
            "Epoch 4, Batch 307, G Loss: 0.6936755776405334, D Loss: 1.3865084648132324\n",
            "Epoch 4, Batch 308, G Loss: 0.6935982704162598, D Loss: 1.3864636421203613\n",
            "Epoch 4, Batch 309, G Loss: 0.6935139894485474, D Loss: 1.3865330219268799\n",
            "Epoch 4, Batch 310, G Loss: 0.6934235095977783, D Loss: 1.38649582862854\n",
            "Epoch 4, Batch 311, G Loss: 0.6933205127716064, D Loss: 1.386336326599121\n",
            "Epoch 4, Batch 312, G Loss: 0.6932260990142822, D Loss: 1.3865556716918945\n",
            "Epoch 4, Batch 313, G Loss: 0.6931412220001221, D Loss: 1.386420726776123\n",
            "Epoch 4, Batch 314, G Loss: 0.6930465698242188, D Loss: 1.3862340450286865\n",
            "Epoch 4, Batch 315, G Loss: 0.6929612159729004, D Loss: 1.3863402605056763\n",
            "Epoch 4, Batch 316, G Loss: 0.6928888559341431, D Loss: 1.3862297534942627\n",
            "Epoch 4, Batch 317, G Loss: 0.6928245425224304, D Loss: 1.3861140012741089\n",
            "Epoch 4, Batch 318, G Loss: 0.692765474319458, D Loss: 1.3860564231872559\n",
            "Epoch 4, Batch 319, G Loss: 0.6927238702774048, D Loss: 1.3863296508789062\n",
            "Epoch 4, Batch 320, G Loss: 0.6926974654197693, D Loss: 1.3860883712768555\n",
            "Epoch 4, Batch 321, G Loss: 0.6926655173301697, D Loss: 1.386348009109497\n",
            "Epoch 4, Batch 322, G Loss: 0.6926398873329163, D Loss: 1.3862253427505493\n",
            "Epoch 4, Batch 323, G Loss: 0.6926218271255493, D Loss: 1.3864099979400635\n",
            "Epoch 4, Batch 324, G Loss: 0.6925923824310303, D Loss: 1.3862812519073486\n",
            "Epoch 4, Batch 325, G Loss: 0.6925715208053589, D Loss: 1.3860949277877808\n",
            "Epoch 4, Batch 326, G Loss: 0.6925591230392456, D Loss: 1.3862409591674805\n",
            "Epoch 4, Batch 327, G Loss: 0.6925615668296814, D Loss: 1.3861315250396729\n",
            "Epoch 4, Batch 328, G Loss: 0.6925572752952576, D Loss: 1.386394739151001\n",
            "Epoch 4, Batch 329, G Loss: 0.6925536394119263, D Loss: 1.3861254453659058\n",
            "Epoch 4, Batch 330, G Loss: 0.6925506591796875, D Loss: 1.3859167098999023\n",
            "Epoch 4, Batch 331, G Loss: 0.6925624012947083, D Loss: 1.3861143589019775\n",
            "Epoch 4, Batch 332, G Loss: 0.6925894021987915, D Loss: 1.3861544132232666\n",
            "Epoch 4, Batch 333, G Loss: 0.6926133036613464, D Loss: 1.3863046169281006\n",
            "Epoch 4, Batch 334, G Loss: 0.6926447153091431, D Loss: 1.386156678199768\n",
            "Epoch 4, Batch 335, G Loss: 0.6926587224006653, D Loss: 1.3862926959991455\n",
            "Epoch 4, Batch 336, G Loss: 0.6926839351654053, D Loss: 1.3862954378128052\n",
            "Epoch 4, Batch 337, G Loss: 0.6926963329315186, D Loss: 1.3864316940307617\n",
            "Epoch 4, Batch 338, G Loss: 0.6927032470703125, D Loss: 1.385859727859497\n",
            "Epoch 4, Batch 339, G Loss: 0.6927273869514465, D Loss: 1.3860174417495728\n",
            "Epoch 4, Batch 340, G Loss: 0.6927496194839478, D Loss: 1.3858121633529663\n",
            "Epoch 4, Batch 341, G Loss: 0.6927988529205322, D Loss: 1.3866398334503174\n",
            "Epoch 4, Batch 342, G Loss: 0.6928234696388245, D Loss: 1.3865268230438232\n",
            "Epoch 4, Batch 343, G Loss: 0.6928346753120422, D Loss: 1.3866032361984253\n",
            "Epoch 4, Batch 344, G Loss: 0.6928250193595886, D Loss: 1.3865132331848145\n",
            "Epoch 4, Batch 345, G Loss: 0.6928052306175232, D Loss: 1.3862416744232178\n",
            "Epoch 4, Batch 346, G Loss: 0.692793071269989, D Loss: 1.3861262798309326\n",
            "Epoch 4, Batch 347, G Loss: 0.692771852016449, D Loss: 1.3861639499664307\n",
            "Epoch 4, Batch 348, G Loss: 0.6927669048309326, D Loss: 1.3862115144729614\n",
            "Epoch 4, Batch 349, G Loss: 0.6927610635757446, D Loss: 1.3860794305801392\n",
            "Epoch 4, Batch 350, G Loss: 0.6927746534347534, D Loss: 1.3861894607543945\n",
            "Epoch 4, Batch 351, G Loss: 0.6927791237831116, D Loss: 1.3861749172210693\n",
            "Epoch 4, Batch 352, G Loss: 0.6927878856658936, D Loss: 1.3861196041107178\n",
            "Epoch 4, Batch 353, G Loss: 0.6928019523620605, D Loss: 1.3862686157226562\n",
            "Epoch 4, Batch 354, G Loss: 0.6928167343139648, D Loss: 1.3863227367401123\n",
            "Epoch 4, Batch 355, G Loss: 0.6928220391273499, D Loss: 1.3865814208984375\n",
            "Epoch 4, Batch 356, G Loss: 0.6928151845932007, D Loss: 1.3865065574645996\n",
            "Epoch 4, Batch 357, G Loss: 0.6927927136421204, D Loss: 1.3863085508346558\n",
            "Epoch 4, Batch 358, G Loss: 0.6927695274353027, D Loss: 1.386400580406189\n",
            "Epoch 4, Batch 359, G Loss: 0.6927413940429688, D Loss: 1.3864415884017944\n",
            "Epoch 4, Batch 360, G Loss: 0.6927127242088318, D Loss: 1.3863601684570312\n",
            "Epoch 4, Batch 361, G Loss: 0.6926669478416443, D Loss: 1.3865208625793457\n",
            "Epoch 4, Batch 362, G Loss: 0.6926168203353882, D Loss: 1.3863199949264526\n",
            "Epoch 4, Batch 363, G Loss: 0.6925721168518066, D Loss: 1.386223316192627\n",
            "Epoch 4, Batch 364, G Loss: 0.6925323009490967, D Loss: 1.3862563371658325\n",
            "Epoch 4, Batch 365, G Loss: 0.6924997568130493, D Loss: 1.386286735534668\n",
            "Epoch 4, Batch 366, G Loss: 0.6924713253974915, D Loss: 1.3862760066986084\n",
            "Epoch 4, Batch 367, G Loss: 0.6924469470977783, D Loss: 1.3861956596374512\n",
            "Epoch 4, Batch 368, G Loss: 0.6924290657043457, D Loss: 1.3861544132232666\n",
            "Epoch 4, Batch 369, G Loss: 0.6924155354499817, D Loss: 1.3861424922943115\n",
            "Epoch 4, Batch 370, G Loss: 0.6924314498901367, D Loss: 1.3860533237457275\n",
            "Epoch 4, Batch 371, G Loss: 0.692449152469635, D Loss: 1.3860737085342407\n",
            "Epoch 4, Batch 372, G Loss: 0.6924822926521301, D Loss: 1.3862487077713013\n",
            "Epoch 4, Batch 373, G Loss: 0.6925084590911865, D Loss: 1.3862403631210327\n",
            "Epoch 4, Batch 374, G Loss: 0.6925389766693115, D Loss: 1.3861665725708008\n",
            "Epoch 4, Batch 375, G Loss: 0.6925687193870544, D Loss: 1.3862178325653076\n",
            "Epoch 4, Batch 376, G Loss: 0.6926015615463257, D Loss: 1.3862403631210327\n",
            "Epoch 4, Batch 377, G Loss: 0.6926386952400208, D Loss: 1.3862314224243164\n",
            "Epoch 4, Batch 378, G Loss: 0.6926799416542053, D Loss: 1.386151909828186\n",
            "Epoch 4, Batch 379, G Loss: 0.692700207233429, D Loss: 1.386179804801941\n",
            "Epoch 4, Batch 380, G Loss: 0.69273841381073, D Loss: 1.3861950635910034\n",
            "Epoch 4, Batch 381, G Loss: 0.6927703022956848, D Loss: 1.386084794998169\n",
            "Epoch 4, Batch 382, G Loss: 0.6928148865699768, D Loss: 1.3863203525543213\n",
            "Epoch 4, Batch 383, G Loss: 0.6928433775901794, D Loss: 1.386253833770752\n",
            "Epoch 4, Batch 384, G Loss: 0.6928781867027283, D Loss: 1.3861587047576904\n",
            "Epoch 4, Batch 385, G Loss: 0.69290691614151, D Loss: 1.3862571716308594\n",
            "Epoch 4, Batch 386, G Loss: 0.6929273009300232, D Loss: 1.3863322734832764\n",
            "Epoch 4, Batch 387, G Loss: 0.6929431557655334, D Loss: 1.3862254619598389\n",
            "Epoch 4, Batch 388, G Loss: 0.6929565668106079, D Loss: 1.3861942291259766\n",
            "Epoch 4, Batch 389, G Loss: 0.6929670572280884, D Loss: 1.3861591815948486\n",
            "Epoch 4, Batch 390, G Loss: 0.6929898858070374, D Loss: 1.385951042175293\n",
            "Epoch 4, Batch 391, G Loss: 0.6930257678031921, D Loss: 1.3861445188522339\n",
            "Epoch 4, Batch 392, G Loss: 0.6930626034736633, D Loss: 1.3861180543899536\n",
            "Epoch 4, Batch 393, G Loss: 0.6931055188179016, D Loss: 1.3861393928527832\n",
            "Epoch 4, Batch 394, G Loss: 0.6931434869766235, D Loss: 1.3861753940582275\n",
            "Epoch 4, Batch 395, G Loss: 0.6931926012039185, D Loss: 1.3861212730407715\n",
            "Epoch 4, Batch 396, G Loss: 0.6932270526885986, D Loss: 1.3865883350372314\n",
            "Epoch 4, Batch 397, G Loss: 0.6932424306869507, D Loss: 1.3864827156066895\n",
            "Epoch 4, Batch 398, G Loss: 0.6932293772697449, D Loss: 1.386347770690918\n",
            "Epoch 4, Batch 399, G Loss: 0.6932092905044556, D Loss: 1.3863558769226074\n",
            "Epoch 4, Batch 400, G Loss: 0.6931782960891724, D Loss: 1.3861076831817627\n",
            "Epoch 4, Batch 401, G Loss: 0.6931642889976501, D Loss: 1.3862099647521973\n",
            "Epoch 4, Batch 402, G Loss: 0.6931495666503906, D Loss: 1.3862521648406982\n",
            "Epoch 4, Batch 403, G Loss: 0.6931360960006714, D Loss: 1.386091947555542\n",
            "Epoch 4, Batch 404, G Loss: 0.6931208372116089, D Loss: 1.3859916925430298\n",
            "Epoch 4, Batch 405, G Loss: 0.6931369304656982, D Loss: 1.385986566543579\n",
            "Epoch 4, Batch 406, G Loss: 0.6931586861610413, D Loss: 1.3860057592391968\n",
            "Epoch 4, Batch 407, G Loss: 0.6931966543197632, D Loss: 1.3863060474395752\n",
            "Epoch 4, Batch 408, G Loss: 0.6932289600372314, D Loss: 1.386214256286621\n",
            "Epoch 4, Batch 409, G Loss: 0.6932511329650879, D Loss: 1.3861589431762695\n",
            "Epoch 4, Batch 410, G Loss: 0.6932804584503174, D Loss: 1.3862051963806152\n",
            "Epoch 4, Batch 411, G Loss: 0.6933081746101379, D Loss: 1.3860880136489868\n",
            "Epoch 4, Batch 412, G Loss: 0.693332850933075, D Loss: 1.386096715927124\n",
            "Epoch 4, Batch 413, G Loss: 0.6933722496032715, D Loss: 1.386091709136963\n",
            "Epoch 4, Batch 414, G Loss: 0.6933987140655518, D Loss: 1.3860470056533813\n",
            "Epoch 4, Batch 415, G Loss: 0.6934530138969421, D Loss: 1.385995864868164\n",
            "Epoch 4, Batch 416, G Loss: 0.6934962868690491, D Loss: 1.385977864265442\n",
            "Epoch 4, Batch 417, G Loss: 0.6935518383979797, D Loss: 1.386213779449463\n",
            "Epoch 4, Batch 418, G Loss: 0.6935939192771912, D Loss: 1.3862191438674927\n",
            "Epoch 4, Batch 419, G Loss: 0.6936460137367249, D Loss: 1.3862900733947754\n",
            "Epoch 4, Batch 420, G Loss: 0.6936663389205933, D Loss: 1.386260747909546\n",
            "Epoch 4, Batch 421, G Loss: 0.6936826109886169, D Loss: 1.386141061782837\n",
            "Epoch 4, Batch 422, G Loss: 0.6936909556388855, D Loss: 1.3860573768615723\n",
            "Epoch 4, Batch 423, G Loss: 0.6937099099159241, D Loss: 1.3861219882965088\n",
            "Epoch 4, Batch 424, G Loss: 0.6937249898910522, D Loss: 1.3864381313323975\n",
            "Epoch 4, Batch 425, G Loss: 0.6937319040298462, D Loss: 1.3863738775253296\n",
            "Epoch 4, Batch 426, G Loss: 0.6937134265899658, D Loss: 1.3862961530685425\n",
            "Epoch 4, Batch 427, G Loss: 0.6936858892440796, D Loss: 1.3863195180892944\n",
            "Epoch 4, Batch 428, G Loss: 0.6936611533164978, D Loss: 1.3861916065216064\n",
            "Epoch 4, Batch 429, G Loss: 0.6936312913894653, D Loss: 1.3862775564193726\n",
            "Epoch 4, Batch 430, G Loss: 0.6935906410217285, D Loss: 1.386056661605835\n",
            "Epoch 4, Batch 431, G Loss: 0.6935580372810364, D Loss: 1.3860700130462646\n",
            "Epoch 4, Batch 432, G Loss: 0.6935331225395203, D Loss: 1.386178731918335\n",
            "Epoch 4, Batch 433, G Loss: 0.693524956703186, D Loss: 1.386031150817871\n",
            "Epoch 4, Batch 434, G Loss: 0.6935149431228638, D Loss: 1.3860434293746948\n",
            "Epoch 4, Batch 435, G Loss: 0.6935250163078308, D Loss: 1.3859539031982422\n",
            "Epoch 4, Batch 436, G Loss: 0.6935275197029114, D Loss: 1.3861322402954102\n",
            "Epoch 4, Batch 437, G Loss: 0.6935369372367859, D Loss: 1.3864054679870605\n",
            "Epoch 4, Batch 438, G Loss: 0.6935261487960815, D Loss: 1.3863351345062256\n",
            "Epoch 4, Batch 439, G Loss: 0.6935216188430786, D Loss: 1.386353611946106\n",
            "Epoch 4, Batch 440, G Loss: 0.6934968829154968, D Loss: 1.3863646984100342\n",
            "Epoch 4, Batch 441, G Loss: 0.693467915058136, D Loss: 1.3864576816558838\n",
            "Epoch 4, Batch 442, G Loss: 0.6934303641319275, D Loss: 1.3864033222198486\n",
            "Epoch 4, Batch 443, G Loss: 0.6933677792549133, D Loss: 1.3863351345062256\n",
            "Epoch 4, Batch 444, G Loss: 0.693322479724884, D Loss: 1.3864593505859375\n",
            "Epoch 4, Batch 445, G Loss: 0.6932500600814819, D Loss: 1.3864258527755737\n",
            "Epoch 4, Batch 446, G Loss: 0.693175733089447, D Loss: 1.3863368034362793\n",
            "Epoch 4, Batch 447, G Loss: 0.6931012272834778, D Loss: 1.3863554000854492\n",
            "Epoch 4, Batch 448, G Loss: 0.6930305361747742, D Loss: 1.3862226009368896\n",
            "Epoch 4, Batch 449, G Loss: 0.692960262298584, D Loss: 1.3861992359161377\n",
            "Epoch 4, Batch 450, G Loss: 0.6929081678390503, D Loss: 1.3862261772155762\n",
            "Epoch 4, Batch 451, G Loss: 0.6928542256355286, D Loss: 1.38618803024292\n",
            "Epoch 4, Batch 452, G Loss: 0.6928125619888306, D Loss: 1.386368989944458\n",
            "Epoch 4, Batch 453, G Loss: 0.6927681565284729, D Loss: 1.3862769603729248\n",
            "Epoch 4, Batch 454, G Loss: 0.6927192211151123, D Loss: 1.386281967163086\n",
            "Epoch 4, Batch 455, G Loss: 0.6926805377006531, D Loss: 1.3860392570495605\n",
            "Epoch 4, Batch 456, G Loss: 0.692659318447113, D Loss: 1.3860790729522705\n",
            "Epoch 4, Batch 457, G Loss: 0.6926508545875549, D Loss: 1.3861150741577148\n",
            "Epoch 4, Batch 458, G Loss: 0.6926557421684265, D Loss: 1.3864027261734009\n",
            "Epoch 4, Batch 459, G Loss: 0.6926534175872803, D Loss: 1.3863754272460938\n",
            "Epoch 4, Batch 460, G Loss: 0.6926259994506836, D Loss: 1.3864010572433472\n",
            "Epoch 4, Batch 461, G Loss: 0.69260174036026, D Loss: 1.3863489627838135\n",
            "Epoch 4, Batch 462, G Loss: 0.6925745606422424, D Loss: 1.3861321210861206\n",
            "Epoch 4, Batch 463, G Loss: 0.692553699016571, D Loss: 1.3860993385314941\n",
            "Epoch 4, Batch 464, G Loss: 0.6925528645515442, D Loss: 1.3861503601074219\n",
            "Epoch 4, Batch 465, G Loss: 0.6925646662712097, D Loss: 1.386063814163208\n",
            "Epoch 4, Batch 466, G Loss: 0.6925861835479736, D Loss: 1.3860650062561035\n",
            "Epoch 4, Batch 467, G Loss: 0.6926237344741821, D Loss: 1.3860656023025513\n",
            "Epoch 4, Batch 468, G Loss: 0.692679762840271, D Loss: 1.3860739469528198\n",
            "Epoch 4, Batch 469, G Loss: 0.6927402019500732, D Loss: 1.3859885931015015\n",
            "Epoch 4, Batch 470, G Loss: 0.6928138136863708, D Loss: 1.3861958980560303\n",
            "Epoch 4, Batch 471, G Loss: 0.6928890943527222, D Loss: 1.3861545324325562\n",
            "Epoch 4, Batch 472, G Loss: 0.6929597854614258, D Loss: 1.3861517906188965\n",
            "Epoch 4, Batch 473, G Loss: 0.6930239796638489, D Loss: 1.3861368894577026\n",
            "Epoch 4, Batch 474, G Loss: 0.693091094493866, D Loss: 1.3861390352249146\n",
            "Epoch 4, Batch 475, G Loss: 0.6931605339050293, D Loss: 1.3862550258636475\n",
            "Epoch 4, Batch 476, G Loss: 0.6932144165039062, D Loss: 1.3862824440002441\n",
            "Epoch 4, Batch 477, G Loss: 0.693256676197052, D Loss: 1.386256217956543\n",
            "Epoch 4, Batch 478, G Loss: 0.6932881474494934, D Loss: 1.3861640691757202\n",
            "Epoch 4, Batch 479, G Loss: 0.6933196187019348, D Loss: 1.385878562927246\n",
            "Epoch 4, Batch 480, G Loss: 0.6933761835098267, D Loss: 1.3859500885009766\n",
            "Epoch 4, Batch 481, G Loss: 0.6934208869934082, D Loss: 1.3859466314315796\n",
            "Epoch 4, Batch 482, G Loss: 0.6934964656829834, D Loss: 1.386229395866394\n",
            "Epoch 4, Batch 483, G Loss: 0.6935625672340393, D Loss: 1.386164903640747\n",
            "Epoch 4, Batch 484, G Loss: 0.693603515625, D Loss: 1.386073350906372\n",
            "Epoch 4, Batch 485, G Loss: 0.6936548948287964, D Loss: 1.3860905170440674\n",
            "Epoch 4, Batch 486, G Loss: 0.6937087178230286, D Loss: 1.3861055374145508\n",
            "Epoch 4, Batch 487, G Loss: 0.6937565803527832, D Loss: 1.3860716819763184\n",
            "Epoch 4, Batch 488, G Loss: 0.6938038468360901, D Loss: 1.3860666751861572\n",
            "Epoch 4, Batch 489, G Loss: 0.6938414573669434, D Loss: 1.3860783576965332\n",
            "Epoch 4, Batch 490, G Loss: 0.6938815116882324, D Loss: 1.3860735893249512\n",
            "Epoch 4, Batch 491, G Loss: 0.6939142346382141, D Loss: 1.3860161304473877\n",
            "Epoch 4, Batch 492, G Loss: 0.6939554214477539, D Loss: 1.3860931396484375\n",
            "Epoch 4, Batch 493, G Loss: 0.6939957141876221, D Loss: 1.3859162330627441\n",
            "Epoch 4, Batch 494, G Loss: 0.6940297484397888, D Loss: 1.3859331607818604\n",
            "Epoch 4, Batch 495, G Loss: 0.6940737962722778, D Loss: 1.3859362602233887\n",
            "Epoch 4, Batch 496, G Loss: 0.6941164135932922, D Loss: 1.385977029800415\n",
            "Epoch 4, Batch 497, G Loss: 0.6941654682159424, D Loss: 1.386070966720581\n",
            "Epoch 4, Batch 498, G Loss: 0.6942105889320374, D Loss: 1.3860893249511719\n",
            "Epoch 4, Batch 499, G Loss: 0.6942331790924072, D Loss: 1.3860106468200684\n",
            "Epoch 4, Batch 500, G Loss: 0.6942697763442993, D Loss: 1.386217713356018\n",
            "Epoch 4, Batch 501, G Loss: 0.6942933797836304, D Loss: 1.3863446712493896\n",
            "Epoch 4, Batch 502, G Loss: 0.6942962408065796, D Loss: 1.3863368034362793\n",
            "Epoch 4, Batch 503, G Loss: 0.694275438785553, D Loss: 1.3862385749816895\n",
            "Epoch 4, Batch 504, G Loss: 0.6942606568336487, D Loss: 1.386139154434204\n",
            "Epoch 4, Batch 505, G Loss: 0.6942405104637146, D Loss: 1.3864243030548096\n",
            "Epoch 4, Batch 506, G Loss: 0.6941955089569092, D Loss: 1.3864059448242188\n",
            "Epoch 4, Batch 507, G Loss: 0.6941511034965515, D Loss: 1.3866167068481445\n",
            "Epoch 4, Batch 508, G Loss: 0.6940715312957764, D Loss: 1.3864271640777588\n",
            "Epoch 4, Batch 509, G Loss: 0.6939932107925415, D Loss: 1.3861067295074463\n",
            "Epoch 4, Batch 510, G Loss: 0.6939247250556946, D Loss: 1.386162519454956\n",
            "Epoch 4, Batch 511, G Loss: 0.6938565373420715, D Loss: 1.3860552310943604\n",
            "Epoch 4, Batch 512, G Loss: 0.6937854290008545, D Loss: 1.3858909606933594\n",
            "Epoch 4, Batch 513, G Loss: 0.6937461495399475, D Loss: 1.3860869407653809\n",
            "Epoch 4, Batch 514, G Loss: 0.693708598613739, D Loss: 1.3860623836517334\n",
            "Epoch 4, Batch 515, G Loss: 0.6936703324317932, D Loss: 1.3862810134887695\n",
            "Epoch 4, Batch 516, G Loss: 0.6936325430870056, D Loss: 1.386425495147705\n",
            "Epoch 4, Batch 517, G Loss: 0.6935892105102539, D Loss: 1.386455774307251\n",
            "Epoch 4, Batch 518, G Loss: 0.6935381889343262, D Loss: 1.386232614517212\n",
            "Epoch 4, Batch 519, G Loss: 0.6934764385223389, D Loss: 1.3863191604614258\n",
            "Epoch 4, Batch 520, G Loss: 0.693415105342865, D Loss: 1.386289358139038\n",
            "Epoch 4, Batch 521, G Loss: 0.6933619976043701, D Loss: 1.3862589597702026\n",
            "Epoch 4, Batch 522, G Loss: 0.6932958364486694, D Loss: 1.386082410812378\n",
            "Epoch 4, Batch 523, G Loss: 0.6932603716850281, D Loss: 1.385977864265442\n",
            "Epoch 4, Batch 524, G Loss: 0.6932206153869629, D Loss: 1.3860197067260742\n",
            "Epoch 4, Batch 525, G Loss: 0.6932027339935303, D Loss: 1.3858726024627686\n",
            "Epoch 4, Batch 526, G Loss: 0.6932012438774109, D Loss: 1.3858757019042969\n",
            "Epoch 4, Batch 527, G Loss: 0.6932176351547241, D Loss: 1.3860630989074707\n",
            "Epoch 4, Batch 528, G Loss: 0.6932283639907837, D Loss: 1.3863120079040527\n",
            "Epoch 4, Batch 529, G Loss: 0.6932346820831299, D Loss: 1.386518955230713\n",
            "Epoch 4, Batch 530, G Loss: 0.6932226419448853, D Loss: 1.386291742324829\n",
            "Epoch 4, Batch 531, G Loss: 0.6932197213172913, D Loss: 1.3861973285675049\n",
            "Epoch 4, Batch 532, G Loss: 0.6932063102722168, D Loss: 1.3858636617660522\n",
            "Epoch 4, Batch 533, G Loss: 0.6932077407836914, D Loss: 1.3861651420593262\n",
            "Epoch 4, Batch 534, G Loss: 0.6932180523872375, D Loss: 1.3862210512161255\n",
            "Epoch 4, Batch 535, G Loss: 0.693217396736145, D Loss: 1.3860862255096436\n",
            "Epoch 4, Batch 536, G Loss: 0.6932175159454346, D Loss: 1.3860021829605103\n",
            "Epoch 4, Batch 537, G Loss: 0.6932306885719299, D Loss: 1.3862742185592651\n",
            "Epoch 4, Batch 538, G Loss: 0.693243682384491, D Loss: 1.3863577842712402\n",
            "Epoch 4, Batch 539, G Loss: 0.6932382583618164, D Loss: 1.3862762451171875\n",
            "Epoch 4, Batch 540, G Loss: 0.6932250261306763, D Loss: 1.3862998485565186\n",
            "Epoch 4, Batch 541, G Loss: 0.6932123303413391, D Loss: 1.3862873315811157\n",
            "Epoch 4, Batch 542, G Loss: 0.6931957602500916, D Loss: 1.3862388134002686\n",
            "Epoch 4, Batch 543, G Loss: 0.693180501461029, D Loss: 1.3862555027008057\n",
            "Epoch 4, Batch 544, G Loss: 0.6931558847427368, D Loss: 1.3862662315368652\n",
            "Epoch 4, Batch 545, G Loss: 0.6931338906288147, D Loss: 1.3862484693527222\n",
            "Epoch 4, Batch 546, G Loss: 0.6931094527244568, D Loss: 1.3862242698669434\n",
            "Epoch 4, Batch 547, G Loss: 0.6930849552154541, D Loss: 1.3862264156341553\n",
            "Epoch 4, Batch 548, G Loss: 0.6930601000785828, D Loss: 1.3864585161209106\n",
            "Epoch 4, Batch 549, G Loss: 0.6930359601974487, D Loss: 1.386204719543457\n",
            "Epoch 4, Batch 550, G Loss: 0.6930054426193237, D Loss: 1.3859986066818237\n",
            "Epoch 4, Batch 551, G Loss: 0.6929863095283508, D Loss: 1.3860437870025635\n",
            "Epoch 4, Batch 552, G Loss: 0.6929808855056763, D Loss: 1.386035680770874\n",
            "Epoch 4, Batch 553, G Loss: 0.6929789781570435, D Loss: 1.3862844705581665\n",
            "Epoch 4, Batch 554, G Loss: 0.6929802298545837, D Loss: 1.3863344192504883\n",
            "Epoch 4, Batch 555, G Loss: 0.692982017993927, D Loss: 1.3863905668258667\n",
            "Epoch 4, Batch 556, G Loss: 0.6929590106010437, D Loss: 1.3861346244812012\n",
            "Epoch 4, Batch 557, G Loss: 0.6929492354393005, D Loss: 1.386075496673584\n",
            "Epoch 4, Batch 558, G Loss: 0.6929458379745483, D Loss: 1.3862733840942383\n",
            "Epoch 4, Batch 559, G Loss: 0.6929417252540588, D Loss: 1.386154294013977\n",
            "Epoch 4, Batch 560, G Loss: 0.6929466724395752, D Loss: 1.386177897453308\n",
            "Epoch 4, Batch 561, G Loss: 0.6929451823234558, D Loss: 1.3860158920288086\n",
            "Epoch 4, Batch 562, G Loss: 0.6929610967636108, D Loss: 1.3859013319015503\n",
            "Epoch 4, Batch 563, G Loss: 0.6929873824119568, D Loss: 1.3858816623687744\n",
            "Epoch 4, Batch 564, G Loss: 0.6930240988731384, D Loss: 1.3857297897338867\n",
            "Epoch 4, Batch 565, G Loss: 0.6930892467498779, D Loss: 1.386298656463623\n",
            "Epoch 4, Batch 566, G Loss: 0.6931322813034058, D Loss: 1.386791706085205\n",
            "Epoch 4, Batch 567, G Loss: 0.6931479573249817, D Loss: 1.3865187168121338\n",
            "Epoch 4, Batch 568, G Loss: 0.693143904209137, D Loss: 1.3865786790847778\n",
            "Epoch 4, Batch 569, G Loss: 0.693123459815979, D Loss: 1.386112093925476\n",
            "Epoch 4, Batch 570, G Loss: 0.6931030750274658, D Loss: 1.3862030506134033\n",
            "Epoch 4, Batch 571, G Loss: 0.6930837035179138, D Loss: 1.3860973119735718\n",
            "Epoch 4, Batch 572, G Loss: 0.6930817365646362, D Loss: 1.3867123126983643\n",
            "Epoch 4, Batch 573, G Loss: 0.6930442452430725, D Loss: 1.3865795135498047\n",
            "Epoch 4, Batch 574, G Loss: 0.6929901838302612, D Loss: 1.3867413997650146\n",
            "Epoch 4, Batch 575, G Loss: 0.6929188370704651, D Loss: 1.386626958847046\n",
            "Epoch 4, Batch 576, G Loss: 0.6928275227546692, D Loss: 1.3863078355789185\n",
            "Epoch 4, Batch 577, G Loss: 0.6927457451820374, D Loss: 1.386305570602417\n",
            "Epoch 4, Batch 578, G Loss: 0.6926690936088562, D Loss: 1.386163353919983\n",
            "Epoch 4, Batch 579, G Loss: 0.6926066875457764, D Loss: 1.3861066102981567\n",
            "Epoch 4, Batch 580, G Loss: 0.6925644874572754, D Loss: 1.386486530303955\n",
            "Epoch 4, Batch 581, G Loss: 0.692503809928894, D Loss: 1.3864259719848633\n",
            "Epoch 4, Batch 582, G Loss: 0.6924505233764648, D Loss: 1.386445164680481\n",
            "Epoch 4, Batch 583, G Loss: 0.6923906803131104, D Loss: 1.3864331245422363\n",
            "Epoch 4, Batch 584, G Loss: 0.6923227906227112, D Loss: 1.3862545490264893\n",
            "Epoch 4, Batch 585, G Loss: 0.6922666430473328, D Loss: 1.3862171173095703\n",
            "Epoch 4, Batch 586, G Loss: 0.6922240257263184, D Loss: 1.3861579895019531\n",
            "Epoch 4, Batch 587, G Loss: 0.6921928524971008, D Loss: 1.3860950469970703\n",
            "Epoch 4, Batch 588, G Loss: 0.6921819448471069, D Loss: 1.3860924243927002\n",
            "Epoch 4, Batch 589, G Loss: 0.692191481590271, D Loss: 1.386063814163208\n",
            "Epoch 4, Batch 590, G Loss: 0.6922139525413513, D Loss: 1.386286973953247\n",
            "Epoch 4, Batch 591, G Loss: 0.6922330260276794, D Loss: 1.3862979412078857\n",
            "Epoch 4, Batch 592, G Loss: 0.6922481060028076, D Loss: 1.386246681213379\n",
            "Epoch 4, Batch 593, G Loss: 0.6922695636749268, D Loss: 1.3861687183380127\n",
            "Epoch 4, Batch 594, G Loss: 0.6922797560691833, D Loss: 1.3861591815948486\n",
            "Epoch 4, Batch 595, G Loss: 0.6923130750656128, D Loss: 1.3860645294189453\n",
            "Epoch 4, Batch 596, G Loss: 0.6923524737358093, D Loss: 1.3862546682357788\n",
            "Epoch 4, Batch 597, G Loss: 0.6923902034759521, D Loss: 1.386258840560913\n",
            "Epoch 4, Batch 598, G Loss: 0.6924226880073547, D Loss: 1.3862030506134033\n",
            "Epoch 4, Batch 599, G Loss: 0.6924516558647156, D Loss: 1.3861792087554932\n",
            "Epoch 4, Batch 600, G Loss: 0.6924969553947449, D Loss: 1.386120080947876\n",
            "Epoch 4, Batch 601, G Loss: 0.6925391554832458, D Loss: 1.3861414194107056\n",
            "Epoch 4, Batch 602, G Loss: 0.6925898790359497, D Loss: 1.386096477508545\n",
            "Epoch 4, Batch 603, G Loss: 0.6926453113555908, D Loss: 1.38618803024292\n",
            "Epoch 4, Batch 604, G Loss: 0.6927019953727722, D Loss: 1.3863285779953003\n",
            "Epoch 4, Batch 605, G Loss: 0.6927379965782166, D Loss: 1.386167049407959\n",
            "Epoch 4, Batch 606, G Loss: 0.692775309085846, D Loss: 1.3861913681030273\n",
            "Epoch 4, Batch 607, G Loss: 0.6928219199180603, D Loss: 1.3861885070800781\n",
            "Epoch 4, Batch 608, G Loss: 0.6928457021713257, D Loss: 1.3862477540969849\n",
            "Epoch 4, Batch 609, G Loss: 0.6928776502609253, D Loss: 1.3861579895019531\n",
            "Epoch 4, Batch 610, G Loss: 0.6929138898849487, D Loss: 1.3861074447631836\n",
            "Epoch 4, Batch 611, G Loss: 0.6929447650909424, D Loss: 1.3862383365631104\n",
            "Epoch 4, Batch 612, G Loss: 0.6929839849472046, D Loss: 1.3863167762756348\n",
            "Epoch 4, Batch 613, G Loss: 0.6930007934570312, D Loss: 1.3863070011138916\n",
            "Epoch 4, Batch 614, G Loss: 0.6930036544799805, D Loss: 1.3862695693969727\n",
            "Epoch 4, Batch 615, G Loss: 0.6930000185966492, D Loss: 1.386242151260376\n",
            "Epoch 4, Batch 616, G Loss: 0.692991316318512, D Loss: 1.3862547874450684\n",
            "Epoch 4, Batch 617, G Loss: 0.692981481552124, D Loss: 1.3862818479537964\n",
            "Epoch 4, Batch 618, G Loss: 0.6929590702056885, D Loss: 1.38620924949646\n",
            "Epoch 4, Batch 619, G Loss: 0.6929402351379395, D Loss: 1.3862507343292236\n",
            "Epoch 4, Batch 620, G Loss: 0.6929229497909546, D Loss: 1.3862491846084595\n",
            "Epoch 4, Batch 621, G Loss: 0.6929066777229309, D Loss: 1.3861517906188965\n",
            "Epoch 4, Batch 622, G Loss: 0.6928901076316833, D Loss: 1.3860368728637695\n",
            "Epoch 4, Batch 623, G Loss: 0.6929042339324951, D Loss: 1.3861442804336548\n",
            "Epoch 4, Batch 624, G Loss: 0.6929234266281128, D Loss: 1.3860986232757568\n",
            "Epoch 4, Batch 625, G Loss: 0.6929467916488647, D Loss: 1.386217713356018\n",
            "Epoch 4, Batch 626, G Loss: 0.6929791569709778, D Loss: 1.3861875534057617\n",
            "Epoch 4, Batch 627, G Loss: 0.6930036544799805, D Loss: 1.3862183094024658\n",
            "Epoch 4, Batch 628, G Loss: 0.6930260062217712, D Loss: 1.386218547821045\n",
            "Epoch 4, Batch 629, G Loss: 0.6930499076843262, D Loss: 1.3861846923828125\n",
            "Epoch 4, Batch 630, G Loss: 0.6930551528930664, D Loss: 1.386138916015625\n",
            "Epoch 4, Batch 631, G Loss: 0.6930816769599915, D Loss: 1.3861346244812012\n",
            "Epoch 4, Batch 632, G Loss: 0.693111002445221, D Loss: 1.386274814605713\n",
            "Epoch 4, Batch 633, G Loss: 0.6931304931640625, D Loss: 1.3862972259521484\n",
            "Epoch 4, Batch 634, G Loss: 0.6931279897689819, D Loss: 1.3862751722335815\n",
            "Epoch 4, Batch 635, G Loss: 0.6931213736534119, D Loss: 1.3862190246582031\n",
            "Epoch 4, Batch 636, G Loss: 0.6931048035621643, D Loss: 1.3861513137817383\n",
            "Epoch 4, Batch 637, G Loss: 0.6930987238883972, D Loss: 1.3861613273620605\n",
            "Epoch 4, Batch 638, G Loss: 0.6930999755859375, D Loss: 1.3861150741577148\n",
            "Epoch 4, Batch 639, G Loss: 0.6931174993515015, D Loss: 1.386183738708496\n",
            "Epoch 4, Batch 640, G Loss: 0.6931239366531372, D Loss: 1.386134147644043\n",
            "Epoch 4, Batch 641, G Loss: 0.6931472420692444, D Loss: 1.386146068572998\n",
            "Epoch 4, Batch 642, G Loss: 0.6931674480438232, D Loss: 1.3861539363861084\n",
            "Epoch 4, Batch 643, G Loss: 0.6931933760643005, D Loss: 1.3861602544784546\n",
            "Epoch 4, Batch 644, G Loss: 0.6932242512702942, D Loss: 1.3862131834030151\n",
            "Epoch 4, Batch 645, G Loss: 0.6932435035705566, D Loss: 1.3861463069915771\n",
            "Epoch 4, Batch 646, G Loss: 0.693265974521637, D Loss: 1.3862202167510986\n",
            "Epoch 4, Batch 647, G Loss: 0.6932864785194397, D Loss: 1.3862248659133911\n",
            "Epoch 4, Batch 648, G Loss: 0.693295955657959, D Loss: 1.3861980438232422\n",
            "Epoch 4, Batch 649, G Loss: 0.693297266960144, D Loss: 1.3861167430877686\n",
            "Epoch 4, Batch 650, G Loss: 0.6933207511901855, D Loss: 1.386218786239624\n",
            "Epoch 4, Batch 651, G Loss: 0.6933260560035706, D Loss: 1.3861579895019531\n",
            "Epoch 4, Batch 652, G Loss: 0.6933403611183167, D Loss: 1.3860255479812622\n",
            "Epoch 4, Batch 653, G Loss: 0.6933718919754028, D Loss: 1.3859825134277344\n",
            "Epoch 4, Batch 654, G Loss: 0.6934205889701843, D Loss: 1.3860750198364258\n",
            "Epoch 4, Batch 655, G Loss: 0.6934701800346375, D Loss: 1.3862342834472656\n",
            "Epoch 4, Batch 656, G Loss: 0.693513035774231, D Loss: 1.386246919631958\n",
            "Epoch 4, Batch 657, G Loss: 0.693544328212738, D Loss: 1.3863070011138916\n",
            "Epoch 4, Batch 658, G Loss: 0.6935569643974304, D Loss: 1.386304497718811\n",
            "Epoch 4, Batch 659, G Loss: 0.6935582160949707, D Loss: 1.3862500190734863\n",
            "Epoch 4, Batch 660, G Loss: 0.6935446858406067, D Loss: 1.3862379789352417\n",
            "Epoch 4, Batch 661, G Loss: 0.6935288310050964, D Loss: 1.3862500190734863\n",
            "Epoch 4, Batch 662, G Loss: 0.6935051083564758, D Loss: 1.3861596584320068\n",
            "Epoch 4, Batch 663, G Loss: 0.6934829950332642, D Loss: 1.3861932754516602\n",
            "Epoch 4, Batch 664, G Loss: 0.6934598684310913, D Loss: 1.3861216306686401\n",
            "Epoch 4, Batch 665, G Loss: 0.6934447288513184, D Loss: 1.386096477508545\n",
            "Epoch 4, Batch 666, G Loss: 0.6934344172477722, D Loss: 1.3863469362258911\n",
            "Epoch 4, Batch 667, G Loss: 0.6934112310409546, D Loss: 1.3863774538040161\n",
            "Epoch 4, Batch 668, G Loss: 0.6933685541152954, D Loss: 1.3862425088882446\n",
            "Epoch 4, Batch 669, G Loss: 0.6933199763298035, D Loss: 1.3862931728363037\n",
            "Epoch 4, Batch 670, G Loss: 0.6932687759399414, D Loss: 1.3862477540969849\n",
            "Epoch 4, Batch 671, G Loss: 0.6932116746902466, D Loss: 1.3861727714538574\n",
            "Epoch 4, Batch 672, G Loss: 0.6931674480438232, D Loss: 1.386075496673584\n",
            "Epoch 4, Batch 673, G Loss: 0.6931337714195251, D Loss: 1.386115550994873\n",
            "Epoch 4, Batch 674, G Loss: 0.6931206583976746, D Loss: 1.3860979080200195\n",
            "Epoch 4, Batch 675, G Loss: 0.6931175589561462, D Loss: 1.386257529258728\n",
            "Epoch 4, Batch 676, G Loss: 0.6931124329566956, D Loss: 1.3862316608428955\n",
            "Epoch 4, Batch 677, G Loss: 0.6930977702140808, D Loss: 1.386195421218872\n",
            "Epoch 4, Batch 678, G Loss: 0.6930903196334839, D Loss: 1.3861932754516602\n",
            "Epoch 4, Batch 679, G Loss: 0.6930888295173645, D Loss: 1.386198878288269\n",
            "Epoch 4, Batch 680, G Loss: 0.693078339099884, D Loss: 1.3862175941467285\n",
            "Epoch 4, Batch 681, G Loss: 0.6930650472640991, D Loss: 1.3861525058746338\n",
            "Epoch 4, Batch 682, G Loss: 0.6930639147758484, D Loss: 1.3862107992172241\n",
            "Epoch 4, Batch 683, G Loss: 0.6930496096611023, D Loss: 1.3862043619155884\n",
            "Epoch 4, Batch 684, G Loss: 0.6930526494979858, D Loss: 1.3861920833587646\n",
            "Epoch 4, Batch 685, G Loss: 0.6930466294288635, D Loss: 1.38614821434021\n",
            "Epoch 4, Batch 686, G Loss: 0.6930456161499023, D Loss: 1.3861284255981445\n",
            "Epoch 4, Batch 687, G Loss: 0.6930521130561829, D Loss: 1.386114239692688\n",
            "Epoch 4, Batch 688, G Loss: 0.6930797100067139, D Loss: 1.3860769271850586\n",
            "Epoch 4, Batch 689, G Loss: 0.693111002445221, D Loss: 1.3861842155456543\n",
            "Epoch 4, Batch 690, G Loss: 0.6931354999542236, D Loss: 1.3861210346221924\n",
            "Epoch 4, Batch 691, G Loss: 0.6931706666946411, D Loss: 1.3861558437347412\n",
            "Epoch 4, Batch 692, G Loss: 0.6932066082954407, D Loss: 1.3861885070800781\n",
            "Epoch 4, Batch 693, G Loss: 0.6932360529899597, D Loss: 1.38619863986969\n",
            "Epoch 4, Batch 694, G Loss: 0.6932573914527893, D Loss: 1.386122226715088\n",
            "Epoch 4, Batch 695, G Loss: 0.6932795643806458, D Loss: 1.38621187210083\n",
            "Epoch 4, Batch 696, G Loss: 0.6933051943778992, D Loss: 1.3861520290374756\n",
            "Epoch 4, Batch 697, G Loss: 0.6933229565620422, D Loss: 1.3861665725708008\n",
            "Epoch 4, Batch 698, G Loss: 0.6933448910713196, D Loss: 1.3861510753631592\n",
            "Epoch 4, Batch 699, G Loss: 0.6933587789535522, D Loss: 1.3861231803894043\n",
            "Epoch 4, Batch 700, G Loss: 0.6933894157409668, D Loss: 1.3861243724822998\n",
            "Epoch 4, Batch 701, G Loss: 0.6934033632278442, D Loss: 1.386086344718933\n",
            "Epoch 4, Batch 702, G Loss: 0.6934313178062439, D Loss: 1.3861658573150635\n",
            "Epoch 4, Batch 703, G Loss: 0.6934618353843689, D Loss: 1.3862218856811523\n",
            "Epoch 4, Batch 704, G Loss: 0.6934727430343628, D Loss: 1.3861902952194214\n",
            "Epoch 4, Batch 705, G Loss: 0.6934921741485596, D Loss: 1.3862226009368896\n",
            "Epoch 4, Batch 706, G Loss: 0.6934987306594849, D Loss: 1.3861844539642334\n",
            "Epoch 4, Batch 707, G Loss: 0.6934928297996521, D Loss: 1.3861876726150513\n",
            "Epoch 4, Batch 708, G Loss: 0.6934906840324402, D Loss: 1.3862123489379883\n",
            "Epoch 4, Batch 709, G Loss: 0.693482518196106, D Loss: 1.3861477375030518\n",
            "Epoch 4, Batch 710, G Loss: 0.6934791207313538, D Loss: 1.3861228227615356\n",
            "Epoch 4, Batch 711, G Loss: 0.6934767961502075, D Loss: 1.3860324621200562\n",
            "Epoch 4, Batch 712, G Loss: 0.6934823989868164, D Loss: 1.3861784934997559\n",
            "Epoch 4, Batch 713, G Loss: 0.6934903264045715, D Loss: 1.3861238956451416\n",
            "Epoch 4, Batch 714, G Loss: 0.6934958696365356, D Loss: 1.3861427307128906\n",
            "Epoch 4, Batch 715, G Loss: 0.693500280380249, D Loss: 1.386183500289917\n",
            "Epoch 4, Batch 716, G Loss: 0.6935045719146729, D Loss: 1.3861544132232666\n",
            "Epoch 4, Batch 717, G Loss: 0.6934999823570251, D Loss: 1.3861819505691528\n",
            "Epoch 4, Batch 718, G Loss: 0.6935063004493713, D Loss: 1.3860410451889038\n",
            "Epoch 4, Batch 719, G Loss: 0.6935097575187683, D Loss: 1.3859362602233887\n",
            "Epoch 4, Batch 720, G Loss: 0.6935392022132874, D Loss: 1.3860034942626953\n",
            "Epoch 4, Batch 721, G Loss: 0.6935739517211914, D Loss: 1.3859896659851074\n",
            "Epoch 4, Batch 722, G Loss: 0.6936119198799133, D Loss: 1.386077880859375\n",
            "Epoch 4, Batch 723, G Loss: 0.6936623454093933, D Loss: 1.3861147165298462\n",
            "Epoch 4, Batch 724, G Loss: 0.6937011480331421, D Loss: 1.3860886096954346\n",
            "Epoch 4, Batch 725, G Loss: 0.6937374472618103, D Loss: 1.3862571716308594\n",
            "Epoch 4, Batch 726, G Loss: 0.6937621235847473, D Loss: 1.3861563205718994\n",
            "Epoch 4, Batch 727, G Loss: 0.6937828660011292, D Loss: 1.386142373085022\n",
            "Epoch 4, Batch 728, G Loss: 0.6937962770462036, D Loss: 1.386169672012329\n",
            "Epoch 4, Batch 729, G Loss: 0.6938022971153259, D Loss: 1.3862226009368896\n",
            "Epoch 4, Batch 730, G Loss: 0.6938050985336304, D Loss: 1.3863635063171387\n",
            "Epoch 4, Batch 731, G Loss: 0.6937856674194336, D Loss: 1.386258602142334\n",
            "Epoch 4, Batch 732, G Loss: 0.6937586069107056, D Loss: 1.386329174041748\n",
            "Epoch 4, Batch 733, G Loss: 0.6937227845191956, D Loss: 1.386287808418274\n",
            "Epoch 4, Batch 734, G Loss: 0.6936736106872559, D Loss: 1.3862309455871582\n",
            "Epoch 4, Batch 735, G Loss: 0.6936269998550415, D Loss: 1.3861887454986572\n",
            "Epoch 4, Batch 736, G Loss: 0.6935770511627197, D Loss: 1.3862015008926392\n",
            "Epoch 4, Batch 737, G Loss: 0.6935276985168457, D Loss: 1.386216163635254\n",
            "Epoch 4, Batch 738, G Loss: 0.693477213382721, D Loss: 1.3860135078430176\n",
            "Epoch 4, Batch 739, G Loss: 0.6934458017349243, D Loss: 1.3859827518463135\n",
            "Epoch 4, Batch 740, G Loss: 0.6934236884117126, D Loss: 1.3859624862670898\n",
            "Epoch 4, Batch 741, G Loss: 0.6934219598770142, D Loss: 1.3862793445587158\n",
            "Epoch 4, Batch 742, G Loss: 0.6934084296226501, D Loss: 1.3862617015838623\n",
            "Epoch 4, Batch 743, G Loss: 0.6933876872062683, D Loss: 1.3862160444259644\n",
            "Epoch 4, Batch 744, G Loss: 0.6933693289756775, D Loss: 1.3864402770996094\n",
            "Epoch 4, Batch 745, G Loss: 0.6933237910270691, D Loss: 1.3865079879760742\n",
            "Epoch 4, Batch 746, G Loss: 0.693272054195404, D Loss: 1.3863592147827148\n",
            "Epoch 4, Batch 747, G Loss: 0.6932001709938049, D Loss: 1.3862850666046143\n",
            "Epoch 4, Batch 748, G Loss: 0.6931323409080505, D Loss: 1.3862993717193604\n",
            "Epoch 4, Batch 749, G Loss: 0.6930629014968872, D Loss: 1.3862868547439575\n",
            "Epoch 4, Batch 750, G Loss: 0.6929916143417358, D Loss: 1.3861596584320068\n",
            "Epoch 4, Batch 751, G Loss: 0.6929301023483276, D Loss: 1.3864188194274902\n",
            "Epoch 4, Batch 752, G Loss: 0.692866325378418, D Loss: 1.3862569332122803\n",
            "Epoch 4, Batch 753, G Loss: 0.6927945017814636, D Loss: 1.3863939046859741\n",
            "Epoch 4, Batch 754, G Loss: 0.6927259564399719, D Loss: 1.3863499164581299\n",
            "Epoch 4, Batch 755, G Loss: 0.6926442980766296, D Loss: 1.3863375186920166\n",
            "Epoch 4, Batch 756, G Loss: 0.6925646662712097, D Loss: 1.3862988948822021\n",
            "Epoch 4, Batch 757, G Loss: 0.692484974861145, D Loss: 1.3862926959991455\n",
            "Epoch 4, Batch 758, G Loss: 0.6924037337303162, D Loss: 1.386244535446167\n",
            "Epoch 4, Batch 759, G Loss: 0.6923394203186035, D Loss: 1.3862800598144531\n",
            "Epoch 4, Batch 760, G Loss: 0.692267119884491, D Loss: 1.3862128257751465\n",
            "Epoch 4, Batch 761, G Loss: 0.6922115683555603, D Loss: 1.3862290382385254\n",
            "Epoch 4, Batch 762, G Loss: 0.6921582818031311, D Loss: 1.3862409591674805\n",
            "Epoch 4, Batch 763, G Loss: 0.6921058893203735, D Loss: 1.3862318992614746\n",
            "Epoch 4, Batch 764, G Loss: 0.6920549273490906, D Loss: 1.3862090110778809\n",
            "Epoch 4, Batch 765, G Loss: 0.6920247077941895, D Loss: 1.3861041069030762\n",
            "Epoch 4, Batch 766, G Loss: 0.6920197010040283, D Loss: 1.3861215114593506\n",
            "Epoch 4, Batch 767, G Loss: 0.6920382380485535, D Loss: 1.3861517906188965\n",
            "Epoch 4, Batch 768, G Loss: 0.6920763850212097, D Loss: 1.3861888647079468\n",
            "Epoch 4, Batch 769, G Loss: 0.6921109557151794, D Loss: 1.3861981630325317\n",
            "Epoch 4, Batch 770, G Loss: 0.6921409964561462, D Loss: 1.3861854076385498\n",
            "Epoch 4, Batch 771, G Loss: 0.6921753883361816, D Loss: 1.3861680030822754\n",
            "Epoch 4, Batch 772, G Loss: 0.692211389541626, D Loss: 1.3861732482910156\n",
            "Epoch 4, Batch 773, G Loss: 0.6922558546066284, D Loss: 1.3861826658248901\n",
            "Epoch 4, Batch 774, G Loss: 0.6922957897186279, D Loss: 1.3861758708953857\n",
            "Epoch 4, Batch 775, G Loss: 0.6923400163650513, D Loss: 1.386169672012329\n",
            "Epoch 4, Batch 776, G Loss: 0.6923813819885254, D Loss: 1.386174201965332\n",
            "Epoch 4, Batch 777, G Loss: 0.6924260258674622, D Loss: 1.386155366897583\n",
            "Epoch 4, Batch 778, G Loss: 0.692486047744751, D Loss: 1.3861947059631348\n",
            "Epoch 4, Batch 779, G Loss: 0.692534327507019, D Loss: 1.3861594200134277\n",
            "Epoch 4, Batch 780, G Loss: 0.6925822496414185, D Loss: 1.3861570358276367\n",
            "Epoch 4, Batch 781, G Loss: 0.6926409602165222, D Loss: 1.3861422538757324\n",
            "Epoch 4, Batch 782, G Loss: 0.6927134394645691, D Loss: 1.3861544132232666\n",
            "Epoch 4, Batch 783, G Loss: 0.6927919983863831, D Loss: 1.386163353919983\n",
            "Epoch 4, Batch 784, G Loss: 0.6928586363792419, D Loss: 1.3861823081970215\n",
            "Epoch 4, Batch 785, G Loss: 0.6929267644882202, D Loss: 1.386159896850586\n",
            "Epoch 4, Batch 786, G Loss: 0.6929991841316223, D Loss: 1.3861525058746338\n",
            "Epoch 4, Batch 787, G Loss: 0.6930616497993469, D Loss: 1.3861666917800903\n",
            "Epoch 4, Batch 788, G Loss: 0.6931250691413879, D Loss: 1.3861461877822876\n",
            "Epoch 4, Batch 789, G Loss: 0.6931798458099365, D Loss: 1.3861615657806396\n",
            "Epoch 4, Batch 790, G Loss: 0.6932416558265686, D Loss: 1.386176347732544\n",
            "Epoch 4, Batch 791, G Loss: 0.6932857036590576, D Loss: 1.3861517906188965\n",
            "Epoch 4, Batch 792, G Loss: 0.6933408379554749, D Loss: 1.3861349821090698\n",
            "Epoch 4, Batch 793, G Loss: 0.6933867931365967, D Loss: 1.3861677646636963\n",
            "Epoch 4, Batch 794, G Loss: 0.6934291124343872, D Loss: 1.3862218856811523\n",
            "Epoch 4, Batch 795, G Loss: 0.6934534311294556, D Loss: 1.3861792087554932\n",
            "Epoch 4, Batch 796, G Loss: 0.6934762001037598, D Loss: 1.386200189590454\n",
            "Epoch 4, Batch 797, G Loss: 0.6934897899627686, D Loss: 1.3861563205718994\n",
            "Epoch 4, Batch 798, G Loss: 0.6935029029846191, D Loss: 1.386149525642395\n",
            "Epoch 4, Batch 799, G Loss: 0.6935189962387085, D Loss: 1.3861943483352661\n",
            "Epoch 4, Batch 800, G Loss: 0.6935256719589233, D Loss: 1.3862332105636597\n",
            "Epoch 4, Batch 801, G Loss: 0.6935111284255981, D Loss: 1.3862690925598145\n",
            "Epoch 4, Batch 802, G Loss: 0.6934906840324402, D Loss: 1.386216640472412\n",
            "Epoch 4, Batch 803, G Loss: 0.6934494376182556, D Loss: 1.3862004280090332\n",
            "Epoch 4, Batch 804, G Loss: 0.6934041976928711, D Loss: 1.386176347732544\n",
            "Epoch 4, Batch 805, G Loss: 0.6933598518371582, D Loss: 1.3862130641937256\n",
            "Epoch 4, Batch 806, G Loss: 0.6933083534240723, D Loss: 1.386162281036377\n",
            "Epoch 4, Batch 807, G Loss: 0.6932711005210876, D Loss: 1.3861942291259766\n",
            "Epoch 4, Batch 808, G Loss: 0.6932262778282166, D Loss: 1.3861515522003174\n",
            "Epoch 4, Batch 809, G Loss: 0.6931995749473572, D Loss: 1.3861379623413086\n",
            "Epoch 4, Batch 810, G Loss: 0.6931737065315247, D Loss: 1.3861596584320068\n",
            "Epoch 4, Batch 811, G Loss: 0.6931632161140442, D Loss: 1.3861303329467773\n",
            "Epoch 4, Batch 812, G Loss: 0.6931509971618652, D Loss: 1.3861708641052246\n",
            "Epoch 4, Batch 813, G Loss: 0.693156361579895, D Loss: 1.3861424922943115\n",
            "Epoch 4, Batch 814, G Loss: 0.6931638717651367, D Loss: 1.3861058950424194\n",
            "Epoch 4, Batch 815, G Loss: 0.6931836009025574, D Loss: 1.3861420154571533\n",
            "Epoch 4, Batch 816, G Loss: 0.693210780620575, D Loss: 1.386129379272461\n",
            "Epoch 4, Batch 817, G Loss: 0.6932564377784729, D Loss: 1.386106014251709\n",
            "Epoch 4, Batch 818, G Loss: 0.6933029294013977, D Loss: 1.386153221130371\n",
            "Epoch 4, Batch 819, G Loss: 0.6933543682098389, D Loss: 1.3861300945281982\n",
            "Epoch 4, Batch 820, G Loss: 0.6934013366699219, D Loss: 1.3862574100494385\n",
            "Epoch 4, Batch 821, G Loss: 0.6934311389923096, D Loss: 1.386183738708496\n",
            "Epoch 4, Batch 822, G Loss: 0.6934484243392944, D Loss: 1.3861844539642334\n",
            "Epoch 4, Batch 823, G Loss: 0.6934608817100525, D Loss: 1.386148452758789\n",
            "Epoch 4, Batch 824, G Loss: 0.6934760212898254, D Loss: 1.3861194849014282\n",
            "Epoch 4, Batch 825, G Loss: 0.6934928894042969, D Loss: 1.3860983848571777\n",
            "Epoch 4, Batch 826, G Loss: 0.6935205459594727, D Loss: 1.3861570358276367\n",
            "Epoch 4, Batch 827, G Loss: 0.6935354471206665, D Loss: 1.3861892223358154\n",
            "Epoch 4, Batch 828, G Loss: 0.6935575604438782, D Loss: 1.3862097263336182\n",
            "Epoch 4, Batch 829, G Loss: 0.6935582160949707, D Loss: 1.3862411975860596\n",
            "Epoch 4, Batch 830, G Loss: 0.6935465335845947, D Loss: 1.3862404823303223\n",
            "Epoch 4, Batch 831, G Loss: 0.6935150623321533, D Loss: 1.3861416578292847\n",
            "Epoch 4, Batch 832, G Loss: 0.6934909224510193, D Loss: 1.3862018585205078\n",
            "Epoch 4, Batch 833, G Loss: 0.6934664845466614, D Loss: 1.386160135269165\n",
            "Epoch 4, Batch 834, G Loss: 0.6934359669685364, D Loss: 1.3861279487609863\n",
            "Epoch 4, Batch 835, G Loss: 0.6934157609939575, D Loss: 1.3861626386642456\n",
            "Epoch 4, Batch 836, G Loss: 0.6933972239494324, D Loss: 1.3861287832260132\n",
            "Epoch 4, Batch 837, G Loss: 0.6933889389038086, D Loss: 1.386134386062622\n",
            "Epoch 4, Batch 838, G Loss: 0.6933820247650146, D Loss: 1.3860795497894287\n",
            "Epoch 4, Batch 839, G Loss: 0.693382740020752, D Loss: 1.3861370086669922\n",
            "Epoch 4, Batch 840, G Loss: 0.6933913826942444, D Loss: 1.3861167430877686\n",
            "Epoch 4, Batch 841, G Loss: 0.6934026479721069, D Loss: 1.3860571384429932\n",
            "Epoch 4, Batch 842, G Loss: 0.6934274435043335, D Loss: 1.3861221075057983\n",
            "Epoch 4, Batch 843, G Loss: 0.6934570074081421, D Loss: 1.3861908912658691\n",
            "Epoch 4, Batch 844, G Loss: 0.6934694647789001, D Loss: 1.3862025737762451\n",
            "Epoch 4, Batch 845, G Loss: 0.6934775710105896, D Loss: 1.386160135269165\n",
            "Epoch 4, Batch 846, G Loss: 0.6934792399406433, D Loss: 1.3861725330352783\n",
            "Epoch 4, Batch 847, G Loss: 0.6934788227081299, D Loss: 1.386130928993225\n",
            "Epoch 4, Batch 848, G Loss: 0.693477213382721, D Loss: 1.3860580921173096\n",
            "Epoch 4, Batch 849, G Loss: 0.6934944987297058, D Loss: 1.3860520124435425\n",
            "Epoch 4, Batch 850, G Loss: 0.6935150623321533, D Loss: 1.3862428665161133\n",
            "Epoch 4, Batch 851, G Loss: 0.6935253739356995, D Loss: 1.3861925601959229\n",
            "Epoch 4, Batch 852, G Loss: 0.6935271620750427, D Loss: 1.386271595954895\n",
            "Epoch 4, Batch 853, G Loss: 0.6935173869132996, D Loss: 1.3862385749816895\n",
            "Epoch 4, Batch 854, G Loss: 0.6934934258460999, D Loss: 1.386234998703003\n",
            "Epoch 4, Batch 855, G Loss: 0.6934563517570496, D Loss: 1.3862286806106567\n",
            "Epoch 4, Batch 856, G Loss: 0.6934213638305664, D Loss: 1.3862241506576538\n",
            "Epoch 4, Batch 857, G Loss: 0.6933820247650146, D Loss: 1.3862264156341553\n",
            "Epoch 4, Batch 858, G Loss: 0.6933367848396301, D Loss: 1.386102318763733\n",
            "Epoch 4, Batch 859, G Loss: 0.6933020949363708, D Loss: 1.3861616849899292\n",
            "Epoch 4, Batch 860, G Loss: 0.6932672262191772, D Loss: 1.3862183094024658\n",
            "Epoch 4, Batch 861, G Loss: 0.6932318806648254, D Loss: 1.3861558437347412\n",
            "Epoch 4, Batch 862, G Loss: 0.6931943297386169, D Loss: 1.386197566986084\n",
            "Epoch 4, Batch 863, G Loss: 0.6931645274162292, D Loss: 1.3861793279647827\n",
            "Epoch 4, Batch 864, G Loss: 0.6931307911872864, D Loss: 1.3861902952194214\n",
            "Epoch 4, Batch 865, G Loss: 0.6930967569351196, D Loss: 1.3861310482025146\n",
            "Epoch 4, Batch 866, G Loss: 0.6930752396583557, D Loss: 1.3861308097839355\n",
            "Epoch 4, Batch 867, G Loss: 0.6930577754974365, D Loss: 1.3861498832702637\n",
            "Epoch 4, Batch 868, G Loss: 0.6930463910102844, D Loss: 1.3862520456314087\n",
            "Epoch 4, Batch 869, G Loss: 0.6930264234542847, D Loss: 1.3862149715423584\n",
            "Epoch 4, Batch 870, G Loss: 0.6930060982704163, D Loss: 1.3863275051116943\n",
            "Epoch 4, Batch 871, G Loss: 0.6929603815078735, D Loss: 1.386286973953247\n",
            "Epoch 4, Batch 872, G Loss: 0.6929022073745728, D Loss: 1.3862736225128174\n",
            "Epoch 4, Batch 873, G Loss: 0.6928331255912781, D Loss: 1.3861794471740723\n",
            "Epoch 4, Batch 874, G Loss: 0.6927660703659058, D Loss: 1.386167287826538\n",
            "Epoch 4, Batch 875, G Loss: 0.6927088499069214, D Loss: 1.3861775398254395\n",
            "Epoch 4, Batch 876, G Loss: 0.6926645040512085, D Loss: 1.3861660957336426\n",
            "Epoch 4, Batch 877, G Loss: 0.6926238536834717, D Loss: 1.3861604928970337\n",
            "Epoch 4, Batch 878, G Loss: 0.6925990581512451, D Loss: 1.386169672012329\n",
            "Epoch 4, Batch 879, G Loss: 0.6925705671310425, D Loss: 1.3861703872680664\n",
            "Epoch 4, Batch 880, G Loss: 0.69255530834198, D Loss: 1.386139154434204\n",
            "Epoch 4, Batch 881, G Loss: 0.6925519108772278, D Loss: 1.3861396312713623\n",
            "Epoch 4, Batch 882, G Loss: 0.692567765712738, D Loss: 1.3861310482025146\n",
            "Epoch 4, Batch 883, G Loss: 0.6925959587097168, D Loss: 1.3861496448516846\n",
            "Epoch 4, Batch 884, G Loss: 0.6926388144493103, D Loss: 1.3861522674560547\n",
            "Epoch 4, Batch 885, G Loss: 0.6926722526550293, D Loss: 1.3861521482467651\n",
            "Epoch 4, Batch 886, G Loss: 0.692718505859375, D Loss: 1.3861455917358398\n",
            "Epoch 4, Batch 887, G Loss: 0.692767858505249, D Loss: 1.3861956596374512\n",
            "Epoch 4, Batch 888, G Loss: 0.6927943229675293, D Loss: 1.3861799240112305\n",
            "Epoch 4, Batch 889, G Loss: 0.6928148865699768, D Loss: 1.3861773014068604\n",
            "Epoch 4, Batch 890, G Loss: 0.6928281784057617, D Loss: 1.3861558437347412\n",
            "Epoch 4, Batch 891, G Loss: 0.6928535103797913, D Loss: 1.3861327171325684\n",
            "Epoch 4, Batch 892, G Loss: 0.6928791999816895, D Loss: 1.3861041069030762\n",
            "Epoch 4, Batch 893, G Loss: 0.6929318308830261, D Loss: 1.386122703552246\n",
            "Epoch 4, Batch 894, G Loss: 0.6929875612258911, D Loss: 1.3861463069915771\n",
            "Epoch 4, Batch 895, G Loss: 0.6930386424064636, D Loss: 1.386164665222168\n",
            "Epoch 4, Batch 896, G Loss: 0.6930896043777466, D Loss: 1.386155128479004\n",
            "Epoch 4, Batch 897, G Loss: 0.6931362748146057, D Loss: 1.386155605316162\n",
            "Epoch 4, Batch 898, G Loss: 0.6931706666946411, D Loss: 1.3861472606658936\n",
            "Epoch 4, Batch 899, G Loss: 0.6932040452957153, D Loss: 1.386172890663147\n",
            "Epoch 4, Batch 900, G Loss: 0.6932312250137329, D Loss: 1.3861641883850098\n",
            "Epoch 4, Batch 901, G Loss: 0.6932454705238342, D Loss: 1.386139154434204\n",
            "Epoch 4, Batch 902, G Loss: 0.6932654976844788, D Loss: 1.3861043453216553\n",
            "Epoch 4, Batch 903, G Loss: 0.6932970285415649, D Loss: 1.3861044645309448\n",
            "Epoch 4, Batch 904, G Loss: 0.6933285593986511, D Loss: 1.3861000537872314\n",
            "Epoch 4, Batch 905, G Loss: 0.6933866739273071, D Loss: 1.386120080947876\n",
            "Epoch 4, Batch 906, G Loss: 0.6934300661087036, D Loss: 1.3861109018325806\n",
            "Epoch 4, Batch 907, G Loss: 0.6934807300567627, D Loss: 1.3861582279205322\n",
            "Epoch 4, Batch 908, G Loss: 0.6935244798660278, D Loss: 1.3860722780227661\n",
            "Epoch 4, Batch 909, G Loss: 0.6935768127441406, D Loss: 1.386054277420044\n",
            "Epoch 4, Batch 910, G Loss: 0.6936357617378235, D Loss: 1.3861554861068726\n",
            "Epoch 4, Batch 911, G Loss: 0.6936808824539185, D Loss: 1.386192798614502\n",
            "Epoch 4, Batch 912, G Loss: 0.6937128305435181, D Loss: 1.386181116104126\n",
            "Epoch 4, Batch 913, G Loss: 0.6937293410301208, D Loss: 1.3862533569335938\n",
            "Epoch 4, Batch 914, G Loss: 0.6937273740768433, D Loss: 1.3862438201904297\n",
            "Epoch 4, Batch 915, G Loss: 0.6937121748924255, D Loss: 1.3860774040222168\n",
            "Epoch 4, Batch 916, G Loss: 0.6936973333358765, D Loss: 1.3860092163085938\n",
            "Epoch 4, Batch 917, G Loss: 0.6937016248703003, D Loss: 1.3859963417053223\n",
            "Epoch 4, Batch 918, G Loss: 0.6937326192855835, D Loss: 1.3859655857086182\n",
            "Epoch 4, Batch 919, G Loss: 0.6937753558158875, D Loss: 1.3860201835632324\n",
            "Epoch 4, Batch 920, G Loss: 0.6938234567642212, D Loss: 1.3862261772155762\n",
            "Epoch 4, Batch 921, G Loss: 0.6938502788543701, D Loss: 1.3861663341522217\n",
            "Epoch 4, Batch 922, G Loss: 0.6938705444335938, D Loss: 1.3861749172210693\n",
            "Epoch 4, Batch 923, G Loss: 0.693873405456543, D Loss: 1.3861167430877686\n",
            "Epoch 4, Batch 924, G Loss: 0.6938825249671936, D Loss: 1.3860914707183838\n",
            "Epoch 4, Batch 925, G Loss: 0.6938828229904175, D Loss: 1.3861552476882935\n",
            "Epoch 4, Batch 926, G Loss: 0.6938769221305847, D Loss: 1.3862013816833496\n",
            "Epoch 4, Batch 927, G Loss: 0.6938671469688416, D Loss: 1.3863139152526855\n",
            "Epoch 4, Batch 928, G Loss: 0.6938294172286987, D Loss: 1.386684536933899\n",
            "Epoch 4, Batch 929, G Loss: 0.6937408447265625, D Loss: 1.3869187831878662\n",
            "Epoch 4, Batch 930, G Loss: 0.6935737729072571, D Loss: 1.386343240737915\n",
            "Epoch 4, Batch 931, G Loss: 0.6934026479721069, D Loss: 1.386052131652832\n",
            "Epoch 4, Batch 932, G Loss: 0.6932497620582581, D Loss: 1.3861491680145264\n",
            "Epoch 4, Batch 933, G Loss: 0.6931352615356445, D Loss: 1.3862210512161255\n",
            "Epoch 4, Batch 934, G Loss: 0.693030595779419, D Loss: 1.386162519454956\n",
            "Epoch 4, Batch 935, G Loss: 0.6929445862770081, D Loss: 1.386216163635254\n",
            "Epoch 4, Batch 936, G Loss: 0.69286048412323, D Loss: 1.3862369060516357\n",
            "Epoch 4, Batch 937, G Loss: 0.6927655935287476, D Loss: 1.3860816955566406\n",
            "Epoch 4, Batch 938, G Loss: 0.6927077770233154, D Loss: 1.3860952854156494\n",
            "Epoch 5, Batch 1, G Loss: 0.6926724910736084, D Loss: 1.3861422538757324\n",
            "Epoch 5, Batch 2, G Loss: 0.6926453113555908, D Loss: 1.3861467838287354\n",
            "Epoch 5, Batch 3, G Loss: 0.692624568939209, D Loss: 1.3861360549926758\n",
            "Epoch 5, Batch 4, G Loss: 0.6926060914993286, D Loss: 1.3861688375473022\n",
            "Epoch 5, Batch 5, G Loss: 0.6925979852676392, D Loss: 1.386137843132019\n",
            "Epoch 5, Batch 6, G Loss: 0.6926015019416809, D Loss: 1.386147141456604\n",
            "Epoch 5, Batch 7, G Loss: 0.6926090717315674, D Loss: 1.3861362934112549\n",
            "Epoch 5, Batch 8, G Loss: 0.6926231980323792, D Loss: 1.386138677597046\n",
            "Epoch 5, Batch 9, G Loss: 0.6926444172859192, D Loss: 1.3861536979675293\n",
            "Epoch 5, Batch 10, G Loss: 0.6926705241203308, D Loss: 1.386128544807434\n",
            "Epoch 5, Batch 11, G Loss: 0.6927026510238647, D Loss: 1.3861595392227173\n",
            "Epoch 5, Batch 12, G Loss: 0.6927416324615479, D Loss: 1.3861806392669678\n",
            "Epoch 5, Batch 13, G Loss: 0.6927520632743835, D Loss: 1.3861708641052246\n",
            "Epoch 5, Batch 14, G Loss: 0.6927657127380371, D Loss: 1.3861758708953857\n",
            "Epoch 5, Batch 15, G Loss: 0.6927785277366638, D Loss: 1.3861584663391113\n",
            "Epoch 5, Batch 16, G Loss: 0.6927862167358398, D Loss: 1.386139154434204\n",
            "Epoch 5, Batch 17, G Loss: 0.69279944896698, D Loss: 1.3861112594604492\n",
            "Epoch 5, Batch 18, G Loss: 0.6928263902664185, D Loss: 1.3861193656921387\n",
            "Epoch 5, Batch 19, G Loss: 0.6928660273551941, D Loss: 1.3861699104309082\n",
            "Epoch 5, Batch 20, G Loss: 0.6928908824920654, D Loss: 1.3862254619598389\n",
            "Epoch 5, Batch 21, G Loss: 0.6928914189338684, D Loss: 1.3861994743347168\n",
            "Epoch 5, Batch 22, G Loss: 0.692877471446991, D Loss: 1.3862242698669434\n",
            "Epoch 5, Batch 23, G Loss: 0.6928311586380005, D Loss: 1.3861405849456787\n",
            "Epoch 5, Batch 24, G Loss: 0.6927857398986816, D Loss: 1.386147379875183\n",
            "Epoch 5, Batch 25, G Loss: 0.6927655339241028, D Loss: 1.386138916015625\n",
            "Epoch 5, Batch 26, G Loss: 0.6927497982978821, D Loss: 1.3861403465270996\n",
            "Epoch 5, Batch 27, G Loss: 0.6927472949028015, D Loss: 1.3861451148986816\n",
            "Epoch 5, Batch 28, G Loss: 0.6927582025527954, D Loss: 1.3861382007598877\n",
            "Epoch 5, Batch 29, G Loss: 0.6927775144577026, D Loss: 1.3861445188522339\n",
            "Epoch 5, Batch 30, G Loss: 0.6927985548973083, D Loss: 1.386152744293213\n",
            "Epoch 5, Batch 31, G Loss: 0.6928237080574036, D Loss: 1.3861538171768188\n",
            "Epoch 5, Batch 32, G Loss: 0.6928371787071228, D Loss: 1.3861581087112427\n",
            "Epoch 5, Batch 33, G Loss: 0.6928374767303467, D Loss: 1.3861465454101562\n",
            "Epoch 5, Batch 34, G Loss: 0.6928330063819885, D Loss: 1.3861459493637085\n",
            "Epoch 5, Batch 35, G Loss: 0.6928280591964722, D Loss: 1.3861479759216309\n",
            "Epoch 5, Batch 36, G Loss: 0.6928342580795288, D Loss: 1.3861287832260132\n",
            "Epoch 5, Batch 37, G Loss: 0.6928281188011169, D Loss: 1.386145830154419\n",
            "Epoch 5, Batch 38, G Loss: 0.6928288340568542, D Loss: 1.3861439228057861\n",
            "Epoch 5, Batch 39, G Loss: 0.6928321719169617, D Loss: 1.3861461877822876\n",
            "Epoch 5, Batch 40, G Loss: 0.6928402781486511, D Loss: 1.3861305713653564\n",
            "Epoch 5, Batch 41, G Loss: 0.6928466558456421, D Loss: 1.3861374855041504\n",
            "Epoch 5, Batch 42, G Loss: 0.6928507685661316, D Loss: 1.3861171007156372\n",
            "Epoch 5, Batch 43, G Loss: 0.6928438544273376, D Loss: 1.3861548900604248\n",
            "Epoch 5, Batch 44, G Loss: 0.692848801612854, D Loss: 1.3861360549926758\n",
            "Epoch 5, Batch 45, G Loss: 0.6928530335426331, D Loss: 1.3861181735992432\n",
            "Epoch 5, Batch 46, G Loss: 0.6928529739379883, D Loss: 1.3861240148544312\n",
            "Epoch 5, Batch 47, G Loss: 0.6928476095199585, D Loss: 1.3861727714538574\n",
            "Epoch 5, Batch 48, G Loss: 0.6928608417510986, D Loss: 1.3861374855041504\n",
            "Epoch 5, Batch 49, G Loss: 0.6928733587265015, D Loss: 1.386167287826538\n",
            "Epoch 5, Batch 50, G Loss: 0.6929015517234802, D Loss: 1.386162519454956\n",
            "Epoch 5, Batch 51, G Loss: 0.6929336190223694, D Loss: 1.3861675262451172\n",
            "Epoch 5, Batch 52, G Loss: 0.692976176738739, D Loss: 1.3861768245697021\n",
            "Epoch 5, Batch 53, G Loss: 0.6930292844772339, D Loss: 1.3861392736434937\n",
            "Epoch 5, Batch 54, G Loss: 0.6930713057518005, D Loss: 1.3861724138259888\n",
            "Epoch 5, Batch 55, G Loss: 0.6931275129318237, D Loss: 1.386169195175171\n",
            "Epoch 5, Batch 56, G Loss: 0.6931880116462708, D Loss: 1.386137843132019\n",
            "Epoch 5, Batch 57, G Loss: 0.6932421326637268, D Loss: 1.3861231803894043\n",
            "Epoch 5, Batch 58, G Loss: 0.6932846307754517, D Loss: 1.3861024379730225\n",
            "Epoch 5, Batch 59, G Loss: 0.6933072805404663, D Loss: 1.3861368894577026\n",
            "Epoch 5, Batch 60, G Loss: 0.6933289170265198, D Loss: 1.3861348628997803\n",
            "Epoch 5, Batch 61, G Loss: 0.6933422684669495, D Loss: 1.386152744293213\n",
            "Epoch 5, Batch 62, G Loss: 0.6933684349060059, D Loss: 1.3861408233642578\n",
            "Epoch 5, Batch 63, G Loss: 0.6933876872062683, D Loss: 1.3861396312713623\n",
            "Epoch 5, Batch 64, G Loss: 0.6934089064598083, D Loss: 1.3861329555511475\n",
            "Epoch 5, Batch 65, G Loss: 0.6934217214584351, D Loss: 1.3861420154571533\n",
            "Epoch 5, Batch 66, G Loss: 0.6934307813644409, D Loss: 1.386149525642395\n",
            "Epoch 5, Batch 67, G Loss: 0.693442702293396, D Loss: 1.3861322402954102\n",
            "Epoch 5, Batch 68, G Loss: 0.6934496760368347, D Loss: 1.3861397504806519\n",
            "Epoch 5, Batch 69, G Loss: 0.6934537291526794, D Loss: 1.3861466646194458\n",
            "Epoch 5, Batch 70, G Loss: 0.6934745907783508, D Loss: 1.3861348628997803\n",
            "Epoch 5, Batch 71, G Loss: 0.6934854984283447, D Loss: 1.3861358165740967\n",
            "Epoch 5, Batch 72, G Loss: 0.6934831738471985, D Loss: 1.3861397504806519\n",
            "Epoch 5, Batch 73, G Loss: 0.693473219871521, D Loss: 1.3861353397369385\n",
            "Epoch 5, Batch 74, G Loss: 0.693449854850769, D Loss: 1.3861277103424072\n",
            "Epoch 5, Batch 75, G Loss: 0.6934075355529785, D Loss: 1.3861278295516968\n",
            "Epoch 5, Batch 76, G Loss: 0.693367600440979, D Loss: 1.3861186504364014\n",
            "Epoch 5, Batch 77, G Loss: 0.6933110952377319, D Loss: 1.3861258029937744\n",
            "Epoch 5, Batch 78, G Loss: 0.6932590007781982, D Loss: 1.386150598526001\n",
            "Epoch 5, Batch 79, G Loss: 0.693215012550354, D Loss: 1.3861310482025146\n",
            "Epoch 5, Batch 80, G Loss: 0.6931780576705933, D Loss: 1.3861157894134521\n",
            "Epoch 5, Batch 81, G Loss: 0.6931356191635132, D Loss: 1.3861134052276611\n",
            "Epoch 5, Batch 82, G Loss: 0.6930932998657227, D Loss: 1.386120080947876\n",
            "Epoch 5, Batch 83, G Loss: 0.6930495500564575, D Loss: 1.3861207962036133\n",
            "Epoch 5, Batch 84, G Loss: 0.6930084228515625, D Loss: 1.386070966720581\n",
            "Epoch 5, Batch 85, G Loss: 0.6929598450660706, D Loss: 1.3860771656036377\n",
            "Epoch 5, Batch 86, G Loss: 0.6928991079330444, D Loss: 1.3860812187194824\n",
            "Epoch 5, Batch 87, G Loss: 0.6928325295448303, D Loss: 1.3860976696014404\n",
            "Epoch 5, Batch 88, G Loss: 0.6927701234817505, D Loss: 1.3861026763916016\n",
            "Epoch 5, Batch 89, G Loss: 0.6927192211151123, D Loss: 1.3861119747161865\n",
            "Epoch 5, Batch 90, G Loss: 0.6926678419113159, D Loss: 1.3860969543457031\n",
            "Epoch 5, Batch 91, G Loss: 0.6926215887069702, D Loss: 1.3861658573150635\n",
            "Epoch 5, Batch 92, G Loss: 0.6925944089889526, D Loss: 1.3861716985702515\n",
            "Epoch 5, Batch 93, G Loss: 0.6925827264785767, D Loss: 1.386141061782837\n",
            "Epoch 5, Batch 94, G Loss: 0.6925773024559021, D Loss: 1.385965347290039\n",
            "Epoch 5, Batch 95, G Loss: 0.6925556659698486, D Loss: 1.385941982269287\n",
            "Epoch 5, Batch 96, G Loss: 0.6925170421600342, D Loss: 1.3859386444091797\n",
            "Epoch 5, Batch 97, G Loss: 0.6924616098403931, D Loss: 1.3859632015228271\n",
            "Epoch 5, Batch 98, G Loss: 0.6923982501029968, D Loss: 1.3860633373260498\n",
            "Epoch 5, Batch 99, G Loss: 0.6923446655273438, D Loss: 1.3860177993774414\n",
            "Epoch 5, Batch 100, G Loss: 0.6922910213470459, D Loss: 1.386037826538086\n",
            "Epoch 5, Batch 101, G Loss: 0.6922391057014465, D Loss: 1.386069416999817\n",
            "Epoch 5, Batch 102, G Loss: 0.6922067403793335, D Loss: 1.3861441612243652\n",
            "Epoch 5, Batch 103, G Loss: 0.6921791434288025, D Loss: 1.3862621784210205\n",
            "Epoch 5, Batch 104, G Loss: 0.692175567150116, D Loss: 1.3860752582550049\n",
            "Epoch 5, Batch 105, G Loss: 0.6921821236610413, D Loss: 1.3864288330078125\n",
            "Epoch 5, Batch 106, G Loss: 0.6922183632850647, D Loss: 1.3863718509674072\n",
            "Epoch 5, Batch 107, G Loss: 0.692279040813446, D Loss: 1.3864514827728271\n",
            "Epoch 5, Batch 108, G Loss: 0.6923635005950928, D Loss: 1.3860615491867065\n",
            "Epoch 5, Batch 109, G Loss: 0.6924455761909485, D Loss: 1.3859785795211792\n",
            "Epoch 5, Batch 110, G Loss: 0.6925097703933716, D Loss: 1.386138677597046\n",
            "Epoch 5, Batch 111, G Loss: 0.6925774216651917, D Loss: 1.3862957954406738\n",
            "Epoch 5, Batch 112, G Loss: 0.6926590204238892, D Loss: 1.386277198791504\n",
            "Epoch 5, Batch 113, G Loss: 0.6927380561828613, D Loss: 1.3863260746002197\n",
            "Epoch 5, Batch 114, G Loss: 0.6928407549858093, D Loss: 1.3862560987472534\n",
            "Epoch 5, Batch 115, G Loss: 0.6929402351379395, D Loss: 1.3864469528198242\n",
            "Epoch 5, Batch 116, G Loss: 0.6930612921714783, D Loss: 1.386347770690918\n",
            "Epoch 5, Batch 117, G Loss: 0.6931907534599304, D Loss: 1.386239767074585\n",
            "Epoch 5, Batch 118, G Loss: 0.6933162212371826, D Loss: 1.3862738609313965\n",
            "Epoch 5, Batch 119, G Loss: 0.6934415102005005, D Loss: 1.3862531185150146\n",
            "Epoch 5, Batch 120, G Loss: 0.6935666799545288, D Loss: 1.3861448764801025\n",
            "Epoch 5, Batch 121, G Loss: 0.6936801075935364, D Loss: 1.3861639499664307\n",
            "Epoch 5, Batch 122, G Loss: 0.6937758922576904, D Loss: 1.386157512664795\n",
            "Epoch 5, Batch 123, G Loss: 0.6938641667366028, D Loss: 1.3860993385314941\n",
            "Epoch 5, Batch 124, G Loss: 0.6939336657524109, D Loss: 1.386021614074707\n",
            "Epoch 5, Batch 125, G Loss: 0.6939661502838135, D Loss: 1.3860642910003662\n",
            "Epoch 5, Batch 126, G Loss: 0.6939862370491028, D Loss: 1.3860578536987305\n",
            "Epoch 5, Batch 127, G Loss: 0.6939764022827148, D Loss: 1.3861029148101807\n",
            "Epoch 5, Batch 128, G Loss: 0.6939588189125061, D Loss: 1.386217474937439\n",
            "Epoch 5, Batch 129, G Loss: 0.6939542889595032, D Loss: 1.3861913681030273\n",
            "Epoch 5, Batch 130, G Loss: 0.6939486861228943, D Loss: 1.3860777616500854\n",
            "Epoch 5, Batch 131, G Loss: 0.6939308643341064, D Loss: 1.3861119747161865\n",
            "Epoch 5, Batch 132, G Loss: 0.6939058899879456, D Loss: 1.3861430883407593\n",
            "Epoch 5, Batch 133, G Loss: 0.6938784122467041, D Loss: 1.3859699964523315\n",
            "Epoch 5, Batch 134, G Loss: 0.6938154101371765, D Loss: 1.3860223293304443\n",
            "Epoch 5, Batch 135, G Loss: 0.6937360167503357, D Loss: 1.3860377073287964\n",
            "Epoch 5, Batch 136, G Loss: 0.6936456561088562, D Loss: 1.3859913349151611\n",
            "Epoch 5, Batch 137, G Loss: 0.693537175655365, D Loss: 1.386094093322754\n",
            "Epoch 5, Batch 138, G Loss: 0.6934338212013245, D Loss: 1.3861291408538818\n",
            "Epoch 5, Batch 139, G Loss: 0.6933431625366211, D Loss: 1.386200189590454\n",
            "Epoch 5, Batch 140, G Loss: 0.6932737231254578, D Loss: 1.3860971927642822\n",
            "Epoch 5, Batch 141, G Loss: 0.6932089924812317, D Loss: 1.3861753940582275\n",
            "Epoch 5, Batch 142, G Loss: 0.6931614875793457, D Loss: 1.3861274719238281\n",
            "Epoch 5, Batch 143, G Loss: 0.6931182742118835, D Loss: 1.3862576484680176\n",
            "Epoch 5, Batch 144, G Loss: 0.6931023597717285, D Loss: 1.3861690759658813\n",
            "Epoch 5, Batch 145, G Loss: 0.6930925250053406, D Loss: 1.3863153457641602\n",
            "Epoch 5, Batch 146, G Loss: 0.693109393119812, D Loss: 1.3861052989959717\n",
            "Epoch 5, Batch 147, G Loss: 0.6931273937225342, D Loss: 1.3860690593719482\n",
            "Epoch 5, Batch 148, G Loss: 0.6931356191635132, D Loss: 1.3860695362091064\n",
            "Epoch 5, Batch 149, G Loss: 0.6931387186050415, D Loss: 1.3860108852386475\n",
            "Epoch 5, Batch 150, G Loss: 0.6931303143501282, D Loss: 1.3862206935882568\n",
            "Epoch 5, Batch 151, G Loss: 0.6931350231170654, D Loss: 1.3861522674560547\n",
            "Epoch 5, Batch 152, G Loss: 0.6931462287902832, D Loss: 1.3860492706298828\n",
            "Epoch 5, Batch 153, G Loss: 0.6931462287902832, D Loss: 1.3860821723937988\n",
            "Epoch 5, Batch 154, G Loss: 0.6931462287902832, D Loss: 1.3861360549926758\n",
            "Epoch 5, Batch 155, G Loss: 0.6931431889533997, D Loss: 1.386141061782837\n",
            "Epoch 5, Batch 156, G Loss: 0.6931498050689697, D Loss: 1.3861570358276367\n",
            "Epoch 5, Batch 157, G Loss: 0.6931610107421875, D Loss: 1.3860504627227783\n",
            "Epoch 5, Batch 158, G Loss: 0.6931617259979248, D Loss: 1.3861675262451172\n",
            "Epoch 5, Batch 159, G Loss: 0.6931732892990112, D Loss: 1.3862361907958984\n",
            "Epoch 5, Batch 160, G Loss: 0.6931924819946289, D Loss: 1.3858654499053955\n",
            "Epoch 5, Batch 161, G Loss: 0.6931809186935425, D Loss: 1.3858284950256348\n",
            "Epoch 5, Batch 162, G Loss: 0.6931241750717163, D Loss: 1.3859790563583374\n",
            "Epoch 5, Batch 163, G Loss: 0.6930596232414246, D Loss: 1.3860177993774414\n",
            "Epoch 5, Batch 164, G Loss: 0.6929925680160522, D Loss: 1.3859615325927734\n",
            "Epoch 5, Batch 165, G Loss: 0.692920446395874, D Loss: 1.3860383033752441\n",
            "Epoch 5, Batch 166, G Loss: 0.6928479075431824, D Loss: 1.3861076831817627\n",
            "Epoch 5, Batch 167, G Loss: 0.6927884221076965, D Loss: 1.386260747909546\n",
            "Epoch 5, Batch 168, G Loss: 0.6927569508552551, D Loss: 1.3861620426177979\n",
            "Epoch 5, Batch 169, G Loss: 0.6927409172058105, D Loss: 1.3860712051391602\n",
            "Epoch 5, Batch 170, G Loss: 0.6927317380905151, D Loss: 1.3860595226287842\n",
            "Epoch 5, Batch 171, G Loss: 0.6927189826965332, D Loss: 1.386101245880127\n",
            "Epoch 5, Batch 172, G Loss: 0.6927093863487244, D Loss: 1.3859899044036865\n",
            "Epoch 5, Batch 173, G Loss: 0.6927038431167603, D Loss: 1.3859440088272095\n",
            "Epoch 5, Batch 174, G Loss: 0.6926841139793396, D Loss: 1.3861401081085205\n",
            "Epoch 5, Batch 175, G Loss: 0.6926752328872681, D Loss: 1.386089563369751\n",
            "Epoch 5, Batch 176, G Loss: 0.6926754713058472, D Loss: 1.386121153831482\n",
            "Epoch 5, Batch 177, G Loss: 0.6926764845848083, D Loss: 1.386196494102478\n",
            "Epoch 5, Batch 178, G Loss: 0.6926882266998291, D Loss: 1.386107087135315\n",
            "Epoch 5, Batch 179, G Loss: 0.6927071213722229, D Loss: 1.3861732482910156\n",
            "Epoch 5, Batch 180, G Loss: 0.692735493183136, D Loss: 1.3861697912216187\n",
            "Epoch 5, Batch 181, G Loss: 0.6927662491798401, D Loss: 1.3861862421035767\n",
            "Epoch 5, Batch 182, G Loss: 0.6928074359893799, D Loss: 1.3861700296401978\n",
            "Epoch 5, Batch 183, G Loss: 0.6928508281707764, D Loss: 1.3861403465270996\n",
            "Epoch 5, Batch 184, G Loss: 0.6928938031196594, D Loss: 1.386204719543457\n",
            "Epoch 5, Batch 185, G Loss: 0.6929412484169006, D Loss: 1.3861554861068726\n",
            "Epoch 5, Batch 186, G Loss: 0.6929935812950134, D Loss: 1.386167049407959\n",
            "Epoch 5, Batch 187, G Loss: 0.6930456161499023, D Loss: 1.386214256286621\n",
            "Epoch 5, Batch 188, G Loss: 0.6931009292602539, D Loss: 1.3859328031539917\n",
            "Epoch 5, Batch 189, G Loss: 0.693137526512146, D Loss: 1.3860687017440796\n",
            "Epoch 5, Batch 190, G Loss: 0.6931667923927307, D Loss: 1.3860788345336914\n",
            "Epoch 5, Batch 191, G Loss: 0.6931949257850647, D Loss: 1.3860182762145996\n",
            "Epoch 5, Batch 192, G Loss: 0.6932061910629272, D Loss: 1.3859879970550537\n",
            "Epoch 5, Batch 193, G Loss: 0.6932100057601929, D Loss: 1.3860676288604736\n",
            "Epoch 5, Batch 194, G Loss: 0.6932128667831421, D Loss: 1.386064052581787\n",
            "Epoch 5, Batch 195, G Loss: 0.693207859992981, D Loss: 1.3859748840332031\n",
            "Epoch 5, Batch 196, G Loss: 0.6931973099708557, D Loss: 1.3860143423080444\n",
            "Epoch 5, Batch 197, G Loss: 0.6931807994842529, D Loss: 1.3859889507293701\n",
            "Epoch 5, Batch 198, G Loss: 0.6931579113006592, D Loss: 1.385984182357788\n",
            "Epoch 5, Batch 199, G Loss: 0.6931251287460327, D Loss: 1.3860831260681152\n",
            "Epoch 5, Batch 200, G Loss: 0.6931040287017822, D Loss: 1.3862404823303223\n",
            "Epoch 5, Batch 201, G Loss: 0.6930911540985107, D Loss: 1.3863739967346191\n",
            "Epoch 5, Batch 202, G Loss: 0.6931050419807434, D Loss: 1.386083960533142\n",
            "Epoch 5, Batch 203, G Loss: 0.693117082118988, D Loss: 1.3858599662780762\n",
            "Epoch 5, Batch 204, G Loss: 0.6931119561195374, D Loss: 1.3858885765075684\n",
            "Epoch 5, Batch 205, G Loss: 0.6931027770042419, D Loss: 1.3857007026672363\n",
            "Epoch 5, Batch 206, G Loss: 0.6930615305900574, D Loss: 1.3860150575637817\n",
            "Epoch 5, Batch 207, G Loss: 0.6930233836174011, D Loss: 1.3859342336654663\n",
            "Epoch 5, Batch 208, G Loss: 0.6929875016212463, D Loss: 1.386006474494934\n",
            "Epoch 5, Batch 209, G Loss: 0.6929486393928528, D Loss: 1.3859972953796387\n",
            "Epoch 5, Batch 210, G Loss: 0.6929107308387756, D Loss: 1.386061668395996\n",
            "Epoch 5, Batch 211, G Loss: 0.6928815245628357, D Loss: 1.385960340499878\n",
            "Epoch 5, Batch 212, G Loss: 0.6928542852401733, D Loss: 1.3857896327972412\n",
            "Epoch 5, Batch 213, G Loss: 0.692814826965332, D Loss: 1.3859885931015015\n",
            "Epoch 5, Batch 214, G Loss: 0.6927804946899414, D Loss: 1.3863537311553955\n",
            "Epoch 5, Batch 215, G Loss: 0.6927704811096191, D Loss: 1.3864988088607788\n",
            "Epoch 5, Batch 216, G Loss: 0.6927904486656189, D Loss: 1.3863131999969482\n",
            "Epoch 5, Batch 217, G Loss: 0.692826509475708, D Loss: 1.386153221130371\n",
            "Epoch 5, Batch 218, G Loss: 0.6928651928901672, D Loss: 1.3861472606658936\n",
            "Epoch 5, Batch 219, G Loss: 0.6929110884666443, D Loss: 1.386301040649414\n",
            "Epoch 5, Batch 220, G Loss: 0.692960798740387, D Loss: 1.3858294486999512\n",
            "Epoch 5, Batch 221, G Loss: 0.6929975152015686, D Loss: 1.3858659267425537\n",
            "Epoch 5, Batch 222, G Loss: 0.6930222511291504, D Loss: 1.3857718706130981\n",
            "Epoch 5, Batch 223, G Loss: 0.6930267810821533, D Loss: 1.385929822921753\n",
            "Epoch 5, Batch 224, G Loss: 0.6930267810821533, D Loss: 1.3859214782714844\n",
            "Epoch 5, Batch 225, G Loss: 0.6930140256881714, D Loss: 1.3860344886779785\n",
            "Epoch 5, Batch 226, G Loss: 0.6930114030838013, D Loss: 1.3861989974975586\n",
            "Epoch 5, Batch 227, G Loss: 0.6930012106895447, D Loss: 1.3863111734390259\n",
            "Epoch 5, Batch 228, G Loss: 0.6930172443389893, D Loss: 1.3863716125488281\n",
            "Epoch 5, Batch 229, G Loss: 0.6930521130561829, D Loss: 1.3862414360046387\n",
            "Epoch 5, Batch 230, G Loss: 0.6930884718894958, D Loss: 1.3860528469085693\n",
            "Epoch 5, Batch 231, G Loss: 0.6931215524673462, D Loss: 1.3862346410751343\n",
            "Epoch 5, Batch 232, G Loss: 0.6931622624397278, D Loss: 1.3861806392669678\n",
            "Epoch 5, Batch 233, G Loss: 0.693202555179596, D Loss: 1.386211633682251\n",
            "Epoch 5, Batch 234, G Loss: 0.6932500600814819, D Loss: 1.3862195014953613\n",
            "Epoch 5, Batch 235, G Loss: 0.6932969093322754, D Loss: 1.3864033222198486\n",
            "Epoch 5, Batch 236, G Loss: 0.6933621764183044, D Loss: 1.3861430883407593\n",
            "Epoch 5, Batch 237, G Loss: 0.6934179067611694, D Loss: 1.386286735534668\n",
            "Epoch 5, Batch 238, G Loss: 0.6934784650802612, D Loss: 1.386370301246643\n",
            "Epoch 5, Batch 239, G Loss: 0.6935498118400574, D Loss: 1.3862687349319458\n",
            "Epoch 5, Batch 240, G Loss: 0.6936221718788147, D Loss: 1.3863177299499512\n",
            "Epoch 5, Batch 241, G Loss: 0.6936958432197571, D Loss: 1.3863427639007568\n",
            "Epoch 5, Batch 242, G Loss: 0.6937795281410217, D Loss: 1.3863239288330078\n",
            "Epoch 5, Batch 243, G Loss: 0.6938579082489014, D Loss: 1.386204719543457\n",
            "Epoch 5, Batch 244, G Loss: 0.693941593170166, D Loss: 1.3862242698669434\n",
            "Epoch 5, Batch 245, G Loss: 0.6940101385116577, D Loss: 1.3861730098724365\n",
            "Epoch 5, Batch 246, G Loss: 0.6940819621086121, D Loss: 1.3861684799194336\n",
            "Epoch 5, Batch 247, G Loss: 0.6941370368003845, D Loss: 1.3861541748046875\n",
            "Epoch 5, Batch 248, G Loss: 0.6941856741905212, D Loss: 1.3862173557281494\n",
            "Epoch 5, Batch 249, G Loss: 0.6942333579063416, D Loss: 1.3860759735107422\n",
            "Epoch 5, Batch 250, G Loss: 0.694270670413971, D Loss: 1.3861894607543945\n",
            "Epoch 5, Batch 251, G Loss: 0.6942941546440125, D Loss: 1.3861007690429688\n",
            "Epoch 5, Batch 252, G Loss: 0.6943085193634033, D Loss: 1.3860359191894531\n",
            "Epoch 5, Batch 253, G Loss: 0.6943049430847168, D Loss: 1.3860491514205933\n",
            "Epoch 5, Batch 254, G Loss: 0.6942839622497559, D Loss: 1.3860986232757568\n",
            "Epoch 5, Batch 255, G Loss: 0.6942532658576965, D Loss: 1.3861407041549683\n",
            "Epoch 5, Batch 256, G Loss: 0.6942239999771118, D Loss: 1.3861243724822998\n",
            "Epoch 5, Batch 257, G Loss: 0.6941909790039062, D Loss: 1.3861403465270996\n",
            "Epoch 5, Batch 258, G Loss: 0.6941612958908081, D Loss: 1.3861453533172607\n",
            "Epoch 5, Batch 259, G Loss: 0.6941277384757996, D Loss: 1.3861273527145386\n",
            "Epoch 5, Batch 260, G Loss: 0.6941001415252686, D Loss: 1.3861483335494995\n",
            "Epoch 5, Batch 261, G Loss: 0.6940709352493286, D Loss: 1.3861956596374512\n",
            "Epoch 5, Batch 262, G Loss: 0.6940580606460571, D Loss: 1.386145830154419\n",
            "Epoch 5, Batch 263, G Loss: 0.6940486431121826, D Loss: 1.386101245880127\n",
            "Epoch 5, Batch 264, G Loss: 0.6940338611602783, D Loss: 1.3861534595489502\n",
            "Epoch 5, Batch 265, G Loss: 0.6940295100212097, D Loss: 1.386160135269165\n",
            "Epoch 5, Batch 266, G Loss: 0.6940368413925171, D Loss: 1.3861258029937744\n",
            "Epoch 5, Batch 267, G Loss: 0.6940327882766724, D Loss: 1.3861560821533203\n",
            "Epoch 5, Batch 268, G Loss: 0.6940403580665588, D Loss: 1.3860841989517212\n",
            "Epoch 5, Batch 269, G Loss: 0.6940377950668335, D Loss: 1.3860626220703125\n",
            "Epoch 5, Batch 270, G Loss: 0.6940213441848755, D Loss: 1.3860851526260376\n",
            "Epoch 5, Batch 271, G Loss: 0.6939951181411743, D Loss: 1.3860514163970947\n",
            "Epoch 5, Batch 272, G Loss: 0.6939530372619629, D Loss: 1.3861109018325806\n",
            "Epoch 5, Batch 273, G Loss: 0.6939137578010559, D Loss: 1.386122465133667\n",
            "Epoch 5, Batch 274, G Loss: 0.6938860416412354, D Loss: 1.3860976696014404\n",
            "Epoch 5, Batch 275, G Loss: 0.693864107131958, D Loss: 1.386110782623291\n",
            "Epoch 5, Batch 276, G Loss: 0.6938519477844238, D Loss: 1.3861002922058105\n",
            "Epoch 5, Batch 277, G Loss: 0.6938564777374268, D Loss: 1.386099934577942\n",
            "Epoch 5, Batch 278, G Loss: 0.6938691139221191, D Loss: 1.3861160278320312\n",
            "Epoch 5, Batch 279, G Loss: 0.6938913464546204, D Loss: 1.3861088752746582\n",
            "Epoch 5, Batch 280, G Loss: 0.6939246654510498, D Loss: 1.3860905170440674\n",
            "Epoch 5, Batch 281, G Loss: 0.6939675211906433, D Loss: 1.386095404624939\n",
            "Epoch 5, Batch 282, G Loss: 0.693996012210846, D Loss: 1.3860933780670166\n",
            "Epoch 5, Batch 283, G Loss: 0.6940249800682068, D Loss: 1.3860803842544556\n",
            "Epoch 5, Batch 284, G Loss: 0.6940569281578064, D Loss: 1.3861037492752075\n",
            "Epoch 5, Batch 285, G Loss: 0.6940723061561584, D Loss: 1.3861185312271118\n",
            "Epoch 5, Batch 286, G Loss: 0.6940813660621643, D Loss: 1.3860832452774048\n",
            "Epoch 5, Batch 287, G Loss: 0.6940882802009583, D Loss: 1.3860433101654053\n",
            "Epoch 5, Batch 288, G Loss: 0.69410240650177, D Loss: 1.3860814571380615\n",
            "Epoch 5, Batch 289, G Loss: 0.6941070556640625, D Loss: 1.3860511779785156\n",
            "Epoch 5, Batch 290, G Loss: 0.6941201090812683, D Loss: 1.386082410812378\n",
            "Epoch 5, Batch 291, G Loss: 0.6941254138946533, D Loss: 1.3860511779785156\n",
            "Epoch 5, Batch 292, G Loss: 0.6941187381744385, D Loss: 1.3860833644866943\n",
            "Epoch 5, Batch 293, G Loss: 0.6941090822219849, D Loss: 1.3860406875610352\n",
            "Epoch 5, Batch 294, G Loss: 0.6940958499908447, D Loss: 1.3860466480255127\n",
            "Epoch 5, Batch 295, G Loss: 0.6940801739692688, D Loss: 1.3860323429107666\n",
            "Epoch 5, Batch 296, G Loss: 0.6940689086914062, D Loss: 1.386081337928772\n",
            "Epoch 5, Batch 297, G Loss: 0.6940503716468811, D Loss: 1.3860421180725098\n",
            "Epoch 5, Batch 298, G Loss: 0.6940321922302246, D Loss: 1.3859467506408691\n",
            "Epoch 5, Batch 299, G Loss: 0.6940273642539978, D Loss: 1.3859877586364746\n",
            "Epoch 5, Batch 300, G Loss: 0.6940274834632874, D Loss: 1.3859843015670776\n",
            "Epoch 5, Batch 301, G Loss: 0.6940271854400635, D Loss: 1.3863317966461182\n",
            "Epoch 5, Batch 302, G Loss: 0.693996012210846, D Loss: 1.3861355781555176\n",
            "Epoch 5, Batch 303, G Loss: 0.6939554214477539, D Loss: 1.386292576789856\n",
            "Epoch 5, Batch 304, G Loss: 0.6938825249671936, D Loss: 1.3862420320510864\n",
            "Epoch 5, Batch 305, G Loss: 0.6937995553016663, D Loss: 1.3862518072128296\n",
            "Epoch 5, Batch 306, G Loss: 0.6936985850334167, D Loss: 1.386284589767456\n",
            "Epoch 5, Batch 307, G Loss: 0.6935837268829346, D Loss: 1.3862314224243164\n",
            "Epoch 5, Batch 308, G Loss: 0.6934641599655151, D Loss: 1.3861948251724243\n",
            "Epoch 5, Batch 309, G Loss: 0.6933461427688599, D Loss: 1.3862204551696777\n",
            "Epoch 5, Batch 310, G Loss: 0.6932353377342224, D Loss: 1.3861982822418213\n",
            "Epoch 5, Batch 311, G Loss: 0.693116307258606, D Loss: 1.3861398696899414\n",
            "Epoch 5, Batch 312, G Loss: 0.6930091977119446, D Loss: 1.3862088918685913\n",
            "Epoch 5, Batch 313, G Loss: 0.692894458770752, D Loss: 1.3861489295959473\n",
            "Epoch 5, Batch 314, G Loss: 0.6927856206893921, D Loss: 1.3861159086227417\n",
            "Epoch 5, Batch 315, G Loss: 0.6926940083503723, D Loss: 1.3861274719238281\n",
            "Epoch 5, Batch 316, G Loss: 0.692609965801239, D Loss: 1.3860950469970703\n",
            "Epoch 5, Batch 317, G Loss: 0.6925413012504578, D Loss: 1.3860727548599243\n",
            "Epoch 5, Batch 318, G Loss: 0.6924880743026733, D Loss: 1.3860474824905396\n",
            "Epoch 5, Batch 319, G Loss: 0.6924608945846558, D Loss: 1.3861114978790283\n",
            "Epoch 5, Batch 320, G Loss: 0.69243323802948, D Loss: 1.386077642440796\n",
            "Epoch 5, Batch 321, G Loss: 0.6924262642860413, D Loss: 1.3861031532287598\n",
            "Epoch 5, Batch 322, G Loss: 0.6924124956130981, D Loss: 1.3860950469970703\n",
            "Epoch 5, Batch 323, G Loss: 0.6924065351486206, D Loss: 1.3861334323883057\n",
            "Epoch 5, Batch 324, G Loss: 0.6923973560333252, D Loss: 1.3861181735992432\n",
            "Epoch 5, Batch 325, G Loss: 0.6923932433128357, D Loss: 1.3860803842544556\n",
            "Epoch 5, Batch 326, G Loss: 0.6924020648002625, D Loss: 1.3861000537872314\n",
            "Epoch 5, Batch 327, G Loss: 0.6924118399620056, D Loss: 1.3860902786254883\n",
            "Epoch 5, Batch 328, G Loss: 0.6924346685409546, D Loss: 1.386109471321106\n",
            "Epoch 5, Batch 329, G Loss: 0.692450225353241, D Loss: 1.3860855102539062\n",
            "Epoch 5, Batch 330, G Loss: 0.692474365234375, D Loss: 1.386054515838623\n",
            "Epoch 5, Batch 331, G Loss: 0.6925209164619446, D Loss: 1.3860790729522705\n",
            "Epoch 5, Batch 332, G Loss: 0.6925711631774902, D Loss: 1.3860795497894287\n",
            "Epoch 5, Batch 333, G Loss: 0.6926286816596985, D Loss: 1.3860862255096436\n",
            "Epoch 5, Batch 334, G Loss: 0.692676842212677, D Loss: 1.3860657215118408\n",
            "Epoch 5, Batch 335, G Loss: 0.6927281022071838, D Loss: 1.3860931396484375\n",
            "Epoch 5, Batch 336, G Loss: 0.6927722096443176, D Loss: 1.386103630065918\n",
            "Epoch 5, Batch 337, G Loss: 0.6928104758262634, D Loss: 1.386094331741333\n",
            "Epoch 5, Batch 338, G Loss: 0.6928300857543945, D Loss: 1.3860490322113037\n",
            "Epoch 5, Batch 339, G Loss: 0.6928751468658447, D Loss: 1.3860689401626587\n",
            "Epoch 5, Batch 340, G Loss: 0.6929284930229187, D Loss: 1.386047601699829\n",
            "Epoch 5, Batch 341, G Loss: 0.6930069327354431, D Loss: 1.386145830154419\n",
            "Epoch 5, Batch 342, G Loss: 0.6930519938468933, D Loss: 1.3861324787139893\n",
            "Epoch 5, Batch 343, G Loss: 0.6930719614028931, D Loss: 1.3861441612243652\n",
            "Epoch 5, Batch 344, G Loss: 0.6930656433105469, D Loss: 1.3861229419708252\n",
            "Epoch 5, Batch 345, G Loss: 0.6930409669876099, D Loss: 1.38607656955719\n",
            "Epoch 5, Batch 346, G Loss: 0.6930174827575684, D Loss: 1.3860728740692139\n",
            "Epoch 5, Batch 347, G Loss: 0.6930063366889954, D Loss: 1.386070966720581\n",
            "Epoch 5, Batch 348, G Loss: 0.6929993629455566, D Loss: 1.386080026626587\n",
            "Epoch 5, Batch 349, G Loss: 0.6929962635040283, D Loss: 1.3860819339752197\n",
            "Epoch 5, Batch 350, G Loss: 0.6930074095726013, D Loss: 1.3860828876495361\n",
            "Epoch 5, Batch 351, G Loss: 0.6930213570594788, D Loss: 1.3860838413238525\n",
            "Epoch 5, Batch 352, G Loss: 0.6930429935455322, D Loss: 1.3860883712768555\n",
            "Epoch 5, Batch 353, G Loss: 0.69306480884552, D Loss: 1.386091947555542\n",
            "Epoch 5, Batch 354, G Loss: 0.6930869817733765, D Loss: 1.3860931396484375\n",
            "Epoch 5, Batch 355, G Loss: 0.6930996775627136, D Loss: 1.3860938549041748\n",
            "Epoch 5, Batch 356, G Loss: 0.6930857300758362, D Loss: 1.3860845565795898\n",
            "Epoch 5, Batch 357, G Loss: 0.6930558085441589, D Loss: 1.3860993385314941\n",
            "Epoch 5, Batch 358, G Loss: 0.6930211782455444, D Loss: 1.3860816955566406\n",
            "Epoch 5, Batch 359, G Loss: 0.6929833292961121, D Loss: 1.3860652446746826\n",
            "Epoch 5, Batch 360, G Loss: 0.6929326057434082, D Loss: 1.386062502861023\n",
            "Epoch 5, Batch 361, G Loss: 0.6928759217262268, D Loss: 1.3860251903533936\n",
            "Epoch 5, Batch 362, G Loss: 0.6928077936172485, D Loss: 1.386071801185608\n",
            "Epoch 5, Batch 363, G Loss: 0.6927388906478882, D Loss: 1.3861005306243896\n",
            "Epoch 5, Batch 364, G Loss: 0.6926899552345276, D Loss: 1.386093020439148\n",
            "Epoch 5, Batch 365, G Loss: 0.6926459670066833, D Loss: 1.3860695362091064\n",
            "Epoch 5, Batch 366, G Loss: 0.6926109790802002, D Loss: 1.3860610723495483\n",
            "Epoch 5, Batch 367, G Loss: 0.6925816535949707, D Loss: 1.3861079216003418\n",
            "Epoch 5, Batch 368, G Loss: 0.6925634145736694, D Loss: 1.3861253261566162\n",
            "Epoch 5, Batch 369, G Loss: 0.6925671100616455, D Loss: 1.3861162662506104\n",
            "Epoch 5, Batch 370, G Loss: 0.6925811171531677, D Loss: 1.3861620426177979\n",
            "Epoch 5, Batch 371, G Loss: 0.6926162838935852, D Loss: 1.3861513137817383\n",
            "Epoch 5, Batch 372, G Loss: 0.6926729679107666, D Loss: 1.3860397338867188\n",
            "Epoch 5, Batch 373, G Loss: 0.6927213668823242, D Loss: 1.3860483169555664\n",
            "Epoch 5, Batch 374, G Loss: 0.6927677989006042, D Loss: 1.386098861694336\n",
            "Epoch 5, Batch 375, G Loss: 0.6928186416625977, D Loss: 1.3860812187194824\n",
            "Epoch 5, Batch 376, G Loss: 0.6928721070289612, D Loss: 1.3860840797424316\n",
            "Epoch 5, Batch 377, G Loss: 0.6929148435592651, D Loss: 1.3860859870910645\n",
            "Epoch 5, Batch 378, G Loss: 0.6929623484611511, D Loss: 1.3861265182495117\n",
            "Epoch 5, Batch 379, G Loss: 0.6930082440376282, D Loss: 1.3861160278320312\n",
            "Epoch 5, Batch 380, G Loss: 0.6930574774742126, D Loss: 1.3860969543457031\n",
            "Epoch 5, Batch 381, G Loss: 0.6931091547012329, D Loss: 1.386162519454956\n",
            "Epoch 5, Batch 382, G Loss: 0.6931648850440979, D Loss: 1.3860235214233398\n",
            "Epoch 5, Batch 383, G Loss: 0.6932032108306885, D Loss: 1.3860695362091064\n",
            "Epoch 5, Batch 384, G Loss: 0.6932356953620911, D Loss: 1.3861148357391357\n",
            "Epoch 5, Batch 385, G Loss: 0.6932740211486816, D Loss: 1.3860595226287842\n",
            "Epoch 5, Batch 386, G Loss: 0.6933017373085022, D Loss: 1.3860307931900024\n",
            "Epoch 5, Batch 387, G Loss: 0.693309485912323, D Loss: 1.3860950469970703\n",
            "Epoch 5, Batch 388, G Loss: 0.6933201551437378, D Loss: 1.386113166809082\n",
            "Epoch 5, Batch 389, G Loss: 0.6933311223983765, D Loss: 1.3861169815063477\n",
            "Epoch 5, Batch 390, G Loss: 0.6933444142341614, D Loss: 1.3862366676330566\n",
            "Epoch 5, Batch 391, G Loss: 0.6933897137641907, D Loss: 1.3861209154129028\n",
            "Epoch 5, Batch 392, G Loss: 0.6934333443641663, D Loss: 1.3861435651779175\n",
            "Epoch 5, Batch 393, G Loss: 0.6934819221496582, D Loss: 1.3860995769500732\n",
            "Epoch 5, Batch 394, G Loss: 0.6935324668884277, D Loss: 1.3860828876495361\n",
            "Epoch 5, Batch 395, G Loss: 0.6935821771621704, D Loss: 1.3860995769500732\n",
            "Epoch 5, Batch 396, G Loss: 0.69362872838974, D Loss: 1.3859748840332031\n",
            "Epoch 5, Batch 397, G Loss: 0.6936299800872803, D Loss: 1.3860139846801758\n",
            "Epoch 5, Batch 398, G Loss: 0.6936025023460388, D Loss: 1.3860584497451782\n",
            "Epoch 5, Batch 399, G Loss: 0.6935597658157349, D Loss: 1.3860485553741455\n",
            "Epoch 5, Batch 400, G Loss: 0.6935059428215027, D Loss: 1.386127233505249\n",
            "Epoch 5, Batch 401, G Loss: 0.6934682130813599, D Loss: 1.3860805034637451\n",
            "Epoch 5, Batch 402, G Loss: 0.6934348344802856, D Loss: 1.38605797290802\n",
            "Epoch 5, Batch 403, G Loss: 0.693394124507904, D Loss: 1.3861453533172607\n",
            "Epoch 5, Batch 404, G Loss: 0.6933752298355103, D Loss: 1.3861901760101318\n",
            "Epoch 5, Batch 405, G Loss: 0.6933805346488953, D Loss: 1.386192798614502\n",
            "Epoch 5, Batch 406, G Loss: 0.693414032459259, D Loss: 1.3861621618270874\n",
            "Epoch 5, Batch 407, G Loss: 0.6934635639190674, D Loss: 1.3860435485839844\n",
            "Epoch 5, Batch 408, G Loss: 0.6934982538223267, D Loss: 1.3860983848571777\n",
            "Epoch 5, Batch 409, G Loss: 0.6935286521911621, D Loss: 1.3861017227172852\n",
            "Epoch 5, Batch 410, G Loss: 0.6935603618621826, D Loss: 1.3860900402069092\n",
            "Epoch 5, Batch 411, G Loss: 0.6935868263244629, D Loss: 1.3861151933670044\n",
            "Epoch 5, Batch 412, G Loss: 0.6936193108558655, D Loss: 1.38608980178833\n",
            "Epoch 5, Batch 413, G Loss: 0.6936579942703247, D Loss: 1.3861078023910522\n",
            "Epoch 5, Batch 414, G Loss: 0.6937039494514465, D Loss: 1.3860911130905151\n",
            "Epoch 5, Batch 415, G Loss: 0.6937515139579773, D Loss: 1.3860926628112793\n",
            "Epoch 5, Batch 416, G Loss: 0.6938151121139526, D Loss: 1.386070728302002\n",
            "Epoch 5, Batch 417, G Loss: 0.6938848495483398, D Loss: 1.3860703706741333\n",
            "Epoch 5, Batch 418, G Loss: 0.693943977355957, D Loss: 1.3860836029052734\n",
            "Epoch 5, Batch 419, G Loss: 0.6939829587936401, D Loss: 1.3861019611358643\n",
            "Epoch 5, Batch 420, G Loss: 0.6940070390701294, D Loss: 1.386094331741333\n",
            "Epoch 5, Batch 421, G Loss: 0.6940189003944397, D Loss: 1.3860681056976318\n",
            "Epoch 5, Batch 422, G Loss: 0.6940246224403381, D Loss: 1.3860514163970947\n",
            "Epoch 5, Batch 423, G Loss: 0.6940342783927917, D Loss: 1.3860559463500977\n",
            "Epoch 5, Batch 424, G Loss: 0.6940402388572693, D Loss: 1.3861138820648193\n",
            "Epoch 5, Batch 425, G Loss: 0.6940226554870605, D Loss: 1.3861043453216553\n",
            "Epoch 5, Batch 426, G Loss: 0.6939815878868103, D Loss: 1.3860933780670166\n",
            "Epoch 5, Batch 427, G Loss: 0.693930983543396, D Loss: 1.386092185974121\n",
            "Epoch 5, Batch 428, G Loss: 0.6938702464103699, D Loss: 1.38606595993042\n",
            "Epoch 5, Batch 429, G Loss: 0.6938087344169617, D Loss: 1.386075735092163\n",
            "Epoch 5, Batch 430, G Loss: 0.6937405467033386, D Loss: 1.3860392570495605\n",
            "Epoch 5, Batch 431, G Loss: 0.6936864852905273, D Loss: 1.3860468864440918\n",
            "Epoch 5, Batch 432, G Loss: 0.6936498880386353, D Loss: 1.3860646486282349\n",
            "Epoch 5, Batch 433, G Loss: 0.6936145424842834, D Loss: 1.3860244750976562\n",
            "Epoch 5, Batch 434, G Loss: 0.6935977339744568, D Loss: 1.3860163688659668\n",
            "Epoch 5, Batch 435, G Loss: 0.6935904622077942, D Loss: 1.3859939575195312\n",
            "Epoch 5, Batch 436, G Loss: 0.6936041116714478, D Loss: 1.3860344886779785\n",
            "Epoch 5, Batch 437, G Loss: 0.6936177611351013, D Loss: 1.3861536979675293\n",
            "Epoch 5, Batch 438, G Loss: 0.6936136484146118, D Loss: 1.3861247301101685\n",
            "Epoch 5, Batch 439, G Loss: 0.6935957074165344, D Loss: 1.3861420154571533\n",
            "Epoch 5, Batch 440, G Loss: 0.6935649514198303, D Loss: 1.3861507177352905\n",
            "Epoch 5, Batch 441, G Loss: 0.6935228705406189, D Loss: 1.3861548900604248\n",
            "Epoch 5, Batch 442, G Loss: 0.6934619545936584, D Loss: 1.3861427307128906\n",
            "Epoch 5, Batch 443, G Loss: 0.693388044834137, D Loss: 1.3861188888549805\n",
            "Epoch 5, Batch 444, G Loss: 0.6933082342147827, D Loss: 1.386173963546753\n",
            "Epoch 5, Batch 445, G Loss: 0.6932177543640137, D Loss: 1.3861534595489502\n",
            "Epoch 5, Batch 446, G Loss: 0.6931180357933044, D Loss: 1.3861312866210938\n",
            "Epoch 5, Batch 447, G Loss: 0.6930214166641235, D Loss: 1.386127233505249\n",
            "Epoch 5, Batch 448, G Loss: 0.6929237246513367, D Loss: 1.3860828876495361\n",
            "Epoch 5, Batch 449, G Loss: 0.692839503288269, D Loss: 1.3860583305358887\n",
            "Epoch 5, Batch 450, G Loss: 0.6927720904350281, D Loss: 1.3860816955566406\n",
            "Epoch 5, Batch 451, G Loss: 0.6927124857902527, D Loss: 1.3860702514648438\n",
            "Epoch 5, Batch 452, G Loss: 0.6926706433296204, D Loss: 1.3860801458358765\n",
            "Epoch 5, Batch 453, G Loss: 0.6926243901252747, D Loss: 1.3860642910003662\n",
            "Epoch 5, Batch 454, G Loss: 0.6925785541534424, D Loss: 1.3860644102096558\n",
            "Epoch 5, Batch 455, G Loss: 0.6925420761108398, D Loss: 1.386051893234253\n",
            "Epoch 5, Batch 456, G Loss: 0.6925316452980042, D Loss: 1.3860430717468262\n",
            "Epoch 5, Batch 457, G Loss: 0.6925458312034607, D Loss: 1.3860493898391724\n",
            "Epoch 5, Batch 458, G Loss: 0.6925758719444275, D Loss: 1.3860869407653809\n",
            "Epoch 5, Batch 459, G Loss: 0.6925894021987915, D Loss: 1.3860714435577393\n",
            "Epoch 5, Batch 460, G Loss: 0.6925896406173706, D Loss: 1.3860665559768677\n",
            "Epoch 5, Batch 461, G Loss: 0.692573070526123, D Loss: 1.386068344116211\n",
            "Epoch 5, Batch 462, G Loss: 0.6925463080406189, D Loss: 1.3860951662063599\n",
            "Epoch 5, Batch 463, G Loss: 0.6925404071807861, D Loss: 1.3860960006713867\n",
            "Epoch 5, Batch 464, G Loss: 0.6925556659698486, D Loss: 1.3860853910446167\n",
            "Epoch 5, Batch 465, G Loss: 0.6925845742225647, D Loss: 1.3861199617385864\n",
            "Epoch 5, Batch 466, G Loss: 0.6926397085189819, D Loss: 1.3861184120178223\n",
            "Epoch 5, Batch 467, G Loss: 0.692712128162384, D Loss: 1.3861093521118164\n",
            "Epoch 5, Batch 468, G Loss: 0.6928026080131531, D Loss: 1.3860909938812256\n",
            "Epoch 5, Batch 469, G Loss: 0.6929033994674683, D Loss: 1.3860909938812256\n",
            "Epoch 5, Batch 470, G Loss: 0.6930246949195862, D Loss: 1.3860547542572021\n",
            "Epoch 5, Batch 471, G Loss: 0.6931401491165161, D Loss: 1.3860723972320557\n",
            "Epoch 5, Batch 472, G Loss: 0.6932428479194641, D Loss: 1.3860753774642944\n",
            "Epoch 5, Batch 473, G Loss: 0.6933472156524658, D Loss: 1.3860679864883423\n",
            "Epoch 5, Batch 474, G Loss: 0.6934453248977661, D Loss: 1.386064052581787\n",
            "Epoch 5, Batch 475, G Loss: 0.6935362815856934, D Loss: 1.3860814571380615\n",
            "Epoch 5, Batch 476, G Loss: 0.6936131715774536, D Loss: 1.3860883712768555\n",
            "Epoch 5, Batch 477, G Loss: 0.6936661601066589, D Loss: 1.3860771656036377\n",
            "Epoch 5, Batch 478, G Loss: 0.6937018632888794, D Loss: 1.386064052581787\n",
            "Epoch 5, Batch 479, G Loss: 0.6937349438667297, D Loss: 1.3859776258468628\n",
            "Epoch 5, Batch 480, G Loss: 0.6937964558601379, D Loss: 1.3859901428222656\n",
            "Epoch 5, Batch 481, G Loss: 0.6938650012016296, D Loss: 1.385980248451233\n",
            "Epoch 5, Batch 482, G Loss: 0.6939462423324585, D Loss: 1.386080265045166\n",
            "Epoch 5, Batch 483, G Loss: 0.6940104961395264, D Loss: 1.38606595993042\n",
            "Epoch 5, Batch 484, G Loss: 0.6940656304359436, D Loss: 1.3860207796096802\n",
            "Epoch 5, Batch 485, G Loss: 0.6941146850585938, D Loss: 1.3860235214233398\n",
            "Epoch 5, Batch 486, G Loss: 0.6941593885421753, D Loss: 1.3860241174697876\n",
            "Epoch 5, Batch 487, G Loss: 0.6942031383514404, D Loss: 1.3859964609146118\n",
            "Epoch 5, Batch 488, G Loss: 0.6942388415336609, D Loss: 1.3859846591949463\n",
            "Epoch 5, Batch 489, G Loss: 0.694273829460144, D Loss: 1.3860069513320923\n",
            "Epoch 5, Batch 490, G Loss: 0.6943005919456482, D Loss: 1.3859953880310059\n",
            "Epoch 5, Batch 491, G Loss: 0.6943278312683105, D Loss: 1.385962724685669\n",
            "Epoch 5, Batch 492, G Loss: 0.6943519711494446, D Loss: 1.3859872817993164\n",
            "Epoch 5, Batch 493, G Loss: 0.6943714618682861, D Loss: 1.3858915567398071\n",
            "Epoch 5, Batch 494, G Loss: 0.6943994760513306, D Loss: 1.3858845233917236\n",
            "Epoch 5, Batch 495, G Loss: 0.6944307684898376, D Loss: 1.3858835697174072\n",
            "Epoch 5, Batch 496, G Loss: 0.6944667100906372, D Loss: 1.385899543762207\n",
            "Epoch 5, Batch 497, G Loss: 0.6944982409477234, D Loss: 1.3859895467758179\n",
            "Epoch 5, Batch 498, G Loss: 0.6945254802703857, D Loss: 1.386006236076355\n",
            "Epoch 5, Batch 499, G Loss: 0.6945403814315796, D Loss: 1.385938286781311\n",
            "Epoch 5, Batch 500, G Loss: 0.6945541501045227, D Loss: 1.3860893249511719\n",
            "Epoch 5, Batch 501, G Loss: 0.6945487856864929, D Loss: 1.3861862421035767\n",
            "Epoch 5, Batch 502, G Loss: 0.6945216655731201, D Loss: 1.3861873149871826\n",
            "Epoch 5, Batch 503, G Loss: 0.6944712996482849, D Loss: 1.3861005306243896\n",
            "Epoch 5, Batch 504, G Loss: 0.6944119930267334, D Loss: 1.3860191106796265\n",
            "Epoch 5, Batch 505, G Loss: 0.6943461894989014, D Loss: 1.386263370513916\n",
            "Epoch 5, Batch 506, G Loss: 0.6942583918571472, D Loss: 1.3862675428390503\n",
            "Epoch 5, Batch 507, G Loss: 0.6941530108451843, D Loss: 1.3864328861236572\n",
            "Epoch 5, Batch 508, G Loss: 0.6940275430679321, D Loss: 1.3862700462341309\n",
            "Epoch 5, Batch 509, G Loss: 0.6938860416412354, D Loss: 1.3860056400299072\n",
            "Epoch 5, Batch 510, G Loss: 0.6937612295150757, D Loss: 1.3860325813293457\n",
            "Epoch 5, Batch 511, G Loss: 0.6936429142951965, D Loss: 1.3859446048736572\n",
            "Epoch 5, Batch 512, G Loss: 0.6935423016548157, D Loss: 1.3858246803283691\n",
            "Epoch 5, Batch 513, G Loss: 0.6934676766395569, D Loss: 1.385977864265442\n",
            "Epoch 5, Batch 514, G Loss: 0.6934046745300293, D Loss: 1.3859891891479492\n",
            "Epoch 5, Batch 515, G Loss: 0.6933616399765015, D Loss: 1.3861346244812012\n",
            "Epoch 5, Batch 516, G Loss: 0.6933098435401917, D Loss: 1.3862342834472656\n",
            "Epoch 5, Batch 517, G Loss: 0.6932474970817566, D Loss: 1.3862766027450562\n",
            "Epoch 5, Batch 518, G Loss: 0.6931686997413635, D Loss: 1.3861300945281982\n",
            "Epoch 5, Batch 519, G Loss: 0.6930985450744629, D Loss: 1.386162519454956\n",
            "Epoch 5, Batch 520, G Loss: 0.693023145198822, D Loss: 1.3861398696899414\n",
            "Epoch 5, Batch 521, G Loss: 0.6929478645324707, D Loss: 1.3861147165298462\n",
            "Epoch 5, Batch 522, G Loss: 0.6928822994232178, D Loss: 1.3859906196594238\n",
            "Epoch 5, Batch 523, G Loss: 0.692827045917511, D Loss: 1.385930061340332\n",
            "Epoch 5, Batch 524, G Loss: 0.6927998065948486, D Loss: 1.3859373331069946\n",
            "Epoch 5, Batch 525, G Loss: 0.6927894949913025, D Loss: 1.3858236074447632\n",
            "Epoch 5, Batch 526, G Loss: 0.6928018927574158, D Loss: 1.3858428001403809\n",
            "Epoch 5, Batch 527, G Loss: 0.692833662033081, D Loss: 1.3859906196594238\n",
            "Epoch 5, Batch 528, G Loss: 0.6928751468658447, D Loss: 1.3861899375915527\n",
            "Epoch 5, Batch 529, G Loss: 0.6929061412811279, D Loss: 1.3863306045532227\n",
            "Epoch 5, Batch 530, G Loss: 0.6929152607917786, D Loss: 1.3861682415008545\n",
            "Epoch 5, Batch 531, G Loss: 0.6929123401641846, D Loss: 1.3860855102539062\n",
            "Epoch 5, Batch 532, G Loss: 0.6929153800010681, D Loss: 1.3858373165130615\n",
            "Epoch 5, Batch 533, G Loss: 0.6929387450218201, D Loss: 1.3860399723052979\n",
            "Epoch 5, Batch 534, G Loss: 0.6929574012756348, D Loss: 1.3860971927642822\n",
            "Epoch 5, Batch 535, G Loss: 0.6929739117622375, D Loss: 1.386002779006958\n",
            "Epoch 5, Batch 536, G Loss: 0.6929915547370911, D Loss: 1.3859502077102661\n",
            "Epoch 5, Batch 537, G Loss: 0.6930244565010071, D Loss: 1.3861331939697266\n",
            "Epoch 5, Batch 538, G Loss: 0.6930447220802307, D Loss: 1.3862144947052002\n",
            "Epoch 5, Batch 539, G Loss: 0.6930506229400635, D Loss: 1.386136770248413\n",
            "Epoch 5, Batch 540, G Loss: 0.6930519342422485, D Loss: 1.3861373662948608\n",
            "Epoch 5, Batch 541, G Loss: 0.6930389404296875, D Loss: 1.386154055595398\n",
            "Epoch 5, Batch 542, G Loss: 0.693025529384613, D Loss: 1.3860868215560913\n",
            "Epoch 5, Batch 543, G Loss: 0.6930068135261536, D Loss: 1.3861067295074463\n",
            "Epoch 5, Batch 544, G Loss: 0.6929834485054016, D Loss: 1.386124610900879\n",
            "Epoch 5, Batch 545, G Loss: 0.6929553151130676, D Loss: 1.3861145973205566\n",
            "Epoch 5, Batch 546, G Loss: 0.6929337382316589, D Loss: 1.3861091136932373\n",
            "Epoch 5, Batch 547, G Loss: 0.6929075121879578, D Loss: 1.3861165046691895\n",
            "Epoch 5, Batch 548, G Loss: 0.6928874254226685, D Loss: 1.3862245082855225\n",
            "Epoch 5, Batch 549, G Loss: 0.6928519606590271, D Loss: 1.386063814163208\n",
            "Epoch 5, Batch 550, G Loss: 0.6928215026855469, D Loss: 1.385956048965454\n",
            "Epoch 5, Batch 551, G Loss: 0.6928061842918396, D Loss: 1.3859769105911255\n",
            "Epoch 5, Batch 552, G Loss: 0.6928070783615112, D Loss: 1.3859856128692627\n",
            "Epoch 5, Batch 553, G Loss: 0.6928203105926514, D Loss: 1.3861372470855713\n",
            "Epoch 5, Batch 554, G Loss: 0.6928326487541199, D Loss: 1.3861494064331055\n",
            "Epoch 5, Batch 555, G Loss: 0.6928335428237915, D Loss: 1.3861716985702515\n",
            "Epoch 5, Batch 556, G Loss: 0.6928191184997559, D Loss: 1.3860247135162354\n",
            "Epoch 5, Batch 557, G Loss: 0.6928123831748962, D Loss: 1.3860031366348267\n",
            "Epoch 5, Batch 558, G Loss: 0.6928191184997559, D Loss: 1.3861002922058105\n",
            "Epoch 5, Batch 559, G Loss: 0.6928222179412842, D Loss: 1.3860385417938232\n",
            "Epoch 5, Batch 560, G Loss: 0.692828893661499, D Loss: 1.3860738277435303\n",
            "Epoch 5, Batch 561, G Loss: 0.6928375959396362, D Loss: 1.3859755992889404\n",
            "Epoch 5, Batch 562, G Loss: 0.6928623914718628, D Loss: 1.3859161138534546\n",
            "Epoch 5, Batch 563, G Loss: 0.6929069757461548, D Loss: 1.3858909606933594\n",
            "Epoch 5, Batch 564, G Loss: 0.6929656863212585, D Loss: 1.3858463764190674\n",
            "Epoch 5, Batch 565, G Loss: 0.6930544972419739, D Loss: 1.3861355781555176\n",
            "Epoch 5, Batch 566, G Loss: 0.6931262016296387, D Loss: 1.3863774538040161\n",
            "Epoch 5, Batch 567, G Loss: 0.6931479573249817, D Loss: 1.3862378597259521\n",
            "Epoch 5, Batch 568, G Loss: 0.6931403279304504, D Loss: 1.3862489461898804\n",
            "Epoch 5, Batch 569, G Loss: 0.6931040287017822, D Loss: 1.386023998260498\n",
            "Epoch 5, Batch 570, G Loss: 0.6930776834487915, D Loss: 1.3860530853271484\n",
            "Epoch 5, Batch 571, G Loss: 0.6930524706840515, D Loss: 1.386011004447937\n",
            "Epoch 5, Batch 572, G Loss: 0.6930331587791443, D Loss: 1.386293649673462\n",
            "Epoch 5, Batch 573, G Loss: 0.6929774284362793, D Loss: 1.3862354755401611\n",
            "Epoch 5, Batch 574, G Loss: 0.6929050087928772, D Loss: 1.3862566947937012\n",
            "Epoch 5, Batch 575, G Loss: 0.6927962899208069, D Loss: 1.3862242698669434\n",
            "Epoch 5, Batch 576, G Loss: 0.6926622986793518, D Loss: 1.3860987424850464\n",
            "Epoch 5, Batch 577, G Loss: 0.692518949508667, D Loss: 1.386082649230957\n",
            "Epoch 5, Batch 578, G Loss: 0.6923941969871521, D Loss: 1.3860805034637451\n",
            "Epoch 5, Batch 579, G Loss: 0.6922985911369324, D Loss: 1.3860735893249512\n",
            "Epoch 5, Batch 580, G Loss: 0.6922248005867004, D Loss: 1.3861136436462402\n",
            "Epoch 5, Batch 581, G Loss: 0.6921468377113342, D Loss: 1.3861074447631836\n",
            "Epoch 5, Batch 582, G Loss: 0.6920731067657471, D Loss: 1.3860902786254883\n",
            "Epoch 5, Batch 583, G Loss: 0.6919894814491272, D Loss: 1.3860377073287964\n",
            "Epoch 5, Batch 584, G Loss: 0.691914439201355, D Loss: 1.3860461711883545\n",
            "Epoch 5, Batch 585, G Loss: 0.6918500065803528, D Loss: 1.386059045791626\n",
            "Epoch 5, Batch 586, G Loss: 0.691807210445404, D Loss: 1.3860790729522705\n",
            "Epoch 5, Batch 587, G Loss: 0.6917877197265625, D Loss: 1.3860936164855957\n",
            "Epoch 5, Batch 588, G Loss: 0.69179767370224, D Loss: 1.3861119747161865\n",
            "Epoch 5, Batch 589, G Loss: 0.6918314695358276, D Loss: 1.3861351013183594\n",
            "Epoch 5, Batch 590, G Loss: 0.691888689994812, D Loss: 1.3859838247299194\n",
            "Epoch 5, Batch 591, G Loss: 0.691947340965271, D Loss: 1.385974407196045\n",
            "Epoch 5, Batch 592, G Loss: 0.691997230052948, D Loss: 1.3860161304473877\n",
            "Epoch 5, Batch 593, G Loss: 0.6920514106750488, D Loss: 1.3860654830932617\n",
            "Epoch 5, Batch 594, G Loss: 0.6921114325523376, D Loss: 1.386102318763733\n",
            "Epoch 5, Batch 595, G Loss: 0.6921818256378174, D Loss: 1.3861826658248901\n",
            "Epoch 5, Batch 596, G Loss: 0.6922739148139954, D Loss: 1.3859939575195312\n",
            "Epoch 5, Batch 597, G Loss: 0.6923604011535645, D Loss: 1.3859918117523193\n",
            "Epoch 5, Batch 598, G Loss: 0.6924349069595337, D Loss: 1.3860447406768799\n",
            "Epoch 5, Batch 599, G Loss: 0.6925114989280701, D Loss: 1.3860764503479004\n",
            "Epoch 5, Batch 600, G Loss: 0.6925907731056213, D Loss: 1.3861361742019653\n",
            "Epoch 5, Batch 601, G Loss: 0.6926765441894531, D Loss: 1.3861122131347656\n",
            "Epoch 5, Batch 602, G Loss: 0.6927661299705505, D Loss: 1.3861699104309082\n",
            "Epoch 5, Batch 603, G Loss: 0.6928664445877075, D Loss: 1.3860787153244019\n",
            "Epoch 5, Batch 604, G Loss: 0.6929616332054138, D Loss: 1.3859089612960815\n",
            "Epoch 5, Batch 605, G Loss: 0.6930322647094727, D Loss: 1.3860864639282227\n",
            "Epoch 5, Batch 606, G Loss: 0.6931014060974121, D Loss: 1.386065125465393\n",
            "Epoch 5, Batch 607, G Loss: 0.6931641101837158, D Loss: 1.3860652446746826\n",
            "Epoch 5, Batch 608, G Loss: 0.6932228207588196, D Loss: 1.3860139846801758\n",
            "Epoch 5, Batch 609, G Loss: 0.6932718753814697, D Loss: 1.386098861694336\n",
            "Epoch 5, Batch 610, G Loss: 0.6933178901672363, D Loss: 1.3861455917358398\n",
            "Epoch 5, Batch 611, G Loss: 0.6933733820915222, D Loss: 1.386047601699829\n",
            "Epoch 5, Batch 612, G Loss: 0.6934140920639038, D Loss: 1.3859597444534302\n",
            "Epoch 5, Batch 613, G Loss: 0.6934361457824707, D Loss: 1.3859550952911377\n",
            "Epoch 5, Batch 614, G Loss: 0.6934376955032349, D Loss: 1.3860116004943848\n",
            "Epoch 5, Batch 615, G Loss: 0.693428635597229, D Loss: 1.386033535003662\n",
            "Epoch 5, Batch 616, G Loss: 0.6934120059013367, D Loss: 1.3860175609588623\n",
            "Epoch 5, Batch 617, G Loss: 0.6933882832527161, D Loss: 1.3860020637512207\n",
            "Epoch 5, Batch 618, G Loss: 0.6933547258377075, D Loss: 1.3860955238342285\n",
            "Epoch 5, Batch 619, G Loss: 0.6933242678642273, D Loss: 1.386023759841919\n",
            "Epoch 5, Batch 620, G Loss: 0.6932888627052307, D Loss: 1.3860219717025757\n",
            "Epoch 5, Batch 621, G Loss: 0.693250834941864, D Loss: 1.3861298561096191\n",
            "Epoch 5, Batch 622, G Loss: 0.6932229995727539, D Loss: 1.3863019943237305\n",
            "Epoch 5, Batch 623, G Loss: 0.6932327747344971, D Loss: 1.3861205577850342\n",
            "Epoch 5, Batch 624, G Loss: 0.6932466626167297, D Loss: 1.3862024545669556\n",
            "Epoch 5, Batch 625, G Loss: 0.6932768821716309, D Loss: 1.3860464096069336\n",
            "Epoch 5, Batch 626, G Loss: 0.6933017373085022, D Loss: 1.386090874671936\n",
            "Epoch 5, Batch 627, G Loss: 0.6933318376541138, D Loss: 1.3860440254211426\n",
            "Epoch 5, Batch 628, G Loss: 0.693355143070221, D Loss: 1.386025309562683\n",
            "Epoch 5, Batch 629, G Loss: 0.6933690905570984, D Loss: 1.3860682249069214\n",
            "Epoch 5, Batch 630, G Loss: 0.6933810710906982, D Loss: 1.3861675262451172\n",
            "Epoch 5, Batch 631, G Loss: 0.6934010982513428, D Loss: 1.3861687183380127\n",
            "Epoch 5, Batch 632, G Loss: 0.6934347152709961, D Loss: 1.3859583139419556\n",
            "Epoch 5, Batch 633, G Loss: 0.6934488415718079, D Loss: 1.3859461545944214\n",
            "Epoch 5, Batch 634, G Loss: 0.6934432983398438, D Loss: 1.3859672546386719\n",
            "Epoch 5, Batch 635, G Loss: 0.6934255361557007, D Loss: 1.386040449142456\n",
            "Epoch 5, Batch 636, G Loss: 0.6934019327163696, D Loss: 1.3861372470855713\n",
            "Epoch 5, Batch 637, G Loss: 0.6933883428573608, D Loss: 1.3861041069030762\n",
            "Epoch 5, Batch 638, G Loss: 0.6933863162994385, D Loss: 1.3862004280090332\n",
            "Epoch 5, Batch 639, G Loss: 0.6933960914611816, D Loss: 1.386075735092163\n",
            "Epoch 5, Batch 640, G Loss: 0.6934076547622681, D Loss: 1.3861842155456543\n",
            "Epoch 5, Batch 641, G Loss: 0.6934298872947693, D Loss: 1.3861408233642578\n",
            "Epoch 5, Batch 642, G Loss: 0.6934576034545898, D Loss: 1.3861384391784668\n",
            "Epoch 5, Batch 643, G Loss: 0.6934915781021118, D Loss: 1.386108160018921\n",
            "Epoch 5, Batch 644, G Loss: 0.6935253739356995, D Loss: 1.3860347270965576\n",
            "Epoch 5, Batch 645, G Loss: 0.6935490965843201, D Loss: 1.3861277103424072\n",
            "Epoch 5, Batch 646, G Loss: 0.6935808062553406, D Loss: 1.3860405683517456\n",
            "Epoch 5, Batch 647, G Loss: 0.6936004161834717, D Loss: 1.3860143423080444\n",
            "Epoch 5, Batch 648, G Loss: 0.6936114430427551, D Loss: 1.3860669136047363\n",
            "Epoch 5, Batch 649, G Loss: 0.6936188340187073, D Loss: 1.386163353919983\n",
            "Epoch 5, Batch 650, G Loss: 0.6936365365982056, D Loss: 1.3860650062561035\n",
            "Epoch 5, Batch 651, G Loss: 0.693645715713501, D Loss: 1.3861138820648193\n",
            "Epoch 5, Batch 652, G Loss: 0.6936612725257874, D Loss: 1.3861830234527588\n",
            "Epoch 5, Batch 653, G Loss: 0.693693220615387, D Loss: 1.3862318992614746\n",
            "Epoch 5, Batch 654, G Loss: 0.6937534809112549, D Loss: 1.3861385583877563\n",
            "Epoch 5, Batch 655, G Loss: 0.6938197016716003, D Loss: 1.3860838413238525\n",
            "Epoch 5, Batch 656, G Loss: 0.6938707828521729, D Loss: 1.3860695362091064\n",
            "Epoch 5, Batch 657, G Loss: 0.6939022541046143, D Loss: 1.386047601699829\n",
            "Epoch 5, Batch 658, G Loss: 0.6939132213592529, D Loss: 1.3860430717468262\n",
            "Epoch 5, Batch 659, G Loss: 0.6939010620117188, D Loss: 1.3860950469970703\n",
            "Epoch 5, Batch 660, G Loss: 0.6938766241073608, D Loss: 1.3860870599746704\n",
            "Epoch 5, Batch 661, G Loss: 0.6938438415527344, D Loss: 1.3860793113708496\n",
            "Epoch 5, Batch 662, G Loss: 0.6938037276268005, D Loss: 1.3860819339752197\n",
            "Epoch 5, Batch 663, G Loss: 0.6937652826309204, D Loss: 1.3860679864883423\n",
            "Epoch 5, Batch 664, G Loss: 0.6937260627746582, D Loss: 1.3860602378845215\n",
            "Epoch 5, Batch 665, G Loss: 0.6936963200569153, D Loss: 1.3860828876495361\n",
            "Epoch 5, Batch 666, G Loss: 0.6936771869659424, D Loss: 1.386054515838623\n",
            "Epoch 5, Batch 667, G Loss: 0.6936368346214294, D Loss: 1.386047601699829\n",
            "Epoch 5, Batch 668, G Loss: 0.6935732960700989, D Loss: 1.386073112487793\n",
            "Epoch 5, Batch 669, G Loss: 0.6935043334960938, D Loss: 1.386055588722229\n",
            "Epoch 5, Batch 670, G Loss: 0.6934278011322021, D Loss: 1.3860564231872559\n",
            "Epoch 5, Batch 671, G Loss: 0.6933512687683105, D Loss: 1.386061668395996\n",
            "Epoch 5, Batch 672, G Loss: 0.6932829022407532, D Loss: 1.3861106634140015\n",
            "Epoch 5, Batch 673, G Loss: 0.6932461857795715, D Loss: 1.3861420154571533\n",
            "Epoch 5, Batch 674, G Loss: 0.6932293772697449, D Loss: 1.3861165046691895\n",
            "Epoch 5, Batch 675, G Loss: 0.6932318806648254, D Loss: 1.386063575744629\n",
            "Epoch 5, Batch 676, G Loss: 0.6932260394096375, D Loss: 1.3860496282577515\n",
            "Epoch 5, Batch 677, G Loss: 0.6932141184806824, D Loss: 1.3860989809036255\n",
            "Epoch 5, Batch 678, G Loss: 0.6932082772254944, D Loss: 1.386067271232605\n",
            "Epoch 5, Batch 679, G Loss: 0.6932058334350586, D Loss: 1.3860490322113037\n",
            "Epoch 5, Batch 680, G Loss: 0.6932022571563721, D Loss: 1.386059284210205\n",
            "Epoch 5, Batch 681, G Loss: 0.6931958794593811, D Loss: 1.386063814163208\n",
            "Epoch 5, Batch 682, G Loss: 0.6931997537612915, D Loss: 1.3860714435577393\n",
            "Epoch 5, Batch 683, G Loss: 0.6932002305984497, D Loss: 1.3860865831375122\n",
            "Epoch 5, Batch 684, G Loss: 0.693200409412384, D Loss: 1.3860819339752197\n",
            "Epoch 5, Batch 685, G Loss: 0.6932027339935303, D Loss: 1.3860787153244019\n",
            "Epoch 5, Batch 686, G Loss: 0.6932153701782227, D Loss: 1.3860580921173096\n",
            "Epoch 5, Batch 687, G Loss: 0.6932392716407776, D Loss: 1.3860726356506348\n",
            "Epoch 5, Batch 688, G Loss: 0.6932744979858398, D Loss: 1.3860801458358765\n",
            "Epoch 5, Batch 689, G Loss: 0.6933265328407288, D Loss: 1.3860437870025635\n",
            "Epoch 5, Batch 690, G Loss: 0.6933743357658386, D Loss: 1.386043667793274\n",
            "Epoch 5, Batch 691, G Loss: 0.6934229731559753, D Loss: 1.386042594909668\n",
            "Epoch 5, Batch 692, G Loss: 0.6934770941734314, D Loss: 1.3860445022583008\n",
            "Epoch 5, Batch 693, G Loss: 0.6935213804244995, D Loss: 1.3860636949539185\n",
            "Epoch 5, Batch 694, G Loss: 0.6935529708862305, D Loss: 1.3860468864440918\n",
            "Epoch 5, Batch 695, G Loss: 0.6935932636260986, D Loss: 1.386070966720581\n",
            "Epoch 5, Batch 696, G Loss: 0.6936181783676147, D Loss: 1.386050820350647\n",
            "Epoch 5, Batch 697, G Loss: 0.6936454176902771, D Loss: 1.3860716819763184\n",
            "Epoch 5, Batch 698, G Loss: 0.6936696767807007, D Loss: 1.3860583305358887\n",
            "Epoch 5, Batch 699, G Loss: 0.6936923265457153, D Loss: 1.3860385417938232\n",
            "Epoch 5, Batch 700, G Loss: 0.6937161087989807, D Loss: 1.386022925376892\n",
            "Epoch 5, Batch 701, G Loss: 0.6937385201454163, D Loss: 1.3860176801681519\n",
            "Epoch 5, Batch 702, G Loss: 0.6937711834907532, D Loss: 1.3860580921173096\n",
            "Epoch 5, Batch 703, G Loss: 0.6937932968139648, D Loss: 1.3860936164855957\n",
            "Epoch 5, Batch 704, G Loss: 0.6938040852546692, D Loss: 1.3860867023468018\n",
            "Epoch 5, Batch 705, G Loss: 0.69381183385849, D Loss: 1.3860986232757568\n",
            "Epoch 5, Batch 706, G Loss: 0.6938006281852722, D Loss: 1.3860681056976318\n",
            "Epoch 5, Batch 707, G Loss: 0.6937863230705261, D Loss: 1.3860623836517334\n",
            "Epoch 5, Batch 708, G Loss: 0.6937654614448547, D Loss: 1.3860697746276855\n",
            "Epoch 5, Batch 709, G Loss: 0.6937402486801147, D Loss: 1.3860406875610352\n",
            "Epoch 5, Batch 710, G Loss: 0.6937163472175598, D Loss: 1.386009931564331\n",
            "Epoch 5, Batch 711, G Loss: 0.6936962604522705, D Loss: 1.3859601020812988\n",
            "Epoch 5, Batch 712, G Loss: 0.6936894655227661, D Loss: 1.3860435485839844\n",
            "Epoch 5, Batch 713, G Loss: 0.6936792135238647, D Loss: 1.3860341310501099\n",
            "Epoch 5, Batch 714, G Loss: 0.6936792731285095, D Loss: 1.3860461711883545\n",
            "Epoch 5, Batch 715, G Loss: 0.6936681270599365, D Loss: 1.3860538005828857\n",
            "Epoch 5, Batch 716, G Loss: 0.6936553120613098, D Loss: 1.3860403299331665\n",
            "Epoch 5, Batch 717, G Loss: 0.6936442852020264, D Loss: 1.3860390186309814\n",
            "Epoch 5, Batch 718, G Loss: 0.693626344203949, D Loss: 1.3859589099884033\n",
            "Epoch 5, Batch 719, G Loss: 0.6936194896697998, D Loss: 1.3858642578125\n",
            "Epoch 5, Batch 720, G Loss: 0.693644642829895, D Loss: 1.3859083652496338\n",
            "Epoch 5, Batch 721, G Loss: 0.6936803460121155, D Loss: 1.3859190940856934\n",
            "Epoch 5, Batch 722, G Loss: 0.6937258839607239, D Loss: 1.3860018253326416\n",
            "Epoch 5, Batch 723, G Loss: 0.6937764883041382, D Loss: 1.3860156536102295\n",
            "Epoch 5, Batch 724, G Loss: 0.6938140392303467, D Loss: 1.3859975337982178\n",
            "Epoch 5, Batch 725, G Loss: 0.6938600540161133, D Loss: 1.386123776435852\n",
            "Epoch 5, Batch 726, G Loss: 0.6938743591308594, D Loss: 1.3860609531402588\n",
            "Epoch 5, Batch 727, G Loss: 0.6938843727111816, D Loss: 1.3860502243041992\n",
            "Epoch 5, Batch 728, G Loss: 0.6938881278038025, D Loss: 1.3860745429992676\n",
            "Epoch 5, Batch 729, G Loss: 0.6938855051994324, D Loss: 1.3861080408096313\n",
            "Epoch 5, Batch 730, G Loss: 0.693864643573761, D Loss: 1.3862617015838623\n",
            "Epoch 5, Batch 731, G Loss: 0.6938211917877197, D Loss: 1.3861656188964844\n",
            "Epoch 5, Batch 732, G Loss: 0.6937646865844727, D Loss: 1.3862295150756836\n",
            "Epoch 5, Batch 733, G Loss: 0.6936947703361511, D Loss: 1.386176347732544\n",
            "Epoch 5, Batch 734, G Loss: 0.6936107277870178, D Loss: 1.3861594200134277\n",
            "Epoch 5, Batch 735, G Loss: 0.6935248970985413, D Loss: 1.3861083984375\n",
            "Epoch 5, Batch 736, G Loss: 0.6934434771537781, D Loss: 1.3861159086227417\n",
            "Epoch 5, Batch 737, G Loss: 0.6933587789535522, D Loss: 1.3861349821090698\n",
            "Epoch 5, Batch 738, G Loss: 0.6932772994041443, D Loss: 1.3859412670135498\n",
            "Epoch 5, Batch 739, G Loss: 0.6932160258293152, D Loss: 1.3859193325042725\n",
            "Epoch 5, Batch 740, G Loss: 0.6931820511817932, D Loss: 1.3859012126922607\n",
            "Epoch 5, Batch 741, G Loss: 0.693168044090271, D Loss: 1.3861875534057617\n",
            "Epoch 5, Batch 742, G Loss: 0.693141520023346, D Loss: 1.3861579895019531\n",
            "Epoch 5, Batch 743, G Loss: 0.6931076645851135, D Loss: 1.386143684387207\n",
            "Epoch 5, Batch 744, G Loss: 0.6930798888206482, D Loss: 1.3863228559494019\n",
            "Epoch 5, Batch 745, G Loss: 0.6930229067802429, D Loss: 1.3863952159881592\n",
            "Epoch 5, Batch 746, G Loss: 0.6929364800453186, D Loss: 1.3862559795379639\n",
            "Epoch 5, Batch 747, G Loss: 0.6928538084030151, D Loss: 1.386197805404663\n",
            "Epoch 5, Batch 748, G Loss: 0.6927592754364014, D Loss: 1.3862123489379883\n",
            "Epoch 5, Batch 749, G Loss: 0.6926707029342651, D Loss: 1.3862011432647705\n",
            "Epoch 5, Batch 750, G Loss: 0.6925814747810364, D Loss: 1.3861055374145508\n",
            "Epoch 5, Batch 751, G Loss: 0.6925155520439148, D Loss: 1.3862786293029785\n",
            "Epoch 5, Batch 752, G Loss: 0.6924313902854919, D Loss: 1.3861994743347168\n",
            "Epoch 5, Batch 753, G Loss: 0.692360520362854, D Loss: 1.386251449584961\n",
            "Epoch 5, Batch 754, G Loss: 0.692271888256073, D Loss: 1.3862276077270508\n",
            "Epoch 5, Batch 755, G Loss: 0.6921850442886353, D Loss: 1.386198878288269\n",
            "Epoch 5, Batch 756, G Loss: 0.6920984983444214, D Loss: 1.3861589431762695\n",
            "Epoch 5, Batch 757, G Loss: 0.6920150518417358, D Loss: 1.3861407041549683\n",
            "Epoch 5, Batch 758, G Loss: 0.6919387578964233, D Loss: 1.3861358165740967\n",
            "Epoch 5, Batch 759, G Loss: 0.6918705701828003, D Loss: 1.3861000537872314\n",
            "Epoch 5, Batch 760, G Loss: 0.691811203956604, D Loss: 1.386092185974121\n",
            "Epoch 5, Batch 761, G Loss: 0.6917626261711121, D Loss: 1.3860783576965332\n",
            "Epoch 5, Batch 762, G Loss: 0.691729724407196, D Loss: 1.3860539197921753\n",
            "Epoch 5, Batch 763, G Loss: 0.6916943192481995, D Loss: 1.3860681056976318\n",
            "Epoch 5, Batch 764, G Loss: 0.6916710138320923, D Loss: 1.386082649230957\n",
            "Epoch 5, Batch 765, G Loss: 0.691662609577179, D Loss: 1.3861889839172363\n",
            "Epoch 5, Batch 766, G Loss: 0.6916935443878174, D Loss: 1.3861576318740845\n",
            "Epoch 5, Batch 767, G Loss: 0.6917579770088196, D Loss: 1.3861186504364014\n",
            "Epoch 5, Batch 768, G Loss: 0.6918449997901917, D Loss: 1.3860247135162354\n",
            "Epoch 5, Batch 769, G Loss: 0.6919251084327698, D Loss: 1.3860044479370117\n",
            "Epoch 5, Batch 770, G Loss: 0.6920084953308105, D Loss: 1.386049747467041\n",
            "Epoch 5, Batch 771, G Loss: 0.6920885443687439, D Loss: 1.386066198348999\n",
            "Epoch 5, Batch 772, G Loss: 0.6921696066856384, D Loss: 1.386106014251709\n",
            "Epoch 5, Batch 773, G Loss: 0.6922591328620911, D Loss: 1.3860197067260742\n",
            "Epoch 5, Batch 774, G Loss: 0.692345142364502, D Loss: 1.3860745429992676\n",
            "Epoch 5, Batch 775, G Loss: 0.6924356818199158, D Loss: 1.3860734701156616\n",
            "Epoch 5, Batch 776, G Loss: 0.6925198435783386, D Loss: 1.3860726356506348\n",
            "Epoch 5, Batch 777, G Loss: 0.6926050186157227, D Loss: 1.3861782550811768\n",
            "Epoch 5, Batch 778, G Loss: 0.692701518535614, D Loss: 1.385951280593872\n",
            "Epoch 5, Batch 779, G Loss: 0.6927781105041504, D Loss: 1.386138677597046\n",
            "Epoch 5, Batch 780, G Loss: 0.6928631663322449, D Loss: 1.3861603736877441\n",
            "Epoch 5, Batch 781, G Loss: 0.6929545402526855, D Loss: 1.3862264156341553\n",
            "Epoch 5, Batch 782, G Loss: 0.693055272102356, D Loss: 1.386157751083374\n",
            "Epoch 5, Batch 783, G Loss: 0.6931620836257935, D Loss: 1.3861373662948608\n",
            "Epoch 5, Batch 784, G Loss: 0.6932569146156311, D Loss: 1.3860892057418823\n",
            "Epoch 5, Batch 785, G Loss: 0.6933454275131226, D Loss: 1.3861204385757446\n",
            "Epoch 5, Batch 786, G Loss: 0.693428635597229, D Loss: 1.3861069679260254\n",
            "Epoch 5, Batch 787, G Loss: 0.6935020089149475, D Loss: 1.3860998153686523\n",
            "Epoch 5, Batch 788, G Loss: 0.6935742497444153, D Loss: 1.3861074447631836\n",
            "Epoch 5, Batch 789, G Loss: 0.6936362981796265, D Loss: 1.3861083984375\n",
            "Epoch 5, Batch 790, G Loss: 0.693693220615387, D Loss: 1.3860669136047363\n",
            "Epoch 5, Batch 791, G Loss: 0.6937363147735596, D Loss: 1.3861489295959473\n",
            "Epoch 5, Batch 792, G Loss: 0.6937814950942993, D Loss: 1.3861451148986816\n",
            "Epoch 5, Batch 793, G Loss: 0.6938232183456421, D Loss: 1.386075496673584\n",
            "Epoch 5, Batch 794, G Loss: 0.6938537955284119, D Loss: 1.3860546350479126\n",
            "Epoch 5, Batch 795, G Loss: 0.6938604116439819, D Loss: 1.386145830154419\n",
            "Epoch 5, Batch 796, G Loss: 0.6938618421554565, D Loss: 1.3860807418823242\n",
            "Epoch 5, Batch 797, G Loss: 0.6938497424125671, D Loss: 1.386098861694336\n",
            "Epoch 5, Batch 798, G Loss: 0.6938350200653076, D Loss: 1.3861324787139893\n",
            "Epoch 5, Batch 799, G Loss: 0.6938219666481018, D Loss: 1.3860527276992798\n",
            "Epoch 5, Batch 800, G Loss: 0.6937960386276245, D Loss: 1.3860507011413574\n",
            "Epoch 5, Batch 801, G Loss: 0.6937516927719116, D Loss: 1.3860061168670654\n",
            "Epoch 5, Batch 802, G Loss: 0.6936833262443542, D Loss: 1.3860588073730469\n",
            "Epoch 5, Batch 803, G Loss: 0.6936044692993164, D Loss: 1.386059284210205\n",
            "Epoch 5, Batch 804, G Loss: 0.6935194134712219, D Loss: 1.3860900402069092\n",
            "Epoch 5, Batch 805, G Loss: 0.6934431195259094, D Loss: 1.386000394821167\n",
            "Epoch 5, Batch 806, G Loss: 0.693357527256012, D Loss: 1.3861545324325562\n",
            "Epoch 5, Batch 807, G Loss: 0.6932841539382935, D Loss: 1.386021614074707\n",
            "Epoch 5, Batch 808, G Loss: 0.6932088136672974, D Loss: 1.3861207962036133\n",
            "Epoch 5, Batch 809, G Loss: 0.6931539177894592, D Loss: 1.3861725330352783\n",
            "Epoch 5, Batch 810, G Loss: 0.6931167840957642, D Loss: 1.3861162662506104\n",
            "Epoch 5, Batch 811, G Loss: 0.6930882334709167, D Loss: 1.3861889839172363\n",
            "Epoch 5, Batch 812, G Loss: 0.6930822134017944, D Loss: 1.3860901594161987\n",
            "Epoch 5, Batch 813, G Loss: 0.6930813789367676, D Loss: 1.3861346244812012\n",
            "Epoch 5, Batch 814, G Loss: 0.6930907368659973, D Loss: 1.3862533569335938\n",
            "Epoch 5, Batch 815, G Loss: 0.693128764629364, D Loss: 1.3861407041549683\n",
            "Epoch 5, Batch 816, G Loss: 0.6931718587875366, D Loss: 1.3862147331237793\n",
            "Epoch 5, Batch 817, G Loss: 0.6932306289672852, D Loss: 1.386234998703003\n",
            "Epoch 5, Batch 818, G Loss: 0.6933056712150574, D Loss: 1.3861308097839355\n",
            "Epoch 5, Batch 819, G Loss: 0.693383514881134, D Loss: 1.3861616849899292\n",
            "Epoch 5, Batch 820, G Loss: 0.6934534907341003, D Loss: 1.3860219717025757\n",
            "Epoch 5, Batch 821, G Loss: 0.6934986710548401, D Loss: 1.3860962390899658\n",
            "Epoch 5, Batch 822, G Loss: 0.6935331225395203, D Loss: 1.3861027956008911\n",
            "Epoch 5, Batch 823, G Loss: 0.6935582160949707, D Loss: 1.3861432075500488\n",
            "Epoch 5, Batch 824, G Loss: 0.6935842633247375, D Loss: 1.3861408233642578\n",
            "Epoch 5, Batch 825, G Loss: 0.6936153769493103, D Loss: 1.3861570358276367\n",
            "Epoch 5, Batch 826, G Loss: 0.6936564445495605, D Loss: 1.3861193656921387\n",
            "Epoch 5, Batch 827, G Loss: 0.6936904191970825, D Loss: 1.3860933780670166\n",
            "Epoch 5, Batch 828, G Loss: 0.6937172412872314, D Loss: 1.386073350906372\n",
            "Epoch 5, Batch 829, G Loss: 0.6937227845191956, D Loss: 1.3860859870910645\n",
            "Epoch 5, Batch 830, G Loss: 0.6937055587768555, D Loss: 1.3860714435577393\n",
            "Epoch 5, Batch 831, G Loss: 0.6936776041984558, D Loss: 1.3861355781555176\n",
            "Epoch 5, Batch 832, G Loss: 0.6936485171318054, D Loss: 1.3860950469970703\n",
            "Epoch 5, Batch 833, G Loss: 0.6936119198799133, D Loss: 1.3860875368118286\n",
            "Epoch 5, Batch 834, G Loss: 0.6935752034187317, D Loss: 1.3861404657363892\n",
            "Epoch 5, Batch 835, G Loss: 0.6935529112815857, D Loss: 1.3861124515533447\n",
            "Epoch 5, Batch 836, G Loss: 0.6935274004936218, D Loss: 1.3861305713653564\n",
            "Epoch 5, Batch 837, G Loss: 0.6935076117515564, D Loss: 1.3861148357391357\n",
            "Epoch 5, Batch 838, G Loss: 0.693500280380249, D Loss: 1.3861393928527832\n",
            "Epoch 5, Batch 839, G Loss: 0.6935038566589355, D Loss: 1.3860933780670166\n",
            "Epoch 5, Batch 840, G Loss: 0.693515956401825, D Loss: 1.3860766887664795\n",
            "Epoch 5, Batch 841, G Loss: 0.693530797958374, D Loss: 1.3861091136932373\n",
            "Epoch 5, Batch 842, G Loss: 0.6935600638389587, D Loss: 1.3860944509506226\n",
            "Epoch 5, Batch 843, G Loss: 0.6935920715332031, D Loss: 1.3860961198806763\n",
            "Epoch 5, Batch 844, G Loss: 0.6936113238334656, D Loss: 1.3860671520233154\n",
            "Epoch 5, Batch 845, G Loss: 0.6936197280883789, D Loss: 1.3860950469970703\n",
            "Epoch 5, Batch 846, G Loss: 0.6936180591583252, D Loss: 1.3861101865768433\n",
            "Epoch 5, Batch 847, G Loss: 0.6936136484146118, D Loss: 1.3860986232757568\n",
            "Epoch 5, Batch 848, G Loss: 0.6936126947402954, D Loss: 1.386096477508545\n",
            "Epoch 5, Batch 849, G Loss: 0.6936240792274475, D Loss: 1.3860667943954468\n",
            "Epoch 5, Batch 850, G Loss: 0.6936551928520203, D Loss: 1.3861477375030518\n",
            "Epoch 5, Batch 851, G Loss: 0.693658709526062, D Loss: 1.386141061782837\n",
            "Epoch 5, Batch 852, G Loss: 0.6936535239219666, D Loss: 1.386145830154419\n",
            "Epoch 5, Batch 853, G Loss: 0.693632960319519, D Loss: 1.386154055595398\n",
            "Epoch 5, Batch 854, G Loss: 0.6935948133468628, D Loss: 1.386152982711792\n",
            "Epoch 5, Batch 855, G Loss: 0.6935427188873291, D Loss: 1.3861761093139648\n",
            "Epoch 5, Batch 856, G Loss: 0.6934857964515686, D Loss: 1.3861522674560547\n",
            "Epoch 5, Batch 857, G Loss: 0.6934221386909485, D Loss: 1.3861627578735352\n",
            "Epoch 5, Batch 858, G Loss: 0.6933549642562866, D Loss: 1.386084794998169\n",
            "Epoch 5, Batch 859, G Loss: 0.6933013200759888, D Loss: 1.3861193656921387\n",
            "Epoch 5, Batch 860, G Loss: 0.6932528018951416, D Loss: 1.3861091136932373\n",
            "Epoch 5, Batch 861, G Loss: 0.6931986212730408, D Loss: 1.386117696762085\n",
            "Epoch 5, Batch 862, G Loss: 0.693147599697113, D Loss: 1.3861310482025146\n",
            "Epoch 5, Batch 863, G Loss: 0.6930977702140808, D Loss: 1.3861346244812012\n",
            "Epoch 5, Batch 864, G Loss: 0.6930556893348694, D Loss: 1.3861184120178223\n",
            "Epoch 5, Batch 865, G Loss: 0.6930102705955505, D Loss: 1.3861358165740967\n",
            "Epoch 5, Batch 866, G Loss: 0.6929788589477539, D Loss: 1.3861322402954102\n",
            "Epoch 5, Batch 867, G Loss: 0.6929621696472168, D Loss: 1.386117935180664\n",
            "Epoch 5, Batch 868, G Loss: 0.6929535865783691, D Loss: 1.386174201965332\n",
            "Epoch 5, Batch 869, G Loss: 0.6929325461387634, D Loss: 1.3861734867095947\n",
            "Epoch 5, Batch 870, G Loss: 0.6929094791412354, D Loss: 1.38614821434021\n",
            "Epoch 5, Batch 871, G Loss: 0.6928558945655823, D Loss: 1.3861483335494995\n",
            "Epoch 5, Batch 872, G Loss: 0.6927880048751831, D Loss: 1.386136770248413\n",
            "Epoch 5, Batch 873, G Loss: 0.692706823348999, D Loss: 1.386122703552246\n",
            "Epoch 5, Batch 874, G Loss: 0.6926316022872925, D Loss: 1.3861092329025269\n",
            "Epoch 5, Batch 875, G Loss: 0.6925684213638306, D Loss: 1.3861286640167236\n",
            "Epoch 5, Batch 876, G Loss: 0.6925167441368103, D Loss: 1.3861428499221802\n",
            "Epoch 5, Batch 877, G Loss: 0.6924822926521301, D Loss: 1.3861432075500488\n",
            "Epoch 5, Batch 878, G Loss: 0.6924619674682617, D Loss: 1.386115550994873\n",
            "Epoch 5, Batch 879, G Loss: 0.692450225353241, D Loss: 1.3860876560211182\n",
            "Epoch 5, Batch 880, G Loss: 0.6924433708190918, D Loss: 1.3861756324768066\n",
            "Epoch 5, Batch 881, G Loss: 0.6924497485160828, D Loss: 1.3862197399139404\n",
            "Epoch 5, Batch 882, G Loss: 0.6924946904182434, D Loss: 1.3861780166625977\n",
            "Epoch 5, Batch 883, G Loss: 0.69255530834198, D Loss: 1.386149525642395\n",
            "Epoch 5, Batch 884, G Loss: 0.6926243305206299, D Loss: 1.3861044645309448\n",
            "Epoch 5, Batch 885, G Loss: 0.6926904916763306, D Loss: 1.386148452758789\n",
            "Epoch 5, Batch 886, G Loss: 0.6927650570869446, D Loss: 1.386141300201416\n",
            "Epoch 5, Batch 887, G Loss: 0.6928393244743347, D Loss: 1.386063575744629\n",
            "Epoch 5, Batch 888, G Loss: 0.6928924918174744, D Loss: 1.3861134052276611\n",
            "Epoch 5, Batch 889, G Loss: 0.6929319500923157, D Loss: 1.3861072063446045\n",
            "Epoch 5, Batch 890, G Loss: 0.6929662227630615, D Loss: 1.3861608505249023\n",
            "Epoch 5, Batch 891, G Loss: 0.6930004954338074, D Loss: 1.3861703872680664\n",
            "Epoch 5, Batch 892, G Loss: 0.6930471658706665, D Loss: 1.386218547821045\n",
            "Epoch 5, Batch 893, G Loss: 0.6931206583976746, D Loss: 1.3861653804779053\n",
            "Epoch 5, Batch 894, G Loss: 0.6932001113891602, D Loss: 1.386138677597046\n",
            "Epoch 5, Batch 895, G Loss: 0.6932793259620667, D Loss: 1.3861018419265747\n",
            "Epoch 5, Batch 896, G Loss: 0.6933367252349854, D Loss: 1.38612699508667\n",
            "Epoch 5, Batch 897, G Loss: 0.6934010982513428, D Loss: 1.3861103057861328\n",
            "Epoch 5, Batch 898, G Loss: 0.6934489607810974, D Loss: 1.386156439781189\n",
            "Epoch 5, Batch 899, G Loss: 0.6934862732887268, D Loss: 1.3860803842544556\n",
            "Epoch 5, Batch 900, G Loss: 0.6935163140296936, D Loss: 1.3861043453216553\n",
            "Epoch 5, Batch 901, G Loss: 0.6935270428657532, D Loss: 1.386146068572998\n",
            "Epoch 5, Batch 902, G Loss: 0.6935426592826843, D Loss: 1.3861606121063232\n",
            "Epoch 5, Batch 903, G Loss: 0.6935742497444153, D Loss: 1.3861713409423828\n",
            "Epoch 5, Batch 904, G Loss: 0.6936140060424805, D Loss: 1.3861582279205322\n",
            "Epoch 5, Batch 905, G Loss: 0.693657398223877, D Loss: 1.3861045837402344\n",
            "Epoch 5, Batch 906, G Loss: 0.6937073469161987, D Loss: 1.3861734867095947\n",
            "Epoch 5, Batch 907, G Loss: 0.6937527656555176, D Loss: 1.3861241340637207\n",
            "Epoch 5, Batch 908, G Loss: 0.6937883496284485, D Loss: 1.3861455917358398\n",
            "Epoch 5, Batch 909, G Loss: 0.6938331723213196, D Loss: 1.386117935180664\n",
            "Epoch 5, Batch 910, G Loss: 0.6938921809196472, D Loss: 1.3861069679260254\n",
            "Epoch 5, Batch 911, G Loss: 0.6939299702644348, D Loss: 1.3861126899719238\n",
            "Epoch 5, Batch 912, G Loss: 0.6939440369606018, D Loss: 1.3861591815948486\n",
            "Epoch 5, Batch 913, G Loss: 0.6939454078674316, D Loss: 1.3861610889434814\n",
            "Epoch 5, Batch 914, G Loss: 0.6939189434051514, D Loss: 1.386167049407959\n",
            "Epoch 5, Batch 915, G Loss: 0.6938647627830505, D Loss: 1.3861069679260254\n",
            "Epoch 5, Batch 916, G Loss: 0.6938208937644958, D Loss: 1.3861136436462402\n",
            "Epoch 5, Batch 917, G Loss: 0.6938056945800781, D Loss: 1.3860890865325928\n",
            "Epoch 5, Batch 918, G Loss: 0.6938127875328064, D Loss: 1.3860588073730469\n",
            "Epoch 5, Batch 919, G Loss: 0.6938425302505493, D Loss: 1.3860329389572144\n",
            "Epoch 5, Batch 920, G Loss: 0.6938852071762085, D Loss: 1.3861445188522339\n",
            "Epoch 5, Batch 921, G Loss: 0.6939002871513367, D Loss: 1.3861427307128906\n",
            "Epoch 5, Batch 922, G Loss: 0.6938956379890442, D Loss: 1.386157512664795\n",
            "Epoch 5, Batch 923, G Loss: 0.6938835382461548, D Loss: 1.386106014251709\n",
            "Epoch 5, Batch 924, G Loss: 0.6938604116439819, D Loss: 1.386078119277954\n",
            "Epoch 5, Batch 925, G Loss: 0.6938362717628479, D Loss: 1.3861678838729858\n",
            "Epoch 5, Batch 926, G Loss: 0.6938161849975586, D Loss: 1.386203646659851\n",
            "Epoch 5, Batch 927, G Loss: 0.6937776803970337, D Loss: 1.3862297534942627\n",
            "Epoch 5, Batch 928, G Loss: 0.6937081217765808, D Loss: 1.386526107788086\n",
            "Epoch 5, Batch 929, G Loss: 0.6935696601867676, D Loss: 1.3867136240005493\n",
            "Epoch 5, Batch 930, G Loss: 0.6933358907699585, D Loss: 1.3862617015838623\n",
            "Epoch 5, Batch 931, G Loss: 0.6930855512619019, D Loss: 1.3860974311828613\n",
            "Epoch 5, Batch 932, G Loss: 0.6928849220275879, D Loss: 1.3861479759216309\n",
            "Epoch 5, Batch 933, G Loss: 0.6927134990692139, D Loss: 1.3861839771270752\n",
            "Epoch 5, Batch 934, G Loss: 0.6925753951072693, D Loss: 1.386173963546753\n",
            "Epoch 5, Batch 935, G Loss: 0.6924583315849304, D Loss: 1.386183500289917\n",
            "Epoch 5, Batch 936, G Loss: 0.6923513412475586, D Loss: 1.3861814737319946\n",
            "Epoch 5, Batch 937, G Loss: 0.6922439336776733, D Loss: 1.3861432075500488\n",
            "Epoch 5, Batch 938, G Loss: 0.6921826601028442, D Loss: 1.386141061782837\n",
            "Epoch 6, Batch 1, G Loss: 0.6921444535255432, D Loss: 1.386160135269165\n",
            "Epoch 6, Batch 2, G Loss: 0.692130446434021, D Loss: 1.3861594200134277\n",
            "Epoch 6, Batch 3, G Loss: 0.6921241879463196, D Loss: 1.3861769437789917\n",
            "Epoch 6, Batch 4, G Loss: 0.6921369433403015, D Loss: 1.386133074760437\n",
            "Epoch 6, Batch 5, G Loss: 0.6921595335006714, D Loss: 1.3861842155456543\n",
            "Epoch 6, Batch 6, G Loss: 0.6921894550323486, D Loss: 1.386182427406311\n",
            "Epoch 6, Batch 7, G Loss: 0.6922335028648376, D Loss: 1.3861899375915527\n",
            "Epoch 6, Batch 8, G Loss: 0.6922899484634399, D Loss: 1.3861818313598633\n",
            "Epoch 6, Batch 9, G Loss: 0.6923639178276062, D Loss: 1.38614821434021\n",
            "Epoch 6, Batch 10, G Loss: 0.6924287676811218, D Loss: 1.3862007856369019\n",
            "Epoch 6, Batch 11, G Loss: 0.6925135850906372, D Loss: 1.3861656188964844\n",
            "Epoch 6, Batch 12, G Loss: 0.6925913691520691, D Loss: 1.3861043453216553\n",
            "Epoch 6, Batch 13, G Loss: 0.6926530599594116, D Loss: 1.3861604928970337\n",
            "Epoch 6, Batch 14, G Loss: 0.6927112936973572, D Loss: 1.3861465454101562\n",
            "Epoch 6, Batch 15, G Loss: 0.6927587389945984, D Loss: 1.3861637115478516\n",
            "Epoch 6, Batch 16, G Loss: 0.6928020119667053, D Loss: 1.3861958980560303\n",
            "Epoch 6, Batch 17, G Loss: 0.6928535103797913, D Loss: 1.3862192630767822\n",
            "Epoch 6, Batch 18, G Loss: 0.692915141582489, D Loss: 1.38618803024292\n",
            "Epoch 6, Batch 19, G Loss: 0.6929852366447449, D Loss: 1.386141300201416\n",
            "Epoch 6, Batch 20, G Loss: 0.6930463910102844, D Loss: 1.3860807418823242\n",
            "Epoch 6, Batch 21, G Loss: 0.6930633783340454, D Loss: 1.386106014251709\n",
            "Epoch 6, Batch 22, G Loss: 0.6930564045906067, D Loss: 1.3860416412353516\n",
            "Epoch 6, Batch 23, G Loss: 0.693011999130249, D Loss: 1.3861756324768066\n",
            "Epoch 6, Batch 24, G Loss: 0.6929788589477539, D Loss: 1.3861956596374512\n",
            "Epoch 6, Batch 25, G Loss: 0.6929500102996826, D Loss: 1.3861758708953857\n",
            "Epoch 6, Batch 26, G Loss: 0.6929298043251038, D Loss: 1.3862181901931763\n",
            "Epoch 6, Batch 27, G Loss: 0.6929308176040649, D Loss: 1.386223316192627\n",
            "Epoch 6, Batch 28, G Loss: 0.6929357647895813, D Loss: 1.386184573173523\n",
            "Epoch 6, Batch 29, G Loss: 0.6929600238800049, D Loss: 1.3862295150756836\n",
            "Epoch 6, Batch 30, G Loss: 0.6929863095283508, D Loss: 1.3861567974090576\n",
            "Epoch 6, Batch 31, G Loss: 0.6930102705955505, D Loss: 1.3860803842544556\n",
            "Epoch 6, Batch 32, G Loss: 0.6930140256881714, D Loss: 1.386064887046814\n",
            "Epoch 6, Batch 33, G Loss: 0.6929978132247925, D Loss: 1.3861172199249268\n",
            "Epoch 6, Batch 34, G Loss: 0.6929880380630493, D Loss: 1.3861567974090576\n",
            "Epoch 6, Batch 35, G Loss: 0.6929657459259033, D Loss: 1.38620126247406\n",
            "Epoch 6, Batch 36, G Loss: 0.6929646730422974, D Loss: 1.3860595226287842\n",
            "Epoch 6, Batch 37, G Loss: 0.6929507255554199, D Loss: 1.3861820697784424\n",
            "Epoch 6, Batch 38, G Loss: 0.6929433345794678, D Loss: 1.3861865997314453\n",
            "Epoch 6, Batch 39, G Loss: 0.6929455399513245, D Loss: 1.3861773014068604\n",
            "Epoch 6, Batch 40, G Loss: 0.6929464340209961, D Loss: 1.3860878944396973\n",
            "Epoch 6, Batch 41, G Loss: 0.6929511427879333, D Loss: 1.3861466646194458\n",
            "Epoch 6, Batch 42, G Loss: 0.6929511427879333, D Loss: 1.386045217514038\n",
            "Epoch 6, Batch 43, G Loss: 0.6929470896720886, D Loss: 1.3862360715866089\n",
            "Epoch 6, Batch 44, G Loss: 0.6929539442062378, D Loss: 1.386146903038025\n",
            "Epoch 6, Batch 45, G Loss: 0.6929648518562317, D Loss: 1.3860546350479126\n",
            "Epoch 6, Batch 46, G Loss: 0.6929640769958496, D Loss: 1.3861110210418701\n",
            "Epoch 6, Batch 47, G Loss: 0.6929638385772705, D Loss: 1.3863147497177124\n",
            "Epoch 6, Batch 48, G Loss: 0.6929836869239807, D Loss: 1.38616943359375\n",
            "Epoch 6, Batch 49, G Loss: 0.6930059194564819, D Loss: 1.3862866163253784\n",
            "Epoch 6, Batch 50, G Loss: 0.6930453181266785, D Loss: 1.3862793445587158\n",
            "Epoch 6, Batch 51, G Loss: 0.693092405796051, D Loss: 1.3863022327423096\n",
            "Epoch 6, Batch 52, G Loss: 0.6931554079055786, D Loss: 1.3863449096679688\n",
            "Epoch 6, Batch 53, G Loss: 0.6932302117347717, D Loss: 1.3861883878707886\n",
            "Epoch 6, Batch 54, G Loss: 0.6932896375656128, D Loss: 1.3863277435302734\n",
            "Epoch 6, Batch 55, G Loss: 0.6933627724647522, D Loss: 1.3863260746002197\n",
            "Epoch 6, Batch 56, G Loss: 0.69344562292099, D Loss: 1.3861773014068604\n",
            "Epoch 6, Batch 57, G Loss: 0.6935125589370728, D Loss: 1.3860907554626465\n",
            "Epoch 6, Batch 58, G Loss: 0.6935617923736572, D Loss: 1.3859800100326538\n",
            "Epoch 6, Batch 59, G Loss: 0.6935914754867554, D Loss: 1.3861932754516602\n",
            "Epoch 6, Batch 60, G Loss: 0.6936138868331909, D Loss: 1.3861873149871826\n",
            "Epoch 6, Batch 61, G Loss: 0.6936381459236145, D Loss: 1.386244773864746\n",
            "Epoch 6, Batch 62, G Loss: 0.6936581134796143, D Loss: 1.386204481124878\n",
            "Epoch 6, Batch 63, G Loss: 0.6936765313148499, D Loss: 1.3862042427062988\n",
            "Epoch 6, Batch 64, G Loss: 0.6936999559402466, D Loss: 1.3861336708068848\n",
            "Epoch 6, Batch 65, G Loss: 0.6937029361724854, D Loss: 1.3862266540527344\n",
            "Epoch 6, Batch 66, G Loss: 0.693708598613739, D Loss: 1.3862630128860474\n",
            "Epoch 6, Batch 67, G Loss: 0.6937198042869568, D Loss: 1.3861379623413086\n",
            "Epoch 6, Batch 68, G Loss: 0.6937151551246643, D Loss: 1.3862037658691406\n",
            "Epoch 6, Batch 69, G Loss: 0.6937133073806763, D Loss: 1.3862571716308594\n",
            "Epoch 6, Batch 70, G Loss: 0.6937217712402344, D Loss: 1.3861937522888184\n",
            "Epoch 6, Batch 71, G Loss: 0.6937283873558044, D Loss: 1.3860925436019897\n",
            "Epoch 6, Batch 72, G Loss: 0.6937065124511719, D Loss: 1.3861913681030273\n",
            "Epoch 6, Batch 73, G Loss: 0.6936838626861572, D Loss: 1.3861157894134521\n",
            "Epoch 6, Batch 74, G Loss: 0.6936412453651428, D Loss: 1.3860474824905396\n",
            "Epoch 6, Batch 75, G Loss: 0.6935707926750183, D Loss: 1.3861346244812012\n",
            "Epoch 6, Batch 76, G Loss: 0.6935015320777893, D Loss: 1.3861005306243896\n",
            "Epoch 6, Batch 77, G Loss: 0.6934192180633545, D Loss: 1.386134147644043\n",
            "Epoch 6, Batch 78, G Loss: 0.6933369040489197, D Loss: 1.3862605094909668\n",
            "Epoch 6, Batch 79, G Loss: 0.6932723522186279, D Loss: 1.3861688375473022\n",
            "Epoch 6, Batch 80, G Loss: 0.6932157278060913, D Loss: 1.3861114978790283\n",
            "Epoch 6, Batch 81, G Loss: 0.6931536793708801, D Loss: 1.3861104249954224\n",
            "Epoch 6, Batch 82, G Loss: 0.693095326423645, D Loss: 1.3861382007598877\n",
            "Epoch 6, Batch 83, G Loss: 0.6930347084999084, D Loss: 1.386141061782837\n",
            "Epoch 6, Batch 84, G Loss: 0.6929795145988464, D Loss: 1.386003017425537\n",
            "Epoch 6, Batch 85, G Loss: 0.6929115056991577, D Loss: 1.386050820350647\n",
            "Epoch 6, Batch 86, G Loss: 0.6928430795669556, D Loss: 1.3860664367675781\n",
            "Epoch 6, Batch 87, G Loss: 0.6927639842033386, D Loss: 1.3861007690429688\n",
            "Epoch 6, Batch 88, G Loss: 0.6926922798156738, D Loss: 1.3861188888549805\n",
            "Epoch 6, Batch 89, G Loss: 0.6926299333572388, D Loss: 1.38615083694458\n",
            "Epoch 6, Batch 90, G Loss: 0.6925813555717468, D Loss: 1.3861241340637207\n",
            "Epoch 6, Batch 91, G Loss: 0.6925448775291443, D Loss: 1.3862566947937012\n",
            "Epoch 6, Batch 92, G Loss: 0.6925346851348877, D Loss: 1.3862550258636475\n",
            "Epoch 6, Batch 93, G Loss: 0.6925361752510071, D Loss: 1.3862016201019287\n",
            "Epoch 6, Batch 94, G Loss: 0.6925498843193054, D Loss: 1.3859148025512695\n",
            "Epoch 6, Batch 95, G Loss: 0.6925488114356995, D Loss: 1.385867714881897\n",
            "Epoch 6, Batch 96, G Loss: 0.6925235390663147, D Loss: 1.38588285446167\n",
            "Epoch 6, Batch 97, G Loss: 0.6924791932106018, D Loss: 1.385920763015747\n",
            "Epoch 6, Batch 98, G Loss: 0.6924240589141846, D Loss: 1.3861039876937866\n",
            "Epoch 6, Batch 99, G Loss: 0.6923804879188538, D Loss: 1.3860242366790771\n",
            "Epoch 6, Batch 100, G Loss: 0.6923409104347229, D Loss: 1.3860676288604736\n",
            "Epoch 6, Batch 101, G Loss: 0.6923032999038696, D Loss: 1.386103630065918\n",
            "Epoch 6, Batch 102, G Loss: 0.6922827959060669, D Loss: 1.3862202167510986\n",
            "Epoch 6, Batch 103, G Loss: 0.6922816038131714, D Loss: 1.386417031288147\n",
            "Epoch 6, Batch 104, G Loss: 0.6923074722290039, D Loss: 1.3861119747161865\n",
            "Epoch 6, Batch 105, G Loss: 0.6923388242721558, D Loss: 1.3866548538208008\n",
            "Epoch 6, Batch 106, G Loss: 0.6924077272415161, D Loss: 1.3865556716918945\n",
            "Epoch 6, Batch 107, G Loss: 0.6925084590911865, D Loss: 1.3866722583770752\n",
            "Epoch 6, Batch 108, G Loss: 0.6926440596580505, D Loss: 1.3861126899719238\n",
            "Epoch 6, Batch 109, G Loss: 0.6927648782730103, D Loss: 1.3859858512878418\n",
            "Epoch 6, Batch 110, G Loss: 0.692889928817749, D Loss: 1.3862192630767822\n",
            "Epoch 6, Batch 111, G Loss: 0.6930063962936401, D Loss: 1.386439323425293\n",
            "Epoch 6, Batch 112, G Loss: 0.6931387186050415, D Loss: 1.3864079713821411\n",
            "Epoch 6, Batch 113, G Loss: 0.6932687759399414, D Loss: 1.3864679336547852\n",
            "Epoch 6, Batch 114, G Loss: 0.6934183239936829, D Loss: 1.3863685131072998\n",
            "Epoch 6, Batch 115, G Loss: 0.6935744881629944, D Loss: 1.386625051498413\n",
            "Epoch 6, Batch 116, G Loss: 0.6937469244003296, D Loss: 1.3864935636520386\n",
            "Epoch 6, Batch 117, G Loss: 0.6939232349395752, D Loss: 1.3863461017608643\n",
            "Epoch 6, Batch 118, G Loss: 0.6940945386886597, D Loss: 1.3863942623138428\n",
            "Epoch 6, Batch 119, G Loss: 0.6942497491836548, D Loss: 1.3863654136657715\n",
            "Epoch 6, Batch 120, G Loss: 0.6944056749343872, D Loss: 1.386220097541809\n",
            "Epoch 6, Batch 121, G Loss: 0.6945340633392334, D Loss: 1.386242389678955\n",
            "Epoch 6, Batch 122, G Loss: 0.6946382522583008, D Loss: 1.3862303495407104\n",
            "Epoch 6, Batch 123, G Loss: 0.6947181820869446, D Loss: 1.3861684799194336\n",
            "Epoch 6, Batch 124, G Loss: 0.6947717666625977, D Loss: 1.3860993385314941\n",
            "Epoch 6, Batch 125, G Loss: 0.6947734355926514, D Loss: 1.3861593008041382\n",
            "Epoch 6, Batch 126, G Loss: 0.6947421431541443, D Loss: 1.3861393928527832\n",
            "Epoch 6, Batch 127, G Loss: 0.6946755051612854, D Loss: 1.3861925601959229\n",
            "Epoch 6, Batch 128, G Loss: 0.6945939064025879, D Loss: 1.386279582977295\n",
            "Epoch 6, Batch 129, G Loss: 0.6945241093635559, D Loss: 1.3862518072128296\n",
            "Epoch 6, Batch 130, G Loss: 0.6944668292999268, D Loss: 1.386154055595398\n",
            "Epoch 6, Batch 131, G Loss: 0.6943778395652771, D Loss: 1.3861885070800781\n",
            "Epoch 6, Batch 132, G Loss: 0.6942830085754395, D Loss: 1.3862204551696777\n",
            "Epoch 6, Batch 133, G Loss: 0.6941899657249451, D Loss: 1.3861315250396729\n",
            "Epoch 6, Batch 134, G Loss: 0.6940480470657349, D Loss: 1.3861560821533203\n",
            "Epoch 6, Batch 135, G Loss: 0.6938819289207458, D Loss: 1.386156439781189\n",
            "Epoch 6, Batch 136, G Loss: 0.6936989426612854, D Loss: 1.3861112594604492\n",
            "Epoch 6, Batch 137, G Loss: 0.6935019493103027, D Loss: 1.3861716985702515\n",
            "Epoch 6, Batch 138, G Loss: 0.6933125257492065, D Loss: 1.3862130641937256\n",
            "Epoch 6, Batch 139, G Loss: 0.6931435465812683, D Loss: 1.3862721920013428\n",
            "Epoch 6, Batch 140, G Loss: 0.6930035948753357, D Loss: 1.3861651420593262\n",
            "Epoch 6, Batch 141, G Loss: 0.6928759217262268, D Loss: 1.386242151260376\n",
            "Epoch 6, Batch 142, G Loss: 0.6927803158760071, D Loss: 1.386186957359314\n",
            "Epoch 6, Batch 143, G Loss: 0.6926984190940857, D Loss: 1.3863003253936768\n",
            "Epoch 6, Batch 144, G Loss: 0.6926555037498474, D Loss: 1.3862059116363525\n",
            "Epoch 6, Batch 145, G Loss: 0.692635178565979, D Loss: 1.3863623142242432\n",
            "Epoch 6, Batch 146, G Loss: 0.6926599740982056, D Loss: 1.3861709833145142\n",
            "Epoch 6, Batch 147, G Loss: 0.6926761269569397, D Loss: 1.386136770248413\n",
            "Epoch 6, Batch 148, G Loss: 0.6926981210708618, D Loss: 1.3861333131790161\n",
            "Epoch 6, Batch 149, G Loss: 0.6927148699760437, D Loss: 1.386084794998169\n",
            "Epoch 6, Batch 150, G Loss: 0.6927196979522705, D Loss: 1.386286973953247\n",
            "Epoch 6, Batch 151, G Loss: 0.6927451491355896, D Loss: 1.3862297534942627\n",
            "Epoch 6, Batch 152, G Loss: 0.6927858591079712, D Loss: 1.3861256837844849\n",
            "Epoch 6, Batch 153, G Loss: 0.6928130984306335, D Loss: 1.3861411809921265\n",
            "Epoch 6, Batch 154, G Loss: 0.6928353309631348, D Loss: 1.3861794471740723\n",
            "Epoch 6, Batch 155, G Loss: 0.6928684711456299, D Loss: 1.3861908912658691\n",
            "Epoch 6, Batch 156, G Loss: 0.6929054260253906, D Loss: 1.3861967325210571\n",
            "Epoch 6, Batch 157, G Loss: 0.6929510235786438, D Loss: 1.3861358165740967\n",
            "Epoch 6, Batch 158, G Loss: 0.6929799318313599, D Loss: 1.3862287998199463\n",
            "Epoch 6, Batch 159, G Loss: 0.6930221319198608, D Loss: 1.3862922191619873\n",
            "Epoch 6, Batch 160, G Loss: 0.6930789351463318, D Loss: 1.3860249519348145\n",
            "Epoch 6, Batch 161, G Loss: 0.6930813193321228, D Loss: 1.3860232830047607\n",
            "Epoch 6, Batch 162, G Loss: 0.693030834197998, D Loss: 1.386089563369751\n",
            "Epoch 6, Batch 163, G Loss: 0.6929640769958496, D Loss: 1.386085033416748\n",
            "Epoch 6, Batch 164, G Loss: 0.6928911805152893, D Loss: 1.3860554695129395\n",
            "Epoch 6, Batch 165, G Loss: 0.6928063035011292, D Loss: 1.3861241340637207\n",
            "Epoch 6, Batch 166, G Loss: 0.692724883556366, D Loss: 1.3861663341522217\n",
            "Epoch 6, Batch 167, G Loss: 0.6926491260528564, D Loss: 1.3863186836242676\n",
            "Epoch 6, Batch 168, G Loss: 0.6926184892654419, D Loss: 1.386218547821045\n",
            "Epoch 6, Batch 169, G Loss: 0.6926003098487854, D Loss: 1.386142611503601\n",
            "Epoch 6, Batch 170, G Loss: 0.6925894021987915, D Loss: 1.3861528635025024\n",
            "Epoch 6, Batch 171, G Loss: 0.6925882697105408, D Loss: 1.3862018585205078\n",
            "Epoch 6, Batch 172, G Loss: 0.6925944089889526, D Loss: 1.3860763311386108\n",
            "Epoch 6, Batch 173, G Loss: 0.6925849318504333, D Loss: 1.386020302772522\n",
            "Epoch 6, Batch 174, G Loss: 0.6925727725028992, D Loss: 1.3862212896347046\n",
            "Epoch 6, Batch 175, G Loss: 0.6925705671310425, D Loss: 1.3861606121063232\n",
            "Epoch 6, Batch 176, G Loss: 0.6925766468048096, D Loss: 1.3861961364746094\n",
            "Epoch 6, Batch 177, G Loss: 0.6925930976867676, D Loss: 1.3862721920013428\n",
            "Epoch 6, Batch 178, G Loss: 0.6926169991493225, D Loss: 1.3861799240112305\n",
            "Epoch 6, Batch 179, G Loss: 0.6926528215408325, D Loss: 1.3862500190734863\n",
            "Epoch 6, Batch 180, G Loss: 0.6926953792572021, D Loss: 1.3862509727478027\n",
            "Epoch 6, Batch 181, G Loss: 0.6927406191825867, D Loss: 1.386276364326477\n",
            "Epoch 6, Batch 182, G Loss: 0.6927947402000427, D Loss: 1.3862619400024414\n",
            "Epoch 6, Batch 183, G Loss: 0.6928591132164001, D Loss: 1.3862366676330566\n",
            "Epoch 6, Batch 184, G Loss: 0.6929207444190979, D Loss: 1.3863096237182617\n",
            "Epoch 6, Batch 185, G Loss: 0.6929883360862732, D Loss: 1.3862438201904297\n",
            "Epoch 6, Batch 186, G Loss: 0.6930590271949768, D Loss: 1.3862553834915161\n",
            "Epoch 6, Batch 187, G Loss: 0.6931248307228088, D Loss: 1.386307954788208\n",
            "Epoch 6, Batch 188, G Loss: 0.6931965351104736, D Loss: 1.386001467704773\n",
            "Epoch 6, Batch 189, G Loss: 0.6932383179664612, D Loss: 1.3861559629440308\n",
            "Epoch 6, Batch 190, G Loss: 0.6932716965675354, D Loss: 1.3861627578735352\n",
            "Epoch 6, Batch 191, G Loss: 0.693299412727356, D Loss: 1.386088490486145\n",
            "Epoch 6, Batch 192, G Loss: 0.6933075189590454, D Loss: 1.3860368728637695\n",
            "Epoch 6, Batch 193, G Loss: 0.6933057308197021, D Loss: 1.3861205577850342\n",
            "Epoch 6, Batch 194, G Loss: 0.6932952404022217, D Loss: 1.3861510753631592\n",
            "Epoch 6, Batch 195, G Loss: 0.6932770013809204, D Loss: 1.3860187530517578\n",
            "Epoch 6, Batch 196, G Loss: 0.6932465434074402, D Loss: 1.386069655418396\n",
            "Epoch 6, Batch 197, G Loss: 0.6932138800621033, D Loss: 1.3860225677490234\n",
            "Epoch 6, Batch 198, G Loss: 0.6931727528572083, D Loss: 1.3860260248184204\n",
            "Epoch 6, Batch 199, G Loss: 0.6931225657463074, D Loss: 1.3861451148986816\n",
            "Epoch 6, Batch 200, G Loss: 0.6930795907974243, D Loss: 1.3863364458084106\n",
            "Epoch 6, Batch 201, G Loss: 0.693058431148529, D Loss: 1.3864984512329102\n",
            "Epoch 6, Batch 202, G Loss: 0.6930670738220215, D Loss: 1.3861364126205444\n",
            "Epoch 6, Batch 203, G Loss: 0.6930742859840393, D Loss: 1.3858928680419922\n",
            "Epoch 6, Batch 204, G Loss: 0.6930636763572693, D Loss: 1.3859169483184814\n",
            "Epoch 6, Batch 205, G Loss: 0.6930371522903442, D Loss: 1.3857102394104004\n",
            "Epoch 6, Batch 206, G Loss: 0.6929817795753479, D Loss: 1.386080026626587\n",
            "Epoch 6, Batch 207, G Loss: 0.6929255723953247, D Loss: 1.3859550952911377\n",
            "Epoch 6, Batch 208, G Loss: 0.692875325679779, D Loss: 1.3860533237457275\n",
            "Epoch 6, Batch 209, G Loss: 0.6928226947784424, D Loss: 1.3860417604446411\n",
            "Epoch 6, Batch 210, G Loss: 0.6927828192710876, D Loss: 1.3861174583435059\n",
            "Epoch 6, Batch 211, G Loss: 0.6927487254142761, D Loss: 1.3860023021697998\n",
            "Epoch 6, Batch 212, G Loss: 0.6927132606506348, D Loss: 1.385791540145874\n",
            "Epoch 6, Batch 213, G Loss: 0.6926760673522949, D Loss: 1.386027455329895\n",
            "Epoch 6, Batch 214, G Loss: 0.692650556564331, D Loss: 1.3864994049072266\n",
            "Epoch 6, Batch 215, G Loss: 0.6926625967025757, D Loss: 1.3866674900054932\n",
            "Epoch 6, Batch 216, G Loss: 0.6927082538604736, D Loss: 1.3864363431930542\n",
            "Epoch 6, Batch 217, G Loss: 0.6927703619003296, D Loss: 1.3862426280975342\n",
            "Epoch 6, Batch 218, G Loss: 0.6928481459617615, D Loss: 1.3862199783325195\n",
            "Epoch 6, Batch 219, G Loss: 0.6929262280464172, D Loss: 1.3864182233810425\n",
            "Epoch 6, Batch 220, G Loss: 0.6930180788040161, D Loss: 1.385865569114685\n",
            "Epoch 6, Batch 221, G Loss: 0.6930824518203735, D Loss: 1.3859069347381592\n",
            "Epoch 6, Batch 222, G Loss: 0.6931284070014954, D Loss: 1.3857873678207397\n",
            "Epoch 6, Batch 223, G Loss: 0.6931454539299011, D Loss: 1.386000156402588\n",
            "Epoch 6, Batch 224, G Loss: 0.6931532025337219, D Loss: 1.3859734535217285\n",
            "Epoch 6, Batch 225, G Loss: 0.6931467056274414, D Loss: 1.3861057758331299\n",
            "Epoch 6, Batch 226, G Loss: 0.6931397318840027, D Loss: 1.3863096237182617\n",
            "Epoch 6, Batch 227, G Loss: 0.693143904209137, D Loss: 1.3864266872406006\n",
            "Epoch 6, Batch 228, G Loss: 0.6931686401367188, D Loss: 1.386509895324707\n",
            "Epoch 6, Batch 229, G Loss: 0.6932064890861511, D Loss: 1.386366844177246\n",
            "Epoch 6, Batch 230, G Loss: 0.6932612657546997, D Loss: 1.386135458946228\n",
            "Epoch 6, Batch 231, G Loss: 0.6933028697967529, D Loss: 1.3863656520843506\n",
            "Epoch 6, Batch 232, G Loss: 0.693354070186615, D Loss: 1.3862996101379395\n",
            "Epoch 6, Batch 233, G Loss: 0.6934009194374084, D Loss: 1.3863298892974854\n",
            "Epoch 6, Batch 234, G Loss: 0.6934539079666138, D Loss: 1.386347770690918\n",
            "Epoch 6, Batch 235, G Loss: 0.6935080885887146, D Loss: 1.386576533317566\n",
            "Epoch 6, Batch 236, G Loss: 0.6935814023017883, D Loss: 1.3862454891204834\n",
            "Epoch 6, Batch 237, G Loss: 0.6936526894569397, D Loss: 1.3864006996154785\n",
            "Epoch 6, Batch 238, G Loss: 0.6937210559844971, D Loss: 1.386509656906128\n",
            "Epoch 6, Batch 239, G Loss: 0.6938026547431946, D Loss: 1.3863868713378906\n",
            "Epoch 6, Batch 240, G Loss: 0.6938876509666443, D Loss: 1.3864309787750244\n",
            "Epoch 6, Batch 241, G Loss: 0.6939711570739746, D Loss: 1.3864550590515137\n",
            "Epoch 6, Batch 242, G Loss: 0.6940690875053406, D Loss: 1.3864465951919556\n",
            "Epoch 6, Batch 243, G Loss: 0.6941648125648499, D Loss: 1.3862946033477783\n",
            "Epoch 6, Batch 244, G Loss: 0.6942510008811951, D Loss: 1.3863153457641602\n",
            "Epoch 6, Batch 245, G Loss: 0.6943300366401672, D Loss: 1.3862543106079102\n",
            "Epoch 6, Batch 246, G Loss: 0.6943987011909485, D Loss: 1.3862323760986328\n",
            "Epoch 6, Batch 247, G Loss: 0.6944535970687866, D Loss: 1.3862162828445435\n",
            "Epoch 6, Batch 248, G Loss: 0.694495677947998, D Loss: 1.3862829208374023\n",
            "Epoch 6, Batch 249, G Loss: 0.6945368051528931, D Loss: 1.3861668109893799\n",
            "Epoch 6, Batch 250, G Loss: 0.6945546865463257, D Loss: 1.386286735534668\n",
            "Epoch 6, Batch 251, G Loss: 0.6945627331733704, D Loss: 1.3861916065216064\n",
            "Epoch 6, Batch 252, G Loss: 0.6945525407791138, D Loss: 1.3861351013183594\n",
            "Epoch 6, Batch 253, G Loss: 0.6945159435272217, D Loss: 1.3861360549926758\n",
            "Epoch 6, Batch 254, G Loss: 0.6944552063941956, D Loss: 1.3861949443817139\n",
            "Epoch 6, Batch 255, G Loss: 0.6943852305412292, D Loss: 1.3862154483795166\n",
            "Epoch 6, Batch 256, G Loss: 0.6943132281303406, D Loss: 1.3861987590789795\n",
            "Epoch 6, Batch 257, G Loss: 0.6942403316497803, D Loss: 1.3861899375915527\n",
            "Epoch 6, Batch 258, G Loss: 0.6941637396812439, D Loss: 1.38621187210083\n",
            "Epoch 6, Batch 259, G Loss: 0.6940930485725403, D Loss: 1.3861860036849976\n",
            "Epoch 6, Batch 260, G Loss: 0.6940284371376038, D Loss: 1.3861796855926514\n",
            "Epoch 6, Batch 261, G Loss: 0.6939641237258911, D Loss: 1.3862495422363281\n",
            "Epoch 6, Batch 262, G Loss: 0.6939225196838379, D Loss: 1.3861994743347168\n",
            "Epoch 6, Batch 263, G Loss: 0.6938840746879578, D Loss: 1.3861258029937744\n",
            "Epoch 6, Batch 264, G Loss: 0.6938482522964478, D Loss: 1.386162281036377\n",
            "Epoch 6, Batch 265, G Loss: 0.6938208937644958, D Loss: 1.3862102031707764\n",
            "Epoch 6, Batch 266, G Loss: 0.6938096880912781, D Loss: 1.3862109184265137\n",
            "Epoch 6, Batch 267, G Loss: 0.6937929391860962, D Loss: 1.3862042427062988\n",
            "Epoch 6, Batch 268, G Loss: 0.693787693977356, D Loss: 1.3861737251281738\n",
            "Epoch 6, Batch 269, G Loss: 0.6937721967697144, D Loss: 1.386153221130371\n",
            "Epoch 6, Batch 270, G Loss: 0.6937358379364014, D Loss: 1.3861722946166992\n",
            "Epoch 6, Batch 271, G Loss: 0.6936902403831482, D Loss: 1.386185646057129\n",
            "Epoch 6, Batch 272, G Loss: 0.6936215162277222, D Loss: 1.3861547708511353\n",
            "Epoch 6, Batch 273, G Loss: 0.6935691833496094, D Loss: 1.3861790895462036\n",
            "Epoch 6, Batch 274, G Loss: 0.6935304999351501, D Loss: 1.3861525058746338\n",
            "Epoch 6, Batch 275, G Loss: 0.6934921145439148, D Loss: 1.3861141204833984\n",
            "Epoch 6, Batch 276, G Loss: 0.6934773921966553, D Loss: 1.3861050605773926\n",
            "Epoch 6, Batch 277, G Loss: 0.6934877634048462, D Loss: 1.386122465133667\n",
            "Epoch 6, Batch 278, G Loss: 0.6935005784034729, D Loss: 1.3861768245697021\n",
            "Epoch 6, Batch 279, G Loss: 0.6935346722602844, D Loss: 1.3861584663391113\n",
            "Epoch 6, Batch 280, G Loss: 0.6935794353485107, D Loss: 1.3861373662948608\n",
            "Epoch 6, Batch 281, G Loss: 0.6936401724815369, D Loss: 1.3862228393554688\n",
            "Epoch 6, Batch 282, G Loss: 0.6936854124069214, D Loss: 1.3861932754516602\n",
            "Epoch 6, Batch 283, G Loss: 0.693718433380127, D Loss: 1.3861719369888306\n",
            "Epoch 6, Batch 284, G Loss: 0.6937494874000549, D Loss: 1.3862680196762085\n",
            "Epoch 6, Batch 285, G Loss: 0.6937727928161621, D Loss: 1.3863035440444946\n",
            "Epoch 6, Batch 286, G Loss: 0.6937817931175232, D Loss: 1.3862110376358032\n",
            "Epoch 6, Batch 287, G Loss: 0.6937786936759949, D Loss: 1.3861315250396729\n",
            "Epoch 6, Batch 288, G Loss: 0.6937822103500366, D Loss: 1.386232614517212\n",
            "Epoch 6, Batch 289, G Loss: 0.6937719583511353, D Loss: 1.3861770629882812\n",
            "Epoch 6, Batch 290, G Loss: 0.6937805414199829, D Loss: 1.3862439393997192\n",
            "Epoch 6, Batch 291, G Loss: 0.6937551498413086, D Loss: 1.3861980438232422\n",
            "Epoch 6, Batch 292, G Loss: 0.6937322020530701, D Loss: 1.3862597942352295\n",
            "Epoch 6, Batch 293, G Loss: 0.6936916708946228, D Loss: 1.3861989974975586\n",
            "Epoch 6, Batch 294, G Loss: 0.6936544179916382, D Loss: 1.3862260580062866\n",
            "Epoch 6, Batch 295, G Loss: 0.6936085820198059, D Loss: 1.3862053155899048\n",
            "Epoch 6, Batch 296, G Loss: 0.6935722827911377, D Loss: 1.3862833976745605\n",
            "Epoch 6, Batch 297, G Loss: 0.6935269832611084, D Loss: 1.386228084564209\n",
            "Epoch 6, Batch 298, G Loss: 0.6934686899185181, D Loss: 1.3860960006713867\n",
            "Epoch 6, Batch 299, G Loss: 0.6934356093406677, D Loss: 1.3861671686172485\n",
            "Epoch 6, Batch 300, G Loss: 0.6934080123901367, D Loss: 1.3861796855926514\n",
            "Epoch 6, Batch 301, G Loss: 0.6933911442756653, D Loss: 1.386665940284729\n",
            "Epoch 6, Batch 302, G Loss: 0.6933091878890991, D Loss: 1.3864166736602783\n",
            "Epoch 6, Batch 303, G Loss: 0.6932233572006226, D Loss: 1.386622667312622\n",
            "Epoch 6, Batch 304, G Loss: 0.6931124329566956, D Loss: 1.386566162109375\n",
            "Epoch 6, Batch 305, G Loss: 0.6929763555526733, D Loss: 1.3865692615509033\n",
            "Epoch 6, Batch 306, G Loss: 0.6928240656852722, D Loss: 1.386599063873291\n",
            "Epoch 6, Batch 307, G Loss: 0.6926634907722473, D Loss: 1.3865344524383545\n",
            "Epoch 6, Batch 308, G Loss: 0.6924943923950195, D Loss: 1.3864699602127075\n",
            "Epoch 6, Batch 309, G Loss: 0.6923368573188782, D Loss: 1.386483907699585\n",
            "Epoch 6, Batch 310, G Loss: 0.6921788454055786, D Loss: 1.3864498138427734\n",
            "Epoch 6, Batch 311, G Loss: 0.6920316815376282, D Loss: 1.3863866329193115\n",
            "Epoch 6, Batch 312, G Loss: 0.6918988227844238, D Loss: 1.386436939239502\n",
            "Epoch 6, Batch 313, G Loss: 0.6917765140533447, D Loss: 1.3863465785980225\n",
            "Epoch 6, Batch 314, G Loss: 0.6916539669036865, D Loss: 1.386352300643921\n",
            "Epoch 6, Batch 315, G Loss: 0.6915640830993652, D Loss: 1.3863155841827393\n",
            "Epoch 6, Batch 316, G Loss: 0.6914688348770142, D Loss: 1.3862977027893066\n",
            "Epoch 6, Batch 317, G Loss: 0.6914036273956299, D Loss: 1.3863003253936768\n",
            "Epoch 6, Batch 318, G Loss: 0.6913548707962036, D Loss: 1.3862755298614502\n",
            "Epoch 6, Batch 319, G Loss: 0.6913402080535889, D Loss: 1.3862643241882324\n",
            "Epoch 6, Batch 320, G Loss: 0.6913325190544128, D Loss: 1.3863110542297363\n",
            "Epoch 6, Batch 321, G Loss: 0.6913427114486694, D Loss: 1.3862109184265137\n",
            "Epoch 6, Batch 322, G Loss: 0.6913578510284424, D Loss: 1.3862576484680176\n",
            "Epoch 6, Batch 323, G Loss: 0.691382646560669, D Loss: 1.386193037033081\n",
            "Epoch 6, Batch 324, G Loss: 0.6913999319076538, D Loss: 1.3862435817718506\n",
            "Epoch 6, Batch 325, G Loss: 0.6914275288581848, D Loss: 1.3862996101379395\n",
            "Epoch 6, Batch 326, G Loss: 0.6914671063423157, D Loss: 1.3862282037734985\n",
            "Epoch 6, Batch 327, G Loss: 0.6915199756622314, D Loss: 1.3862719535827637\n",
            "Epoch 6, Batch 328, G Loss: 0.6915801763534546, D Loss: 1.3860855102539062\n",
            "Epoch 6, Batch 329, G Loss: 0.6916292309761047, D Loss: 1.3862701654434204\n",
            "Epoch 6, Batch 330, G Loss: 0.6916950941085815, D Loss: 1.3864043951034546\n",
            "Epoch 6, Batch 331, G Loss: 0.6917768120765686, D Loss: 1.3862762451171875\n",
            "Epoch 6, Batch 332, G Loss: 0.6918731331825256, D Loss: 1.3862276077270508\n",
            "Epoch 6, Batch 333, G Loss: 0.6919699907302856, D Loss: 1.3860912322998047\n",
            "Epoch 6, Batch 334, G Loss: 0.6920567750930786, D Loss: 1.3861892223358154\n",
            "Epoch 6, Batch 335, G Loss: 0.692143976688385, D Loss: 1.3861050605773926\n",
            "Epoch 6, Batch 336, G Loss: 0.6922286748886108, D Loss: 1.3861106634140015\n",
            "Epoch 6, Batch 337, G Loss: 0.6923008561134338, D Loss: 1.3859293460845947\n",
            "Epoch 6, Batch 338, G Loss: 0.6923419237136841, D Loss: 1.3865368366241455\n",
            "Epoch 6, Batch 339, G Loss: 0.6924171447753906, D Loss: 1.386380672454834\n",
            "Epoch 6, Batch 340, G Loss: 0.6924946308135986, D Loss: 1.3866288661956787\n",
            "Epoch 6, Batch 341, G Loss: 0.692609965801239, D Loss: 1.3857300281524658\n",
            "Epoch 6, Batch 342, G Loss: 0.6926769614219666, D Loss: 1.385850191116333\n",
            "Epoch 6, Batch 343, G Loss: 0.6927193403244019, D Loss: 1.385745882987976\n",
            "Epoch 6, Batch 344, G Loss: 0.6927225589752197, D Loss: 1.3858319520950317\n",
            "Epoch 6, Batch 345, G Loss: 0.6927070617675781, D Loss: 1.3861265182495117\n",
            "Epoch 6, Batch 346, G Loss: 0.6927067637443542, D Loss: 1.3862727880477905\n",
            "Epoch 6, Batch 347, G Loss: 0.6927016377449036, D Loss: 1.3862390518188477\n",
            "Epoch 6, Batch 348, G Loss: 0.6926997303962708, D Loss: 1.3861916065216064\n",
            "Epoch 6, Batch 349, G Loss: 0.6927105784416199, D Loss: 1.3864102363586426\n",
            "Epoch 6, Batch 350, G Loss: 0.6927340626716614, D Loss: 1.3862261772155762\n",
            "Epoch 6, Batch 351, G Loss: 0.6927590370178223, D Loss: 1.3862690925598145\n",
            "Epoch 6, Batch 352, G Loss: 0.6927943825721741, D Loss: 1.3863825798034668\n",
            "Epoch 6, Batch 353, G Loss: 0.6928408145904541, D Loss: 1.3861305713653564\n",
            "Epoch 6, Batch 354, G Loss: 0.6928610801696777, D Loss: 1.3860584497451782\n",
            "Epoch 6, Batch 355, G Loss: 0.6929038166999817, D Loss: 1.3856050968170166\n",
            "Epoch 6, Batch 356, G Loss: 0.6928955316543579, D Loss: 1.385713815689087\n",
            "Epoch 6, Batch 357, G Loss: 0.6928774118423462, D Loss: 1.3861083984375\n",
            "Epoch 6, Batch 358, G Loss: 0.6928644180297852, D Loss: 1.3858919143676758\n",
            "Epoch 6, Batch 359, G Loss: 0.6928319334983826, D Loss: 1.385798454284668\n",
            "Epoch 6, Batch 360, G Loss: 0.6928060054779053, D Loss: 1.3859308958053589\n",
            "Epoch 6, Batch 361, G Loss: 0.6927680373191833, D Loss: 1.3855561017990112\n",
            "Epoch 6, Batch 362, G Loss: 0.6927177309989929, D Loss: 1.3860454559326172\n",
            "Epoch 6, Batch 363, G Loss: 0.6926705241203308, D Loss: 1.3862977027893066\n",
            "Epoch 6, Batch 364, G Loss: 0.6926384568214417, D Loss: 1.386237621307373\n",
            "Epoch 6, Batch 365, G Loss: 0.6926129460334778, D Loss: 1.386138916015625\n",
            "Epoch 6, Batch 366, G Loss: 0.6926141977310181, D Loss: 1.3861427307128906\n",
            "Epoch 6, Batch 367, G Loss: 0.6926191449165344, D Loss: 1.3864128589630127\n",
            "Epoch 6, Batch 368, G Loss: 0.6926417350769043, D Loss: 1.3865387439727783\n",
            "Epoch 6, Batch 369, G Loss: 0.6926731467247009, D Loss: 1.3865901231765747\n",
            "Epoch 6, Batch 370, G Loss: 0.6927288770675659, D Loss: 1.3868608474731445\n",
            "Epoch 6, Batch 371, G Loss: 0.6928108930587769, D Loss: 1.3868021965026855\n",
            "Epoch 6, Batch 372, G Loss: 0.692912220954895, D Loss: 1.3862037658691406\n",
            "Epoch 6, Batch 373, G Loss: 0.6930015683174133, D Loss: 1.3862665891647339\n",
            "Epoch 6, Batch 374, G Loss: 0.6931061744689941, D Loss: 1.3865149021148682\n",
            "Epoch 6, Batch 375, G Loss: 0.6932032108306885, D Loss: 1.3863850831985474\n",
            "Epoch 6, Batch 376, G Loss: 0.6933050155639648, D Loss: 1.386355996131897\n",
            "Epoch 6, Batch 377, G Loss: 0.6934059262275696, D Loss: 1.3863600492477417\n",
            "Epoch 6, Batch 378, G Loss: 0.6934939026832581, D Loss: 1.386603832244873\n",
            "Epoch 6, Batch 379, G Loss: 0.6935943365097046, D Loss: 1.3865535259246826\n",
            "Epoch 6, Batch 380, G Loss: 0.6936972737312317, D Loss: 1.3864766359329224\n",
            "Epoch 6, Batch 381, G Loss: 0.6937904357910156, D Loss: 1.386838436126709\n",
            "Epoch 6, Batch 382, G Loss: 0.6938894391059875, D Loss: 1.3861052989959717\n",
            "Epoch 6, Batch 383, G Loss: 0.6939651370048523, D Loss: 1.3863284587860107\n",
            "Epoch 6, Batch 384, G Loss: 0.6940284967422485, D Loss: 1.3865892887115479\n",
            "Epoch 6, Batch 385, G Loss: 0.6941050291061401, D Loss: 1.3862985372543335\n",
            "Epoch 6, Batch 386, G Loss: 0.6941601634025574, D Loss: 1.3861232995986938\n",
            "Epoch 6, Batch 387, G Loss: 0.6941947937011719, D Loss: 1.3864202499389648\n",
            "Epoch 6, Batch 388, G Loss: 0.6942177414894104, D Loss: 1.3865150213241577\n",
            "Epoch 6, Batch 389, G Loss: 0.694246232509613, D Loss: 1.386579990386963\n",
            "Epoch 6, Batch 390, G Loss: 0.694275975227356, D Loss: 1.3871134519577026\n",
            "Epoch 6, Batch 391, G Loss: 0.6943321824073792, D Loss: 1.386592149734497\n",
            "Epoch 6, Batch 392, G Loss: 0.6943997740745544, D Loss: 1.3866541385650635\n",
            "Epoch 6, Batch 393, G Loss: 0.694455623626709, D Loss: 1.3865375518798828\n",
            "Epoch 6, Batch 394, G Loss: 0.6945235729217529, D Loss: 1.3864498138427734\n",
            "Epoch 6, Batch 395, G Loss: 0.6945777535438538, D Loss: 1.3865346908569336\n",
            "Epoch 6, Batch 396, G Loss: 0.6946349143981934, D Loss: 1.3857338428497314\n",
            "Epoch 6, Batch 397, G Loss: 0.6946296095848083, D Loss: 1.3859360218048096\n",
            "Epoch 6, Batch 398, G Loss: 0.6945879459381104, D Loss: 1.3861780166625977\n",
            "Epoch 6, Batch 399, G Loss: 0.6945332884788513, D Loss: 1.3861496448516846\n",
            "Epoch 6, Batch 400, G Loss: 0.6944602727890015, D Loss: 1.3865361213684082\n",
            "Epoch 6, Batch 401, G Loss: 0.6944040656089783, D Loss: 1.3863451480865479\n",
            "Epoch 6, Batch 402, G Loss: 0.6943456530570984, D Loss: 1.3862677812576294\n",
            "Epoch 6, Batch 403, G Loss: 0.6942954063415527, D Loss: 1.386527419090271\n",
            "Epoch 6, Batch 404, G Loss: 0.6942592263221741, D Loss: 1.3866758346557617\n",
            "Epoch 6, Batch 405, G Loss: 0.6942633986473083, D Loss: 1.3866498470306396\n",
            "Epoch 6, Batch 406, G Loss: 0.6942935585975647, D Loss: 1.3865712881088257\n",
            "Epoch 6, Batch 407, G Loss: 0.6943536996841431, D Loss: 1.386175513267517\n",
            "Epoch 6, Batch 408, G Loss: 0.694389283657074, D Loss: 1.38631010055542\n",
            "Epoch 6, Batch 409, G Loss: 0.6944223642349243, D Loss: 1.386327862739563\n",
            "Epoch 6, Batch 410, G Loss: 0.6944560408592224, D Loss: 1.3862788677215576\n",
            "Epoch 6, Batch 411, G Loss: 0.6944818496704102, D Loss: 1.3863420486450195\n",
            "Epoch 6, Batch 412, G Loss: 0.6945204734802246, D Loss: 1.3862746953964233\n",
            "Epoch 6, Batch 413, G Loss: 0.6945662498474121, D Loss: 1.3862831592559814\n",
            "Epoch 6, Batch 414, G Loss: 0.6946151852607727, D Loss: 1.3862390518188477\n",
            "Epoch 6, Batch 415, G Loss: 0.6946733593940735, D Loss: 1.386225938796997\n",
            "Epoch 6, Batch 416, G Loss: 0.6947428584098816, D Loss: 1.386176347732544\n",
            "Epoch 6, Batch 417, G Loss: 0.6948318481445312, D Loss: 1.3861510753631592\n",
            "Epoch 6, Batch 418, G Loss: 0.6948950886726379, D Loss: 1.386170744895935\n",
            "Epoch 6, Batch 419, G Loss: 0.6949349045753479, D Loss: 1.3861973285675049\n",
            "Epoch 6, Batch 420, G Loss: 0.6949492692947388, D Loss: 1.3861675262451172\n",
            "Epoch 6, Batch 421, G Loss: 0.6949416399002075, D Loss: 1.3861050605773926\n",
            "Epoch 6, Batch 422, G Loss: 0.6949267387390137, D Loss: 1.386044979095459\n",
            "Epoch 6, Batch 423, G Loss: 0.6949223279953003, D Loss: 1.3860585689544678\n",
            "Epoch 6, Batch 424, G Loss: 0.6949124336242676, D Loss: 1.3862396478652954\n",
            "Epoch 6, Batch 425, G Loss: 0.6948618292808533, D Loss: 1.3862006664276123\n",
            "Epoch 6, Batch 426, G Loss: 0.6947731971740723, D Loss: 1.3861582279205322\n",
            "Epoch 6, Batch 427, G Loss: 0.6946710348129272, D Loss: 1.3861808776855469\n",
            "Epoch 6, Batch 428, G Loss: 0.6945539116859436, D Loss: 1.3860909938812256\n",
            "Epoch 6, Batch 429, G Loss: 0.6944358348846436, D Loss: 1.3861560821533203\n",
            "Epoch 6, Batch 430, G Loss: 0.6943158507347107, D Loss: 1.3859679698944092\n",
            "Epoch 6, Batch 431, G Loss: 0.6942128539085388, D Loss: 1.385969877243042\n",
            "Epoch 6, Batch 432, G Loss: 0.6941311359405518, D Loss: 1.3860704898834229\n",
            "Epoch 6, Batch 433, G Loss: 0.694049596786499, D Loss: 1.3859156370162964\n",
            "Epoch 6, Batch 434, G Loss: 0.6939961910247803, D Loss: 1.3859200477600098\n",
            "Epoch 6, Batch 435, G Loss: 0.6939626336097717, D Loss: 1.3858121633529663\n",
            "Epoch 6, Batch 436, G Loss: 0.6939465403556824, D Loss: 1.3860223293304443\n",
            "Epoch 6, Batch 437, G Loss: 0.6939533948898315, D Loss: 1.3864318132400513\n",
            "Epoch 6, Batch 438, G Loss: 0.6938990354537964, D Loss: 1.3863524198532104\n",
            "Epoch 6, Batch 439, G Loss: 0.6938372254371643, D Loss: 1.3864123821258545\n",
            "Epoch 6, Batch 440, G Loss: 0.6937680244445801, D Loss: 1.3864370584487915\n",
            "Epoch 6, Batch 441, G Loss: 0.6936724781990051, D Loss: 1.3865488767623901\n",
            "Epoch 6, Batch 442, G Loss: 0.6935577392578125, D Loss: 1.3865008354187012\n",
            "Epoch 6, Batch 443, G Loss: 0.693414032459259, D Loss: 1.3864057064056396\n",
            "Epoch 6, Batch 444, G Loss: 0.6932652592658997, D Loss: 1.3866511583328247\n",
            "Epoch 6, Batch 445, G Loss: 0.6930856704711914, D Loss: 1.386618733406067\n",
            "Epoch 6, Batch 446, G Loss: 0.6929109692573547, D Loss: 1.38650643825531\n",
            "Epoch 6, Batch 447, G Loss: 0.6927257180213928, D Loss: 1.3865439891815186\n",
            "Epoch 6, Batch 448, G Loss: 0.692574143409729, D Loss: 1.386326789855957\n",
            "Epoch 6, Batch 449, G Loss: 0.6924362182617188, D Loss: 1.3862636089324951\n",
            "Epoch 6, Batch 450, G Loss: 0.6923251748085022, D Loss: 1.386347770690918\n",
            "Epoch 6, Batch 451, G Loss: 0.6922268271446228, D Loss: 1.3862805366516113\n",
            "Epoch 6, Batch 452, G Loss: 0.6921471357345581, D Loss: 1.3865734338760376\n",
            "Epoch 6, Batch 453, G Loss: 0.6920634508132935, D Loss: 1.3864305019378662\n",
            "Epoch 6, Batch 454, G Loss: 0.6919711232185364, D Loss: 1.3864507675170898\n",
            "Epoch 6, Batch 455, G Loss: 0.6919013261795044, D Loss: 1.386069893836975\n",
            "Epoch 6, Batch 456, G Loss: 0.6918686032295227, D Loss: 1.3861287832260132\n",
            "Epoch 6, Batch 457, G Loss: 0.6918758749961853, D Loss: 1.3861922025680542\n",
            "Epoch 6, Batch 458, G Loss: 0.6918907761573792, D Loss: 1.386716365814209\n",
            "Epoch 6, Batch 459, G Loss: 0.6918935179710388, D Loss: 1.3866671323776245\n",
            "Epoch 6, Batch 460, G Loss: 0.6918694376945496, D Loss: 1.386689305305481\n",
            "Epoch 6, Batch 461, G Loss: 0.6918266415596008, D Loss: 1.3866190910339355\n",
            "Epoch 6, Batch 462, G Loss: 0.6917783617973328, D Loss: 1.3863276243209839\n",
            "Epoch 6, Batch 463, G Loss: 0.6917668581008911, D Loss: 1.38625967502594\n",
            "Epoch 6, Batch 464, G Loss: 0.6917729377746582, D Loss: 1.386332631111145\n",
            "Epoch 6, Batch 465, G Loss: 0.6917949318885803, D Loss: 1.38625967502594\n",
            "Epoch 6, Batch 466, G Loss: 0.6918548345565796, D Loss: 1.3862740993499756\n",
            "Epoch 6, Batch 467, G Loss: 0.6919400691986084, D Loss: 1.3862724304199219\n",
            "Epoch 6, Batch 468, G Loss: 0.6920484304428101, D Loss: 1.3862783908843994\n",
            "Epoch 6, Batch 469, G Loss: 0.6921706795692444, D Loss: 1.3861808776855469\n",
            "Epoch 6, Batch 470, G Loss: 0.6923194527626038, D Loss: 1.3863747119903564\n",
            "Epoch 6, Batch 471, G Loss: 0.6924509406089783, D Loss: 1.3863728046417236\n",
            "Epoch 6, Batch 472, G Loss: 0.6925649642944336, D Loss: 1.3863911628723145\n",
            "Epoch 6, Batch 473, G Loss: 0.692676305770874, D Loss: 1.38637375831604\n",
            "Epoch 6, Batch 474, G Loss: 0.6927851438522339, D Loss: 1.3863698244094849\n",
            "Epoch 6, Batch 475, G Loss: 0.6928762197494507, D Loss: 1.386475682258606\n",
            "Epoch 6, Batch 476, G Loss: 0.6929469108581543, D Loss: 1.38649320602417\n",
            "Epoch 6, Batch 477, G Loss: 0.6929853558540344, D Loss: 1.3864567279815674\n",
            "Epoch 6, Batch 478, G Loss: 0.692999005317688, D Loss: 1.3863906860351562\n",
            "Epoch 6, Batch 479, G Loss: 0.6930025815963745, D Loss: 1.3861870765686035\n",
            "Epoch 6, Batch 480, G Loss: 0.6930381655693054, D Loss: 1.3862473964691162\n",
            "Epoch 6, Batch 481, G Loss: 0.6930873394012451, D Loss: 1.3862524032592773\n",
            "Epoch 6, Batch 482, G Loss: 0.6931437253952026, D Loss: 1.3864421844482422\n",
            "Epoch 6, Batch 483, G Loss: 0.6931846737861633, D Loss: 1.3864197731018066\n",
            "Epoch 6, Batch 484, G Loss: 0.6932073831558228, D Loss: 1.3863627910614014\n",
            "Epoch 6, Batch 485, G Loss: 0.6932240724563599, D Loss: 1.3863729238510132\n",
            "Epoch 6, Batch 486, G Loss: 0.6932373642921448, D Loss: 1.3863770961761475\n",
            "Epoch 6, Batch 487, G Loss: 0.6932402849197388, D Loss: 1.386352777481079\n",
            "Epoch 6, Batch 488, G Loss: 0.6932408809661865, D Loss: 1.386340618133545\n",
            "Epoch 6, Batch 489, G Loss: 0.6932339072227478, D Loss: 1.386385202407837\n",
            "Epoch 6, Batch 490, G Loss: 0.6932260990142822, D Loss: 1.3863670825958252\n",
            "Epoch 6, Batch 491, G Loss: 0.6932171583175659, D Loss: 1.3863544464111328\n",
            "Epoch 6, Batch 492, G Loss: 0.6932050585746765, D Loss: 1.3863542079925537\n",
            "Epoch 6, Batch 493, G Loss: 0.6931878924369812, D Loss: 1.386319637298584\n",
            "Epoch 6, Batch 494, G Loss: 0.6931820511817932, D Loss: 1.3863164186477661\n",
            "Epoch 6, Batch 495, G Loss: 0.6931843757629395, D Loss: 1.3863184452056885\n",
            "Epoch 6, Batch 496, G Loss: 0.6931934952735901, D Loss: 1.386328101158142\n",
            "Epoch 6, Batch 497, G Loss: 0.6932024359703064, D Loss: 1.3863924741744995\n",
            "Epoch 6, Batch 498, G Loss: 0.6932010054588318, D Loss: 1.3864035606384277\n",
            "Epoch 6, Batch 499, G Loss: 0.6931929588317871, D Loss: 1.3863818645477295\n",
            "Epoch 6, Batch 500, G Loss: 0.6931910514831543, D Loss: 1.3864119052886963\n",
            "Epoch 6, Batch 501, G Loss: 0.693165123462677, D Loss: 1.3864325284957886\n",
            "Epoch 6, Batch 502, G Loss: 0.6931158304214478, D Loss: 1.3864386081695557\n",
            "Epoch 6, Batch 503, G Loss: 0.6930461525917053, D Loss: 1.3864049911499023\n",
            "Epoch 6, Batch 504, G Loss: 0.6929657459259033, D Loss: 1.3863816261291504\n",
            "Epoch 6, Batch 505, G Loss: 0.6928851008415222, D Loss: 1.3864319324493408\n",
            "Epoch 6, Batch 506, G Loss: 0.6927893757820129, D Loss: 1.3864428997039795\n",
            "Epoch 6, Batch 507, G Loss: 0.6926758289337158, D Loss: 1.3864178657531738\n",
            "Epoch 6, Batch 508, G Loss: 0.6925410032272339, D Loss: 1.3863897323608398\n",
            "Epoch 6, Batch 509, G Loss: 0.6924012899398804, D Loss: 1.3863613605499268\n",
            "Epoch 6, Batch 510, G Loss: 0.6922701001167297, D Loss: 1.3863351345062256\n",
            "Epoch 6, Batch 511, G Loss: 0.6921665668487549, D Loss: 1.3863263130187988\n",
            "Epoch 6, Batch 512, G Loss: 0.6920750141143799, D Loss: 1.3863877058029175\n",
            "Epoch 6, Batch 513, G Loss: 0.6920283436775208, D Loss: 1.3863248825073242\n",
            "Epoch 6, Batch 514, G Loss: 0.69198077917099, D Loss: 1.386378526687622\n",
            "Epoch 6, Batch 515, G Loss: 0.6919647455215454, D Loss: 1.3862783908843994\n",
            "Epoch 6, Batch 516, G Loss: 0.6919522881507874, D Loss: 1.386191725730896\n",
            "Epoch 6, Batch 517, G Loss: 0.6919270753860474, D Loss: 1.3861864805221558\n",
            "Epoch 6, Batch 518, G Loss: 0.691895067691803, D Loss: 1.3862900733947754\n",
            "Epoch 6, Batch 519, G Loss: 0.6918700337409973, D Loss: 1.3862009048461914\n",
            "Epoch 6, Batch 520, G Loss: 0.6918492317199707, D Loss: 1.3862059116363525\n",
            "Epoch 6, Batch 521, G Loss: 0.6918302774429321, D Loss: 1.386197805404663\n",
            "Epoch 6, Batch 522, G Loss: 0.6918141841888428, D Loss: 1.3863506317138672\n",
            "Epoch 6, Batch 523, G Loss: 0.6918115615844727, D Loss: 1.3864374160766602\n",
            "Epoch 6, Batch 524, G Loss: 0.6918536424636841, D Loss: 1.3863842487335205\n",
            "Epoch 6, Batch 525, G Loss: 0.6919004321098328, D Loss: 1.3865017890930176\n",
            "Epoch 6, Batch 526, G Loss: 0.6919720768928528, D Loss: 1.3865294456481934\n",
            "Epoch 6, Batch 527, G Loss: 0.6920798420906067, D Loss: 1.386365532875061\n",
            "Epoch 6, Batch 528, G Loss: 0.692182719707489, D Loss: 1.3861918449401855\n",
            "Epoch 6, Batch 529, G Loss: 0.6922823786735535, D Loss: 1.3859940767288208\n",
            "Epoch 6, Batch 530, G Loss: 0.6923432350158691, D Loss: 1.3862172365188599\n",
            "Epoch 6, Batch 531, G Loss: 0.6924077272415161, D Loss: 1.3862555027008057\n",
            "Epoch 6, Batch 532, G Loss: 0.6924611330032349, D Loss: 1.3865785598754883\n",
            "Epoch 6, Batch 533, G Loss: 0.6925339698791504, D Loss: 1.3862872123718262\n",
            "Epoch 6, Batch 534, G Loss: 0.6926100850105286, D Loss: 1.3862359523773193\n",
            "Epoch 6, Batch 535, G Loss: 0.6926761269569397, D Loss: 1.386380672454834\n",
            "Epoch 6, Batch 536, G Loss: 0.6927309632301331, D Loss: 1.3865009546279907\n",
            "Epoch 6, Batch 537, G Loss: 0.6928075551986694, D Loss: 1.386202335357666\n",
            "Epoch 6, Batch 538, G Loss: 0.6928672194480896, D Loss: 1.3861422538757324\n",
            "Epoch 6, Batch 539, G Loss: 0.6929137706756592, D Loss: 1.3862006664276123\n",
            "Epoch 6, Batch 540, G Loss: 0.6929539442062378, D Loss: 1.3861668109893799\n",
            "Epoch 6, Batch 541, G Loss: 0.6929723024368286, D Loss: 1.3862168788909912\n",
            "Epoch 6, Batch 542, G Loss: 0.6929941177368164, D Loss: 1.3862147331237793\n",
            "Epoch 6, Batch 543, G Loss: 0.6930009722709656, D Loss: 1.38621187210083\n",
            "Epoch 6, Batch 544, G Loss: 0.6929974555969238, D Loss: 1.386230230331421\n",
            "Epoch 6, Batch 545, G Loss: 0.6930041909217834, D Loss: 1.3862459659576416\n",
            "Epoch 6, Batch 546, G Loss: 0.6929982900619507, D Loss: 1.3862971067428589\n",
            "Epoch 6, Batch 547, G Loss: 0.6929903030395508, D Loss: 1.3863165378570557\n",
            "Epoch 6, Batch 548, G Loss: 0.6929904222488403, D Loss: 1.3859783411026\n",
            "Epoch 6, Batch 549, G Loss: 0.6929680705070496, D Loss: 1.38629150390625\n",
            "Epoch 6, Batch 550, G Loss: 0.6929570436477661, D Loss: 1.3866066932678223\n",
            "Epoch 6, Batch 551, G Loss: 0.6929553151130676, D Loss: 1.3865419626235962\n",
            "Epoch 6, Batch 552, G Loss: 0.6929752826690674, D Loss: 1.3865764141082764\n",
            "Epoch 6, Batch 553, G Loss: 0.6930065751075745, D Loss: 1.3862472772598267\n",
            "Epoch 6, Batch 554, G Loss: 0.693030059337616, D Loss: 1.386157512664795\n",
            "Epoch 6, Batch 555, G Loss: 0.6930395364761353, D Loss: 1.3860599994659424\n",
            "Epoch 6, Batch 556, G Loss: 0.6930441856384277, D Loss: 1.3864479064941406\n",
            "Epoch 6, Batch 557, G Loss: 0.6930693984031677, D Loss: 1.3865410089492798\n",
            "Epoch 6, Batch 558, G Loss: 0.6930926442146301, D Loss: 1.3862476348876953\n",
            "Epoch 6, Batch 559, G Loss: 0.6931242942810059, D Loss: 1.3864283561706543\n",
            "Epoch 6, Batch 560, G Loss: 0.6931470632553101, D Loss: 1.3864238262176514\n",
            "Epoch 6, Batch 561, G Loss: 0.6931790709495544, D Loss: 1.3866550922393799\n",
            "Epoch 6, Batch 562, G Loss: 0.693230152130127, D Loss: 1.3868118524551392\n",
            "Epoch 6, Batch 563, G Loss: 0.6933026909828186, D Loss: 1.3868131637573242\n",
            "Epoch 6, Batch 564, G Loss: 0.6933884620666504, D Loss: 1.3870720863342285\n",
            "Epoch 6, Batch 565, G Loss: 0.693510115146637, D Loss: 1.3862712383270264\n",
            "Epoch 6, Batch 566, G Loss: 0.6936123967170715, D Loss: 1.3856418132781982\n",
            "Epoch 6, Batch 567, G Loss: 0.6936617493629456, D Loss: 1.386000394821167\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Batch 568, G Loss: 0.6936879754066467, D Loss: 1.3859058618545532\n",
            "Epoch 6, Batch 569, G Loss: 0.6936749815940857, D Loss: 1.3864972591400146\n",
            "Epoch 6, Batch 570, G Loss: 0.6936684250831604, D Loss: 1.3863790035247803\n",
            "Epoch 6, Batch 571, G Loss: 0.6936642527580261, D Loss: 1.3865281343460083\n",
            "Epoch 6, Batch 572, G Loss: 0.6936714053153992, D Loss: 1.385742425918579\n",
            "Epoch 6, Batch 573, G Loss: 0.6936350464820862, D Loss: 1.3859248161315918\n",
            "Epoch 6, Batch 574, G Loss: 0.6935725212097168, D Loss: 1.3856682777404785\n",
            "Epoch 6, Batch 575, G Loss: 0.6934798955917358, D Loss: 1.3858301639556885\n",
            "Epoch 6, Batch 576, G Loss: 0.6933658123016357, D Loss: 1.3862559795379639\n",
            "Epoch 6, Batch 577, G Loss: 0.6932563185691833, D Loss: 1.3862390518188477\n",
            "Epoch 6, Batch 578, G Loss: 0.6931647658348083, D Loss: 1.386519193649292\n",
            "Epoch 6, Batch 579, G Loss: 0.693095326423645, D Loss: 1.386625051498413\n",
            "Epoch 6, Batch 580, G Loss: 0.6930592656135559, D Loss: 1.385937213897705\n",
            "Epoch 6, Batch 581, G Loss: 0.6930083632469177, D Loss: 1.386075735092163\n",
            "Epoch 6, Batch 582, G Loss: 0.6929657459259033, D Loss: 1.3860138654708862\n",
            "Epoch 6, Batch 583, G Loss: 0.6929168701171875, D Loss: 1.3859529495239258\n",
            "Epoch 6, Batch 584, G Loss: 0.6928695440292358, D Loss: 1.3863320350646973\n",
            "Epoch 6, Batch 585, G Loss: 0.6928343772888184, D Loss: 1.3864333629608154\n",
            "Epoch 6, Batch 586, G Loss: 0.692815363407135, D Loss: 1.3865737915039062\n",
            "Epoch 6, Batch 587, G Loss: 0.6928377151489258, D Loss: 1.3867206573486328\n",
            "Epoch 6, Batch 588, G Loss: 0.6928741931915283, D Loss: 1.3867518901824951\n",
            "Epoch 6, Batch 589, G Loss: 0.6929391622543335, D Loss: 1.3868327140808105\n",
            "Epoch 6, Batch 590, G Loss: 0.6930409073829651, D Loss: 1.3861939907073975\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Batch 591, G Loss: 0.6931222677230835, D Loss: 1.3861750364303589\n",
            "Epoch 6, Batch 592, G Loss: 0.6932082176208496, D Loss: 1.386317253112793\n",
            "Epoch 6, Batch 593, G Loss: 0.6932845115661621, D Loss: 1.3865094184875488\n",
            "Epoch 6, Batch 594, G Loss: 0.6933764815330505, D Loss: 1.386574387550354\n",
            "Epoch 6, Batch 595, G Loss: 0.6934688091278076, D Loss: 1.3868091106414795\n",
            "Epoch 6, Batch 596, G Loss: 0.6935811042785645, D Loss: 1.3862721920013428\n",
            "Epoch 6, Batch 597, G Loss: 0.6936848163604736, D Loss: 1.386265516281128\n",
            "Epoch 6, Batch 598, G Loss: 0.6937784552574158, D Loss: 1.3863985538482666\n",
            "Epoch 6, Batch 599, G Loss: 0.6938549876213074, D Loss: 1.3864753246307373\n",
            "Epoch 6, Batch 600, G Loss: 0.6939411759376526, D Loss: 1.3865917921066284\n",
            "Epoch 6, Batch 601, G Loss: 0.694038450717926, D Loss: 1.3865230083465576\n",
            "Epoch 6, Batch 602, G Loss: 0.6941304206848145, D Loss: 1.3866153955459595\n",
            "Epoch 6, Batch 603, G Loss: 0.6942331790924072, D Loss: 1.3864140510559082\n",
            "Epoch 6, Batch 604, G Loss: 0.6943293809890747, D Loss: 1.3861021995544434\n",
            "Epoch 6, Batch 605, G Loss: 0.6943846940994263, D Loss: 1.3864085674285889\n",
            "Epoch 6, Batch 606, G Loss: 0.6944411993026733, D Loss: 1.386362075805664\n",
            "Epoch 6, Batch 607, G Loss: 0.6944897174835205, D Loss: 1.3863465785980225\n",
            "Epoch 6, Batch 608, G Loss: 0.6945264339447021, D Loss: 1.3862621784210205\n",
            "Epoch 6, Batch 609, G Loss: 0.6945462822914124, D Loss: 1.3863673210144043\n",
            "Epoch 6, Batch 610, G Loss: 0.6945673823356628, D Loss: 1.3864092826843262\n",
            "Epoch 6, Batch 611, G Loss: 0.6945950388908386, D Loss: 1.3862841129302979\n",
            "Epoch 6, Batch 612, G Loss: 0.6946095824241638, D Loss: 1.3861817121505737\n",
            "Epoch 6, Batch 613, G Loss: 0.6945902109146118, D Loss: 1.3861851692199707\n",
            "Epoch 6, Batch 614, G Loss: 0.6945446729660034, D Loss: 1.3862354755401611\n",
            "Epoch 6, Batch 615, G Loss: 0.6944877505302429, D Loss: 1.3862426280975342\n",
            "Epoch 6, Batch 616, G Loss: 0.6944195628166199, D Loss: 1.386232852935791\n",
            "Epoch 6, Batch 617, G Loss: 0.6943453550338745, D Loss: 1.3862413167953491\n",
            "Epoch 6, Batch 618, G Loss: 0.6942555904388428, D Loss: 1.3862861394882202\n",
            "Epoch 6, Batch 619, G Loss: 0.6941741704940796, D Loss: 1.3862428665161133\n",
            "Epoch 6, Batch 620, G Loss: 0.6940884590148926, D Loss: 1.3862406015396118\n",
            "Epoch 6, Batch 621, G Loss: 0.6940056681632996, D Loss: 1.3862342834472656\n",
            "Epoch 6, Batch 622, G Loss: 0.6939342021942139, D Loss: 1.386232614517212\n",
            "Epoch 6, Batch 623, G Loss: 0.6939175724983215, D Loss: 1.3861925601959229\n",
            "Epoch 6, Batch 624, G Loss: 0.693916380405426, D Loss: 1.3861784934997559\n",
            "Epoch 6, Batch 625, G Loss: 0.6939419507980347, D Loss: 1.3862106800079346\n",
            "Epoch 6, Batch 626, G Loss: 0.6939641237258911, D Loss: 1.3861968517303467\n",
            "Epoch 6, Batch 627, G Loss: 0.6939818859100342, D Loss: 1.386202335357666\n",
            "Epoch 6, Batch 628, G Loss: 0.6939986348152161, D Loss: 1.3861839771270752\n",
            "Epoch 6, Batch 629, G Loss: 0.6939888596534729, D Loss: 1.386170506477356\n",
            "Epoch 6, Batch 630, G Loss: 0.6939941644668579, D Loss: 1.3861171007156372\n",
            "Epoch 6, Batch 631, G Loss: 0.6940139532089233, D Loss: 1.3860926628112793\n",
            "Epoch 6, Batch 632, G Loss: 0.6940355896949768, D Loss: 1.3862509727478027\n",
            "Epoch 6, Batch 633, G Loss: 0.6940343379974365, D Loss: 1.3863149881362915\n",
            "Epoch 6, Batch 634, G Loss: 0.693999171257019, D Loss: 1.3862812519073486\n",
            "Epoch 6, Batch 635, G Loss: 0.6939471364021301, D Loss: 1.3862004280090332\n",
            "Epoch 6, Batch 636, G Loss: 0.6938786506652832, D Loss: 1.386082410812378\n",
            "Epoch 6, Batch 637, G Loss: 0.6938329339027405, D Loss: 1.3860857486724854\n",
            "Epoch 6, Batch 638, G Loss: 0.6938029527664185, D Loss: 1.3860232830047607\n",
            "Epoch 6, Batch 639, G Loss: 0.6937790513038635, D Loss: 1.386159896850586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend.py:5818: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6, Batch 640, G Loss: 0.6937702894210815, D Loss: 1.3860671520233154\n",
            "Epoch 6, Batch 641, G Loss: 0.6937707662582397, D Loss: 1.3860793113708496\n",
            "Epoch 6, Batch 642, G Loss: 0.6937719583511353, D Loss: 1.386102557182312\n",
            "Epoch 6, Batch 643, G Loss: 0.6937882304191589, D Loss: 1.3860995769500732\n",
            "Epoch 6, Batch 644, G Loss: 0.6937997937202454, D Loss: 1.3862370252609253\n",
            "Epoch 6, Batch 645, G Loss: 0.6937988996505737, D Loss: 1.3860812187194824\n",
            "Epoch 6, Batch 646, G Loss: 0.6937894821166992, D Loss: 1.3862738609313965\n",
            "Epoch 6, Batch 647, G Loss: 0.6937816143035889, D Loss: 1.3862683773040771\n",
            "Epoch 6, Batch 648, G Loss: 0.6937346458435059, D Loss: 1.3862526416778564\n",
            "Epoch 6, Batch 649, G Loss: 0.6937130689620972, D Loss: 1.3860185146331787\n",
            "Epoch 6, Batch 650, G Loss: 0.6936851739883423, D Loss: 1.386335849761963\n",
            "Epoch 6, Batch 651, G Loss: 0.6936488151550293, D Loss: 1.3861733675003052\n",
            "Epoch 6, Batch 652, G Loss: 0.6936156749725342, D Loss: 1.3857882022857666\n",
            "Epoch 6, Batch 653, G Loss: 0.6935954093933105, D Loss: 1.3857240676879883\n",
            "Epoch 6, Batch 654, G Loss: 0.6936432123184204, D Loss: 1.3859754800796509\n",
            "Epoch 6, Batch 655, G Loss: 0.6936858296394348, D Loss: 1.3864455223083496\n",
            "Epoch 6, Batch 656, G Loss: 0.6937088966369629, D Loss: 1.3864798545837402\n",
            "Epoch 6, Batch 657, G Loss: 0.6936955451965332, D Loss: 1.386663556098938\n",
            "Epoch 6, Batch 658, G Loss: 0.6936444640159607, D Loss: 1.3866591453552246\n",
            "Epoch 6, Batch 659, G Loss: 0.6935564875602722, D Loss: 1.3865766525268555\n",
            "Epoch 6, Batch 660, G Loss: 0.693450927734375, D Loss: 1.3865398168563843\n",
            "Epoch 6, Batch 661, G Loss: 0.693345308303833, D Loss: 1.3865910768508911\n",
            "Epoch 6, Batch 662, G Loss: 0.6932129859924316, D Loss: 1.3863552808761597\n",
            "Epoch 6, Batch 663, G Loss: 0.6930959224700928, D Loss: 1.386446237564087\n",
            "Epoch 6, Batch 664, G Loss: 0.6929847598075867, D Loss: 1.3862452507019043\n",
            "Epoch 6, Batch 665, G Loss: 0.6928887367248535, D Loss: 1.386195182800293\n",
            "Epoch 6, Batch 666, G Loss: 0.6928056478500366, D Loss: 1.386872410774231\n",
            "Epoch 6, Batch 667, G Loss: 0.6926999688148499, D Loss: 1.3869527578353882\n",
            "Epoch 6, Batch 668, G Loss: 0.6925442814826965, D Loss: 1.3866403102874756\n",
            "Epoch 6, Batch 669, G Loss: 0.6924117803573608, D Loss: 1.3867483139038086\n",
            "Epoch 6, Batch 670, G Loss: 0.6922694444656372, D Loss: 1.386641263961792\n",
            "Epoch 6, Batch 671, G Loss: 0.6921325922012329, D Loss: 1.386444091796875\n",
            "Epoch 6, Batch 672, G Loss: 0.6920209527015686, D Loss: 1.3862248659133911\n",
            "Epoch 6, Batch 673, G Loss: 0.6919529438018799, D Loss: 1.386354923248291\n",
            "Epoch 6, Batch 674, G Loss: 0.6919130682945251, D Loss: 1.3863062858581543\n",
            "Epoch 6, Batch 675, G Loss: 0.6919150948524475, D Loss: 1.3866550922393799\n",
            "Epoch 6, Batch 676, G Loss: 0.6918824911117554, D Loss: 1.3866013288497925\n",
            "Epoch 6, Batch 677, G Loss: 0.6918602585792542, D Loss: 1.3865424394607544\n",
            "Epoch 6, Batch 678, G Loss: 0.6918487548828125, D Loss: 1.3865220546722412\n",
            "Epoch 6, Batch 679, G Loss: 0.69183748960495, D Loss: 1.3865251541137695\n",
            "Epoch 6, Batch 680, G Loss: 0.6918278336524963, D Loss: 1.3865511417388916\n",
            "Epoch 6, Batch 681, G Loss: 0.6918113827705383, D Loss: 1.3864234685897827\n",
            "Epoch 6, Batch 682, G Loss: 0.6918061971664429, D Loss: 1.3865374326705933\n",
            "Epoch 6, Batch 683, G Loss: 0.6918020248413086, D Loss: 1.3865091800689697\n",
            "Epoch 6, Batch 684, G Loss: 0.6917979121208191, D Loss: 1.3864896297454834\n",
            "Epoch 6, Batch 685, G Loss: 0.6917997002601624, D Loss: 1.386427402496338\n",
            "Epoch 6, Batch 686, G Loss: 0.6918085217475891, D Loss: 1.3863853216171265\n",
            "Epoch 6, Batch 687, G Loss: 0.6918349266052246, D Loss: 1.3863776922225952\n",
            "Epoch 6, Batch 688, G Loss: 0.6918778419494629, D Loss: 1.386359691619873\n",
            "Epoch 6, Batch 689, G Loss: 0.6919409036636353, D Loss: 1.3864076137542725\n",
            "Epoch 6, Batch 690, G Loss: 0.6919978260993958, D Loss: 1.386367678642273\n",
            "Epoch 6, Batch 691, G Loss: 0.6920620799064636, D Loss: 1.3863718509674072\n",
            "Epoch 6, Batch 692, G Loss: 0.6921254396438599, D Loss: 1.3863778114318848\n",
            "Epoch 6, Batch 693, G Loss: 0.6921839118003845, D Loss: 1.3863842487335205\n",
            "Epoch 6, Batch 694, G Loss: 0.6922245621681213, D Loss: 1.3863734006881714\n",
            "Epoch 6, Batch 695, G Loss: 0.6922703981399536, D Loss: 1.386368989944458\n",
            "Epoch 6, Batch 696, G Loss: 0.6923069953918457, D Loss: 1.3863565921783447\n",
            "Epoch 6, Batch 697, G Loss: 0.6923392415046692, D Loss: 1.386366605758667\n",
            "Epoch 6, Batch 698, G Loss: 0.6923716068267822, D Loss: 1.3863540887832642\n",
            "Epoch 6, Batch 699, G Loss: 0.6923996210098267, D Loss: 1.3863558769226074\n",
            "Epoch 6, Batch 700, G Loss: 0.6924329400062561, D Loss: 1.3863246440887451\n",
            "Epoch 6, Batch 701, G Loss: 0.6924642324447632, D Loss: 1.3863680362701416\n",
            "Epoch 6, Batch 702, G Loss: 0.6925013065338135, D Loss: 1.3863158226013184\n",
            "Epoch 6, Batch 703, G Loss: 0.6925307512283325, D Loss: 1.3862690925598145\n",
            "Epoch 6, Batch 704, G Loss: 0.6925548911094666, D Loss: 1.3862926959991455\n",
            "Epoch 6, Batch 705, G Loss: 0.692566990852356, D Loss: 1.3862414360046387\n",
            "Epoch 6, Batch 706, G Loss: 0.692571222782135, D Loss: 1.3862464427947998\n",
            "Epoch 6, Batch 707, G Loss: 0.6925660967826843, D Loss: 1.3862268924713135\n",
            "Epoch 6, Batch 708, G Loss: 0.6925488710403442, D Loss: 1.3861911296844482\n",
            "Epoch 6, Batch 709, G Loss: 0.6925279498100281, D Loss: 1.3862695693969727\n",
            "Epoch 6, Batch 710, G Loss: 0.6925094127655029, D Loss: 1.3862805366516113\n",
            "Epoch 6, Batch 711, G Loss: 0.6925010085105896, D Loss: 1.3864152431488037\n",
            "Epoch 6, Batch 712, G Loss: 0.6925109624862671, D Loss: 1.3861806392669678\n",
            "Epoch 6, Batch 713, G Loss: 0.692512035369873, D Loss: 1.3862848281860352\n",
            "Epoch 6, Batch 714, G Loss: 0.6925163865089417, D Loss: 1.3862652778625488\n",
            "Epoch 6, Batch 715, G Loss: 0.6925330758094788, D Loss: 1.3861687183380127\n",
            "Epoch 6, Batch 716, G Loss: 0.692527711391449, D Loss: 1.3862214088439941\n",
            "Epoch 6, Batch 717, G Loss: 0.6925351619720459, D Loss: 1.3861651420593262\n",
            "Epoch 6, Batch 718, G Loss: 0.6925448179244995, D Loss: 1.386427402496338\n",
            "Epoch 6, Batch 719, G Loss: 0.6925679445266724, D Loss: 1.3866117000579834\n",
            "Epoch 6, Batch 720, G Loss: 0.6926150321960449, D Loss: 1.3864634037017822\n",
            "Epoch 6, Batch 721, G Loss: 0.6926727890968323, D Loss: 1.3865209817886353\n",
            "Epoch 6, Batch 722, G Loss: 0.6927410960197449, D Loss: 1.3863834142684937\n",
            "Epoch 6, Batch 723, G Loss: 0.6928102970123291, D Loss: 1.3862876892089844\n",
            "Epoch 6, Batch 724, G Loss: 0.6928784251213074, D Loss: 1.3863284587860107\n",
            "Epoch 6, Batch 725, G Loss: 0.6929413676261902, D Loss: 1.3860417604446411\n",
            "Epoch 6, Batch 726, G Loss: 0.6929901838302612, D Loss: 1.3862297534942627\n",
            "Epoch 6, Batch 727, G Loss: 0.693031370639801, D Loss: 1.3862452507019043\n",
            "Epoch 6, Batch 728, G Loss: 0.6930564641952515, D Loss: 1.3862147331237793\n",
            "Epoch 6, Batch 729, G Loss: 0.6930823922157288, D Loss: 1.3861206769943237\n",
            "Epoch 6, Batch 730, G Loss: 0.6930912733078003, D Loss: 1.3858997821807861\n",
            "Epoch 6, Batch 731, G Loss: 0.6930842399597168, D Loss: 1.3860676288604736\n",
            "Epoch 6, Batch 732, G Loss: 0.6930481195449829, D Loss: 1.3859519958496094\n",
            "Epoch 6, Batch 733, G Loss: 0.6930171251296997, D Loss: 1.3859913349151611\n",
            "Epoch 6, Batch 734, G Loss: 0.6929613947868347, D Loss: 1.386137843132019\n",
            "Epoch 6, Batch 735, G Loss: 0.6929239630699158, D Loss: 1.3861796855926514\n",
            "Epoch 6, Batch 736, G Loss: 0.692872941493988, D Loss: 1.3861567974090576\n",
            "Epoch 6, Batch 737, G Loss: 0.6928200125694275, D Loss: 1.386141300201416\n",
            "Epoch 6, Batch 738, G Loss: 0.6927751898765564, D Loss: 1.3865569829940796\n",
            "Epoch 6, Batch 739, G Loss: 0.6927568316459656, D Loss: 1.386620283126831\n",
            "Epoch 6, Batch 740, G Loss: 0.6927626729011536, D Loss: 1.3866819143295288\n",
            "Epoch 6, Batch 741, G Loss: 0.6927800178527832, D Loss: 1.3860042095184326\n",
            "Epoch 6, Batch 742, G Loss: 0.6927930116653442, D Loss: 1.3860325813293457\n",
            "Epoch 6, Batch 743, G Loss: 0.6928038597106934, D Loss: 1.3861720561981201\n",
            "Epoch 6, Batch 744, G Loss: 0.6928158402442932, D Loss: 1.3856520652770996\n",
            "Epoch 6, Batch 745, G Loss: 0.6928043961524963, D Loss: 1.3855336904525757\n",
            "Epoch 6, Batch 746, G Loss: 0.6927785277366638, D Loss: 1.3858110904693604\n",
            "Epoch 6, Batch 747, G Loss: 0.6927431225776672, D Loss: 1.3860095739364624\n",
            "Epoch 6, Batch 748, G Loss: 0.6926970481872559, D Loss: 1.385965347290039\n",
            "Epoch 6, Batch 749, G Loss: 0.6926575303077698, D Loss: 1.3859983682632446\n",
            "Epoch 6, Batch 750, G Loss: 0.6926279664039612, D Loss: 1.386361837387085\n",
            "Epoch 6, Batch 751, G Loss: 0.6926088929176331, D Loss: 1.385576844215393\n",
            "Epoch 6, Batch 752, G Loss: 0.6925781965255737, D Loss: 1.3860975503921509\n",
            "Epoch 6, Batch 753, G Loss: 0.6925511360168457, D Loss: 1.3856126070022583\n",
            "Epoch 6, Batch 754, G Loss: 0.692523717880249, D Loss: 1.385718584060669\n",
            "Epoch 6, Batch 755, G Loss: 0.6925025582313538, D Loss: 1.3857276439666748\n",
            "Epoch 6, Batch 756, G Loss: 0.6924777626991272, D Loss: 1.385831356048584\n",
            "Epoch 6, Batch 757, G Loss: 0.6924481987953186, D Loss: 1.3858282566070557\n",
            "Epoch 6, Batch 758, G Loss: 0.6924267411231995, D Loss: 1.386117935180664\n",
            "Epoch 6, Batch 759, G Loss: 0.6924408078193665, D Loss: 1.385777473449707\n",
            "Epoch 6, Batch 760, G Loss: 0.6924233436584473, D Loss: 1.386225700378418\n",
            "Epoch 6, Batch 761, G Loss: 0.6924440264701843, D Loss: 1.3860573768615723\n",
            "Epoch 6, Batch 762, G Loss: 0.6924746632575989, D Loss: 1.3859223127365112\n",
            "Epoch 6, Batch 763, G Loss: 0.6924958229064941, D Loss: 1.3860303163528442\n",
            "Epoch 6, Batch 764, G Loss: 0.6925408244132996, D Loss: 1.3862931728363037\n",
            "Epoch 6, Batch 765, G Loss: 0.6925879716873169, D Loss: 1.3874592781066895\n",
            "Epoch 6, Batch 766, G Loss: 0.6926918625831604, D Loss: 1.3872597217559814\n",
            "Epoch 6, Batch 767, G Loss: 0.6928118467330933, D Loss: 1.3869085311889648\n",
            "Epoch 6, Batch 768, G Loss: 0.6929585933685303, D Loss: 1.3863017559051514\n",
            "Epoch 6, Batch 769, G Loss: 0.6930897235870361, D Loss: 1.3861792087554932\n",
            "Epoch 6, Batch 770, G Loss: 0.6932298541069031, D Loss: 1.3864336013793945\n",
            "Epoch 6, Batch 771, G Loss: 0.6933525204658508, D Loss: 1.3866424560546875\n",
            "Epoch 6, Batch 772, G Loss: 0.693485677242279, D Loss: 1.386695146560669\n",
            "Epoch 6, Batch 773, G Loss: 0.693621039390564, D Loss: 1.3863906860351562\n",
            "Epoch 6, Batch 774, G Loss: 0.6937414407730103, D Loss: 1.3866134881973267\n",
            "Epoch 6, Batch 775, G Loss: 0.6938630938529968, D Loss: 1.386673927307129\n",
            "Epoch 6, Batch 776, G Loss: 0.6939674019813538, D Loss: 1.3866814374923706\n",
            "Epoch 6, Batch 777, G Loss: 0.6940871477127075, D Loss: 1.3871142864227295\n",
            "Epoch 6, Batch 778, G Loss: 0.694214940071106, D Loss: 1.386225700378418\n",
            "Epoch 6, Batch 779, G Loss: 0.6943134665489197, D Loss: 1.3869013786315918\n",
            "Epoch 6, Batch 780, G Loss: 0.6944195628166199, D Loss: 1.3869686126708984\n",
            "Epoch 6, Batch 781, G Loss: 0.6945129632949829, D Loss: 1.3872075080871582\n",
            "Epoch 6, Batch 782, G Loss: 0.6946135759353638, D Loss: 1.3869526386260986\n",
            "Epoch 6, Batch 783, G Loss: 0.6947329640388489, D Loss: 1.3867990970611572\n",
            "Epoch 6, Batch 784, G Loss: 0.6948376893997192, D Loss: 1.3866205215454102\n",
            "Epoch 6, Batch 785, G Loss: 0.6949352622032166, D Loss: 1.3867862224578857\n",
            "Epoch 6, Batch 786, G Loss: 0.6950171589851379, D Loss: 1.3867411613464355\n",
            "Epoch 6, Batch 787, G Loss: 0.6950913071632385, D Loss: 1.3866559267044067\n",
            "Epoch 6, Batch 788, G Loss: 0.695158839225769, D Loss: 1.3867310285568237\n",
            "Epoch 6, Batch 789, G Loss: 0.6952224373817444, D Loss: 1.3866499662399292\n",
            "Epoch 6, Batch 790, G Loss: 0.6952765583992004, D Loss: 1.3865017890930176\n",
            "Epoch 6, Batch 791, G Loss: 0.6953079700469971, D Loss: 1.3866629600524902\n",
            "Epoch 6, Batch 792, G Loss: 0.695349931716919, D Loss: 1.3866485357284546\n",
            "Epoch 6, Batch 793, G Loss: 0.6953734755516052, D Loss: 1.3864834308624268\n",
            "Epoch 6, Batch 794, G Loss: 0.6953961253166199, D Loss: 1.3863131999969482\n",
            "Epoch 6, Batch 795, G Loss: 0.6953933238983154, D Loss: 1.3864598274230957\n",
            "Epoch 6, Batch 796, G Loss: 0.6953740119934082, D Loss: 1.3863582611083984\n",
            "Epoch 6, Batch 797, G Loss: 0.695351243019104, D Loss: 1.3864092826843262\n",
            "Epoch 6, Batch 798, G Loss: 0.6953184008598328, D Loss: 1.3864079713821411\n",
            "Epoch 6, Batch 799, G Loss: 0.695295512676239, D Loss: 1.3862853050231934\n",
            "Epoch 6, Batch 800, G Loss: 0.6952553391456604, D Loss: 1.386240005493164\n",
            "Epoch 6, Batch 801, G Loss: 0.6951903700828552, D Loss: 1.386204719543457\n",
            "Epoch 6, Batch 802, G Loss: 0.6950945854187012, D Loss: 1.3862357139587402\n",
            "Epoch 6, Batch 803, G Loss: 0.6949841380119324, D Loss: 1.3862242698669434\n",
            "Epoch 6, Batch 804, G Loss: 0.6948732733726501, D Loss: 1.3862106800079346\n",
            "Epoch 6, Batch 805, G Loss: 0.6947636008262634, D Loss: 1.3861826658248901\n",
            "Epoch 6, Batch 806, G Loss: 0.6946445107460022, D Loss: 1.386185884475708\n",
            "Epoch 6, Batch 807, G Loss: 0.6945493817329407, D Loss: 1.3861536979675293\n",
            "Epoch 6, Batch 808, G Loss: 0.6944525837898254, D Loss: 1.386112928390503\n",
            "Epoch 6, Batch 809, G Loss: 0.6943817138671875, D Loss: 1.3860762119293213\n",
            "Epoch 6, Batch 810, G Loss: 0.6943349838256836, D Loss: 1.3860722780227661\n",
            "Epoch 6, Batch 811, G Loss: 0.6943154335021973, D Loss: 1.3859963417053223\n",
            "Epoch 6, Batch 812, G Loss: 0.6943216919898987, D Loss: 1.3860478401184082\n",
            "Epoch 6, Batch 813, G Loss: 0.6943376660346985, D Loss: 1.3859481811523438\n",
            "Epoch 6, Batch 814, G Loss: 0.6943702101707458, D Loss: 1.3857921361923218\n",
            "Epoch 6, Batch 815, G Loss: 0.6944429874420166, D Loss: 1.3858696222305298\n",
            "Epoch 6, Batch 816, G Loss: 0.6945196390151978, D Loss: 1.3857762813568115\n",
            "Epoch 6, Batch 817, G Loss: 0.6946266293525696, D Loss: 1.3856664896011353\n",
            "Epoch 6, Batch 818, G Loss: 0.6947441101074219, D Loss: 1.3858411312103271\n",
            "Epoch 6, Batch 819, G Loss: 0.6948542594909668, D Loss: 1.3857297897338867\n",
            "Epoch 6, Batch 820, G Loss: 0.6949582695960999, D Loss: 1.386305809020996\n",
            "Epoch 6, Batch 821, G Loss: 0.6950309872627258, D Loss: 1.3859355449676514\n",
            "Epoch 6, Batch 822, G Loss: 0.695056676864624, D Loss: 1.3859268426895142\n",
            "Epoch 6, Batch 823, G Loss: 0.6950649619102478, D Loss: 1.3857297897338867\n",
            "Epoch 6, Batch 824, G Loss: 0.6950686573982239, D Loss: 1.385583519935608\n",
            "Epoch 6, Batch 825, G Loss: 0.6950808167457581, D Loss: 1.385448694229126\n",
            "Epoch 6, Batch 826, G Loss: 0.6950986981391907, D Loss: 1.385776400566101\n",
            "Epoch 6, Batch 827, G Loss: 0.6950945258140564, D Loss: 1.385939598083496\n",
            "Epoch 6, Batch 828, G Loss: 0.695080041885376, D Loss: 1.3860987424850464\n",
            "Epoch 6, Batch 829, G Loss: 0.6950269341468811, D Loss: 1.3862720727920532\n",
            "Epoch 6, Batch 830, G Loss: 0.694950520992279, D Loss: 1.386297583580017\n",
            "Epoch 6, Batch 831, G Loss: 0.6948117613792419, D Loss: 1.3856854438781738\n",
            "Epoch 6, Batch 832, G Loss: 0.694689154624939, D Loss: 1.3860739469528198\n",
            "Epoch 6, Batch 833, G Loss: 0.6945646405220032, D Loss: 1.3858468532562256\n",
            "Epoch 6, Batch 834, G Loss: 0.6943783760070801, D Loss: 1.3856616020202637\n",
            "Epoch 6, Batch 835, G Loss: 0.6942898631095886, D Loss: 1.385898232460022\n",
            "Epoch 6, Batch 836, G Loss: 0.6941655278205872, D Loss: 1.3856768608093262\n",
            "Epoch 6, Batch 837, G Loss: 0.6940363645553589, D Loss: 1.385791540145874\n",
            "Epoch 6, Batch 838, G Loss: 0.6939448118209839, D Loss: 1.3854011297225952\n",
            "Epoch 6, Batch 839, G Loss: 0.6938649415969849, D Loss: 1.385856032371521\n",
            "Epoch 6, Batch 840, G Loss: 0.6938014030456543, D Loss: 1.385746955871582\n",
            "Epoch 6, Batch 841, G Loss: 0.6937286853790283, D Loss: 1.3853533267974854\n",
            "Epoch 6, Batch 842, G Loss: 0.6937015056610107, D Loss: 1.3858442306518555\n",
            "Epoch 6, Batch 843, G Loss: 0.6936792135238647, D Loss: 1.3863893747329712\n",
            "Epoch 6, Batch 844, G Loss: 0.6936224102973938, D Loss: 1.3864778280258179\n",
            "Epoch 6, Batch 845, G Loss: 0.6935195922851562, D Loss: 1.3862591981887817\n",
            "Epoch 6, Batch 846, G Loss: 0.6934590935707092, D Loss: 1.3863401412963867\n",
            "Epoch 6, Batch 847, G Loss: 0.6933450698852539, D Loss: 1.3861339092254639\n",
            "Epoch 6, Batch 848, G Loss: 0.6932499408721924, D Loss: 1.3856751918792725\n",
            "Epoch 6, Batch 849, G Loss: 0.6931853294372559, D Loss: 1.3857121467590332\n",
            "Epoch 6, Batch 850, G Loss: 0.6931297183036804, D Loss: 1.3869669437408447\n",
            "Epoch 6, Batch 851, G Loss: 0.6930403709411621, D Loss: 1.3867045640945435\n",
            "Epoch 6, Batch 852, G Loss: 0.6929754018783569, D Loss: 1.387199878692627\n",
            "Epoch 6, Batch 853, G Loss: 0.6928449273109436, D Loss: 1.3870415687561035\n",
            "Epoch 6, Batch 854, G Loss: 0.6927010416984558, D Loss: 1.3870599269866943\n",
            "Epoch 6, Batch 855, G Loss: 0.6925582885742188, D Loss: 1.387007713317871\n",
            "Epoch 6, Batch 856, G Loss: 0.692363977432251, D Loss: 1.3870494365692139\n",
            "Epoch 6, Batch 857, G Loss: 0.6922065019607544, D Loss: 1.3871121406555176\n",
            "Epoch 6, Batch 858, G Loss: 0.692042887210846, D Loss: 1.3864169120788574\n",
            "Epoch 6, Batch 859, G Loss: 0.6919021606445312, D Loss: 1.3867990970611572\n",
            "Epoch 6, Batch 860, G Loss: 0.6917856931686401, D Loss: 1.3871662616729736\n",
            "Epoch 6, Batch 861, G Loss: 0.6916642189025879, D Loss: 1.3868143558502197\n",
            "Epoch 6, Batch 862, G Loss: 0.6915550827980042, D Loss: 1.3870518207550049\n",
            "Epoch 6, Batch 863, G Loss: 0.6914587020874023, D Loss: 1.386974573135376\n",
            "Epoch 6, Batch 864, G Loss: 0.6913586258888245, D Loss: 1.3870667219161987\n",
            "Epoch 6, Batch 865, G Loss: 0.6912768483161926, D Loss: 1.3866815567016602\n",
            "Epoch 6, Batch 866, G Loss: 0.6912064552307129, D Loss: 1.386711597442627\n",
            "Epoch 6, Batch 867, G Loss: 0.6911638379096985, D Loss: 1.3868234157562256\n",
            "Epoch 6, Batch 868, G Loss: 0.6911370158195496, D Loss: 1.3873744010925293\n",
            "Epoch 6, Batch 869, G Loss: 0.6911082863807678, D Loss: 1.387153148651123\n",
            "Epoch 6, Batch 870, G Loss: 0.6910699605941772, D Loss: 1.3877606391906738\n",
            "Epoch 6, Batch 871, G Loss: 0.6909958720207214, D Loss: 1.3875174522399902\n",
            "Epoch 6, Batch 872, G Loss: 0.6908888816833496, D Loss: 1.3874404430389404\n",
            "Epoch 6, Batch 873, G Loss: 0.6907840371131897, D Loss: 1.386928677558899\n",
            "Epoch 6, Batch 874, G Loss: 0.6907073259353638, D Loss: 1.386825680732727\n",
            "Epoch 6, Batch 875, G Loss: 0.6906455159187317, D Loss: 1.3868215084075928\n",
            "Epoch 6, Batch 876, G Loss: 0.6906174421310425, D Loss: 1.3867552280426025\n",
            "Epoch 6, Batch 877, G Loss: 0.6906029582023621, D Loss: 1.3866890668869019\n",
            "Epoch 6, Batch 878, G Loss: 0.6906177997589111, D Loss: 1.386725664138794\n",
            "Epoch 6, Batch 879, G Loss: 0.6906418800354004, D Loss: 1.3866822719573975\n",
            "Epoch 6, Batch 880, G Loss: 0.6906743049621582, D Loss: 1.386488676071167\n",
            "Epoch 6, Batch 881, G Loss: 0.6907328367233276, D Loss: 1.3864688873291016\n",
            "Epoch 6, Batch 882, G Loss: 0.6908182501792908, D Loss: 1.3864290714263916\n",
            "Epoch 6, Batch 883, G Loss: 0.6909250617027283, D Loss: 1.386481523513794\n",
            "Epoch 6, Batch 884, G Loss: 0.6910415887832642, D Loss: 1.3864604234695435\n",
            "Epoch 6, Batch 885, G Loss: 0.6911569833755493, D Loss: 1.3864307403564453\n",
            "Epoch 6, Batch 886, G Loss: 0.6912760734558105, D Loss: 1.3864120244979858\n",
            "Epoch 6, Batch 887, G Loss: 0.6913958787918091, D Loss: 1.386413335800171\n",
            "Epoch 6, Batch 888, G Loss: 0.6914954781532288, D Loss: 1.3863577842712402\n",
            "Epoch 6, Batch 889, G Loss: 0.6915672421455383, D Loss: 1.38631010055542\n",
            "Epoch 6, Batch 890, G Loss: 0.6916340589523315, D Loss: 1.38632071018219\n",
            "Epoch 6, Batch 891, G Loss: 0.6917040348052979, D Loss: 1.3863211870193481\n",
            "Epoch 6, Batch 892, G Loss: 0.6917809247970581, D Loss: 1.3864010572433472\n",
            "Epoch 6, Batch 893, G Loss: 0.6918762922286987, D Loss: 1.3863121271133423\n",
            "Epoch 6, Batch 894, G Loss: 0.6919764280319214, D Loss: 1.386216402053833\n",
            "Epoch 6, Batch 895, G Loss: 0.6920682787895203, D Loss: 1.3861303329467773\n",
            "Epoch 6, Batch 896, G Loss: 0.6921600699424744, D Loss: 1.38612699508667\n",
            "Epoch 6, Batch 897, G Loss: 0.6922186017036438, D Loss: 1.3860883712768555\n",
            "Epoch 6, Batch 898, G Loss: 0.692277193069458, D Loss: 1.38615882396698\n",
            "Epoch 6, Batch 899, G Loss: 0.6923253536224365, D Loss: 1.385936975479126\n",
            "Epoch 6, Batch 900, G Loss: 0.6923537254333496, D Loss: 1.385972023010254\n",
            "Epoch 6, Batch 901, G Loss: 0.6923617720603943, D Loss: 1.3861382007598877\n",
            "Epoch 6, Batch 902, G Loss: 0.6923823356628418, D Loss: 1.386311650276184\n",
            "Epoch 6, Batch 903, G Loss: 0.6924169063568115, D Loss: 1.3863211870193481\n",
            "Epoch 6, Batch 904, G Loss: 0.6924675703048706, D Loss: 1.3863390684127808\n",
            "Epoch 6, Batch 905, G Loss: 0.6924974918365479, D Loss: 1.3860931396484375\n",
            "Epoch 6, Batch 906, G Loss: 0.6925475597381592, D Loss: 1.3862664699554443\n",
            "Epoch 6, Batch 907, G Loss: 0.6926003098487854, D Loss: 1.385901927947998\n",
            "Epoch 6, Batch 908, G Loss: 0.6926339864730835, D Loss: 1.3863987922668457\n",
            "Epoch 6, Batch 909, G Loss: 0.692688524723053, D Loss: 1.3864281177520752\n",
            "Epoch 6, Batch 910, G Loss: 0.6927335858345032, D Loss: 1.3858925104141235\n",
            "Epoch 6, Batch 911, G Loss: 0.6927835941314697, D Loss: 1.3857085704803467\n",
            "Epoch 6, Batch 912, G Loss: 0.6927998661994934, D Loss: 1.3858447074890137\n",
            "Epoch 6, Batch 913, G Loss: 0.6928189992904663, D Loss: 1.3854670524597168\n",
            "Epoch 6, Batch 914, G Loss: 0.6927935481071472, D Loss: 1.3855276107788086\n",
            "Epoch 6, Batch 915, G Loss: 0.6928007006645203, D Loss: 1.3862533569335938\n",
            "Epoch 6, Batch 916, G Loss: 0.6927552223205566, D Loss: 1.3867030143737793\n",
            "Epoch 6, Batch 917, G Loss: 0.6927563548088074, D Loss: 1.386763334274292\n",
            "Epoch 6, Batch 918, G Loss: 0.6927897930145264, D Loss: 1.3868460655212402\n",
            "Epoch 6, Batch 919, G Loss: 0.6928510069847107, D Loss: 1.38646399974823\n",
            "Epoch 6, Batch 920, G Loss: 0.6929125189781189, D Loss: 1.385530710220337\n",
            "Epoch 6, Batch 921, G Loss: 0.6929483413696289, D Loss: 1.3858705759048462\n",
            "Epoch 6, Batch 922, G Loss: 0.6929653882980347, D Loss: 1.3858469724655151\n",
            "Epoch 6, Batch 923, G Loss: 0.6929682493209839, D Loss: 1.3861157894134521\n",
            "Epoch 6, Batch 924, G Loss: 0.6929847598075867, D Loss: 1.386155128479004\n",
            "Epoch 6, Batch 925, G Loss: 0.692990779876709, D Loss: 1.385988712310791\n",
            "Epoch 6, Batch 926, G Loss: 0.6930102705955505, D Loss: 1.3858405351638794\n",
            "Epoch 6, Batch 927, G Loss: 0.6930025219917297, D Loss: 1.3853163719177246\n",
            "Epoch 6, Batch 928, G Loss: 0.6929742693901062, D Loss: 1.3838694095611572\n",
            "Epoch 6, Batch 929, G Loss: 0.6928924918174744, D Loss: 1.3827626705169678\n",
            "Epoch 6, Batch 930, G Loss: 0.692744791507721, D Loss: 1.385057806968689\n",
            "Epoch 6, Batch 931, G Loss: 0.6925738453865051, D Loss: 1.3865814208984375\n",
            "Epoch 6, Batch 932, G Loss: 0.6924623250961304, D Loss: 1.3860719203948975\n",
            "Epoch 6, Batch 933, G Loss: 0.692374587059021, D Loss: 1.385563850402832\n",
            "Epoch 6, Batch 934, G Loss: 0.6922744512557983, D Loss: 1.3860589265823364\n",
            "Epoch 6, Batch 935, G Loss: 0.6922005414962769, D Loss: 1.385521411895752\n",
            "Epoch 6, Batch 936, G Loss: 0.6921463012695312, D Loss: 1.38525390625\n",
            "Epoch 6, Batch 937, G Loss: 0.6921119093894958, D Loss: 1.3869435787200928\n",
            "Epoch 6, Batch 938, G Loss: 0.6921005845069885, D Loss: 1.3868236541748047\n",
            "Epoch 7, Batch 1, G Loss: 0.6921226382255554, D Loss: 1.3862669467926025\n",
            "Epoch 7, Batch 2, G Loss: 0.6921665668487549, D Loss: 1.3862242698669434\n",
            "Epoch 7, Batch 3, G Loss: 0.6922134757041931, D Loss: 1.3864645957946777\n",
            "Epoch 7, Batch 4, G Loss: 0.6922900676727295, D Loss: 1.3859820365905762\n",
            "Epoch 7, Batch 5, G Loss: 0.6923714280128479, D Loss: 1.3866558074951172\n",
            "Epoch 7, Batch 6, G Loss: 0.6924779415130615, D Loss: 1.3864715099334717\n",
            "Epoch 7, Batch 7, G Loss: 0.6926038861274719, D Loss: 1.3866850137710571\n",
            "Epoch 7, Batch 8, G Loss: 0.6927291750907898, D Loss: 1.3866726160049438\n",
            "Epoch 7, Batch 9, G Loss: 0.6928472518920898, D Loss: 1.386412501335144\n",
            "Epoch 7, Batch 10, G Loss: 0.6929641366004944, D Loss: 1.3869682550430298\n",
            "Epoch 7, Batch 11, G Loss: 0.6931159496307373, D Loss: 1.3864004611968994\n",
            "Epoch 7, Batch 12, G Loss: 0.6932403445243835, D Loss: 1.385789155960083\n",
            "Epoch 7, Batch 13, G Loss: 0.6933528184890747, D Loss: 1.386300802230835\n",
            "Epoch 7, Batch 14, G Loss: 0.6934735774993896, D Loss: 1.3861913681030273\n",
            "Epoch 7, Batch 15, G Loss: 0.6935788989067078, D Loss: 1.3864566087722778\n",
            "Epoch 7, Batch 16, G Loss: 0.6936787366867065, D Loss: 1.3868904113769531\n",
            "Epoch 7, Batch 17, G Loss: 0.6937800049781799, D Loss: 1.387412667274475\n",
            "Epoch 7, Batch 18, G Loss: 0.6938693523406982, D Loss: 1.3871915340423584\n",
            "Epoch 7, Batch 19, G Loss: 0.693972110748291, D Loss: 1.3862273693084717\n",
            "Epoch 7, Batch 20, G Loss: 0.6940528750419617, D Loss: 1.385279893875122\n",
            "Epoch 7, Batch 21, G Loss: 0.6941194534301758, D Loss: 1.385685682296753\n",
            "Epoch 7, Batch 22, G Loss: 0.6941279768943787, D Loss: 1.3850393295288086\n",
            "Epoch 7, Batch 23, G Loss: 0.6941508650779724, D Loss: 1.3868322372436523\n",
            "Epoch 7, Batch 24, G Loss: 0.6941277384757996, D Loss: 1.3870325088500977\n",
            "Epoch 7, Batch 25, G Loss: 0.6941136121749878, D Loss: 1.3870124816894531\n",
            "Epoch 7, Batch 26, G Loss: 0.694135308265686, D Loss: 1.387215495109558\n",
            "Epoch 7, Batch 27, G Loss: 0.6941455006599426, D Loss: 1.3872270584106445\n",
            "Epoch 7, Batch 28, G Loss: 0.6942115426063538, D Loss: 1.3870573043823242\n",
            "Epoch 7, Batch 29, G Loss: 0.6942205429077148, D Loss: 1.3872075080871582\n",
            "Epoch 7, Batch 30, G Loss: 0.6942591667175293, D Loss: 1.38673734664917\n",
            "Epoch 7, Batch 31, G Loss: 0.6943337321281433, D Loss: 1.386275291442871\n",
            "Epoch 7, Batch 32, G Loss: 0.6943445801734924, D Loss: 1.3861172199249268\n",
            "Epoch 7, Batch 33, G Loss: 0.6943601369857788, D Loss: 1.3865498304367065\n",
            "Epoch 7, Batch 34, G Loss: 0.6943624019622803, D Loss: 1.3868169784545898\n",
            "Epoch 7, Batch 35, G Loss: 0.6943740844726562, D Loss: 1.387007713317871\n",
            "Epoch 7, Batch 36, G Loss: 0.6943906545639038, D Loss: 1.386521816253662\n",
            "Epoch 7, Batch 37, G Loss: 0.694408655166626, D Loss: 1.3869367837905884\n",
            "Epoch 7, Batch 38, G Loss: 0.6944168210029602, D Loss: 1.3869948387145996\n",
            "Epoch 7, Batch 39, G Loss: 0.6944226026535034, D Loss: 1.3869338035583496\n",
            "Epoch 7, Batch 40, G Loss: 0.6944438219070435, D Loss: 1.3866515159606934\n",
            "Epoch 7, Batch 41, G Loss: 0.6944661140441895, D Loss: 1.3867954015731812\n",
            "Epoch 7, Batch 42, G Loss: 0.6945023536682129, D Loss: 1.3865272998809814\n",
            "Epoch 7, Batch 43, G Loss: 0.6945278644561768, D Loss: 1.3869850635528564\n",
            "Epoch 7, Batch 44, G Loss: 0.6945593953132629, D Loss: 1.386749505996704\n",
            "Epoch 7, Batch 45, G Loss: 0.694585919380188, D Loss: 1.3865118026733398\n",
            "Epoch 7, Batch 46, G Loss: 0.6946032643318176, D Loss: 1.3866856098175049\n",
            "Epoch 7, Batch 47, G Loss: 0.6946314573287964, D Loss: 1.387089490890503\n",
            "Epoch 7, Batch 48, G Loss: 0.6946718096733093, D Loss: 1.3867666721343994\n",
            "Epoch 7, Batch 49, G Loss: 0.6947239637374878, D Loss: 1.3869458436965942\n",
            "Epoch 7, Batch 50, G Loss: 0.6947962641716003, D Loss: 1.3868722915649414\n",
            "Epoch 7, Batch 51, G Loss: 0.69487464427948, D Loss: 1.3868153095245361\n",
            "Epoch 7, Batch 52, G Loss: 0.6949684023857117, D Loss: 1.3868160247802734\n",
            "Epoch 7, Batch 53, G Loss: 0.6950650811195374, D Loss: 1.3865466117858887\n",
            "Epoch 7, Batch 54, G Loss: 0.6951563358306885, D Loss: 1.386692762374878\n",
            "Epoch 7, Batch 55, G Loss: 0.6952582597732544, D Loss: 1.3866183757781982\n",
            "Epoch 7, Batch 56, G Loss: 0.6953684091567993, D Loss: 1.386413812637329\n",
            "Epoch 7, Batch 57, G Loss: 0.695470929145813, D Loss: 1.3862836360931396\n",
            "Epoch 7, Batch 58, G Loss: 0.6955440044403076, D Loss: 1.3861888647079468\n",
            "Epoch 7, Batch 59, G Loss: 0.6955761313438416, D Loss: 1.3862786293029785\n",
            "Epoch 7, Batch 60, G Loss: 0.6956029534339905, D Loss: 1.3862248659133911\n",
            "Epoch 7, Batch 61, G Loss: 0.6956260800361633, D Loss: 1.386194109916687\n",
            "Epoch 7, Batch 62, G Loss: 0.6956517696380615, D Loss: 1.3861298561096191\n",
            "Epoch 7, Batch 63, G Loss: 0.6956721544265747, D Loss: 1.3860788345336914\n",
            "Epoch 7, Batch 64, G Loss: 0.6956865191459656, D Loss: 1.3860893249511719\n",
            "Epoch 7, Batch 65, G Loss: 0.6956870555877686, D Loss: 1.3859896659851074\n",
            "Epoch 7, Batch 66, G Loss: 0.6956804394721985, D Loss: 1.3859143257141113\n",
            "Epoch 7, Batch 67, G Loss: 0.6956749558448792, D Loss: 1.3859676122665405\n",
            "Epoch 7, Batch 68, G Loss: 0.6956596970558167, D Loss: 1.385838508605957\n",
            "Epoch 7, Batch 69, G Loss: 0.6956391334533691, D Loss: 1.385676622390747\n",
            "Epoch 7, Batch 70, G Loss: 0.6956383585929871, D Loss: 1.385750412940979\n",
            "Epoch 7, Batch 71, G Loss: 0.6956327557563782, D Loss: 1.386099100112915\n",
            "Epoch 7, Batch 72, G Loss: 0.695593535900116, D Loss: 1.3857884407043457\n",
            "Epoch 7, Batch 73, G Loss: 0.6955466866493225, D Loss: 1.3860063552856445\n",
            "Epoch 7, Batch 74, G Loss: 0.6954602599143982, D Loss: 1.3862223625183105\n",
            "Epoch 7, Batch 75, G Loss: 0.695343017578125, D Loss: 1.3857779502868652\n",
            "Epoch 7, Batch 76, G Loss: 0.6952306628227234, D Loss: 1.3859291076660156\n",
            "Epoch 7, Batch 77, G Loss: 0.6951029300689697, D Loss: 1.3858753442764282\n",
            "Epoch 7, Batch 78, G Loss: 0.6949717998504639, D Loss: 1.3854036331176758\n",
            "Epoch 7, Batch 79, G Loss: 0.6948660016059875, D Loss: 1.3856638669967651\n",
            "Epoch 7, Batch 80, G Loss: 0.6947639584541321, D Loss: 1.385890245437622\n",
            "Epoch 7, Batch 81, G Loss: 0.6946477890014648, D Loss: 1.3858535289764404\n",
            "Epoch 7, Batch 82, G Loss: 0.6945397257804871, D Loss: 1.3857940435409546\n",
            "Epoch 7, Batch 83, G Loss: 0.6943967938423157, D Loss: 1.38570237159729\n",
            "Epoch 7, Batch 84, G Loss: 0.6942958235740662, D Loss: 1.3862700462341309\n",
            "Epoch 7, Batch 85, G Loss: 0.6941470503807068, D Loss: 1.38621187210083\n",
            "Epoch 7, Batch 86, G Loss: 0.6939725279808044, D Loss: 1.3862346410751343\n",
            "Epoch 7, Batch 87, G Loss: 0.6938214898109436, D Loss: 1.3859264850616455\n",
            "Epoch 7, Batch 88, G Loss: 0.6936644911766052, D Loss: 1.385878324508667\n",
            "Epoch 7, Batch 89, G Loss: 0.6935551166534424, D Loss: 1.385725975036621\n",
            "Epoch 7, Batch 90, G Loss: 0.6934619545936584, D Loss: 1.385927438735962\n",
            "Epoch 7, Batch 91, G Loss: 0.6933822631835938, D Loss: 1.385506510734558\n",
            "Epoch 7, Batch 92, G Loss: 0.6933031678199768, D Loss: 1.3854572772979736\n",
            "Epoch 7, Batch 93, G Loss: 0.693265438079834, D Loss: 1.385645866394043\n",
            "Epoch 7, Batch 94, G Loss: 0.6932656168937683, D Loss: 1.3868975639343262\n",
            "Epoch 7, Batch 95, G Loss: 0.6932096481323242, D Loss: 1.3869425058364868\n",
            "Epoch 7, Batch 96, G Loss: 0.6931242942810059, D Loss: 1.3869322538375854\n",
            "Epoch 7, Batch 97, G Loss: 0.6929847002029419, D Loss: 1.3867216110229492\n",
            "Epoch 7, Batch 98, G Loss: 0.6928367614746094, D Loss: 1.3862583637237549\n",
            "Epoch 7, Batch 99, G Loss: 0.6926920413970947, D Loss: 1.3864097595214844\n",
            "Epoch 7, Batch 100, G Loss: 0.6925644278526306, D Loss: 1.3863494396209717\n",
            "Epoch 7, Batch 101, G Loss: 0.6924552917480469, D Loss: 1.3861818313598633\n",
            "Epoch 7, Batch 102, G Loss: 0.6923484206199646, D Loss: 1.3858625888824463\n",
            "Epoch 7, Batch 103, G Loss: 0.6922591924667358, D Loss: 1.3855786323547363\n",
            "Epoch 7, Batch 104, G Loss: 0.6922494769096375, D Loss: 1.3861857652664185\n",
            "Epoch 7, Batch 105, G Loss: 0.6922264695167542, D Loss: 1.385071039199829\n",
            "Epoch 7, Batch 106, G Loss: 0.6922749280929565, D Loss: 1.3852510452270508\n",
            "Epoch 7, Batch 107, G Loss: 0.6923708319664001, D Loss: 1.385011911392212\n",
            "Epoch 7, Batch 108, G Loss: 0.6925415396690369, D Loss: 1.3863976001739502\n",
            "Epoch 7, Batch 109, G Loss: 0.6926725506782532, D Loss: 1.3866840600967407\n",
            "Epoch 7, Batch 110, G Loss: 0.6927782893180847, D Loss: 1.386189341545105\n",
            "Epoch 7, Batch 111, G Loss: 0.6928779482841492, D Loss: 1.3856284618377686\n",
            "Epoch 7, Batch 112, G Loss: 0.6929990649223328, D Loss: 1.3856730461120605\n",
            "Epoch 7, Batch 113, G Loss: 0.6930985450744629, D Loss: 1.3855414390563965\n",
            "Epoch 7, Batch 114, G Loss: 0.6932332515716553, D Loss: 1.385777473449707\n",
            "Epoch 7, Batch 115, G Loss: 0.6933736801147461, D Loss: 1.385028600692749\n",
            "Epoch 7, Batch 116, G Loss: 0.6935409307479858, D Loss: 1.3854057788848877\n",
            "Epoch 7, Batch 117, G Loss: 0.6936885118484497, D Loss: 1.3858730792999268\n",
            "Epoch 7, Batch 118, G Loss: 0.6938300132751465, D Loss: 1.3857471942901611\n",
            "Epoch 7, Batch 119, G Loss: 0.6939566731452942, D Loss: 1.3858380317687988\n",
            "Epoch 7, Batch 120, G Loss: 0.694096565246582, D Loss: 1.386423110961914\n",
            "Epoch 7, Batch 121, G Loss: 0.694151759147644, D Loss: 1.3863507509231567\n",
            "Epoch 7, Batch 122, G Loss: 0.6942006349563599, D Loss: 1.3864378929138184\n",
            "Epoch 7, Batch 123, G Loss: 0.6941986083984375, D Loss: 1.3869011402130127\n",
            "Epoch 7, Batch 124, G Loss: 0.6941587328910828, D Loss: 1.3876457214355469\n",
            "Epoch 7, Batch 125, G Loss: 0.6940460205078125, D Loss: 1.3874356746673584\n",
            "Epoch 7, Batch 126, G Loss: 0.6938837170600891, D Loss: 1.3874127864837646\n",
            "Epoch 7, Batch 127, G Loss: 0.6936891674995422, D Loss: 1.387077808380127\n",
            "Epoch 7, Batch 128, G Loss: 0.6934629678726196, D Loss: 1.38603937625885\n",
            "Epoch 7, Batch 129, G Loss: 0.6932728886604309, D Loss: 1.3862333297729492\n",
            "Epoch 7, Batch 130, G Loss: 0.6930935978889465, D Loss: 1.387291431427002\n",
            "Epoch 7, Batch 131, G Loss: 0.6928667426109314, D Loss: 1.387024998664856\n",
            "Epoch 7, Batch 132, G Loss: 0.692662239074707, D Loss: 1.3868117332458496\n",
            "Epoch 7, Batch 133, G Loss: 0.6924700140953064, D Loss: 1.388465404510498\n",
            "Epoch 7, Batch 134, G Loss: 0.6922272443771362, D Loss: 1.3879024982452393\n",
            "Epoch 7, Batch 135, G Loss: 0.6919560432434082, D Loss: 1.3876514434814453\n",
            "Epoch 7, Batch 136, G Loss: 0.691662073135376, D Loss: 1.387817144393921\n",
            "Epoch 7, Batch 137, G Loss: 0.6913701295852661, D Loss: 1.3870773315429688\n",
            "Epoch 7, Batch 138, G Loss: 0.6910966634750366, D Loss: 1.386933445930481\n",
            "Epoch 7, Batch 139, G Loss: 0.6908645629882812, D Loss: 1.3866069316864014\n",
            "Epoch 7, Batch 140, G Loss: 0.6906892657279968, D Loss: 1.3869922161102295\n",
            "Epoch 7, Batch 141, G Loss: 0.6905431747436523, D Loss: 1.3867034912109375\n",
            "Epoch 7, Batch 142, G Loss: 0.6904443502426147, D Loss: 1.3868162631988525\n",
            "Epoch 7, Batch 143, G Loss: 0.6903805136680603, D Loss: 1.3863873481750488\n",
            "Epoch 7, Batch 144, G Loss: 0.6903554201126099, D Loss: 1.3866069316864014\n",
            "Epoch 7, Batch 145, G Loss: 0.6903777122497559, D Loss: 1.3862636089324951\n",
            "Epoch 7, Batch 146, G Loss: 0.6904578804969788, D Loss: 1.3868281841278076\n",
            "Epoch 7, Batch 147, G Loss: 0.6905455589294434, D Loss: 1.3868627548217773\n",
            "Epoch 7, Batch 148, G Loss: 0.6906325817108154, D Loss: 1.386840581893921\n",
            "Epoch 7, Batch 149, G Loss: 0.6907168626785278, D Loss: 1.3869503736495972\n",
            "Epoch 7, Batch 150, G Loss: 0.6907961368560791, D Loss: 1.3865129947662354\n",
            "Epoch 7, Batch 151, G Loss: 0.6908994317054749, D Loss: 1.3866372108459473\n",
            "Epoch 7, Batch 152, G Loss: 0.6910117864608765, D Loss: 1.3867583274841309\n",
            "Epoch 7, Batch 153, G Loss: 0.6911188960075378, D Loss: 1.3866503238677979\n",
            "Epoch 7, Batch 154, G Loss: 0.6912187337875366, D Loss: 1.3865234851837158\n",
            "Epoch 7, Batch 155, G Loss: 0.6913208961486816, D Loss: 1.3865175247192383\n",
            "Epoch 7, Batch 156, G Loss: 0.6914269328117371, D Loss: 1.3864691257476807\n",
            "Epoch 7, Batch 157, G Loss: 0.6915306448936462, D Loss: 1.3865501880645752\n",
            "Epoch 7, Batch 158, G Loss: 0.6916241645812988, D Loss: 1.3864434957504272\n",
            "Epoch 7, Batch 159, G Loss: 0.6917162537574768, D Loss: 1.3864097595214844\n",
            "Epoch 7, Batch 160, G Loss: 0.6918177604675293, D Loss: 1.386454701423645\n",
            "Epoch 7, Batch 161, G Loss: 0.6918702125549316, D Loss: 1.386371374130249\n",
            "Epoch 7, Batch 162, G Loss: 0.691865086555481, D Loss: 1.3862855434417725\n",
            "Epoch 7, Batch 163, G Loss: 0.6918461918830872, D Loss: 1.3862125873565674\n",
            "Epoch 7, Batch 164, G Loss: 0.6918205618858337, D Loss: 1.386108636856079\n",
            "Epoch 7, Batch 165, G Loss: 0.6917691826820374, D Loss: 1.3861274719238281\n",
            "Epoch 7, Batch 166, G Loss: 0.6917381882667542, D Loss: 1.386141300201416\n",
            "Epoch 7, Batch 167, G Loss: 0.6916961669921875, D Loss: 1.3863275051116943\n",
            "Epoch 7, Batch 168, G Loss: 0.6917020082473755, D Loss: 1.386143445968628\n",
            "Epoch 7, Batch 169, G Loss: 0.6917070746421814, D Loss: 1.3859697580337524\n",
            "Epoch 7, Batch 170, G Loss: 0.6917116045951843, D Loss: 1.3859164714813232\n",
            "Epoch 7, Batch 171, G Loss: 0.6917295455932617, D Loss: 1.3859455585479736\n",
            "Epoch 7, Batch 172, G Loss: 0.691737949848175, D Loss: 1.3856762647628784\n",
            "Epoch 7, Batch 173, G Loss: 0.691744327545166, D Loss: 1.3855520486831665\n",
            "Epoch 7, Batch 174, G Loss: 0.6917672157287598, D Loss: 1.385946273803711\n",
            "Epoch 7, Batch 175, G Loss: 0.6917843222618103, D Loss: 1.3857909440994263\n",
            "Epoch 7, Batch 176, G Loss: 0.6918022036552429, D Loss: 1.3858572244644165\n",
            "Epoch 7, Batch 177, G Loss: 0.6918428540229797, D Loss: 1.3860125541687012\n",
            "Epoch 7, Batch 178, G Loss: 0.691892147064209, D Loss: 1.3857512474060059\n",
            "Epoch 7, Batch 179, G Loss: 0.6919481158256531, D Loss: 1.385899305343628\n",
            "Epoch 7, Batch 180, G Loss: 0.6919779181480408, D Loss: 1.3859137296676636\n",
            "Epoch 7, Batch 181, G Loss: 0.6920413374900818, D Loss: 1.3859425783157349\n",
            "Epoch 7, Batch 182, G Loss: 0.692110538482666, D Loss: 1.3859083652496338\n",
            "Epoch 7, Batch 183, G Loss: 0.6922070980072021, D Loss: 1.3858036994934082\n",
            "Epoch 7, Batch 184, G Loss: 0.6922782063484192, D Loss: 1.3860182762145996\n",
            "Epoch 7, Batch 185, G Loss: 0.6923621296882629, D Loss: 1.3858466148376465\n",
            "Epoch 7, Batch 186, G Loss: 0.6924481987953186, D Loss: 1.3858987092971802\n",
            "Epoch 7, Batch 187, G Loss: 0.6925341486930847, D Loss: 1.3860852718353271\n",
            "Epoch 7, Batch 188, G Loss: 0.6926285028457642, D Loss: 1.385003924369812\n",
            "Epoch 7, Batch 189, G Loss: 0.692700982093811, D Loss: 1.385519027709961\n",
            "Epoch 7, Batch 190, G Loss: 0.692772388458252, D Loss: 1.3855350017547607\n",
            "Epoch 7, Batch 191, G Loss: 0.6928070187568665, D Loss: 1.3853294849395752\n",
            "Epoch 7, Batch 192, G Loss: 0.692878246307373, D Loss: 1.3851697444915771\n",
            "Epoch 7, Batch 193, G Loss: 0.6928861737251282, D Loss: 1.3855342864990234\n",
            "Epoch 7, Batch 194, G Loss: 0.6929318308830261, D Loss: 1.385470986366272\n",
            "Epoch 7, Batch 195, G Loss: 0.6929243206977844, D Loss: 1.3851449489593506\n",
            "Epoch 7, Batch 196, G Loss: 0.6929417252540588, D Loss: 1.385339617729187\n",
            "Epoch 7, Batch 197, G Loss: 0.6929497718811035, D Loss: 1.385257601737976\n",
            "Epoch 7, Batch 198, G Loss: 0.6929544806480408, D Loss: 1.3852384090423584\n",
            "Epoch 7, Batch 199, G Loss: 0.6929287314414978, D Loss: 1.3856685161590576\n",
            "Epoch 7, Batch 200, G Loss: 0.6929388046264648, D Loss: 1.3863725662231445\n",
            "Epoch 7, Batch 201, G Loss: 0.6929506659507751, D Loss: 1.386936902999878\n",
            "Epoch 7, Batch 202, G Loss: 0.6929807066917419, D Loss: 1.38572359085083\n",
            "Epoch 7, Batch 203, G Loss: 0.6929966807365417, D Loss: 1.3846931457519531\n",
            "Epoch 7, Batch 204, G Loss: 0.6930040121078491, D Loss: 1.3848466873168945\n",
            "Epoch 7, Batch 205, G Loss: 0.6929908394813538, D Loss: 1.3840503692626953\n",
            "Epoch 7, Batch 206, G Loss: 0.692992627620697, D Loss: 1.3854484558105469\n",
            "Epoch 7, Batch 207, G Loss: 0.6929581165313721, D Loss: 1.3851996660232544\n",
            "Epoch 7, Batch 208, G Loss: 0.6929572224617004, D Loss: 1.3855347633361816\n",
            "Epoch 7, Batch 209, G Loss: 0.6929519176483154, D Loss: 1.3855068683624268\n",
            "Epoch 7, Batch 210, G Loss: 0.6929291486740112, D Loss: 1.3858144283294678\n",
            "Epoch 7, Batch 211, G Loss: 0.6929036974906921, D Loss: 1.3854186534881592\n",
            "Epoch 7, Batch 212, G Loss: 0.6929371356964111, D Loss: 1.3847250938415527\n",
            "Epoch 7, Batch 213, G Loss: 0.6929314136505127, D Loss: 1.3856176137924194\n",
            "Epoch 7, Batch 214, G Loss: 0.6929259300231934, D Loss: 1.387143850326538\n",
            "Epoch 7, Batch 215, G Loss: 0.6929344534873962, D Loss: 1.3878085613250732\n",
            "Epoch 7, Batch 216, G Loss: 0.6929817199707031, D Loss: 1.387115240097046\n",
            "Epoch 7, Batch 217, G Loss: 0.6930674910545349, D Loss: 1.3864967823028564\n",
            "Epoch 7, Batch 218, G Loss: 0.6931366324424744, D Loss: 1.3865362405776978\n",
            "Epoch 7, Batch 219, G Loss: 0.6932029128074646, D Loss: 1.3872305154800415\n",
            "Epoch 7, Batch 220, G Loss: 0.6932790875434875, D Loss: 1.3852529525756836\n",
            "Epoch 7, Batch 221, G Loss: 0.6933654546737671, D Loss: 1.385434865951538\n",
            "Epoch 7, Batch 222, G Loss: 0.6934471130371094, D Loss: 1.385099172592163\n",
            "Epoch 7, Batch 223, G Loss: 0.6935123801231384, D Loss: 1.3857626914978027\n",
            "Epoch 7, Batch 224, G Loss: 0.6935452818870544, D Loss: 1.385852575302124\n",
            "Epoch 7, Batch 225, G Loss: 0.6935787796974182, D Loss: 1.3863637447357178\n",
            "Epoch 7, Batch 226, G Loss: 0.6936256885528564, D Loss: 1.3871099948883057\n",
            "Epoch 7, Batch 227, G Loss: 0.6936851143836975, D Loss: 1.3875545263290405\n",
            "Epoch 7, Batch 228, G Loss: 0.6937578916549683, D Loss: 1.3878018856048584\n",
            "Epoch 7, Batch 229, G Loss: 0.6938120722770691, D Loss: 1.3873679637908936\n",
            "Epoch 7, Batch 230, G Loss: 0.6938688158988953, D Loss: 1.3866956233978271\n",
            "Epoch 7, Batch 231, G Loss: 0.693983256816864, D Loss: 1.3873724937438965\n",
            "Epoch 7, Batch 232, G Loss: 0.6940566301345825, D Loss: 1.387218952178955\n",
            "Epoch 7, Batch 233, G Loss: 0.6941421031951904, D Loss: 1.3873733282089233\n",
            "Epoch 7, Batch 234, G Loss: 0.694227933883667, D Loss: 1.3874292373657227\n",
            "Epoch 7, Batch 235, G Loss: 0.6942886710166931, D Loss: 1.3881428241729736\n",
            "Epoch 7, Batch 236, G Loss: 0.6944074630737305, D Loss: 1.387249231338501\n",
            "Epoch 7, Batch 237, G Loss: 0.6945310235023499, D Loss: 1.3877613544464111\n",
            "Epoch 7, Batch 238, G Loss: 0.6945992112159729, D Loss: 1.388091802597046\n",
            "Epoch 7, Batch 239, G Loss: 0.694697916507721, D Loss: 1.3877382278442383\n",
            "Epoch 7, Batch 240, G Loss: 0.6948031187057495, D Loss: 1.3879485130310059\n",
            "Epoch 7, Batch 241, G Loss: 0.6949103474617004, D Loss: 1.3880505561828613\n",
            "Epoch 7, Batch 242, G Loss: 0.6950470805168152, D Loss: 1.3879218101501465\n",
            "Epoch 7, Batch 243, G Loss: 0.6951451301574707, D Loss: 1.387484073638916\n",
            "Epoch 7, Batch 244, G Loss: 0.6952544450759888, D Loss: 1.3875744342803955\n",
            "Epoch 7, Batch 245, G Loss: 0.6953473687171936, D Loss: 1.3873448371887207\n",
            "Epoch 7, Batch 246, G Loss: 0.6954360008239746, D Loss: 1.3873869180679321\n",
            "Epoch 7, Batch 247, G Loss: 0.6955369710922241, D Loss: 1.3872624635696411\n",
            "Epoch 7, Batch 248, G Loss: 0.6956039667129517, D Loss: 1.3874709606170654\n",
            "Epoch 7, Batch 249, G Loss: 0.6956592202186584, D Loss: 1.3868496417999268\n",
            "Epoch 7, Batch 250, G Loss: 0.6956902146339417, D Loss: 1.3872783184051514\n",
            "Epoch 7, Batch 251, G Loss: 0.6957545280456543, D Loss: 1.386871576309204\n",
            "Epoch 7, Batch 252, G Loss: 0.6957640647888184, D Loss: 1.3866066932678223\n",
            "Epoch 7, Batch 253, G Loss: 0.6957732439041138, D Loss: 1.3866431713104248\n",
            "Epoch 7, Batch 254, G Loss: 0.6957497000694275, D Loss: 1.3867602348327637\n",
            "Epoch 7, Batch 255, G Loss: 0.695692241191864, D Loss: 1.3869010210037231\n",
            "Epoch 7, Batch 256, G Loss: 0.6956748962402344, D Loss: 1.3868088722229004\n",
            "Epoch 7, Batch 257, G Loss: 0.6956257224082947, D Loss: 1.3868438005447388\n",
            "Epoch 7, Batch 258, G Loss: 0.6955817937850952, D Loss: 1.3868038654327393\n",
            "Epoch 7, Batch 259, G Loss: 0.695544958114624, D Loss: 1.386721134185791\n",
            "Epoch 7, Batch 260, G Loss: 0.6955075263977051, D Loss: 1.3867461681365967\n",
            "Epoch 7, Batch 261, G Loss: 0.6954865455627441, D Loss: 1.3867316246032715\n",
            "Epoch 7, Batch 262, G Loss: 0.6954779624938965, D Loss: 1.3865904808044434\n",
            "Epoch 7, Batch 263, G Loss: 0.695471465587616, D Loss: 1.3864984512329102\n",
            "Epoch 7, Batch 264, G Loss: 0.6954646706581116, D Loss: 1.3865010738372803\n",
            "Epoch 7, Batch 265, G Loss: 0.6954707503318787, D Loss: 1.3864178657531738\n",
            "Epoch 7, Batch 266, G Loss: 0.6954874992370605, D Loss: 1.3863236904144287\n",
            "Epoch 7, Batch 267, G Loss: 0.6954965591430664, D Loss: 1.38627290725708\n",
            "Epoch 7, Batch 268, G Loss: 0.6955187320709229, D Loss: 1.3862154483795166\n",
            "Epoch 7, Batch 269, G Loss: 0.695525586605072, D Loss: 1.386183500289917\n",
            "Epoch 7, Batch 270, G Loss: 0.6955068111419678, D Loss: 1.386125087738037\n",
            "Epoch 7, Batch 271, G Loss: 0.6954752802848816, D Loss: 1.386175513267517\n",
            "Epoch 7, Batch 272, G Loss: 0.6954182386398315, D Loss: 1.3859320878982544\n",
            "Epoch 7, Batch 273, G Loss: 0.69537353515625, D Loss: 1.3858786821365356\n",
            "Epoch 7, Batch 274, G Loss: 0.6953381896018982, D Loss: 1.385859489440918\n",
            "Epoch 7, Batch 275, G Loss: 0.6953129768371582, D Loss: 1.385642409324646\n",
            "Epoch 7, Batch 276, G Loss: 0.6953185796737671, D Loss: 1.385573148727417\n",
            "Epoch 7, Batch 277, G Loss: 0.6953256726264954, D Loss: 1.3855392932891846\n",
            "Epoch 7, Batch 278, G Loss: 0.6953611373901367, D Loss: 1.3854703903198242\n",
            "Epoch 7, Batch 279, G Loss: 0.6954046487808228, D Loss: 1.3853297233581543\n",
            "Epoch 7, Batch 280, G Loss: 0.6954815983772278, D Loss: 1.3852455615997314\n",
            "Epoch 7, Batch 281, G Loss: 0.6955574154853821, D Loss: 1.3856468200683594\n",
            "Epoch 7, Batch 282, G Loss: 0.6956023573875427, D Loss: 1.3854284286499023\n",
            "Epoch 7, Batch 283, G Loss: 0.6956474184989929, D Loss: 1.3852550983428955\n",
            "Epoch 7, Batch 284, G Loss: 0.6956917643547058, D Loss: 1.3855422735214233\n",
            "Epoch 7, Batch 285, G Loss: 0.6957313418388367, D Loss: 1.3855926990509033\n",
            "Epoch 7, Batch 286, G Loss: 0.6957328915596008, D Loss: 1.3852713108062744\n",
            "Epoch 7, Batch 287, G Loss: 0.6957265138626099, D Loss: 1.3849153518676758\n",
            "Epoch 7, Batch 288, G Loss: 0.6957430839538574, D Loss: 1.3852410316467285\n",
            "Epoch 7, Batch 289, G Loss: 0.6957255005836487, D Loss: 1.3849568367004395\n",
            "Epoch 7, Batch 290, G Loss: 0.6957380175590515, D Loss: 1.3853089809417725\n",
            "Epoch 7, Batch 291, G Loss: 0.6957300901412964, D Loss: 1.3852012157440186\n",
            "Epoch 7, Batch 292, G Loss: 0.6957008838653564, D Loss: 1.3853437900543213\n",
            "Epoch 7, Batch 293, G Loss: 0.6956273317337036, D Loss: 1.385040044784546\n",
            "Epoch 7, Batch 294, G Loss: 0.6955997943878174, D Loss: 1.3850315809249878\n",
            "Epoch 7, Batch 295, G Loss: 0.6955348253250122, D Loss: 1.3849456310272217\n",
            "Epoch 7, Batch 296, G Loss: 0.6954901814460754, D Loss: 1.3853315114974976\n",
            "Epoch 7, Batch 297, G Loss: 0.6954560279846191, D Loss: 1.3850083351135254\n",
            "Epoch 7, Batch 298, G Loss: 0.695402979850769, D Loss: 1.3843575716018677\n",
            "Epoch 7, Batch 299, G Loss: 0.6953582763671875, D Loss: 1.3847079277038574\n",
            "Epoch 7, Batch 300, G Loss: 0.695356547832489, D Loss: 1.3846814632415771\n",
            "Epoch 7, Batch 301, G Loss: 0.6952890157699585, D Loss: 1.3870421648025513\n",
            "Epoch 7, Batch 302, G Loss: 0.6951884031295776, D Loss: 1.385746717453003\n",
            "Epoch 7, Batch 303, G Loss: 0.695080578327179, D Loss: 1.3868730068206787\n",
            "Epoch 7, Batch 304, G Loss: 0.6948860883712769, D Loss: 1.3864641189575195\n",
            "Epoch 7, Batch 305, G Loss: 0.694685161113739, D Loss: 1.3866232633590698\n",
            "Epoch 7, Batch 306, G Loss: 0.694415807723999, D Loss: 1.3869209289550781\n",
            "Epoch 7, Batch 307, G Loss: 0.694133460521698, D Loss: 1.386580228805542\n",
            "Epoch 7, Batch 308, G Loss: 0.6938928365707397, D Loss: 1.386467456817627\n",
            "Epoch 7, Batch 309, G Loss: 0.6936063170433044, D Loss: 1.3867499828338623\n",
            "Epoch 7, Batch 310, G Loss: 0.693303644657135, D Loss: 1.386716365814209\n",
            "Epoch 7, Batch 311, G Loss: 0.6930280923843384, D Loss: 1.3862404823303223\n",
            "Epoch 7, Batch 312, G Loss: 0.6927393674850464, D Loss: 1.3870034217834473\n",
            "Epoch 7, Batch 313, G Loss: 0.6924852728843689, D Loss: 1.3866328001022339\n",
            "Epoch 7, Batch 314, G Loss: 0.692246675491333, D Loss: 1.3860749006271362\n",
            "Epoch 7, Batch 315, G Loss: 0.6920115351676941, D Loss: 1.3864734172821045\n",
            "Epoch 7, Batch 316, G Loss: 0.6918331980705261, D Loss: 1.3861470222473145\n",
            "Epoch 7, Batch 317, G Loss: 0.6916963458061218, D Loss: 1.3858144283294678\n",
            "Epoch 7, Batch 318, G Loss: 0.6915608048439026, D Loss: 1.3856606483459473\n",
            "Epoch 7, Batch 319, G Loss: 0.6915275454521179, D Loss: 1.3866299390792847\n",
            "Epoch 7, Batch 320, G Loss: 0.6914604306221008, D Loss: 1.3859233856201172\n",
            "Epoch 7, Batch 321, G Loss: 0.6914702653884888, D Loss: 1.386806607246399\n",
            "Epoch 7, Batch 322, G Loss: 0.6914939284324646, D Loss: 1.38643479347229\n",
            "Epoch 7, Batch 323, G Loss: 0.691495954990387, D Loss: 1.3871560096740723\n",
            "Epoch 7, Batch 324, G Loss: 0.6915156841278076, D Loss: 1.3867416381835938\n",
            "Epoch 7, Batch 325, G Loss: 0.6915287971496582, D Loss: 1.3861395120620728\n",
            "Epoch 7, Batch 326, G Loss: 0.6916054487228394, D Loss: 1.3866684436798096\n",
            "Epoch 7, Batch 327, G Loss: 0.6916524767875671, D Loss: 1.3863781690597534\n",
            "Epoch 7, Batch 328, G Loss: 0.6917294859886169, D Loss: 1.387300729751587\n",
            "Epoch 7, Batch 329, G Loss: 0.6917902231216431, D Loss: 1.3864272832870483\n",
            "Epoch 7, Batch 330, G Loss: 0.691860020160675, D Loss: 1.3857333660125732\n",
            "Epoch 7, Batch 331, G Loss: 0.691938579082489, D Loss: 1.386440634727478\n",
            "Epoch 7, Batch 332, G Loss: 0.6920350790023804, D Loss: 1.3866252899169922\n",
            "Epoch 7, Batch 333, G Loss: 0.6921486854553223, D Loss: 1.3871190547943115\n",
            "Epoch 7, Batch 334, G Loss: 0.6921941041946411, D Loss: 1.3866938352584839\n",
            "Epoch 7, Batch 335, G Loss: 0.6922416090965271, D Loss: 1.387162208557129\n",
            "Epoch 7, Batch 336, G Loss: 0.6922685503959656, D Loss: 1.3872411251068115\n",
            "Epoch 7, Batch 337, G Loss: 0.692298173904419, D Loss: 1.3876409530639648\n",
            "Epoch 7, Batch 338, G Loss: 0.6922666430473328, D Loss: 1.3859071731567383\n",
            "Epoch 7, Batch 339, G Loss: 0.692299485206604, D Loss: 1.3864078521728516\n",
            "Epoch 7, Batch 340, G Loss: 0.6923365592956543, D Loss: 1.3857823610305786\n",
            "Epoch 7, Batch 341, G Loss: 0.6923739314079285, D Loss: 1.388288974761963\n",
            "Epoch 7, Batch 342, G Loss: 0.692377507686615, D Loss: 1.387944221496582\n",
            "Epoch 7, Batch 343, G Loss: 0.6923167109489441, D Loss: 1.3881618976593018\n",
            "Epoch 7, Batch 344, G Loss: 0.6922263503074646, D Loss: 1.3878767490386963\n",
            "Epoch 7, Batch 345, G Loss: 0.6920888423919678, D Loss: 1.387117862701416\n",
            "Epoch 7, Batch 346, G Loss: 0.6919808387756348, D Loss: 1.3868093490600586\n",
            "Epoch 7, Batch 347, G Loss: 0.6918683648109436, D Loss: 1.3868772983551025\n",
            "Epoch 7, Batch 348, G Loss: 0.6917867660522461, D Loss: 1.3869998455047607\n",
            "Epoch 7, Batch 349, G Loss: 0.6916877627372742, D Loss: 1.3866684436798096\n",
            "Epoch 7, Batch 350, G Loss: 0.6916366815567017, D Loss: 1.3869460821151733\n",
            "Epoch 7, Batch 351, G Loss: 0.6915873289108276, D Loss: 1.3868962526321411\n",
            "Epoch 7, Batch 352, G Loss: 0.691551923751831, D Loss: 1.386765480041504\n",
            "Epoch 7, Batch 353, G Loss: 0.6915313005447388, D Loss: 1.387050986289978\n",
            "Epoch 7, Batch 354, G Loss: 0.6915008425712585, D Loss: 1.3871495723724365\n",
            "Epoch 7, Batch 355, G Loss: 0.6914632320404053, D Loss: 1.3876051902770996\n",
            "Epoch 7, Batch 356, G Loss: 0.6914041638374329, D Loss: 1.3874163627624512\n",
            "Epoch 7, Batch 357, G Loss: 0.6913372874259949, D Loss: 1.3870213031768799\n",
            "Epoch 7, Batch 358, G Loss: 0.6912478804588318, D Loss: 1.387134075164795\n",
            "Epoch 7, Batch 359, G Loss: 0.6911846995353699, D Loss: 1.3871192932128906\n",
            "Epoch 7, Batch 360, G Loss: 0.6911072731018066, D Loss: 1.3869502544403076\n",
            "Epoch 7, Batch 361, G Loss: 0.6910166144371033, D Loss: 1.3870868682861328\n",
            "Epoch 7, Batch 362, G Loss: 0.6909332275390625, D Loss: 1.3867876529693604\n",
            "Epoch 7, Batch 363, G Loss: 0.6908562183380127, D Loss: 1.3866466283798218\n",
            "Epoch 7, Batch 364, G Loss: 0.6908005475997925, D Loss: 1.3866357803344727\n",
            "Epoch 7, Batch 365, G Loss: 0.6907631158828735, D Loss: 1.3865923881530762\n",
            "Epoch 7, Batch 366, G Loss: 0.6907392144203186, D Loss: 1.3865256309509277\n",
            "Epoch 7, Batch 367, G Loss: 0.6907253861427307, D Loss: 1.38645601272583\n",
            "Epoch 7, Batch 368, G Loss: 0.6907350420951843, D Loss: 1.3864099979400635\n",
            "Epoch 7, Batch 369, G Loss: 0.690767765045166, D Loss: 1.3863650560379028\n",
            "Epoch 7, Batch 370, G Loss: 0.6908256411552429, D Loss: 1.3863518238067627\n",
            "Epoch 7, Batch 371, G Loss: 0.6909098625183105, D Loss: 1.3863388299942017\n",
            "Epoch 7, Batch 372, G Loss: 0.6910169720649719, D Loss: 1.3862284421920776\n",
            "Epoch 7, Batch 373, G Loss: 0.6911228895187378, D Loss: 1.3862049579620361\n",
            "Epoch 7, Batch 374, G Loss: 0.6912315487861633, D Loss: 1.3862361907958984\n",
            "Epoch 7, Batch 375, G Loss: 0.6913439631462097, D Loss: 1.3861744403839111\n",
            "Epoch 7, Batch 376, G Loss: 0.6914626359939575, D Loss: 1.386141300201416\n",
            "Epoch 7, Batch 377, G Loss: 0.6915756464004517, D Loss: 1.3860989809036255\n",
            "Epoch 7, Batch 378, G Loss: 0.6916818618774414, D Loss: 1.3861788511276245\n",
            "Epoch 7, Batch 379, G Loss: 0.6917898058891296, D Loss: 1.3861260414123535\n",
            "Epoch 7, Batch 380, G Loss: 0.6919001340866089, D Loss: 1.386061429977417\n",
            "Epoch 7, Batch 381, G Loss: 0.6920003890991211, D Loss: 1.3862707614898682\n",
            "Epoch 7, Batch 382, G Loss: 0.6921060681343079, D Loss: 1.3857684135437012\n",
            "Epoch 7, Batch 383, G Loss: 0.6921942830085754, D Loss: 1.3858928680419922\n",
            "Epoch 7, Batch 384, G Loss: 0.692284882068634, D Loss: 1.386062502861023\n",
            "Epoch 7, Batch 385, G Loss: 0.6923538446426392, D Loss: 1.3858145475387573\n",
            "Epoch 7, Batch 386, G Loss: 0.6924211978912354, D Loss: 1.3856122493743896\n",
            "Epoch 7, Batch 387, G Loss: 0.6924710273742676, D Loss: 1.3858671188354492\n",
            "Epoch 7, Batch 388, G Loss: 0.6925046443939209, D Loss: 1.3859472274780273\n",
            "Epoch 7, Batch 389, G Loss: 0.6925483345985413, D Loss: 1.386025071144104\n",
            "Epoch 7, Batch 390, G Loss: 0.6925914287567139, D Loss: 1.386676549911499\n",
            "Epoch 7, Batch 391, G Loss: 0.692653238773346, D Loss: 1.3860416412353516\n",
            "Epoch 7, Batch 392, G Loss: 0.6927188038825989, D Loss: 1.3861324787139893\n",
            "Epoch 7, Batch 393, G Loss: 0.6927818655967712, D Loss: 1.3860156536102295\n",
            "Epoch 7, Batch 394, G Loss: 0.6928423047065735, D Loss: 1.3859000205993652\n",
            "Epoch 7, Batch 395, G Loss: 0.6928749680519104, D Loss: 1.3860664367675781\n",
            "Epoch 7, Batch 396, G Loss: 0.6929165720939636, D Loss: 1.3845261335372925\n",
            "Epoch 7, Batch 397, G Loss: 0.6929349303245544, D Loss: 1.384828805923462\n",
            "Epoch 7, Batch 398, G Loss: 0.6929351091384888, D Loss: 1.3852674961090088\n",
            "Epoch 7, Batch 399, G Loss: 0.6928969621658325, D Loss: 1.3852105140686035\n",
            "Epoch 7, Batch 400, G Loss: 0.6928583979606628, D Loss: 1.386175513267517\n",
            "Epoch 7, Batch 401, G Loss: 0.6928296685218811, D Loss: 1.3857412338256836\n",
            "Epoch 7, Batch 402, G Loss: 0.6928408741950989, D Loss: 1.3855090141296387\n",
            "Epoch 7, Batch 403, G Loss: 0.6928088068962097, D Loss: 1.386244773864746\n",
            "Epoch 7, Batch 404, G Loss: 0.6928244829177856, D Loss: 1.3867524862289429\n",
            "Epoch 7, Batch 405, G Loss: 0.6928383111953735, D Loss: 1.386776089668274\n",
            "Epoch 7, Batch 406, G Loss: 0.6928718090057373, D Loss: 1.38669753074646\n",
            "Epoch 7, Batch 407, G Loss: 0.692925751209259, D Loss: 1.3852640390396118\n",
            "Epoch 7, Batch 408, G Loss: 0.6929543018341064, D Loss: 1.3857221603393555\n",
            "Epoch 7, Batch 409, G Loss: 0.6929632425308228, D Loss: 1.3860234022140503\n",
            "Epoch 7, Batch 410, G Loss: 0.6930177807807922, D Loss: 1.385781168937683\n",
            "Epoch 7, Batch 411, G Loss: 0.6930503249168396, D Loss: 1.3863263130187988\n",
            "Epoch 7, Batch 412, G Loss: 0.6930962204933167, D Loss: 1.386298418045044\n",
            "Epoch 7, Batch 413, G Loss: 0.6931360960006714, D Loss: 1.3863134384155273\n",
            "Epoch 7, Batch 414, G Loss: 0.6931900978088379, D Loss: 1.3865466117858887\n",
            "Epoch 7, Batch 415, G Loss: 0.6932365298271179, D Loss: 1.3867249488830566\n",
            "Epoch 7, Batch 416, G Loss: 0.6933188438415527, D Loss: 1.386786699295044\n",
            "Epoch 7, Batch 417, G Loss: 0.6933692097663879, D Loss: 1.3858106136322021\n",
            "Epoch 7, Batch 418, G Loss: 0.6934133172035217, D Loss: 1.385842204093933\n",
            "Epoch 7, Batch 419, G Loss: 0.6934781670570374, D Loss: 1.3855197429656982\n",
            "Epoch 7, Batch 420, G Loss: 0.6935155391693115, D Loss: 1.3856770992279053\n",
            "Epoch 7, Batch 421, G Loss: 0.6935479044914246, D Loss: 1.3861498832702637\n",
            "Epoch 7, Batch 422, G Loss: 0.6935707926750183, D Loss: 1.3865177631378174\n",
            "Epoch 7, Batch 423, G Loss: 0.6935867667198181, D Loss: 1.3862801790237427\n",
            "Epoch 7, Batch 424, G Loss: 0.6936060190200806, D Loss: 1.3851161003112793\n",
            "Epoch 7, Batch 425, G Loss: 0.693623423576355, D Loss: 1.3853230476379395\n",
            "Epoch 7, Batch 426, G Loss: 0.6935923099517822, D Loss: 1.3856509923934937\n",
            "Epoch 7, Batch 427, G Loss: 0.6935697197914124, D Loss: 1.3855929374694824\n",
            "Epoch 7, Batch 428, G Loss: 0.6935390830039978, D Loss: 1.386060118675232\n",
            "Epoch 7, Batch 429, G Loss: 0.6935147643089294, D Loss: 1.3857333660125732\n",
            "Epoch 7, Batch 430, G Loss: 0.6934723854064941, D Loss: 1.3866331577301025\n",
            "Epoch 7, Batch 431, G Loss: 0.6934484243392944, D Loss: 1.3866301774978638\n",
            "Epoch 7, Batch 432, G Loss: 0.6934397220611572, D Loss: 1.386243224143982\n",
            "Epoch 7, Batch 433, G Loss: 0.6934118270874023, D Loss: 1.386815071105957\n",
            "Epoch 7, Batch 434, G Loss: 0.6934230327606201, D Loss: 1.3867683410644531\n",
            "Epoch 7, Batch 435, G Loss: 0.6934056282043457, D Loss: 1.3871140480041504\n",
            "Epoch 7, Batch 436, G Loss: 0.6934331655502319, D Loss: 1.3864688873291016\n",
            "Epoch 7, Batch 437, G Loss: 0.6934577226638794, D Loss: 1.3855000734329224\n",
            "Epoch 7, Batch 438, G Loss: 0.6934769153594971, D Loss: 1.385805606842041\n",
            "Epoch 7, Batch 439, G Loss: 0.6934971213340759, D Loss: 1.3857262134552002\n",
            "Epoch 7, Batch 440, G Loss: 0.6934726238250732, D Loss: 1.3857436180114746\n",
            "Epoch 7, Batch 441, G Loss: 0.693479597568512, D Loss: 1.3853919506072998\n",
            "Epoch 7, Batch 442, G Loss: 0.6934628486633301, D Loss: 1.3855743408203125\n",
            "Epoch 7, Batch 443, G Loss: 0.6934118270874023, D Loss: 1.3859037160873413\n",
            "Epoch 7, Batch 444, G Loss: 0.6933964490890503, D Loss: 1.3853851556777954\n",
            "Epoch 7, Batch 445, G Loss: 0.6933619976043701, D Loss: 1.385525107383728\n",
            "Epoch 7, Batch 446, G Loss: 0.6933120489120483, D Loss: 1.3859124183654785\n",
            "Epoch 7, Batch 447, G Loss: 0.6932624578475952, D Loss: 1.385852575302124\n",
            "Epoch 7, Batch 448, G Loss: 0.6932350397109985, D Loss: 1.386397123336792\n",
            "Epoch 7, Batch 449, G Loss: 0.6931938529014587, D Loss: 1.3865554332733154\n",
            "Epoch 7, Batch 450, G Loss: 0.693182647228241, D Loss: 1.3864386081695557\n",
            "Epoch 7, Batch 451, G Loss: 0.6931712031364441, D Loss: 1.3866586685180664\n",
            "Epoch 7, Batch 452, G Loss: 0.6931772232055664, D Loss: 1.3857908248901367\n",
            "Epoch 7, Batch 453, G Loss: 0.6931757926940918, D Loss: 1.386260986328125\n",
            "Epoch 7, Batch 454, G Loss: 0.6932040452957153, D Loss: 1.3862512111663818\n",
            "Epoch 7, Batch 455, G Loss: 0.6932181715965271, D Loss: 1.3875327110290527\n",
            "Epoch 7, Batch 456, G Loss: 0.6932541728019714, D Loss: 1.387339472770691\n",
            "Epoch 7, Batch 457, G Loss: 0.693317711353302, D Loss: 1.3871819972991943\n",
            "Epoch 7, Batch 458, G Loss: 0.6933807134628296, D Loss: 1.3856990337371826\n",
            "Epoch 7, Batch 459, G Loss: 0.6934448480606079, D Loss: 1.385817527770996\n",
            "Epoch 7, Batch 460, G Loss: 0.6934913992881775, D Loss: 1.3857364654541016\n",
            "Epoch 7, Batch 461, G Loss: 0.693538248538971, D Loss: 1.3860018253326416\n",
            "Epoch 7, Batch 462, G Loss: 0.693547785282135, D Loss: 1.387239694595337\n",
            "Epoch 7, Batch 463, G Loss: 0.6935948133468628, D Loss: 1.3874616622924805\n",
            "Epoch 7, Batch 464, G Loss: 0.6936736106872559, D Loss: 1.387145757675171\n",
            "Epoch 7, Batch 465, G Loss: 0.6937395334243774, D Loss: 1.387664556503296\n",
            "Epoch 7, Batch 466, G Loss: 0.693824052810669, D Loss: 1.38767671585083\n",
            "Epoch 7, Batch 467, G Loss: 0.6939018368721008, D Loss: 1.3876785039901733\n",
            "Epoch 7, Batch 468, G Loss: 0.6940348148345947, D Loss: 1.3875571489334106\n",
            "Epoch 7, Batch 469, G Loss: 0.6941604614257812, D Loss: 1.3879643678665161\n",
            "Epoch 7, Batch 470, G Loss: 0.6943110823631287, D Loss: 1.3869072198867798\n",
            "Epoch 7, Batch 471, G Loss: 0.6944480538368225, D Loss: 1.3870677947998047\n",
            "Epoch 7, Batch 472, G Loss: 0.6945586800575256, D Loss: 1.3870747089385986\n",
            "Epoch 7, Batch 473, G Loss: 0.6946724057197571, D Loss: 1.3871136903762817\n",
            "Epoch 7, Batch 474, G Loss: 0.6947720050811768, D Loss: 1.3870887756347656\n",
            "Epoch 7, Batch 475, G Loss: 0.6948748826980591, D Loss: 1.3866556882858276\n",
            "Epoch 7, Batch 476, G Loss: 0.6949546337127686, D Loss: 1.3865749835968018\n",
            "Epoch 7, Batch 477, G Loss: 0.6950093507766724, D Loss: 1.3866440057754517\n",
            "Epoch 7, Batch 478, G Loss: 0.6950377821922302, D Loss: 1.3869010210037231\n",
            "Epoch 7, Batch 479, G Loss: 0.6950773000717163, D Loss: 1.387589693069458\n",
            "Epoch 7, Batch 480, G Loss: 0.6951268315315247, D Loss: 1.3873149156570435\n",
            "Epoch 7, Batch 481, G Loss: 0.6951876282691956, D Loss: 1.3872722387313843\n",
            "Epoch 7, Batch 482, G Loss: 0.6952446699142456, D Loss: 1.386635661125183\n",
            "Epoch 7, Batch 483, G Loss: 0.6953022480010986, D Loss: 1.3866970539093018\n",
            "Epoch 7, Batch 484, G Loss: 0.6953317523002625, D Loss: 1.3868186473846436\n",
            "Epoch 7, Batch 485, G Loss: 0.6953649520874023, D Loss: 1.3867368698120117\n",
            "Epoch 7, Batch 486, G Loss: 0.6954012513160706, D Loss: 1.3866515159606934\n",
            "Epoch 7, Batch 487, G Loss: 0.695414662361145, D Loss: 1.3866376876831055\n",
            "Epoch 7, Batch 488, G Loss: 0.6954325437545776, D Loss: 1.3865809440612793\n",
            "Epoch 7, Batch 489, G Loss: 0.6954482793807983, D Loss: 1.3865209817886353\n",
            "Epoch 7, Batch 490, G Loss: 0.6954497694969177, D Loss: 1.3864715099334717\n",
            "Epoch 7, Batch 491, G Loss: 0.6954509019851685, D Loss: 1.3864400386810303\n",
            "Epoch 7, Batch 492, G Loss: 0.6954585313796997, D Loss: 1.3863557577133179\n",
            "Epoch 7, Batch 493, G Loss: 0.6954518556594849, D Loss: 1.3863221406936646\n",
            "Epoch 7, Batch 494, G Loss: 0.6954622864723206, D Loss: 1.386245846748352\n",
            "Epoch 7, Batch 495, G Loss: 0.6954783201217651, D Loss: 1.386171579360962\n",
            "Epoch 7, Batch 496, G Loss: 0.6954983472824097, D Loss: 1.3861134052276611\n",
            "Epoch 7, Batch 497, G Loss: 0.6955214738845825, D Loss: 1.3860907554626465\n",
            "Epoch 7, Batch 498, G Loss: 0.6955358386039734, D Loss: 1.3860502243041992\n",
            "Epoch 7, Batch 499, G Loss: 0.6955413818359375, D Loss: 1.3859440088272095\n",
            "Epoch 7, Batch 500, G Loss: 0.6955437064170837, D Loss: 1.386042833328247\n",
            "Epoch 7, Batch 501, G Loss: 0.6955233216285706, D Loss: 1.3861238956451416\n",
            "Epoch 7, Batch 502, G Loss: 0.6954806447029114, D Loss: 1.3860955238342285\n",
            "Epoch 7, Batch 503, G Loss: 0.6954012513160706, D Loss: 1.3859758377075195\n",
            "Epoch 7, Batch 504, G Loss: 0.695319652557373, D Loss: 1.3858442306518555\n",
            "Epoch 7, Batch 505, G Loss: 0.6952245235443115, D Loss: 1.3861557245254517\n",
            "Epoch 7, Batch 506, G Loss: 0.6951125860214233, D Loss: 1.3861198425292969\n",
            "Epoch 7, Batch 507, G Loss: 0.6949862837791443, D Loss: 1.386385202407837\n",
            "Epoch 7, Batch 508, G Loss: 0.694815993309021, D Loss: 1.3861284255981445\n",
            "Epoch 7, Batch 509, G Loss: 0.6946233510971069, D Loss: 1.3856860399246216\n",
            "Epoch 7, Batch 510, G Loss: 0.6944718360900879, D Loss: 1.3857452869415283\n",
            "Epoch 7, Batch 511, G Loss: 0.6943320631980896, D Loss: 1.3855676651000977\n",
            "Epoch 7, Batch 512, G Loss: 0.6942183971405029, D Loss: 1.385261058807373\n",
            "Epoch 7, Batch 513, G Loss: 0.6941272020339966, D Loss: 1.3855758905410767\n",
            "Epoch 7, Batch 514, G Loss: 0.6940633654594421, D Loss: 1.3855278491973877\n",
            "Epoch 7, Batch 515, G Loss: 0.694023609161377, D Loss: 1.3858990669250488\n",
            "Epoch 7, Batch 516, G Loss: 0.6939936876296997, D Loss: 1.3861596584320068\n",
            "Epoch 7, Batch 517, G Loss: 0.693957507610321, D Loss: 1.3862345218658447\n",
            "Epoch 7, Batch 518, G Loss: 0.6939001083374023, D Loss: 1.3858208656311035\n",
            "Epoch 7, Batch 519, G Loss: 0.6938395500183105, D Loss: 1.385991096496582\n",
            "Epoch 7, Batch 520, G Loss: 0.6937934756278992, D Loss: 1.3859241008758545\n",
            "Epoch 7, Batch 521, G Loss: 0.6937246322631836, D Loss: 1.385888695716858\n",
            "Epoch 7, Batch 522, G Loss: 0.6936807632446289, D Loss: 1.3854717016220093\n",
            "Epoch 7, Batch 523, G Loss: 0.693647563457489, D Loss: 1.385258436203003\n",
            "Epoch 7, Batch 524, G Loss: 0.6936357617378235, D Loss: 1.3853423595428467\n",
            "Epoch 7, Batch 525, G Loss: 0.6936661005020142, D Loss: 1.3849811553955078\n",
            "Epoch 7, Batch 526, G Loss: 0.6937148571014404, D Loss: 1.3849871158599854\n",
            "Epoch 7, Batch 527, G Loss: 0.6937646865844727, D Loss: 1.3854905366897583\n",
            "Epoch 7, Batch 528, G Loss: 0.6938673257827759, D Loss: 1.3860960006713867\n",
            "Epoch 7, Batch 529, G Loss: 0.6939277052879333, D Loss: 1.3866314888000488\n",
            "Epoch 7, Batch 530, G Loss: 0.6939440369606018, D Loss: 1.386060357093811\n",
            "Epoch 7, Batch 531, G Loss: 0.6939723491668701, D Loss: 1.385819911956787\n",
            "Epoch 7, Batch 532, G Loss: 0.6939314007759094, D Loss: 1.3849709033966064\n",
            "Epoch 7, Batch 533, G Loss: 0.6939681768417358, D Loss: 1.3857694864273071\n",
            "Epoch 7, Batch 534, G Loss: 0.6939921379089355, D Loss: 1.3859493732452393\n",
            "Epoch 7, Batch 535, G Loss: 0.6939878463745117, D Loss: 1.3856019973754883\n",
            "Epoch 7, Batch 536, G Loss: 0.6939753293991089, D Loss: 1.3853925466537476\n",
            "Epoch 7, Batch 537, G Loss: 0.6939852833747864, D Loss: 1.3861699104309082\n",
            "Epoch 7, Batch 538, G Loss: 0.6939957141876221, D Loss: 1.3864407539367676\n",
            "Epoch 7, Batch 539, G Loss: 0.6939323544502258, D Loss: 1.3862547874450684\n",
            "Epoch 7, Batch 540, G Loss: 0.6938756704330444, D Loss: 1.3863284587860107\n",
            "Epoch 7, Batch 541, G Loss: 0.6938245892524719, D Loss: 1.3863143920898438\n",
            "Epoch 7, Batch 542, G Loss: 0.6937140822410583, D Loss: 1.3862279653549194\n",
            "Epoch 7, Batch 543, G Loss: 0.6936376690864563, D Loss: 1.3863036632537842\n",
            "Epoch 7, Batch 544, G Loss: 0.6935208439826965, D Loss: 1.3863754272460938\n",
            "Epoch 7, Batch 545, G Loss: 0.6934199333190918, D Loss: 1.3863487243652344\n",
            "Epoch 7, Batch 546, G Loss: 0.6933372616767883, D Loss: 1.3862810134887695\n",
            "Epoch 7, Batch 547, G Loss: 0.6932122111320496, D Loss: 1.386334776878357\n",
            "Epoch 7, Batch 548, G Loss: 0.6930935978889465, D Loss: 1.387040615081787\n",
            "Epoch 7, Batch 549, G Loss: 0.6929654479026794, D Loss: 1.3863223791122437\n",
            "Epoch 7, Batch 550, G Loss: 0.6928316950798035, D Loss: 1.3857362270355225\n",
            "Epoch 7, Batch 551, G Loss: 0.692724883556366, D Loss: 1.3859044313430786\n",
            "Epoch 7, Batch 552, G Loss: 0.6926512718200684, D Loss: 1.385924220085144\n",
            "Epoch 7, Batch 553, G Loss: 0.6926333904266357, D Loss: 1.3866745233535767\n",
            "Epoch 7, Batch 554, G Loss: 0.6925766468048096, D Loss: 1.386866807937622\n",
            "Epoch 7, Batch 555, G Loss: 0.6925245523452759, D Loss: 1.3870844841003418\n",
            "Epoch 7, Batch 556, G Loss: 0.6924459338188171, D Loss: 1.386314868927002\n",
            "Epoch 7, Batch 557, G Loss: 0.692381739616394, D Loss: 1.386164665222168\n",
            "Epoch 7, Batch 558, G Loss: 0.6923748850822449, D Loss: 1.3867583274841309\n",
            "Epoch 7, Batch 559, G Loss: 0.6923131942749023, D Loss: 1.3864502906799316\n",
            "Epoch 7, Batch 560, G Loss: 0.692296028137207, D Loss: 1.3865381479263306\n",
            "Epoch 7, Batch 561, G Loss: 0.6922506093978882, D Loss: 1.3860976696014404\n",
            "Epoch 7, Batch 562, G Loss: 0.6922454237937927, D Loss: 1.3857958316802979\n",
            "Epoch 7, Batch 563, G Loss: 0.6922820210456848, D Loss: 1.3857625722885132\n",
            "Epoch 7, Batch 564, G Loss: 0.6923210620880127, D Loss: 1.385392665863037\n",
            "Epoch 7, Batch 565, G Loss: 0.6924324631690979, D Loss: 1.3870267868041992\n",
            "Epoch 7, Batch 566, G Loss: 0.6925027966499329, D Loss: 1.388361930847168\n",
            "Epoch 7, Batch 567, G Loss: 0.6925139427185059, D Loss: 1.3876087665557861\n",
            "Epoch 7, Batch 568, G Loss: 0.6924781799316406, D Loss: 1.387763500213623\n",
            "Epoch 7, Batch 569, G Loss: 0.6923948526382446, D Loss: 1.3865594863891602\n",
            "Epoch 7, Batch 570, G Loss: 0.6923306584358215, D Loss: 1.386800765991211\n",
            "Epoch 7, Batch 571, G Loss: 0.6922632455825806, D Loss: 1.3865303993225098\n",
            "Epoch 7, Batch 572, G Loss: 0.6922215223312378, D Loss: 1.388092041015625\n",
            "Epoch 7, Batch 573, G Loss: 0.6921330690383911, D Loss: 1.3877429962158203\n",
            "Epoch 7, Batch 574, G Loss: 0.6920197010040283, D Loss: 1.3880982398986816\n",
            "Epoch 7, Batch 575, G Loss: 0.691835343837738, D Loss: 1.3878388404846191\n",
            "Epoch 7, Batch 576, G Loss: 0.6916640400886536, D Loss: 1.3870357275009155\n",
            "Epoch 7, Batch 577, G Loss: 0.6914927959442139, D Loss: 1.3870052099227905\n",
            "Epoch 7, Batch 578, G Loss: 0.6913416385650635, D Loss: 1.3866773843765259\n",
            "Epoch 7, Batch 579, G Loss: 0.6912155151367188, D Loss: 1.386535406112671\n",
            "Epoch 7, Batch 580, G Loss: 0.6911359429359436, D Loss: 1.3873554468154907\n",
            "Epoch 7, Batch 581, G Loss: 0.6910467147827148, D Loss: 1.3871817588806152\n",
            "Epoch 7, Batch 582, G Loss: 0.69096839427948, D Loss: 1.3871902227401733\n",
            "Epoch 7, Batch 583, G Loss: 0.6908760070800781, D Loss: 1.3871264457702637\n",
            "Epoch 7, Batch 584, G Loss: 0.6908125877380371, D Loss: 1.3867378234863281\n",
            "Epoch 7, Batch 585, G Loss: 0.6907630562782288, D Loss: 1.3866535425186157\n",
            "Epoch 7, Batch 586, G Loss: 0.6907470226287842, D Loss: 1.3865243196487427\n",
            "Epoch 7, Batch 587, G Loss: 0.6907563209533691, D Loss: 1.3864086866378784\n",
            "Epoch 7, Batch 588, G Loss: 0.6908137798309326, D Loss: 1.3863916397094727\n",
            "Epoch 7, Batch 589, G Loss: 0.6908936500549316, D Loss: 1.3863606452941895\n",
            "Epoch 7, Batch 590, G Loss: 0.6910144090652466, D Loss: 1.3865957260131836\n",
            "Epoch 7, Batch 591, G Loss: 0.6911216378211975, D Loss: 1.3865773677825928\n",
            "Epoch 7, Batch 592, G Loss: 0.6912326216697693, D Loss: 1.3864984512329102\n",
            "Epoch 7, Batch 593, G Loss: 0.6913372874259949, D Loss: 1.386428952217102\n",
            "Epoch 7, Batch 594, G Loss: 0.6914554238319397, D Loss: 1.3864034414291382\n",
            "Epoch 7, Batch 595, G Loss: 0.6915796995162964, D Loss: 1.3863537311553955\n",
            "Epoch 7, Batch 596, G Loss: 0.6917179822921753, D Loss: 1.3863842487335205\n",
            "Epoch 7, Batch 597, G Loss: 0.6918474435806274, D Loss: 1.386357307434082\n",
            "Epoch 7, Batch 598, G Loss: 0.691962480545044, D Loss: 1.3863343000411987\n",
            "Epoch 7, Batch 599, G Loss: 0.6920692324638367, D Loss: 1.3863215446472168\n",
            "Epoch 7, Batch 600, G Loss: 0.6921737194061279, D Loss: 1.3863341808319092\n",
            "Epoch 7, Batch 601, G Loss: 0.692280650138855, D Loss: 1.386314868927002\n",
            "Epoch 7, Batch 602, G Loss: 0.6923848390579224, D Loss: 1.3863463401794434\n",
            "Epoch 7, Batch 603, G Loss: 0.6924877762794495, D Loss: 1.3862651586532593\n",
            "Epoch 7, Batch 604, G Loss: 0.6925821304321289, D Loss: 1.38608980178833\n",
            "Epoch 7, Batch 605, G Loss: 0.6926426291465759, D Loss: 1.386246919631958\n",
            "Epoch 7, Batch 606, G Loss: 0.6926976442337036, D Loss: 1.3862067461013794\n",
            "Epoch 7, Batch 607, G Loss: 0.6927406191825867, D Loss: 1.386185884475708\n",
            "Epoch 7, Batch 608, G Loss: 0.6927756667137146, D Loss: 1.3860913515090942\n",
            "Epoch 7, Batch 609, G Loss: 0.6927894949913025, D Loss: 1.3862160444259644\n",
            "Epoch 7, Batch 610, G Loss: 0.6928032040596008, D Loss: 1.3862920999526978\n",
            "Epoch 7, Batch 611, G Loss: 0.6928191184997559, D Loss: 1.3860647678375244\n",
            "Epoch 7, Batch 612, G Loss: 0.6928287148475647, D Loss: 1.3858572244644165\n",
            "Epoch 7, Batch 613, G Loss: 0.6928082704544067, D Loss: 1.385838508605957\n",
            "Epoch 7, Batch 614, G Loss: 0.6927779912948608, D Loss: 1.3859025239944458\n",
            "Epoch 7, Batch 615, G Loss: 0.6927353143692017, D Loss: 1.3859400749206543\n",
            "Epoch 7, Batch 616, G Loss: 0.6926938891410828, D Loss: 1.3858749866485596\n",
            "Epoch 7, Batch 617, G Loss: 0.6926344037055969, D Loss: 1.385781168937683\n",
            "Epoch 7, Batch 618, G Loss: 0.6925978660583496, D Loss: 1.3859878778457642\n",
            "Epoch 7, Batch 619, G Loss: 0.6925447583198547, D Loss: 1.3858118057250977\n",
            "Epoch 7, Batch 620, G Loss: 0.6924939751625061, D Loss: 1.385781168937683\n",
            "Epoch 7, Batch 621, G Loss: 0.6924524903297424, D Loss: 1.386132001876831\n",
            "Epoch 7, Batch 622, G Loss: 0.692419707775116, D Loss: 1.3866826295852661\n",
            "Epoch 7, Batch 623, G Loss: 0.6924225687980652, D Loss: 1.3861509561538696\n",
            "Epoch 7, Batch 624, G Loss: 0.6924337148666382, D Loss: 1.3863903284072876\n",
            "Epoch 7, Batch 625, G Loss: 0.6924580335617065, D Loss: 1.3858298063278198\n",
            "Epoch 7, Batch 626, G Loss: 0.6924881935119629, D Loss: 1.385956048965454\n",
            "Epoch 7, Batch 627, G Loss: 0.6925148367881775, D Loss: 1.3857982158660889\n",
            "Epoch 7, Batch 628, G Loss: 0.692559540271759, D Loss: 1.3857630491256714\n",
            "Epoch 7, Batch 629, G Loss: 0.6925946474075317, D Loss: 1.3859152793884277\n",
            "Epoch 7, Batch 630, G Loss: 0.6926298141479492, D Loss: 1.3862531185150146\n",
            "Epoch 7, Batch 631, G Loss: 0.6926696300506592, D Loss: 1.3862636089324951\n",
            "Epoch 7, Batch 632, G Loss: 0.6927292346954346, D Loss: 1.3854238986968994\n",
            "Epoch 7, Batch 633, G Loss: 0.6927747130393982, D Loss: 1.3852739334106445\n",
            "Epoch 7, Batch 634, G Loss: 0.6927981972694397, D Loss: 1.3853914737701416\n",
            "Epoch 7, Batch 635, G Loss: 0.6928028464317322, D Loss: 1.3857231140136719\n",
            "Epoch 7, Batch 636, G Loss: 0.6928115487098694, D Loss: 1.38618803024292\n",
            "Epoch 7, Batch 637, G Loss: 0.6928558349609375, D Loss: 1.386085867881775\n",
            "Epoch 7, Batch 638, G Loss: 0.6928664445877075, D Loss: 1.386460542678833\n",
            "Epoch 7, Batch 639, G Loss: 0.692894697189331, D Loss: 1.3859195709228516\n",
            "Epoch 7, Batch 640, G Loss: 0.6929335594177246, D Loss: 1.3863545656204224\n",
            "Epoch 7, Batch 641, G Loss: 0.6929534673690796, D Loss: 1.3862441778182983\n",
            "Epoch 7, Batch 642, G Loss: 0.6930254101753235, D Loss: 1.3862022161483765\n",
            "Epoch 7, Batch 643, G Loss: 0.6930689811706543, D Loss: 1.3861448764801025\n",
            "Epoch 7, Batch 644, G Loss: 0.6931098103523254, D Loss: 1.3857572078704834\n",
            "Epoch 7, Batch 645, G Loss: 0.6931657791137695, D Loss: 1.3862545490264893\n",
            "Epoch 7, Batch 646, G Loss: 0.6932151317596436, D Loss: 1.385756015777588\n",
            "Epoch 7, Batch 647, G Loss: 0.6932432055473328, D Loss: 1.3857091665267944\n",
            "Epoch 7, Batch 648, G Loss: 0.6932662725448608, D Loss: 1.3859341144561768\n",
            "Epoch 7, Batch 649, G Loss: 0.6932985186576843, D Loss: 1.3865381479263306\n",
            "Epoch 7, Batch 650, G Loss: 0.6933165788650513, D Loss: 1.3858003616333008\n",
            "Epoch 7, Batch 651, G Loss: 0.693329930305481, D Loss: 1.3862452507019043\n",
            "Epoch 7, Batch 652, G Loss: 0.6933514475822449, D Loss: 1.387115478515625\n",
            "Epoch 7, Batch 653, G Loss: 0.6934038996696472, D Loss: 1.3873789310455322\n",
            "Epoch 7, Batch 654, G Loss: 0.6934390664100647, D Loss: 1.3867486715316772\n",
            "Epoch 7, Batch 655, G Loss: 0.6935070753097534, D Loss: 1.3858318328857422\n",
            "Epoch 7, Batch 656, G Loss: 0.6935460567474365, D Loss: 1.3857824802398682\n",
            "Epoch 7, Batch 657, G Loss: 0.6935786008834839, D Loss: 1.3854649066925049\n",
            "Epoch 7, Batch 658, G Loss: 0.6935860514640808, D Loss: 1.3855037689208984\n",
            "Epoch 7, Batch 659, G Loss: 0.6935927867889404, D Loss: 1.385807991027832\n",
            "Epoch 7, Batch 660, G Loss: 0.693566083908081, D Loss: 1.3859055042266846\n",
            "Epoch 7, Batch 661, G Loss: 0.6935562491416931, D Loss: 1.3858225345611572\n",
            "Epoch 7, Batch 662, G Loss: 0.6935316324234009, D Loss: 1.3863201141357422\n",
            "Epoch 7, Batch 663, G Loss: 0.6935132145881653, D Loss: 1.3861382007598877\n",
            "Epoch 7, Batch 664, G Loss: 0.6934816241264343, D Loss: 1.3865556716918945\n",
            "Epoch 7, Batch 665, G Loss: 0.6934789419174194, D Loss: 1.386715054512024\n",
            "Epoch 7, Batch 666, G Loss: 0.6934569478034973, D Loss: 1.385361909866333\n",
            "Epoch 7, Batch 667, G Loss: 0.693449854850769, D Loss: 1.3851745128631592\n",
            "Epoch 7, Batch 668, G Loss: 0.6934031844139099, D Loss: 1.3859658241271973\n",
            "Epoch 7, Batch 669, G Loss: 0.6933682560920715, D Loss: 1.3856679201126099\n",
            "Epoch 7, Batch 670, G Loss: 0.6933304071426392, D Loss: 1.3859225511550903\n",
            "Epoch 7, Batch 671, G Loss: 0.6932704448699951, D Loss: 1.3864271640777588\n",
            "Epoch 7, Batch 672, G Loss: 0.6932563185691833, D Loss: 1.3870887756347656\n",
            "Epoch 7, Batch 673, G Loss: 0.6932447552680969, D Loss: 1.3869203329086304\n",
            "Epoch 7, Batch 674, G Loss: 0.6932457089424133, D Loss: 1.3870209455490112\n",
            "Epoch 7, Batch 675, G Loss: 0.6932673454284668, D Loss: 1.385944128036499\n",
            "Epoch 7, Batch 676, G Loss: 0.6932904720306396, D Loss: 1.3860979080200195\n",
            "Epoch 7, Batch 677, G Loss: 0.6933150291442871, D Loss: 1.3864219188690186\n",
            "Epoch 7, Batch 678, G Loss: 0.693328857421875, D Loss: 1.386401891708374\n",
            "Epoch 7, Batch 679, G Loss: 0.6933727860450745, D Loss: 1.3863012790679932\n",
            "Epoch 7, Batch 680, G Loss: 0.6933911442756653, D Loss: 1.3862320184707642\n",
            "Epoch 7, Batch 681, G Loss: 0.6934183239936829, D Loss: 1.3867053985595703\n",
            "Epoch 7, Batch 682, G Loss: 0.6934328675270081, D Loss: 1.3863275051116943\n",
            "Epoch 7, Batch 683, G Loss: 0.6934643387794495, D Loss: 1.3864600658416748\n",
            "Epoch 7, Batch 684, G Loss: 0.6935065388679504, D Loss: 1.3864805698394775\n",
            "Epoch 7, Batch 685, G Loss: 0.6935279965400696, D Loss: 1.3867956399917603\n",
            "Epoch 7, Batch 686, G Loss: 0.6935864686965942, D Loss: 1.386913537979126\n",
            "Epoch 7, Batch 687, G Loss: 0.6936065554618835, D Loss: 1.3870891332626343\n",
            "Epoch 7, Batch 688, G Loss: 0.6936805844306946, D Loss: 1.3872709274291992\n",
            "Epoch 7, Batch 689, G Loss: 0.693743884563446, D Loss: 1.3865599632263184\n",
            "Epoch 7, Batch 690, G Loss: 0.6938092708587646, D Loss: 1.386972427368164\n",
            "Epoch 7, Batch 691, G Loss: 0.6938685774803162, D Loss: 1.3867586851119995\n",
            "Epoch 7, Batch 692, G Loss: 0.69394451379776, D Loss: 1.3865467309951782\n",
            "Epoch 7, Batch 693, G Loss: 0.6939869523048401, D Loss: 1.3865336179733276\n",
            "Epoch 7, Batch 694, G Loss: 0.6940486431121826, D Loss: 1.3869080543518066\n",
            "Epoch 7, Batch 695, G Loss: 0.6940981149673462, D Loss: 1.3864973783493042\n",
            "Epoch 7, Batch 696, G Loss: 0.6941361427307129, D Loss: 1.3867384195327759\n",
            "Epoch 7, Batch 697, G Loss: 0.6941649913787842, D Loss: 1.3867032527923584\n",
            "Epoch 7, Batch 698, G Loss: 0.6941955089569092, D Loss: 1.3867456912994385\n",
            "Epoch 7, Batch 699, G Loss: 0.6942371726036072, D Loss: 1.386845588684082\n",
            "Epoch 7, Batch 700, G Loss: 0.6942651867866516, D Loss: 1.3867785930633545\n",
            "Epoch 7, Batch 701, G Loss: 0.694275975227356, D Loss: 1.3869564533233643\n",
            "Epoch 7, Batch 702, G Loss: 0.6943141222000122, D Loss: 1.3866674900054932\n",
            "Epoch 7, Batch 703, G Loss: 0.6943432688713074, D Loss: 1.3864588737487793\n",
            "Epoch 7, Batch 704, G Loss: 0.6943641304969788, D Loss: 1.3865702152252197\n",
            "Epoch 7, Batch 705, G Loss: 0.69436115026474, D Loss: 1.386460781097412\n",
            "Epoch 7, Batch 706, G Loss: 0.694362223148346, D Loss: 1.3865405321121216\n",
            "Epoch 7, Batch 707, G Loss: 0.6943530440330505, D Loss: 1.3865385055541992\n",
            "Epoch 7, Batch 708, G Loss: 0.6943398118019104, D Loss: 1.386466145515442\n",
            "Epoch 7, Batch 709, G Loss: 0.694318950176239, D Loss: 1.3866071701049805\n",
            "Epoch 7, Batch 710, G Loss: 0.694291889667511, D Loss: 1.38663911819458\n",
            "Epoch 7, Batch 711, G Loss: 0.6942809820175171, D Loss: 1.38679838180542\n",
            "Epoch 7, Batch 712, G Loss: 0.6942839622497559, D Loss: 1.386488914489746\n",
            "Epoch 7, Batch 713, G Loss: 0.6942881345748901, D Loss: 1.3865635395050049\n",
            "Epoch 7, Batch 714, G Loss: 0.6942890882492065, D Loss: 1.3865160942077637\n",
            "Epoch 7, Batch 715, G Loss: 0.6942920088768005, D Loss: 1.3864331245422363\n",
            "Epoch 7, Batch 716, G Loss: 0.6942903399467468, D Loss: 1.3864521980285645\n",
            "Epoch 7, Batch 717, G Loss: 0.6942931413650513, D Loss: 1.386399745941162\n",
            "Epoch 7, Batch 718, G Loss: 0.6942917108535767, D Loss: 1.3865182399749756\n",
            "Epoch 7, Batch 719, G Loss: 0.6942998170852661, D Loss: 1.386566162109375\n",
            "Epoch 7, Batch 720, G Loss: 0.6943365335464478, D Loss: 1.38645601272583\n",
            "Epoch 7, Batch 721, G Loss: 0.6943891048431396, D Loss: 1.3864201307296753\n",
            "Epoch 7, Batch 722, G Loss: 0.6944550275802612, D Loss: 1.3863416910171509\n",
            "Epoch 7, Batch 723, G Loss: 0.6945226192474365, D Loss: 1.3862841129302979\n",
            "Epoch 7, Batch 724, G Loss: 0.694584310054779, D Loss: 1.3862502574920654\n",
            "Epoch 7, Batch 725, G Loss: 0.694641649723053, D Loss: 1.3862261772155762\n",
            "Epoch 7, Batch 726, G Loss: 0.6946703791618347, D Loss: 1.3862006664276123\n",
            "Epoch 7, Batch 727, G Loss: 0.6946943402290344, D Loss: 1.3861676454544067\n",
            "Epoch 7, Batch 728, G Loss: 0.6947095990180969, D Loss: 1.3861515522003174\n",
            "Epoch 7, Batch 729, G Loss: 0.6947061419487, D Loss: 1.3861613273620605\n",
            "Epoch 7, Batch 730, G Loss: 0.6946950554847717, D Loss: 1.3862385749816895\n",
            "Epoch 7, Batch 731, G Loss: 0.6946477890014648, D Loss: 1.3861597776412964\n",
            "Epoch 7, Batch 732, G Loss: 0.6945832967758179, D Loss: 1.3862099647521973\n",
            "Epoch 7, Batch 733, G Loss: 0.6945064067840576, D Loss: 1.3861684799194336\n",
            "Epoch 7, Batch 734, G Loss: 0.6944146156311035, D Loss: 1.3861126899719238\n",
            "Epoch 7, Batch 735, G Loss: 0.6943293809890747, D Loss: 1.3860604763031006\n",
            "Epoch 7, Batch 736, G Loss: 0.6942574381828308, D Loss: 1.386059284210205\n",
            "Epoch 7, Batch 737, G Loss: 0.6941779255867004, D Loss: 1.386069893836975\n",
            "Epoch 7, Batch 738, G Loss: 0.6941096782684326, D Loss: 1.3858013153076172\n",
            "Epoch 7, Batch 739, G Loss: 0.6940567493438721, D Loss: 1.3857440948486328\n",
            "Epoch 7, Batch 740, G Loss: 0.6940401196479797, D Loss: 1.3856751918792725\n",
            "Epoch 7, Batch 741, G Loss: 0.6940444707870483, D Loss: 1.3861467838287354\n",
            "Epoch 7, Batch 742, G Loss: 0.6940333247184753, D Loss: 1.3861151933670044\n",
            "Epoch 7, Batch 743, G Loss: 0.6940239071846008, D Loss: 1.3860180377960205\n",
            "Epoch 7, Batch 744, G Loss: 0.6939911842346191, D Loss: 1.3864388465881348\n",
            "Epoch 7, Batch 745, G Loss: 0.6939191818237305, D Loss: 1.3865773677825928\n",
            "Epoch 7, Batch 746, G Loss: 0.6938233375549316, D Loss: 1.3863248825073242\n",
            "Epoch 7, Batch 747, G Loss: 0.6937233805656433, D Loss: 1.3861711025238037\n",
            "Epoch 7, Batch 748, G Loss: 0.6936049461364746, D Loss: 1.3862122297286987\n",
            "Epoch 7, Batch 749, G Loss: 0.6934953927993774, D Loss: 1.3862004280090332\n",
            "Epoch 7, Batch 750, G Loss: 0.6933796405792236, D Loss: 1.3859071731567383\n",
            "Epoch 7, Batch 751, G Loss: 0.6932823061943054, D Loss: 1.3865455389022827\n",
            "Epoch 7, Batch 752, G Loss: 0.6931768655776978, D Loss: 1.3861768245697021\n",
            "Epoch 7, Batch 753, G Loss: 0.6930679678916931, D Loss: 1.3865504264831543\n",
            "Epoch 7, Batch 754, G Loss: 0.6929658651351929, D Loss: 1.3864657878875732\n",
            "Epoch 7, Batch 755, G Loss: 0.6928510069847107, D Loss: 1.3864610195159912\n",
            "Epoch 7, Batch 756, G Loss: 0.6927348375320435, D Loss: 1.3863774538040161\n",
            "Epoch 7, Batch 757, G Loss: 0.6926361322402954, D Loss: 1.386383295059204\n",
            "Epoch 7, Batch 758, G Loss: 0.6925249695777893, D Loss: 1.386230230331421\n",
            "Epoch 7, Batch 759, G Loss: 0.6924486756324768, D Loss: 1.3864144086837769\n",
            "Epoch 7, Batch 760, G Loss: 0.6923709511756897, D Loss: 1.3861706256866455\n",
            "Epoch 7, Batch 761, G Loss: 0.69232177734375, D Loss: 1.3862802982330322\n",
            "Epoch 7, Batch 762, G Loss: 0.6922828555107117, D Loss: 1.3863751888275146\n",
            "Epoch 7, Batch 763, G Loss: 0.6922663450241089, D Loss: 1.3863370418548584\n",
            "Epoch 7, Batch 764, G Loss: 0.6922447681427002, D Loss: 1.38621187210083\n",
            "Epoch 7, Batch 765, G Loss: 0.6922521591186523, D Loss: 1.3855700492858887\n",
            "Epoch 7, Batch 766, G Loss: 0.6923186779022217, D Loss: 1.385667324066162\n",
            "Epoch 7, Batch 767, G Loss: 0.6924247145652771, D Loss: 1.385878562927246\n",
            "Epoch 7, Batch 768, G Loss: 0.6925453543663025, D Loss: 1.3862228393554688\n",
            "Epoch 7, Batch 769, G Loss: 0.6926661729812622, D Loss: 1.3863215446472168\n",
            "Epoch 7, Batch 770, G Loss: 0.6927896738052368, D Loss: 1.386192798614502\n",
            "Epoch 7, Batch 771, G Loss: 0.6928871273994446, D Loss: 1.3860845565795898\n",
            "Epoch 7, Batch 772, G Loss: 0.6929990649223328, D Loss: 1.3860900402069092\n",
            "Epoch 7, Batch 773, G Loss: 0.6931101679801941, D Loss: 1.3862504959106445\n",
            "Epoch 7, Batch 774, G Loss: 0.6931999325752258, D Loss: 1.38614821434021\n",
            "Epoch 7, Batch 775, G Loss: 0.6932792067527771, D Loss: 1.3861169815063477\n",
            "Epoch 7, Batch 776, G Loss: 0.6933647394180298, D Loss: 1.3861342668533325\n",
            "Epoch 7, Batch 777, G Loss: 0.6934293508529663, D Loss: 1.3858609199523926\n",
            "Epoch 7, Batch 778, G Loss: 0.6935076117515564, D Loss: 1.3864555358886719\n",
            "Epoch 7, Batch 779, G Loss: 0.6935412883758545, D Loss: 1.3860244750976562\n",
            "Epoch 7, Batch 780, G Loss: 0.6935892701148987, D Loss: 1.3859705924987793\n",
            "Epoch 7, Batch 781, G Loss: 0.6936318278312683, D Loss: 1.3857929706573486\n",
            "Epoch 7, Batch 782, G Loss: 0.6936872005462646, D Loss: 1.3859891891479492\n",
            "Epoch 7, Batch 783, G Loss: 0.6937336921691895, D Loss: 1.3861243724822998\n",
            "Epoch 7, Batch 784, G Loss: 0.6937814950942993, D Loss: 1.38627028465271\n",
            "Epoch 7, Batch 785, G Loss: 0.6937991380691528, D Loss: 1.3860783576965332\n",
            "Epoch 7, Batch 786, G Loss: 0.6938062906265259, D Loss: 1.3861161470413208\n",
            "Epoch 7, Batch 787, G Loss: 0.693819522857666, D Loss: 1.386193871498108\n",
            "Epoch 7, Batch 788, G Loss: 0.6938024163246155, D Loss: 1.3860812187194824\n",
            "Epoch 7, Batch 789, G Loss: 0.69377601146698, D Loss: 1.386182427406311\n",
            "Epoch 7, Batch 790, G Loss: 0.6937551498413086, D Loss: 1.386352300643921\n",
            "Epoch 7, Batch 791, G Loss: 0.693713366985321, D Loss: 1.3861175775527954\n",
            "Epoch 7, Batch 792, G Loss: 0.6936522722244263, D Loss: 1.386103630065918\n",
            "Epoch 7, Batch 793, G Loss: 0.6936264634132385, D Loss: 1.3863190412521362\n",
            "Epoch 7, Batch 794, G Loss: 0.693572998046875, D Loss: 1.3866727352142334\n",
            "Epoch 7, Batch 795, G Loss: 0.6934926509857178, D Loss: 1.3863554000854492\n",
            "Epoch 7, Batch 796, G Loss: 0.6933866143226624, D Loss: 1.3865431547164917\n",
            "Epoch 7, Batch 797, G Loss: 0.6932886838912964, D Loss: 1.38629150390625\n",
            "Epoch 7, Batch 798, G Loss: 0.6932080388069153, D Loss: 1.3862335681915283\n",
            "Epoch 7, Batch 799, G Loss: 0.693115234375, D Loss: 1.3865593671798706\n",
            "Epoch 7, Batch 800, G Loss: 0.6930315494537354, D Loss: 1.3867895603179932\n",
            "Epoch 7, Batch 801, G Loss: 0.6929117441177368, D Loss: 1.3869807720184326\n",
            "Epoch 7, Batch 802, G Loss: 0.692773163318634, D Loss: 1.3867355585098267\n",
            "Epoch 7, Batch 803, G Loss: 0.6926469802856445, D Loss: 1.386612057685852\n",
            "Epoch 7, Batch 804, G Loss: 0.6925013065338135, D Loss: 1.3864758014678955\n",
            "Epoch 7, Batch 805, G Loss: 0.6923848986625671, D Loss: 1.3867053985595703\n",
            "Epoch 7, Batch 806, G Loss: 0.6922666430473328, D Loss: 1.3863128423690796\n",
            "Epoch 7, Batch 807, G Loss: 0.6921570301055908, D Loss: 1.3866143226623535\n",
            "Epoch 7, Batch 808, G Loss: 0.6920806169509888, D Loss: 1.386293888092041\n",
            "Epoch 7, Batch 809, G Loss: 0.6920240521430969, D Loss: 1.3862254619598389\n",
            "Epoch 7, Batch 810, G Loss: 0.6919882893562317, D Loss: 1.3863688707351685\n",
            "Epoch 7, Batch 811, G Loss: 0.691987931728363, D Loss: 1.3862042427062988\n",
            "Epoch 7, Batch 812, G Loss: 0.6920074224472046, D Loss: 1.3864459991455078\n",
            "Epoch 7, Batch 813, G Loss: 0.6920500993728638, D Loss: 1.3862817287445068\n",
            "Epoch 7, Batch 814, G Loss: 0.6920834183692932, D Loss: 1.3860619068145752\n",
            "Epoch 7, Batch 815, G Loss: 0.692169189453125, D Loss: 1.3862879276275635\n",
            "Epoch 7, Batch 816, G Loss: 0.6922603249549866, D Loss: 1.386197566986084\n",
            "Epoch 7, Batch 817, G Loss: 0.6923677921295166, D Loss: 1.3861353397369385\n",
            "Epoch 7, Batch 818, G Loss: 0.6924910545349121, D Loss: 1.3863790035247803\n",
            "Epoch 7, Batch 819, G Loss: 0.692620038986206, D Loss: 1.3862892389297485\n",
            "Epoch 7, Batch 820, G Loss: 0.6927289366722107, D Loss: 1.386806845664978\n",
            "Epoch 7, Batch 821, G Loss: 0.6928016543388367, D Loss: 1.3865184783935547\n",
            "Epoch 7, Batch 822, G Loss: 0.6928548812866211, D Loss: 1.3865103721618652\n",
            "Epoch 7, Batch 823, G Loss: 0.6928848028182983, D Loss: 1.3863894939422607\n",
            "Epoch 7, Batch 824, G Loss: 0.6929282546043396, D Loss: 1.3863003253936768\n",
            "Epoch 7, Batch 825, G Loss: 0.6929548382759094, D Loss: 1.3862502574920654\n",
            "Epoch 7, Batch 826, G Loss: 0.6929908394813538, D Loss: 1.3864305019378662\n",
            "Epoch 7, Batch 827, G Loss: 0.6930204033851624, D Loss: 1.3864986896514893\n",
            "Epoch 7, Batch 828, G Loss: 0.6930336952209473, D Loss: 1.3865551948547363\n",
            "Epoch 7, Batch 829, G Loss: 0.6930148601531982, D Loss: 1.3866119384765625\n",
            "Epoch 7, Batch 830, G Loss: 0.6929763555526733, D Loss: 1.386601209640503\n",
            "Epoch 7, Batch 831, G Loss: 0.6929059028625488, D Loss: 1.3863849639892578\n",
            "Epoch 7, Batch 832, G Loss: 0.6928527355194092, D Loss: 1.3864822387695312\n",
            "Epoch 7, Batch 833, G Loss: 0.6927818059921265, D Loss: 1.3864068984985352\n",
            "Epoch 7, Batch 834, G Loss: 0.6927161812782288, D Loss: 1.3863499164581299\n",
            "Epoch 7, Batch 835, G Loss: 0.6926663517951965, D Loss: 1.3864009380340576\n",
            "Epoch 7, Batch 836, G Loss: 0.6926182508468628, D Loss: 1.3863515853881836\n",
            "Epoch 7, Batch 837, G Loss: 0.6925839781761169, D Loss: 1.3863550424575806\n",
            "Epoch 7, Batch 838, G Loss: 0.6925557255744934, D Loss: 1.3862946033477783\n",
            "Epoch 7, Batch 839, G Loss: 0.6925498247146606, D Loss: 1.3863481283187866\n",
            "Epoch 7, Batch 840, G Loss: 0.6925469040870667, D Loss: 1.3863213062286377\n",
            "Epoch 7, Batch 841, G Loss: 0.6925522089004517, D Loss: 1.3862876892089844\n",
            "Epoch 7, Batch 842, G Loss: 0.6925747990608215, D Loss: 1.3863303661346436\n",
            "Epoch 7, Batch 843, G Loss: 0.6926025748252869, D Loss: 1.386375069618225\n",
            "Epoch 7, Batch 844, G Loss: 0.6926191449165344, D Loss: 1.3863518238067627\n",
            "Epoch 7, Batch 845, G Loss: 0.6926231980323792, D Loss: 1.3863441944122314\n",
            "Epoch 7, Batch 846, G Loss: 0.6926246285438538, D Loss: 1.3863437175750732\n",
            "Epoch 7, Batch 847, G Loss: 0.6926221251487732, D Loss: 1.3863306045532227\n",
            "Epoch 7, Batch 848, G Loss: 0.6926252245903015, D Loss: 1.386326789855957\n",
            "Epoch 7, Batch 849, G Loss: 0.6926404237747192, D Loss: 1.3863112926483154\n",
            "Epoch 7, Batch 850, G Loss: 0.6926677227020264, D Loss: 1.386324167251587\n",
            "Epoch 7, Batch 851, G Loss: 0.692682147026062, D Loss: 1.3863176107406616\n",
            "Epoch 7, Batch 852, G Loss: 0.692686915397644, D Loss: 1.3862769603729248\n",
            "Epoch 7, Batch 853, G Loss: 0.6926733255386353, D Loss: 1.3862738609313965\n",
            "Epoch 7, Batch 854, G Loss: 0.6926484107971191, D Loss: 1.3862597942352295\n",
            "Epoch 7, Batch 855, G Loss: 0.6926147937774658, D Loss: 1.3862638473510742\n",
            "Epoch 7, Batch 856, G Loss: 0.6925761699676514, D Loss: 1.3862309455871582\n",
            "Epoch 7, Batch 857, G Loss: 0.6925349235534668, D Loss: 1.3862167596817017\n",
            "Epoch 7, Batch 858, G Loss: 0.6924919486045837, D Loss: 1.3862998485565186\n",
            "Epoch 7, Batch 859, G Loss: 0.6924634575843811, D Loss: 1.3862333297729492\n",
            "Epoch 7, Batch 860, G Loss: 0.6924487352371216, D Loss: 1.3861331939697266\n",
            "Epoch 7, Batch 861, G Loss: 0.6924386024475098, D Loss: 1.3862226009368896\n",
            "Epoch 7, Batch 862, G Loss: 0.6924331188201904, D Loss: 1.3861515522003174\n",
            "Epoch 7, Batch 863, G Loss: 0.6924285888671875, D Loss: 1.3861650228500366\n",
            "Epoch 7, Batch 864, G Loss: 0.6924270391464233, D Loss: 1.3861216306686401\n",
            "Epoch 7, Batch 865, G Loss: 0.6924243569374084, D Loss: 1.3862617015838623\n",
            "Epoch 7, Batch 866, G Loss: 0.6924424171447754, D Loss: 1.3862452507019043\n",
            "Epoch 7, Batch 867, G Loss: 0.6924583911895752, D Loss: 1.3861902952194214\n",
            "Epoch 7, Batch 868, G Loss: 0.6924863457679749, D Loss: 1.3859522342681885\n",
            "Epoch 7, Batch 869, G Loss: 0.692503035068512, D Loss: 1.3860441446304321\n",
            "Epoch 7, Batch 870, G Loss: 0.6925188899040222, D Loss: 1.3856267929077148\n",
            "Epoch 7, Batch 871, G Loss: 0.6925128102302551, D Loss: 1.385711431503296\n",
            "Epoch 7, Batch 872, G Loss: 0.6924840211868286, D Loss: 1.3856950998306274\n",
            "Epoch 7, Batch 873, G Loss: 0.6924422979354858, D Loss: 1.3860294818878174\n",
            "Epoch 7, Batch 874, G Loss: 0.6924156546592712, D Loss: 1.386060118675232\n",
            "Epoch 7, Batch 875, G Loss: 0.6924028992652893, D Loss: 1.3860220909118652\n",
            "Epoch 7, Batch 876, G Loss: 0.6923946738243103, D Loss: 1.386075496673584\n",
            "Epoch 7, Batch 877, G Loss: 0.6923985481262207, D Loss: 1.3861229419708252\n",
            "Epoch 7, Batch 878, G Loss: 0.6924262642860413, D Loss: 1.3859573602676392\n",
            "Epoch 7, Batch 879, G Loss: 0.6924271583557129, D Loss: 1.3859376907348633\n",
            "Epoch 7, Batch 880, G Loss: 0.6924581527709961, D Loss: 1.3863625526428223\n",
            "Epoch 7, Batch 881, G Loss: 0.6925011277198792, D Loss: 1.386465072631836\n",
            "Epoch 7, Batch 882, G Loss: 0.69255530834198, D Loss: 1.3864691257476807\n",
            "Epoch 7, Batch 883, G Loss: 0.6926417350769043, D Loss: 1.3862338066101074\n",
            "Epoch 7, Batch 884, G Loss: 0.6927277445793152, D Loss: 1.386059284210205\n",
            "Epoch 7, Batch 885, G Loss: 0.6927980780601501, D Loss: 1.3862271308898926\n",
            "Epoch 7, Batch 886, G Loss: 0.6928941607475281, D Loss: 1.38621985912323\n",
            "Epoch 7, Batch 887, G Loss: 0.6929692625999451, D Loss: 1.3854986429214478\n",
            "Epoch 7, Batch 888, G Loss: 0.6930310130119324, D Loss: 1.3858089447021484\n",
            "Epoch 7, Batch 889, G Loss: 0.693078339099884, D Loss: 1.385831594467163\n",
            "Epoch 7, Batch 890, G Loss: 0.6931126117706299, D Loss: 1.3862264156341553\n",
            "Epoch 7, Batch 891, G Loss: 0.69316565990448, D Loss: 1.3864326477050781\n",
            "Epoch 7, Batch 892, G Loss: 0.6932013630867004, D Loss: 1.3869454860687256\n",
            "Epoch 7, Batch 893, G Loss: 0.6932561993598938, D Loss: 1.3865439891815186\n",
            "Epoch 7, Batch 894, G Loss: 0.6933274269104004, D Loss: 1.3862359523773193\n",
            "Epoch 7, Batch 895, G Loss: 0.6933639049530029, D Loss: 1.3860318660736084\n",
            "Epoch 7, Batch 896, G Loss: 0.693416178226471, D Loss: 1.386136531829834\n",
            "Epoch 7, Batch 897, G Loss: 0.6934475302696228, D Loss: 1.386080026626587\n",
            "Epoch 7, Batch 898, G Loss: 0.6934753656387329, D Loss: 1.3862837553024292\n",
            "Epoch 7, Batch 899, G Loss: 0.6934931874275208, D Loss: 1.3858959674835205\n",
            "Epoch 7, Batch 900, G Loss: 0.6934992074966431, D Loss: 1.3860211372375488\n",
            "Epoch 7, Batch 901, G Loss: 0.6934981346130371, D Loss: 1.386364459991455\n",
            "Epoch 7, Batch 902, G Loss: 0.693498969078064, D Loss: 1.386715054512024\n",
            "Epoch 7, Batch 903, G Loss: 0.693513035774231, D Loss: 1.386723518371582\n",
            "Epoch 7, Batch 904, G Loss: 0.6935302019119263, D Loss: 1.3867812156677246\n",
            "Epoch 7, Batch 905, G Loss: 0.6935475468635559, D Loss: 1.386404275894165\n",
            "Epoch 7, Batch 906, G Loss: 0.6935721635818481, D Loss: 1.3866387605667114\n",
            "Epoch 7, Batch 907, G Loss: 0.6935954689979553, D Loss: 1.386160135269165\n",
            "Epoch 7, Batch 908, G Loss: 0.6936028003692627, D Loss: 1.3868324756622314\n",
            "Epoch 7, Batch 909, G Loss: 0.6936248540878296, D Loss: 1.3868818283081055\n",
            "Epoch 7, Batch 910, G Loss: 0.6936573386192322, D Loss: 1.3861973285675049\n",
            "Epoch 7, Batch 911, G Loss: 0.6936782598495483, D Loss: 1.3860070705413818\n",
            "Epoch 7, Batch 912, G Loss: 0.6936802864074707, D Loss: 1.386157512664795\n",
            "Epoch 7, Batch 913, G Loss: 0.6936798095703125, D Loss: 1.3857753276824951\n",
            "Epoch 7, Batch 914, G Loss: 0.6936612725257874, D Loss: 1.3858423233032227\n",
            "Epoch 7, Batch 915, G Loss: 0.6936110854148865, D Loss: 1.3866543769836426\n",
            "Epoch 7, Batch 916, G Loss: 0.6935867071151733, D Loss: 1.3870484828948975\n",
            "Epoch 7, Batch 917, G Loss: 0.6935794353485107, D Loss: 1.387083888053894\n",
            "Epoch 7, Batch 918, G Loss: 0.6935846209526062, D Loss: 1.3871533870697021\n",
            "Epoch 7, Batch 919, G Loss: 0.6936039924621582, D Loss: 1.3868224620819092\n",
            "Epoch 7, Batch 920, G Loss: 0.6936494708061218, D Loss: 1.3860070705413818\n",
            "Epoch 7, Batch 921, G Loss: 0.6936699748039246, D Loss: 1.3862879276275635\n",
            "Epoch 7, Batch 922, G Loss: 0.6936880350112915, D Loss: 1.3862568140029907\n",
            "Epoch 7, Batch 923, G Loss: 0.6936855912208557, D Loss: 1.3864628076553345\n",
            "Epoch 7, Batch 924, G Loss: 0.6936838626861572, D Loss: 1.3865015506744385\n",
            "Epoch 7, Batch 925, G Loss: 0.6936885118484497, D Loss: 1.3863598108291626\n",
            "Epoch 7, Batch 926, G Loss: 0.6936787962913513, D Loss: 1.386267066001892\n",
            "Epoch 7, Batch 927, G Loss: 0.6936668157577515, D Loss: 1.385925531387329\n",
            "Epoch 7, Batch 928, G Loss: 0.6936424374580383, D Loss: 1.3849940299987793\n",
            "Epoch 7, Batch 929, G Loss: 0.6935555338859558, D Loss: 1.3843309879302979\n",
            "Epoch 7, Batch 930, G Loss: 0.6933852434158325, D Loss: 1.3857951164245605\n",
            "Epoch 7, Batch 931, G Loss: 0.6932138204574585, D Loss: 1.386725664138794\n",
            "Epoch 7, Batch 932, G Loss: 0.6930908560752869, D Loss: 1.386413812637329\n",
            "Epoch 7, Batch 933, G Loss: 0.6929700970649719, D Loss: 1.3861308097839355\n",
            "Epoch 7, Batch 934, G Loss: 0.6928703188896179, D Loss: 1.3864142894744873\n",
            "Epoch 7, Batch 935, G Loss: 0.6927894353866577, D Loss: 1.3861137628555298\n",
            "Epoch 7, Batch 936, G Loss: 0.6927222609519958, D Loss: 1.3859691619873047\n",
            "Epoch 7, Batch 937, G Loss: 0.6926655173301697, D Loss: 1.3869194984436035\n",
            "Epoch 7, Batch 938, G Loss: 0.6926473379135132, D Loss: 1.386838674545288\n",
            "Epoch 8, Batch 1, G Loss: 0.6926724314689636, D Loss: 1.3865296840667725\n",
            "Epoch 8, Batch 2, G Loss: 0.692734956741333, D Loss: 1.3864854574203491\n",
            "Epoch 8, Batch 3, G Loss: 0.6927952170372009, D Loss: 1.3865954875946045\n",
            "Epoch 8, Batch 4, G Loss: 0.6928592324256897, D Loss: 1.3863550424575806\n",
            "Epoch 8, Batch 5, G Loss: 0.6929476261138916, D Loss: 1.386671543121338\n",
            "Epoch 8, Batch 6, G Loss: 0.6930414438247681, D Loss: 1.3865723609924316\n",
            "Epoch 8, Batch 7, G Loss: 0.6931474804878235, D Loss: 1.3866639137268066\n",
            "Epoch 8, Batch 8, G Loss: 0.6932559609413147, D Loss: 1.3866448402404785\n",
            "Epoch 8, Batch 9, G Loss: 0.693381667137146, D Loss: 1.3864980936050415\n",
            "Epoch 8, Batch 10, G Loss: 0.6934864521026611, D Loss: 1.3867285251617432\n",
            "Epoch 8, Batch 11, G Loss: 0.6936129927635193, D Loss: 1.3864648342132568\n",
            "Epoch 8, Batch 12, G Loss: 0.693719744682312, D Loss: 1.3861961364746094\n",
            "Epoch 8, Batch 13, G Loss: 0.6938052177429199, D Loss: 1.3863918781280518\n",
            "Epoch 8, Batch 14, G Loss: 0.6938766837120056, D Loss: 1.3863401412963867\n",
            "Epoch 8, Batch 15, G Loss: 0.6939353942871094, D Loss: 1.3864316940307617\n",
            "Epoch 8, Batch 16, G Loss: 0.6939802169799805, D Loss: 1.3865811824798584\n",
            "Epoch 8, Batch 17, G Loss: 0.6940246820449829, D Loss: 1.3867528438568115\n",
            "Epoch 8, Batch 18, G Loss: 0.6940772533416748, D Loss: 1.3866454362869263\n",
            "Epoch 8, Batch 19, G Loss: 0.6941297054290771, D Loss: 1.3863030672073364\n",
            "Epoch 8, Batch 20, G Loss: 0.6941655874252319, D Loss: 1.385988712310791\n",
            "Epoch 8, Batch 21, G Loss: 0.6941563487052917, D Loss: 1.3861205577850342\n",
            "Epoch 8, Batch 22, G Loss: 0.6941202282905579, D Loss: 1.3859102725982666\n",
            "Epoch 8, Batch 23, G Loss: 0.6940335631370544, D Loss: 1.3864514827728271\n",
            "Epoch 8, Batch 24, G Loss: 0.6939572095870972, D Loss: 1.3864960670471191\n",
            "Epoch 8, Batch 25, G Loss: 0.6938966512680054, D Loss: 1.3864693641662598\n",
            "Epoch 8, Batch 26, G Loss: 0.6938406229019165, D Loss: 1.3865175247192383\n",
            "Epoch 8, Batch 27, G Loss: 0.693805456161499, D Loss: 1.3864977359771729\n",
            "Epoch 8, Batch 28, G Loss: 0.6937804222106934, D Loss: 1.38644540309906\n",
            "Epoch 8, Batch 29, G Loss: 0.6937611103057861, D Loss: 1.3864665031433105\n",
            "Epoch 8, Batch 30, G Loss: 0.6937570571899414, D Loss: 1.3863427639007568\n",
            "Epoch 8, Batch 31, G Loss: 0.693749725818634, D Loss: 1.3862411975860596\n",
            "Epoch 8, Batch 32, G Loss: 0.6937311887741089, D Loss: 1.3862037658691406\n",
            "Epoch 8, Batch 33, G Loss: 0.6936920881271362, D Loss: 1.3862855434417725\n",
            "Epoch 8, Batch 34, G Loss: 0.6936488151550293, D Loss: 1.3863284587860107\n",
            "Epoch 8, Batch 35, G Loss: 0.6936128735542297, D Loss: 1.386354923248291\n",
            "Epoch 8, Batch 36, G Loss: 0.6935898065567017, D Loss: 1.386258840560913\n",
            "Epoch 8, Batch 37, G Loss: 0.6935591697692871, D Loss: 1.3863253593444824\n",
            "Epoch 8, Batch 38, G Loss: 0.6935431957244873, D Loss: 1.3863205909729004\n",
            "Epoch 8, Batch 39, G Loss: 0.6935373544692993, D Loss: 1.3863041400909424\n",
            "Epoch 8, Batch 40, G Loss: 0.6935379505157471, D Loss: 1.386260747909546\n",
            "Epoch 8, Batch 41, G Loss: 0.6935374736785889, D Loss: 1.3862756490707397\n",
            "Epoch 8, Batch 42, G Loss: 0.6935411691665649, D Loss: 1.3862419128417969\n",
            "Epoch 8, Batch 43, G Loss: 0.6935393810272217, D Loss: 1.3862812519073486\n",
            "Epoch 8, Batch 44, G Loss: 0.6935505867004395, D Loss: 1.3862571716308594\n",
            "Epoch 8, Batch 45, G Loss: 0.6935669183731079, D Loss: 1.3862406015396118\n",
            "Epoch 8, Batch 46, G Loss: 0.6935732364654541, D Loss: 1.386237621307373\n",
            "Epoch 8, Batch 47, G Loss: 0.6935850381851196, D Loss: 1.3862223625183105\n",
            "Epoch 8, Batch 48, G Loss: 0.6936184763908386, D Loss: 1.3862130641937256\n",
            "Epoch 8, Batch 49, G Loss: 0.6936550736427307, D Loss: 1.3861840963363647\n",
            "Epoch 8, Batch 50, G Loss: 0.6937064528465271, D Loss: 1.386167287826538\n",
            "Epoch 8, Batch 51, G Loss: 0.6937703490257263, D Loss: 1.3861627578735352\n",
            "Epoch 8, Batch 52, G Loss: 0.6938393115997314, D Loss: 1.3861310482025146\n",
            "Epoch 8, Batch 53, G Loss: 0.6939181685447693, D Loss: 1.386179804801941\n",
            "Epoch 8, Batch 54, G Loss: 0.6939870119094849, D Loss: 1.3860690593719482\n",
            "Epoch 8, Batch 55, G Loss: 0.6940658092498779, D Loss: 1.386054515838623\n",
            "Epoch 8, Batch 56, G Loss: 0.6941508650779724, D Loss: 1.386117935180664\n",
            "Epoch 8, Batch 57, G Loss: 0.6942126154899597, D Loss: 1.386213779449463\n",
            "Epoch 8, Batch 58, G Loss: 0.694250762462616, D Loss: 1.3863048553466797\n",
            "Epoch 8, Batch 59, G Loss: 0.6942489147186279, D Loss: 1.386070728302002\n",
            "Epoch 8, Batch 60, G Loss: 0.6942442059516907, D Loss: 1.3860540390014648\n",
            "Epoch 8, Batch 61, G Loss: 0.6942306756973267, D Loss: 1.3859937191009521\n",
            "Epoch 8, Batch 62, G Loss: 0.6942174434661865, D Loss: 1.3860336542129517\n",
            "Epoch 8, Batch 63, G Loss: 0.6942000985145569, D Loss: 1.386016607284546\n",
            "Epoch 8, Batch 64, G Loss: 0.6941739916801453, D Loss: 1.3862004280090332\n",
            "Epoch 8, Batch 65, G Loss: 0.6941437721252441, D Loss: 1.386012077331543\n",
            "Epoch 8, Batch 66, G Loss: 0.6941054463386536, D Loss: 1.3859374523162842\n",
            "Epoch 8, Batch 67, G Loss: 0.6940742135047913, D Loss: 1.3861377239227295\n",
            "Epoch 8, Batch 68, G Loss: 0.6940227150917053, D Loss: 1.385982871055603\n",
            "Epoch 8, Batch 69, G Loss: 0.6939862370491028, D Loss: 1.3857944011688232\n",
            "Epoch 8, Batch 70, G Loss: 0.6939648985862732, D Loss: 1.3859705924987793\n",
            "Epoch 8, Batch 71, G Loss: 0.6939336061477661, D Loss: 1.3864638805389404\n",
            "Epoch 8, Batch 72, G Loss: 0.6938839554786682, D Loss: 1.3861069679260254\n",
            "Epoch 8, Batch 73, G Loss: 0.6938199400901794, D Loss: 1.3864043951034546\n",
            "Epoch 8, Batch 74, G Loss: 0.6937487125396729, D Loss: 1.3866400718688965\n",
            "Epoch 8, Batch 75, G Loss: 0.6936187148094177, D Loss: 1.3862017393112183\n",
            "Epoch 8, Batch 76, G Loss: 0.6935110092163086, D Loss: 1.3863650560379028\n",
            "Epoch 8, Batch 77, G Loss: 0.693382740020752, D Loss: 1.3863246440887451\n",
            "Epoch 8, Batch 78, G Loss: 0.6932688355445862, D Loss: 1.385887861251831\n",
            "Epoch 8, Batch 79, G Loss: 0.6931735277175903, D Loss: 1.3861525058746338\n",
            "Epoch 8, Batch 80, G Loss: 0.6930977702140808, D Loss: 1.386354923248291\n",
            "Epoch 8, Batch 81, G Loss: 0.6930168271064758, D Loss: 1.3863320350646973\n",
            "Epoch 8, Batch 82, G Loss: 0.6929362416267395, D Loss: 1.3862972259521484\n",
            "Epoch 8, Batch 83, G Loss: 0.6928621530532837, D Loss: 1.3862195014953613\n",
            "Epoch 8, Batch 84, G Loss: 0.6927953958511353, D Loss: 1.386667251586914\n",
            "Epoch 8, Batch 85, G Loss: 0.692707359790802, D Loss: 1.3866174221038818\n",
            "Epoch 8, Batch 86, G Loss: 0.6926167011260986, D Loss: 1.3866113424301147\n",
            "Epoch 8, Batch 87, G Loss: 0.6925209760665894, D Loss: 1.3863976001739502\n",
            "Epoch 8, Batch 88, G Loss: 0.6924440860748291, D Loss: 1.3863451480865479\n",
            "Epoch 8, Batch 89, G Loss: 0.6923655271530151, D Loss: 1.386256456375122\n",
            "Epoch 8, Batch 90, G Loss: 0.6923097968101501, D Loss: 1.3863894939422607\n",
            "Epoch 8, Batch 91, G Loss: 0.6922851800918579, D Loss: 1.3861230611801147\n",
            "Epoch 8, Batch 92, G Loss: 0.692284107208252, D Loss: 1.3860830068588257\n",
            "Epoch 8, Batch 93, G Loss: 0.6923192143440247, D Loss: 1.3861804008483887\n",
            "Epoch 8, Batch 94, G Loss: 0.6923527717590332, D Loss: 1.3868954181671143\n",
            "Epoch 8, Batch 95, G Loss: 0.6923680305480957, D Loss: 1.3868927955627441\n",
            "Epoch 8, Batch 96, G Loss: 0.692348062992096, D Loss: 1.3868720531463623\n",
            "Epoch 8, Batch 97, G Loss: 0.6923118233680725, D Loss: 1.3867237567901611\n",
            "Epoch 8, Batch 98, G Loss: 0.6922510862350464, D Loss: 1.3864833116531372\n",
            "Epoch 8, Batch 99, G Loss: 0.6922070980072021, D Loss: 1.3865302801132202\n",
            "Epoch 8, Batch 100, G Loss: 0.6921668648719788, D Loss: 1.386495590209961\n",
            "Epoch 8, Batch 101, G Loss: 0.692145824432373, D Loss: 1.386397123336792\n",
            "Epoch 8, Batch 102, G Loss: 0.692126989364624, D Loss: 1.3862555027008057\n",
            "Epoch 8, Batch 103, G Loss: 0.6921374797821045, D Loss: 1.3861404657363892\n",
            "Epoch 8, Batch 104, G Loss: 0.6921821236610413, D Loss: 1.3863614797592163\n",
            "Epoch 8, Batch 105, G Loss: 0.6922366619110107, D Loss: 1.3859567642211914\n",
            "Epoch 8, Batch 106, G Loss: 0.6923255324363708, D Loss: 1.3860266208648682\n",
            "Epoch 8, Batch 107, G Loss: 0.6924539804458618, D Loss: 1.3859440088272095\n",
            "Epoch 8, Batch 108, G Loss: 0.6926262378692627, D Loss: 1.3863933086395264\n",
            "Epoch 8, Batch 109, G Loss: 0.692772388458252, D Loss: 1.3864713907241821\n",
            "Epoch 8, Batch 110, G Loss: 0.692877471446991, D Loss: 1.3863232135772705\n",
            "Epoch 8, Batch 111, G Loss: 0.6929899454116821, D Loss: 1.386138677597046\n",
            "Epoch 8, Batch 112, G Loss: 0.6930997967720032, D Loss: 1.3861520290374756\n",
            "Epoch 8, Batch 113, G Loss: 0.6932153701782227, D Loss: 1.3861010074615479\n",
            "Epoch 8, Batch 114, G Loss: 0.6933439373970032, D Loss: 1.3861558437347412\n",
            "Epoch 8, Batch 115, G Loss: 0.6934512257575989, D Loss: 1.385941982269287\n",
            "Epoch 8, Batch 116, G Loss: 0.6935826539993286, D Loss: 1.3860520124435425\n",
            "Epoch 8, Batch 117, G Loss: 0.6937174797058105, D Loss: 1.3861725330352783\n",
            "Epoch 8, Batch 118, G Loss: 0.6938369870185852, D Loss: 1.3861262798309326\n",
            "Epoch 8, Batch 119, G Loss: 0.6939377784729004, D Loss: 1.3861432075500488\n",
            "Epoch 8, Batch 120, G Loss: 0.6940256357192993, D Loss: 1.3863239288330078\n",
            "Epoch 8, Batch 121, G Loss: 0.6940864324569702, D Loss: 1.3862786293029785\n",
            "Epoch 8, Batch 122, G Loss: 0.6941177248954773, D Loss: 1.3862978219985962\n",
            "Epoch 8, Batch 123, G Loss: 0.6941084861755371, D Loss: 1.38645339012146\n",
            "Epoch 8, Batch 124, G Loss: 0.6940712332725525, D Loss: 1.3866946697235107\n",
            "Epoch 8, Batch 125, G Loss: 0.6939760446548462, D Loss: 1.3866243362426758\n",
            "Epoch 8, Batch 126, G Loss: 0.6938454508781433, D Loss: 1.3865983486175537\n",
            "Epoch 8, Batch 127, G Loss: 0.6936821341514587, D Loss: 1.3864859342575073\n",
            "Epoch 8, Batch 128, G Loss: 0.6935049295425415, D Loss: 1.3861351013183594\n",
            "Epoch 8, Batch 129, G Loss: 0.6933506727218628, D Loss: 1.386191725730896\n",
            "Epoch 8, Batch 130, G Loss: 0.6932131052017212, D Loss: 1.3865258693695068\n",
            "Epoch 8, Batch 131, G Loss: 0.6930633783340454, D Loss: 1.3864339590072632\n",
            "Epoch 8, Batch 132, G Loss: 0.6929196715354919, D Loss: 1.3863605260849\n",
            "Epoch 8, Batch 133, G Loss: 0.6927826404571533, D Loss: 1.3868768215179443\n",
            "Epoch 8, Batch 134, G Loss: 0.6926110982894897, D Loss: 1.3866932392120361\n",
            "Epoch 8, Batch 135, G Loss: 0.692437469959259, D Loss: 1.3866016864776611\n",
            "Epoch 8, Batch 136, G Loss: 0.6922518014907837, D Loss: 1.386624813079834\n",
            "Epoch 8, Batch 137, G Loss: 0.6920613050460815, D Loss: 1.3864226341247559\n",
            "Epoch 8, Batch 138, G Loss: 0.6919069886207581, D Loss: 1.3863686323165894\n",
            "Epoch 8, Batch 139, G Loss: 0.6917790174484253, D Loss: 1.3862990140914917\n",
            "Epoch 8, Batch 140, G Loss: 0.6916933655738831, D Loss: 1.3863729238510132\n",
            "Epoch 8, Batch 141, G Loss: 0.6916369795799255, D Loss: 1.3863232135772705\n",
            "Epoch 8, Batch 142, G Loss: 0.6916207075119019, D Loss: 1.3863266706466675\n",
            "Epoch 8, Batch 143, G Loss: 0.691624641418457, D Loss: 1.386284589767456\n",
            "Epoch 8, Batch 144, G Loss: 0.6916751265525818, D Loss: 1.386296033859253\n",
            "Epoch 8, Batch 145, G Loss: 0.6917538046836853, D Loss: 1.386289119720459\n",
            "Epoch 8, Batch 146, G Loss: 0.6918737292289734, D Loss: 1.386317491531372\n",
            "Epoch 8, Batch 147, G Loss: 0.6919940114021301, D Loss: 1.386309027671814\n",
            "Epoch 8, Batch 148, G Loss: 0.6921143531799316, D Loss: 1.3863016366958618\n",
            "Epoch 8, Batch 149, G Loss: 0.6922277212142944, D Loss: 1.386291265487671\n",
            "Epoch 8, Batch 150, G Loss: 0.6923243999481201, D Loss: 1.3863284587860107\n",
            "Epoch 8, Batch 151, G Loss: 0.6924349665641785, D Loss: 1.3863096237182617\n",
            "Epoch 8, Batch 152, G Loss: 0.6925450563430786, D Loss: 1.3862638473510742\n",
            "Epoch 8, Batch 153, G Loss: 0.6926379203796387, D Loss: 1.386263370513916\n",
            "Epoch 8, Batch 154, G Loss: 0.6927189826965332, D Loss: 1.386275291442871\n",
            "Epoch 8, Batch 155, G Loss: 0.6927960515022278, D Loss: 1.386277437210083\n",
            "Epoch 8, Batch 156, G Loss: 0.6928673386573792, D Loss: 1.386286973953247\n",
            "Epoch 8, Batch 157, G Loss: 0.6929352283477783, D Loss: 1.3862048387527466\n",
            "Epoch 8, Batch 158, G Loss: 0.6929821372032166, D Loss: 1.3862979412078857\n",
            "Epoch 8, Batch 159, G Loss: 0.6930272579193115, D Loss: 1.386364459991455\n",
            "Epoch 8, Batch 160, G Loss: 0.6930775046348572, D Loss: 1.385970950126648\n",
            "Epoch 8, Batch 161, G Loss: 0.6930742859840393, D Loss: 1.3859034776687622\n",
            "Epoch 8, Batch 162, G Loss: 0.6930214762687683, D Loss: 1.3860498666763306\n",
            "Epoch 8, Batch 163, G Loss: 0.6929479837417603, D Loss: 1.3860843181610107\n",
            "Epoch 8, Batch 164, G Loss: 0.6928659677505493, D Loss: 1.385987401008606\n",
            "Epoch 8, Batch 165, G Loss: 0.692787766456604, D Loss: 1.3860762119293213\n",
            "Epoch 8, Batch 166, G Loss: 0.6926987171173096, D Loss: 1.386175513267517\n",
            "Epoch 8, Batch 167, G Loss: 0.692625880241394, D Loss: 1.3864119052886963\n",
            "Epoch 8, Batch 168, G Loss: 0.692582368850708, D Loss: 1.3862481117248535\n",
            "Epoch 8, Batch 169, G Loss: 0.6925644278526306, D Loss: 1.386091709136963\n",
            "Epoch 8, Batch 170, G Loss: 0.6925467848777771, D Loss: 1.386063814163208\n",
            "Epoch 8, Batch 171, G Loss: 0.6925323605537415, D Loss: 1.3861231803894043\n",
            "Epoch 8, Batch 172, G Loss: 0.692540168762207, D Loss: 1.3858959674835205\n",
            "Epoch 8, Batch 173, G Loss: 0.6925313472747803, D Loss: 1.385824203491211\n",
            "Epoch 8, Batch 174, G Loss: 0.6925268173217773, D Loss: 1.3861854076385498\n",
            "Epoch 8, Batch 175, G Loss: 0.6925237774848938, D Loss: 1.3860859870910645\n",
            "Epoch 8, Batch 176, G Loss: 0.6925356984138489, D Loss: 1.386152982711792\n",
            "Epoch 8, Batch 177, G Loss: 0.6925540566444397, D Loss: 1.3862966299057007\n",
            "Epoch 8, Batch 178, G Loss: 0.6925961375236511, D Loss: 1.386106252670288\n",
            "Epoch 8, Batch 179, G Loss: 0.6926338076591492, D Loss: 1.3862384557724\n",
            "Epoch 8, Batch 180, G Loss: 0.6926891803741455, D Loss: 1.3862332105636597\n",
            "Epoch 8, Batch 181, G Loss: 0.6927348375320435, D Loss: 1.3862717151641846\n",
            "Epoch 8, Batch 182, G Loss: 0.692790150642395, D Loss: 1.3862539529800415\n",
            "Epoch 8, Batch 183, G Loss: 0.6928489804267883, D Loss: 1.3862042427062988\n",
            "Epoch 8, Batch 184, G Loss: 0.6929092407226562, D Loss: 1.3863554000854492\n",
            "Epoch 8, Batch 185, G Loss: 0.692981481552124, D Loss: 1.3862347602844238\n",
            "Epoch 8, Batch 186, G Loss: 0.6930564641952515, D Loss: 1.386266827583313\n",
            "Epoch 8, Batch 187, G Loss: 0.6931182146072388, D Loss: 1.3864026069641113\n",
            "Epoch 8, Batch 188, G Loss: 0.6931838989257812, D Loss: 1.3856921195983887\n",
            "Epoch 8, Batch 189, G Loss: 0.6932282447814941, D Loss: 1.3860442638397217\n",
            "Epoch 8, Batch 190, G Loss: 0.693265974521637, D Loss: 1.3860645294189453\n",
            "Epoch 8, Batch 191, G Loss: 0.6933119893074036, D Loss: 1.3859107494354248\n",
            "Epoch 8, Batch 192, G Loss: 0.6933085918426514, D Loss: 1.385849952697754\n",
            "Epoch 8, Batch 193, G Loss: 0.6933121681213379, D Loss: 1.386070966720581\n",
            "Epoch 8, Batch 194, G Loss: 0.6933145523071289, D Loss: 1.3860435485839844\n",
            "Epoch 8, Batch 195, G Loss: 0.6933057904243469, D Loss: 1.3858208656311035\n",
            "Epoch 8, Batch 196, G Loss: 0.6932873129844666, D Loss: 1.3859516382217407\n",
            "Epoch 8, Batch 197, G Loss: 0.6932576894760132, D Loss: 1.3859102725982666\n",
            "Epoch 8, Batch 198, G Loss: 0.6932433247566223, D Loss: 1.385894775390625\n",
            "Epoch 8, Batch 199, G Loss: 0.6932044625282288, D Loss: 1.3861539363861084\n",
            "Epoch 8, Batch 200, G Loss: 0.6931795477867126, D Loss: 1.3865959644317627\n",
            "Epoch 8, Batch 201, G Loss: 0.6931750178337097, D Loss: 1.3869247436523438\n",
            "Epoch 8, Batch 202, G Loss: 0.6931647658348083, D Loss: 1.3862136602401733\n",
            "Epoch 8, Batch 203, G Loss: 0.6931807994842529, D Loss: 1.3855915069580078\n",
            "Epoch 8, Batch 204, G Loss: 0.6931770443916321, D Loss: 1.38568115234375\n",
            "Epoch 8, Batch 205, G Loss: 0.693180501461029, D Loss: 1.3851923942565918\n",
            "Epoch 8, Batch 206, G Loss: 0.693143904209137, D Loss: 1.3860232830047607\n",
            "Epoch 8, Batch 207, G Loss: 0.6931191086769104, D Loss: 1.385859489440918\n",
            "Epoch 8, Batch 208, G Loss: 0.6930791735649109, D Loss: 1.386073350906372\n",
            "Epoch 8, Batch 209, G Loss: 0.6930803656578064, D Loss: 1.3860433101654053\n",
            "Epoch 8, Batch 210, G Loss: 0.6930666565895081, D Loss: 1.3862152099609375\n",
            "Epoch 8, Batch 211, G Loss: 0.6930596232414246, D Loss: 1.385972499847412\n",
            "Epoch 8, Batch 212, G Loss: 0.6930579543113708, D Loss: 1.3855905532836914\n",
            "Epoch 8, Batch 213, G Loss: 0.69305819272995, D Loss: 1.386087417602539\n",
            "Epoch 8, Batch 214, G Loss: 0.6930539608001709, D Loss: 1.3869521617889404\n",
            "Epoch 8, Batch 215, G Loss: 0.6930735111236572, D Loss: 1.3873095512390137\n",
            "Epoch 8, Batch 216, G Loss: 0.6931252479553223, D Loss: 1.3868937492370605\n",
            "Epoch 8, Batch 217, G Loss: 0.6931926012039185, D Loss: 1.3865325450897217\n",
            "Epoch 8, Batch 218, G Loss: 0.6932530403137207, D Loss: 1.3865453004837036\n",
            "Epoch 8, Batch 219, G Loss: 0.6933298110961914, D Loss: 1.3869078159332275\n",
            "Epoch 8, Batch 220, G Loss: 0.6934242844581604, D Loss: 1.3857884407043457\n",
            "Epoch 8, Batch 221, G Loss: 0.6934826970100403, D Loss: 1.3858933448791504\n",
            "Epoch 8, Batch 222, G Loss: 0.6935389041900635, D Loss: 1.3856992721557617\n",
            "Epoch 8, Batch 223, G Loss: 0.6935603022575378, D Loss: 1.3860647678375244\n",
            "Epoch 8, Batch 224, G Loss: 0.6935776472091675, D Loss: 1.3860911130905151\n",
            "Epoch 8, Batch 225, G Loss: 0.6935839056968689, D Loss: 1.3863525390625\n",
            "Epoch 8, Batch 226, G Loss: 0.6935940384864807, D Loss: 1.3867391347885132\n",
            "Epoch 8, Batch 227, G Loss: 0.6936096549034119, D Loss: 1.3869619369506836\n",
            "Epoch 8, Batch 228, G Loss: 0.6936418414115906, D Loss: 1.387075662612915\n",
            "Epoch 8, Batch 229, G Loss: 0.6936746835708618, D Loss: 1.3868238925933838\n",
            "Epoch 8, Batch 230, G Loss: 0.6937168836593628, D Loss: 1.386443853378296\n",
            "Epoch 8, Batch 231, G Loss: 0.6937627196311951, D Loss: 1.3867924213409424\n",
            "Epoch 8, Batch 232, G Loss: 0.6937873959541321, D Loss: 1.3867096900939941\n",
            "Epoch 8, Batch 233, G Loss: 0.6938372254371643, D Loss: 1.386764645576477\n",
            "Epoch 8, Batch 234, G Loss: 0.6938762068748474, D Loss: 1.3867805004119873\n",
            "Epoch 8, Batch 235, G Loss: 0.6939182877540588, D Loss: 1.3871073722839355\n",
            "Epoch 8, Batch 236, G Loss: 0.6939818859100342, D Loss: 1.3866474628448486\n",
            "Epoch 8, Batch 237, G Loss: 0.694031834602356, D Loss: 1.3868882656097412\n",
            "Epoch 8, Batch 238, G Loss: 0.6940800547599792, D Loss: 1.387017846107483\n",
            "Epoch 8, Batch 239, G Loss: 0.6941424608230591, D Loss: 1.3868223428726196\n",
            "Epoch 8, Batch 240, G Loss: 0.6941903829574585, D Loss: 1.3869109153747559\n",
            "Epoch 8, Batch 241, G Loss: 0.6942562460899353, D Loss: 1.3869308233261108\n",
            "Epoch 8, Batch 242, G Loss: 0.694324791431427, D Loss: 1.3868621587753296\n",
            "Epoch 8, Batch 243, G Loss: 0.6943943500518799, D Loss: 1.3866472244262695\n",
            "Epoch 8, Batch 244, G Loss: 0.6944511532783508, D Loss: 1.3866653442382812\n",
            "Epoch 8, Batch 245, G Loss: 0.6945050358772278, D Loss: 1.3865532875061035\n",
            "Epoch 8, Batch 246, G Loss: 0.6945394277572632, D Loss: 1.3865517377853394\n",
            "Epoch 8, Batch 247, G Loss: 0.6945690512657166, D Loss: 1.3865010738372803\n",
            "Epoch 8, Batch 248, G Loss: 0.6945922374725342, D Loss: 1.3865461349487305\n",
            "Epoch 8, Batch 249, G Loss: 0.6946064829826355, D Loss: 1.386352777481079\n",
            "Epoch 8, Batch 250, G Loss: 0.6945980787277222, D Loss: 1.3864562511444092\n",
            "Epoch 8, Batch 251, G Loss: 0.6945911049842834, D Loss: 1.3863427639007568\n",
            "Epoch 8, Batch 252, G Loss: 0.6945649981498718, D Loss: 1.3862762451171875\n",
            "Epoch 8, Batch 253, G Loss: 0.6945143938064575, D Loss: 1.3862791061401367\n",
            "Epoch 8, Batch 254, G Loss: 0.694444477558136, D Loss: 1.3862953186035156\n",
            "Epoch 8, Batch 255, G Loss: 0.6943650245666504, D Loss: 1.386289358139038\n",
            "Epoch 8, Batch 256, G Loss: 0.6942893862724304, D Loss: 1.3862662315368652\n",
            "Epoch 8, Batch 257, G Loss: 0.6942144632339478, D Loss: 1.3862428665161133\n",
            "Epoch 8, Batch 258, G Loss: 0.6941492557525635, D Loss: 1.386223554611206\n",
            "Epoch 8, Batch 259, G Loss: 0.6940944194793701, D Loss: 1.3861960172653198\n",
            "Epoch 8, Batch 260, G Loss: 0.6940451860427856, D Loss: 1.38614821434021\n",
            "Epoch 8, Batch 261, G Loss: 0.6940110921859741, D Loss: 1.3861117362976074\n",
            "Epoch 8, Batch 262, G Loss: 0.6939946413040161, D Loss: 1.386110544204712\n",
            "Epoch 8, Batch 263, G Loss: 0.693993091583252, D Loss: 1.3860843181610107\n",
            "Epoch 8, Batch 264, G Loss: 0.6939956545829773, D Loss: 1.3859840631484985\n",
            "Epoch 8, Batch 265, G Loss: 0.6940078735351562, D Loss: 1.3859939575195312\n",
            "Epoch 8, Batch 266, G Loss: 0.6940427422523499, D Loss: 1.3860453367233276\n",
            "Epoch 8, Batch 267, G Loss: 0.6940770149230957, D Loss: 1.3859100341796875\n",
            "Epoch 8, Batch 268, G Loss: 0.6941169500350952, D Loss: 1.3861103057861328\n",
            "Epoch 8, Batch 269, G Loss: 0.6941385269165039, D Loss: 1.386175274848938\n",
            "Epoch 8, Batch 270, G Loss: 0.6941413879394531, D Loss: 1.3860771656036377\n",
            "Epoch 8, Batch 271, G Loss: 0.6941373944282532, D Loss: 1.3862903118133545\n",
            "Epoch 8, Batch 272, G Loss: 0.6940889954566956, D Loss: 1.3858110904693604\n",
            "Epoch 8, Batch 273, G Loss: 0.694062352180481, D Loss: 1.3858022689819336\n",
            "Epoch 8, Batch 274, G Loss: 0.6940488219261169, D Loss: 1.3858575820922852\n",
            "Epoch 8, Batch 275, G Loss: 0.6940498352050781, D Loss: 1.3855698108673096\n",
            "Epoch 8, Batch 276, G Loss: 0.6940630674362183, D Loss: 1.3855704069137573\n",
            "Epoch 8, Batch 277, G Loss: 0.6940922737121582, D Loss: 1.3855900764465332\n",
            "Epoch 8, Batch 278, G Loss: 0.6941418051719666, D Loss: 1.385565996170044\n",
            "Epoch 8, Batch 279, G Loss: 0.6942073106765747, D Loss: 1.3854424953460693\n",
            "Epoch 8, Batch 280, G Loss: 0.6942614912986755, D Loss: 1.3854444026947021\n",
            "Epoch 8, Batch 281, G Loss: 0.6943493485450745, D Loss: 1.3859680891036987\n",
            "Epoch 8, Batch 282, G Loss: 0.6943987011909485, D Loss: 1.3857533931732178\n",
            "Epoch 8, Batch 283, G Loss: 0.6944485306739807, D Loss: 1.3855962753295898\n",
            "Epoch 8, Batch 284, G Loss: 0.6944664716720581, D Loss: 1.3859633207321167\n",
            "Epoch 8, Batch 285, G Loss: 0.6944930553436279, D Loss: 1.3860483169555664\n",
            "Epoch 8, Batch 286, G Loss: 0.694465696811676, D Loss: 1.3857600688934326\n",
            "Epoch 8, Batch 287, G Loss: 0.6944355368614197, D Loss: 1.385439395904541\n",
            "Epoch 8, Batch 288, G Loss: 0.694422721862793, D Loss: 1.3857996463775635\n",
            "Epoch 8, Batch 289, G Loss: 0.6943774223327637, D Loss: 1.3855557441711426\n",
            "Epoch 8, Batch 290, G Loss: 0.6943590044975281, D Loss: 1.3859212398529053\n",
            "Epoch 8, Batch 291, G Loss: 0.6943022012710571, D Loss: 1.385864019393921\n",
            "Epoch 8, Batch 292, G Loss: 0.6942354440689087, D Loss: 1.386022686958313\n",
            "Epoch 8, Batch 293, G Loss: 0.6941724419593811, D Loss: 1.3857364654541016\n",
            "Epoch 8, Batch 294, G Loss: 0.6940768361091614, D Loss: 1.3857858180999756\n",
            "Epoch 8, Batch 295, G Loss: 0.6940138936042786, D Loss: 1.3857057094573975\n",
            "Epoch 8, Batch 296, G Loss: 0.693930983543396, D Loss: 1.3860728740692139\n",
            "Epoch 8, Batch 297, G Loss: 0.6938408613204956, D Loss: 1.3858401775360107\n",
            "Epoch 8, Batch 298, G Loss: 0.6937772631645203, D Loss: 1.3853020668029785\n",
            "Epoch 8, Batch 299, G Loss: 0.6937115788459778, D Loss: 1.3856301307678223\n",
            "Epoch 8, Batch 300, G Loss: 0.6937034726142883, D Loss: 1.3856236934661865\n",
            "Epoch 8, Batch 301, G Loss: 0.6936792135238647, D Loss: 1.3875268697738647\n",
            "Epoch 8, Batch 302, G Loss: 0.693574070930481, D Loss: 1.3864953517913818\n",
            "Epoch 8, Batch 303, G Loss: 0.6934438943862915, D Loss: 1.387427806854248\n",
            "Epoch 8, Batch 304, G Loss: 0.693305253982544, D Loss: 1.3870689868927002\n",
            "Epoch 8, Batch 305, G Loss: 0.6931247115135193, D Loss: 1.387190341949463\n",
            "Epoch 8, Batch 306, G Loss: 0.6929282546043396, D Loss: 1.3873785734176636\n",
            "Epoch 8, Batch 307, G Loss: 0.6926904916763306, D Loss: 1.3871183395385742\n",
            "Epoch 8, Batch 308, G Loss: 0.692470133304596, D Loss: 1.3870441913604736\n",
            "Epoch 8, Batch 309, G Loss: 0.692261815071106, D Loss: 1.3872270584106445\n",
            "Epoch 8, Batch 310, G Loss: 0.6920714378356934, D Loss: 1.3871530294418335\n",
            "Epoch 8, Batch 311, G Loss: 0.6918738484382629, D Loss: 1.3868035078048706\n",
            "Epoch 8, Batch 312, G Loss: 0.6917107105255127, D Loss: 1.387284278869629\n",
            "Epoch 8, Batch 313, G Loss: 0.6915341019630432, D Loss: 1.387026309967041\n",
            "Epoch 8, Batch 314, G Loss: 0.6913884878158569, D Loss: 1.3866316080093384\n",
            "Epoch 8, Batch 315, G Loss: 0.691270649433136, D Loss: 1.386852741241455\n",
            "Epoch 8, Batch 316, G Loss: 0.6911585927009583, D Loss: 1.3866487741470337\n",
            "Epoch 8, Batch 317, G Loss: 0.6911263465881348, D Loss: 1.3864099979400635\n",
            "Epoch 8, Batch 318, G Loss: 0.6911152601242065, D Loss: 1.3862844705581665\n",
            "Epoch 8, Batch 319, G Loss: 0.6911486983299255, D Loss: 1.3868632316589355\n",
            "Epoch 8, Batch 320, G Loss: 0.6912169456481934, D Loss: 1.3864068984985352\n",
            "Epoch 8, Batch 321, G Loss: 0.6913148760795593, D Loss: 1.386889934539795\n",
            "Epoch 8, Batch 322, G Loss: 0.6914054155349731, D Loss: 1.3866775035858154\n",
            "Epoch 8, Batch 323, G Loss: 0.691521942615509, D Loss: 1.3870289325714111\n",
            "Epoch 8, Batch 324, G Loss: 0.6916148662567139, D Loss: 1.386789083480835\n",
            "Epoch 8, Batch 325, G Loss: 0.6917462944984436, D Loss: 1.3864169120788574\n",
            "Epoch 8, Batch 326, G Loss: 0.6918510794639587, D Loss: 1.3866915702819824\n",
            "Epoch 8, Batch 327, G Loss: 0.691963791847229, D Loss: 1.3865253925323486\n",
            "Epoch 8, Batch 328, G Loss: 0.6920859813690186, D Loss: 1.3869409561157227\n",
            "Epoch 8, Batch 329, G Loss: 0.6921854019165039, D Loss: 1.3865149021148682\n",
            "Epoch 8, Batch 330, G Loss: 0.692288875579834, D Loss: 1.3861870765686035\n",
            "Epoch 8, Batch 331, G Loss: 0.6924148797988892, D Loss: 1.3864654302597046\n",
            "Epoch 8, Batch 332, G Loss: 0.6925204396247864, D Loss: 1.3865329027175903\n",
            "Epoch 8, Batch 333, G Loss: 0.6926183104515076, D Loss: 1.3867305517196655\n",
            "Epoch 8, Batch 334, G Loss: 0.6926950216293335, D Loss: 1.3865280151367188\n",
            "Epoch 8, Batch 335, G Loss: 0.6927571296691895, D Loss: 1.386688470840454\n",
            "Epoch 8, Batch 336, G Loss: 0.6927894949913025, D Loss: 1.3867014646530151\n",
            "Epoch 8, Batch 337, G Loss: 0.6928063035011292, D Loss: 1.386810064315796\n",
            "Epoch 8, Batch 338, G Loss: 0.6927896738052368, D Loss: 1.386204481124878\n",
            "Epoch 8, Batch 339, G Loss: 0.6927938461303711, D Loss: 1.3863615989685059\n",
            "Epoch 8, Batch 340, G Loss: 0.692785918712616, D Loss: 1.386183738708496\n",
            "Epoch 8, Batch 341, G Loss: 0.6928281784057617, D Loss: 1.38688325881958\n",
            "Epoch 8, Batch 342, G Loss: 0.6928036212921143, D Loss: 1.386758804321289\n",
            "Epoch 8, Batch 343, G Loss: 0.6927441954612732, D Loss: 1.3867747783660889\n",
            "Epoch 8, Batch 344, G Loss: 0.6926506161689758, D Loss: 1.386664867401123\n",
            "Epoch 8, Batch 345, G Loss: 0.6925400495529175, D Loss: 1.3864599466323853\n",
            "Epoch 8, Batch 346, G Loss: 0.6924354434013367, D Loss: 1.3863856792449951\n",
            "Epoch 8, Batch 347, G Loss: 0.6923413872718811, D Loss: 1.3863810300827026\n",
            "Epoch 8, Batch 348, G Loss: 0.6922662854194641, D Loss: 1.386387586593628\n",
            "Epoch 8, Batch 349, G Loss: 0.692200779914856, D Loss: 1.3863403797149658\n",
            "Epoch 8, Batch 350, G Loss: 0.692155659198761, D Loss: 1.3863487243652344\n",
            "Epoch 8, Batch 351, G Loss: 0.6921256184577942, D Loss: 1.3863334655761719\n",
            "Epoch 8, Batch 352, G Loss: 0.6921082139015198, D Loss: 1.386329174041748\n",
            "Epoch 8, Batch 353, G Loss: 0.6921079754829407, D Loss: 1.3862892389297485\n",
            "Epoch 8, Batch 354, G Loss: 0.6921125054359436, D Loss: 1.386260986328125\n",
            "Epoch 8, Batch 355, G Loss: 0.6921188831329346, D Loss: 1.3861424922943115\n",
            "Epoch 8, Batch 356, G Loss: 0.6921083331108093, D Loss: 1.3861106634140015\n",
            "Epoch 8, Batch 357, G Loss: 0.6920877695083618, D Loss: 1.3861744403839111\n",
            "Epoch 8, Batch 358, G Loss: 0.6920723915100098, D Loss: 1.3860700130462646\n",
            "Epoch 8, Batch 359, G Loss: 0.6920554637908936, D Loss: 1.3859858512878418\n",
            "Epoch 8, Batch 360, G Loss: 0.6920400857925415, D Loss: 1.3860012292861938\n",
            "Epoch 8, Batch 361, G Loss: 0.692028284072876, D Loss: 1.3857742547988892\n",
            "Epoch 8, Batch 362, G Loss: 0.6920102834701538, D Loss: 1.385972499847412\n",
            "Epoch 8, Batch 363, G Loss: 0.6920100450515747, D Loss: 1.386068344116211\n",
            "Epoch 8, Batch 364, G Loss: 0.6920127272605896, D Loss: 1.3860034942626953\n",
            "Epoch 8, Batch 365, G Loss: 0.6920344829559326, D Loss: 1.3859038352966309\n",
            "Epoch 8, Batch 366, G Loss: 0.692058801651001, D Loss: 1.385894775390625\n",
            "Epoch 8, Batch 367, G Loss: 0.69211345911026, D Loss: 1.386042594909668\n",
            "Epoch 8, Batch 368, G Loss: 0.6921569108963013, D Loss: 1.386143445968628\n",
            "Epoch 8, Batch 369, G Loss: 0.6922310590744019, D Loss: 1.3861703872680664\n",
            "Epoch 8, Batch 370, G Loss: 0.6923143863677979, D Loss: 1.3863887786865234\n",
            "Epoch 8, Batch 371, G Loss: 0.6924193501472473, D Loss: 1.3863415718078613\n",
            "Epoch 8, Batch 372, G Loss: 0.6925305128097534, D Loss: 1.385800838470459\n",
            "Epoch 8, Batch 373, G Loss: 0.6926379203796387, D Loss: 1.3858320713043213\n",
            "Epoch 8, Batch 374, G Loss: 0.6927362680435181, D Loss: 1.3860671520233154\n",
            "Epoch 8, Batch 375, G Loss: 0.6928248405456543, D Loss: 1.3859246969223022\n",
            "Epoch 8, Batch 376, G Loss: 0.6929325461387634, D Loss: 1.3858497142791748\n",
            "Epoch 8, Batch 377, G Loss: 0.693016529083252, D Loss: 1.3858444690704346\n",
            "Epoch 8, Batch 378, G Loss: 0.6930973529815674, D Loss: 1.3860907554626465\n",
            "Epoch 8, Batch 379, G Loss: 0.6931588053703308, D Loss: 1.386041522026062\n",
            "Epoch 8, Batch 380, G Loss: 0.6932216286659241, D Loss: 1.385967493057251\n",
            "Epoch 8, Batch 381, G Loss: 0.6932888627052307, D Loss: 1.3864049911499023\n",
            "Epoch 8, Batch 382, G Loss: 0.6933377385139465, D Loss: 1.3854742050170898\n",
            "Epoch 8, Batch 383, G Loss: 0.6933794617652893, D Loss: 1.3857464790344238\n",
            "Epoch 8, Batch 384, G Loss: 0.693383514881134, D Loss: 1.3861253261566162\n",
            "Epoch 8, Batch 385, G Loss: 0.6933997273445129, D Loss: 1.3857321739196777\n",
            "Epoch 8, Batch 386, G Loss: 0.6934057474136353, D Loss: 1.3854470252990723\n",
            "Epoch 8, Batch 387, G Loss: 0.6934173703193665, D Loss: 1.385875940322876\n",
            "Epoch 8, Batch 388, G Loss: 0.6933969259262085, D Loss: 1.3860290050506592\n",
            "Epoch 8, Batch 389, G Loss: 0.6933936476707458, D Loss: 1.3861883878707886\n",
            "Epoch 8, Batch 390, G Loss: 0.6933903098106384, D Loss: 1.3871238231658936\n",
            "Epoch 8, Batch 391, G Loss: 0.6933894157409668, D Loss: 1.3862671852111816\n",
            "Epoch 8, Batch 392, G Loss: 0.6933921575546265, D Loss: 1.3864197731018066\n",
            "Epoch 8, Batch 393, G Loss: 0.6934226155281067, D Loss: 1.3862881660461426\n",
            "Epoch 8, Batch 394, G Loss: 0.6934632062911987, D Loss: 1.386131763458252\n",
            "Epoch 8, Batch 395, G Loss: 0.6934404969215393, D Loss: 1.3863731622695923\n",
            "Epoch 8, Batch 396, G Loss: 0.6934661865234375, D Loss: 1.3844382762908936\n",
            "Epoch 8, Batch 397, G Loss: 0.6934356689453125, D Loss: 1.3848884105682373\n",
            "Epoch 8, Batch 398, G Loss: 0.6934038996696472, D Loss: 1.385469675064087\n",
            "Epoch 8, Batch 399, G Loss: 0.6933642625808716, D Loss: 1.3854284286499023\n",
            "Epoch 8, Batch 400, G Loss: 0.6933295726776123, D Loss: 1.386565089225769\n",
            "Epoch 8, Batch 401, G Loss: 0.6933143138885498, D Loss: 1.3860712051391602\n",
            "Epoch 8, Batch 402, G Loss: 0.6932857036590576, D Loss: 1.3858718872070312\n",
            "Epoch 8, Batch 403, G Loss: 0.6932624578475952, D Loss: 1.3866679668426514\n",
            "Epoch 8, Batch 404, G Loss: 0.6932461857795715, D Loss: 1.387242078781128\n",
            "Epoch 8, Batch 405, G Loss: 0.6932457685470581, D Loss: 1.3872735500335693\n",
            "Epoch 8, Batch 406, G Loss: 0.6932794451713562, D Loss: 1.3871887922286987\n",
            "Epoch 8, Batch 407, G Loss: 0.6933072805404663, D Loss: 1.3857603073120117\n",
            "Epoch 8, Batch 408, G Loss: 0.6933674812316895, D Loss: 1.3861937522888184\n",
            "Epoch 8, Batch 409, G Loss: 0.6934054493904114, D Loss: 1.386479377746582\n",
            "Epoch 8, Batch 410, G Loss: 0.6934109330177307, D Loss: 1.3862991333007812\n",
            "Epoch 8, Batch 411, G Loss: 0.6934499740600586, D Loss: 1.3868162631988525\n",
            "Epoch 8, Batch 412, G Loss: 0.6934982538223267, D Loss: 1.3867886066436768\n",
            "Epoch 8, Batch 413, G Loss: 0.693548321723938, D Loss: 1.3867841958999634\n",
            "Epoch 8, Batch 414, G Loss: 0.6935979127883911, D Loss: 1.3869972229003906\n",
            "Epoch 8, Batch 415, G Loss: 0.6936320066452026, D Loss: 1.3871558904647827\n",
            "Epoch 8, Batch 416, G Loss: 0.6936731934547424, D Loss: 1.3872350454330444\n",
            "Epoch 8, Batch 417, G Loss: 0.6937505602836609, D Loss: 1.3863487243652344\n",
            "Epoch 8, Batch 418, G Loss: 0.6937980651855469, D Loss: 1.3863683938980103\n",
            "Epoch 8, Batch 419, G Loss: 0.6938280463218689, D Loss: 1.3861181735992432\n",
            "Epoch 8, Batch 420, G Loss: 0.6938708424568176, D Loss: 1.3862338066101074\n",
            "Epoch 8, Batch 421, G Loss: 0.6938681602478027, D Loss: 1.3866355419158936\n",
            "Epoch 8, Batch 422, G Loss: 0.6938880085945129, D Loss: 1.386906623840332\n",
            "Epoch 8, Batch 423, G Loss: 0.693895161151886, D Loss: 1.3867087364196777\n",
            "Epoch 8, Batch 424, G Loss: 0.6939067244529724, D Loss: 1.3858169317245483\n",
            "Epoch 8, Batch 425, G Loss: 0.6938677430152893, D Loss: 1.3860187530517578\n",
            "Epoch 8, Batch 426, G Loss: 0.6938652396202087, D Loss: 1.3862241506576538\n",
            "Epoch 8, Batch 427, G Loss: 0.6938216686248779, D Loss: 1.3861896991729736\n",
            "Epoch 8, Batch 428, G Loss: 0.6937662363052368, D Loss: 1.3865303993225098\n",
            "Epoch 8, Batch 429, G Loss: 0.6937265992164612, D Loss: 1.386303186416626\n",
            "Epoch 8, Batch 430, G Loss: 0.6936812400817871, D Loss: 1.38690185546875\n",
            "Epoch 8, Batch 431, G Loss: 0.6936573386192322, D Loss: 1.3868777751922607\n",
            "Epoch 8, Batch 432, G Loss: 0.6936167478561401, D Loss: 1.3866376876831055\n",
            "Epoch 8, Batch 433, G Loss: 0.6936036944389343, D Loss: 1.3869755268096924\n",
            "Epoch 8, Batch 434, G Loss: 0.6936036944389343, D Loss: 1.386935830116272\n",
            "Epoch 8, Batch 435, G Loss: 0.6936267018318176, D Loss: 1.3870954513549805\n",
            "Epoch 8, Batch 436, G Loss: 0.6936503648757935, D Loss: 1.386703610420227\n",
            "Epoch 8, Batch 437, G Loss: 0.6936975717544556, D Loss: 1.3861291408538818\n",
            "Epoch 8, Batch 438, G Loss: 0.693712592124939, D Loss: 1.3863017559051514\n",
            "Epoch 8, Batch 439, G Loss: 0.6937181949615479, D Loss: 1.3862648010253906\n",
            "Epoch 8, Batch 440, G Loss: 0.6937164068222046, D Loss: 1.3862590789794922\n",
            "Epoch 8, Batch 441, G Loss: 0.6937049627304077, D Loss: 1.3860856294631958\n",
            "Epoch 8, Batch 442, G Loss: 0.6936836838722229, D Loss: 1.3861724138259888\n",
            "Epoch 8, Batch 443, G Loss: 0.6936366558074951, D Loss: 1.3863260746002197\n",
            "Epoch 8, Batch 444, G Loss: 0.6936033368110657, D Loss: 1.3860840797424316\n",
            "Epoch 8, Batch 445, G Loss: 0.6935532093048096, D Loss: 1.3861501216888428\n",
            "Epoch 8, Batch 446, G Loss: 0.6934965252876282, D Loss: 1.386314868927002\n",
            "Epoch 8, Batch 447, G Loss: 0.6934416890144348, D Loss: 1.3862782716751099\n",
            "Epoch 8, Batch 448, G Loss: 0.6933866739273071, D Loss: 1.3865139484405518\n",
            "Epoch 8, Batch 449, G Loss: 0.6933532953262329, D Loss: 1.386552333831787\n",
            "Epoch 8, Batch 450, G Loss: 0.6933296322822571, D Loss: 1.3864986896514893\n",
            "Epoch 8, Batch 451, G Loss: 0.6933215260505676, D Loss: 1.3865687847137451\n",
            "Epoch 8, Batch 452, G Loss: 0.6933304071426392, D Loss: 1.3862144947052002\n",
            "Epoch 8, Batch 453, G Loss: 0.6933248043060303, D Loss: 1.3863868713378906\n",
            "Epoch 8, Batch 454, G Loss: 0.6933369636535645, D Loss: 1.3863738775253296\n",
            "Epoch 8, Batch 455, G Loss: 0.6933451890945435, D Loss: 1.3868229389190674\n",
            "Epoch 8, Batch 456, G Loss: 0.6933863162994385, D Loss: 1.3867262601852417\n",
            "Epoch 8, Batch 457, G Loss: 0.6934552192687988, D Loss: 1.386644721031189\n",
            "Epoch 8, Batch 458, G Loss: 0.6935343146324158, D Loss: 1.3861597776412964\n",
            "Epoch 8, Batch 459, G Loss: 0.6935892701148987, D Loss: 1.386196255683899\n",
            "Epoch 8, Batch 460, G Loss: 0.6936262249946594, D Loss: 1.3861719369888306\n",
            "Epoch 8, Batch 461, G Loss: 0.6936495900154114, D Loss: 1.3862416744232178\n",
            "Epoch 8, Batch 462, G Loss: 0.6936548948287964, D Loss: 1.3865671157836914\n",
            "Epoch 8, Batch 463, G Loss: 0.6936831474304199, D Loss: 1.3866022825241089\n",
            "Epoch 8, Batch 464, G Loss: 0.6937358379364014, D Loss: 1.3865001201629639\n",
            "Epoch 8, Batch 465, G Loss: 0.6937925219535828, D Loss: 1.3865975141525269\n",
            "Epoch 8, Batch 466, G Loss: 0.6938700079917908, D Loss: 1.386566162109375\n",
            "Epoch 8, Batch 467, G Loss: 0.6939674615859985, D Loss: 1.386520266532898\n",
            "Epoch 8, Batch 468, G Loss: 0.6940767168998718, D Loss: 1.386458158493042\n",
            "Epoch 8, Batch 469, G Loss: 0.6941924095153809, D Loss: 1.386460304260254\n",
            "Epoch 8, Batch 470, G Loss: 0.6943232417106628, D Loss: 1.3863056898117065\n",
            "Epoch 8, Batch 471, G Loss: 0.6944336891174316, D Loss: 1.3862941265106201\n",
            "Epoch 8, Batch 472, G Loss: 0.694530725479126, D Loss: 1.3862602710723877\n",
            "Epoch 8, Batch 473, G Loss: 0.6946123242378235, D Loss: 1.3862249851226807\n",
            "Epoch 8, Batch 474, G Loss: 0.6946797370910645, D Loss: 1.3861873149871826\n",
            "Epoch 8, Batch 475, G Loss: 0.6947342753410339, D Loss: 1.3862093687057495\n",
            "Epoch 8, Batch 476, G Loss: 0.6947563886642456, D Loss: 1.386206865310669\n",
            "Epoch 8, Batch 477, G Loss: 0.6947497725486755, D Loss: 1.3861725330352783\n",
            "Epoch 8, Batch 478, G Loss: 0.6947110295295715, D Loss: 1.3860825300216675\n",
            "Epoch 8, Batch 479, G Loss: 0.6946708559989929, D Loss: 1.385779857635498\n",
            "Epoch 8, Batch 480, G Loss: 0.6946558952331543, D Loss: 1.3858052492141724\n",
            "Epoch 8, Batch 481, G Loss: 0.6946637630462646, D Loss: 1.3857321739196777\n",
            "Epoch 8, Batch 482, G Loss: 0.6946896910667419, D Loss: 1.3860654830932617\n",
            "Epoch 8, Batch 483, G Loss: 0.694688618183136, D Loss: 1.385972023010254\n",
            "Epoch 8, Batch 484, G Loss: 0.6946778297424316, D Loss: 1.385789155960083\n",
            "Epoch 8, Batch 485, G Loss: 0.6946567893028259, D Loss: 1.3857951164245605\n",
            "Epoch 8, Batch 486, G Loss: 0.6946441531181335, D Loss: 1.3857998847961426\n",
            "Epoch 8, Batch 487, G Loss: 0.6946172118186951, D Loss: 1.385715126991272\n",
            "Epoch 8, Batch 488, G Loss: 0.6945880055427551, D Loss: 1.3856925964355469\n",
            "Epoch 8, Batch 489, G Loss: 0.6945815682411194, D Loss: 1.3856743574142456\n",
            "Epoch 8, Batch 490, G Loss: 0.6945468187332153, D Loss: 1.3856501579284668\n",
            "Epoch 8, Batch 491, G Loss: 0.6945286989212036, D Loss: 1.3854975700378418\n",
            "Epoch 8, Batch 492, G Loss: 0.6945134997367859, D Loss: 1.3856377601623535\n",
            "Epoch 8, Batch 493, G Loss: 0.6944875717163086, D Loss: 1.3852592706680298\n",
            "Epoch 8, Batch 494, G Loss: 0.6944831609725952, D Loss: 1.3852622509002686\n",
            "Epoch 8, Batch 495, G Loss: 0.6944894790649414, D Loss: 1.3852574825286865\n",
            "Epoch 8, Batch 496, G Loss: 0.6945019364356995, D Loss: 1.385335922241211\n",
            "Epoch 8, Batch 497, G Loss: 0.6945264935493469, D Loss: 1.3855750560760498\n",
            "Epoch 8, Batch 498, G Loss: 0.6945422291755676, D Loss: 1.3856251239776611\n",
            "Epoch 8, Batch 499, G Loss: 0.6945500373840332, D Loss: 1.3854026794433594\n",
            "Epoch 8, Batch 500, G Loss: 0.6945416331291199, D Loss: 1.3859171867370605\n",
            "Epoch 8, Batch 501, G Loss: 0.6945050954818726, D Loss: 1.3862667083740234\n",
            "Epoch 8, Batch 502, G Loss: 0.6944409608840942, D Loss: 1.3862638473510742\n",
            "Epoch 8, Batch 503, G Loss: 0.6943410038948059, D Loss: 1.3860034942626953\n",
            "Epoch 8, Batch 504, G Loss: 0.6942212581634521, D Loss: 1.3857734203338623\n",
            "Epoch 8, Batch 505, G Loss: 0.6941096186637878, D Loss: 1.3865498304367065\n",
            "Epoch 8, Batch 506, G Loss: 0.693970263004303, D Loss: 1.3865188360214233\n",
            "Epoch 8, Batch 507, G Loss: 0.6937934756278992, D Loss: 1.3871281147003174\n",
            "Epoch 8, Batch 508, G Loss: 0.6936039328575134, D Loss: 1.3866117000579834\n",
            "Epoch 8, Batch 509, G Loss: 0.693384051322937, D Loss: 1.385770559310913\n",
            "Epoch 8, Batch 510, G Loss: 0.6931909322738647, D Loss: 1.3859591484069824\n",
            "Epoch 8, Batch 511, G Loss: 0.6930480003356934, D Loss: 1.385685682296753\n",
            "Epoch 8, Batch 512, G Loss: 0.6929349303245544, D Loss: 1.385209560394287\n",
            "Epoch 8, Batch 513, G Loss: 0.6928390860557556, D Loss: 1.3858282566070557\n",
            "Epoch 8, Batch 514, G Loss: 0.6927971243858337, D Loss: 1.3857989311218262\n",
            "Epoch 8, Batch 515, G Loss: 0.6928011178970337, D Loss: 1.3864200115203857\n",
            "Epoch 8, Batch 516, G Loss: 0.692747950553894, D Loss: 1.3868908882141113\n",
            "Epoch 8, Batch 517, G Loss: 0.6927247047424316, D Loss: 1.3870179653167725\n",
            "Epoch 8, Batch 518, G Loss: 0.6926920413970947, D Loss: 1.3864145278930664\n",
            "Epoch 8, Batch 519, G Loss: 0.6926175951957703, D Loss: 1.3866890668869019\n",
            "Epoch 8, Batch 520, G Loss: 0.6925657391548157, D Loss: 1.3866257667541504\n",
            "Epoch 8, Batch 521, G Loss: 0.6925382018089294, D Loss: 1.386556625366211\n",
            "Epoch 8, Batch 522, G Loss: 0.6924829483032227, D Loss: 1.3860526084899902\n",
            "Epoch 8, Batch 523, G Loss: 0.6924648880958557, D Loss: 1.3858146667480469\n",
            "Epoch 8, Batch 524, G Loss: 0.6924850940704346, D Loss: 1.3859384059906006\n",
            "Epoch 8, Batch 525, G Loss: 0.6925244331359863, D Loss: 1.3855605125427246\n",
            "Epoch 8, Batch 526, G Loss: 0.6926088929176331, D Loss: 1.3856076002120972\n",
            "Epoch 8, Batch 527, G Loss: 0.6927260756492615, D Loss: 1.3861925601959229\n",
            "Epoch 8, Batch 528, G Loss: 0.6928443908691406, D Loss: 1.3869080543518066\n",
            "Epoch 8, Batch 529, G Loss: 0.6929415464401245, D Loss: 1.3874790668487549\n",
            "Epoch 8, Batch 530, G Loss: 0.6929718255996704, D Loss: 1.3868664503097534\n",
            "Epoch 8, Batch 531, G Loss: 0.6929720044136047, D Loss: 1.3866491317749023\n",
            "Epoch 8, Batch 532, G Loss: 0.692936897277832, D Loss: 1.3857935667037964\n",
            "Epoch 8, Batch 533, G Loss: 0.6929764151573181, D Loss: 1.3865978717803955\n",
            "Epoch 8, Batch 534, G Loss: 0.6929819583892822, D Loss: 1.38678777217865\n",
            "Epoch 8, Batch 535, G Loss: 0.6929957270622253, D Loss: 1.3864316940307617\n",
            "Epoch 8, Batch 536, G Loss: 0.6929830312728882, D Loss: 1.3862322568893433\n",
            "Epoch 8, Batch 537, G Loss: 0.6929411888122559, D Loss: 1.3869599103927612\n",
            "Epoch 8, Batch 538, G Loss: 0.6929274797439575, D Loss: 1.3871846199035645\n",
            "Epoch 8, Batch 539, G Loss: 0.692868709564209, D Loss: 1.3869810104370117\n",
            "Epoch 8, Batch 540, G Loss: 0.6928001642227173, D Loss: 1.3870117664337158\n",
            "Epoch 8, Batch 541, G Loss: 0.6927103996276855, D Loss: 1.386998176574707\n",
            "Epoch 8, Batch 542, G Loss: 0.6925832629203796, D Loss: 1.3869162797927856\n",
            "Epoch 8, Batch 543, G Loss: 0.6925085186958313, D Loss: 1.3869465589523315\n",
            "Epoch 8, Batch 544, G Loss: 0.6924203634262085, D Loss: 1.3869584798812866\n",
            "Epoch 8, Batch 545, G Loss: 0.692310631275177, D Loss: 1.386927604675293\n",
            "Epoch 8, Batch 546, G Loss: 0.6922292113304138, D Loss: 1.386864185333252\n",
            "Epoch 8, Batch 547, G Loss: 0.6921578049659729, D Loss: 1.386857509613037\n",
            "Epoch 8, Batch 548, G Loss: 0.6920681595802307, D Loss: 1.3872976303100586\n",
            "Epoch 8, Batch 549, G Loss: 0.6919834613800049, D Loss: 1.3867979049682617\n",
            "Epoch 8, Batch 550, G Loss: 0.6919065713882446, D Loss: 1.3864130973815918\n",
            "Epoch 8, Batch 551, G Loss: 0.6918726563453674, D Loss: 1.3864874839782715\n",
            "Epoch 8, Batch 552, G Loss: 0.6918769478797913, D Loss: 1.3864679336547852\n",
            "Epoch 8, Batch 553, G Loss: 0.6918729543685913, D Loss: 1.3869075775146484\n",
            "Epoch 8, Batch 554, G Loss: 0.691902756690979, D Loss: 1.3869585990905762\n",
            "Epoch 8, Batch 555, G Loss: 0.6919083595275879, D Loss: 1.3870420455932617\n",
            "Epoch 8, Batch 556, G Loss: 0.6919136047363281, D Loss: 1.386613368988037\n",
            "Epoch 8, Batch 557, G Loss: 0.6919225454330444, D Loss: 1.386521816253662\n",
            "Epoch 8, Batch 558, G Loss: 0.6919614672660828, D Loss: 1.3867788314819336\n",
            "Epoch 8, Batch 559, G Loss: 0.6919882893562317, D Loss: 1.3866145610809326\n",
            "Epoch 8, Batch 560, G Loss: 0.6920298337936401, D Loss: 1.3866283893585205\n",
            "Epoch 8, Batch 561, G Loss: 0.6920692920684814, D Loss: 1.3864306211471558\n",
            "Epoch 8, Batch 562, G Loss: 0.6921338438987732, D Loss: 1.3862966299057007\n",
            "Epoch 8, Batch 563, G Loss: 0.6922107338905334, D Loss: 1.3862816095352173\n",
            "Epoch 8, Batch 564, G Loss: 0.6923067569732666, D Loss: 1.3861674070358276\n",
            "Epoch 8, Batch 565, G Loss: 0.6924408674240112, D Loss: 1.3866592645645142\n",
            "Epoch 8, Batch 566, G Loss: 0.692531406879425, D Loss: 1.3870160579681396\n",
            "Epoch 8, Batch 567, G Loss: 0.6925699710845947, D Loss: 1.38676118850708\n",
            "Epoch 8, Batch 568, G Loss: 0.6925813555717468, D Loss: 1.3867385387420654\n",
            "Epoch 8, Batch 569, G Loss: 0.6925557851791382, D Loss: 1.3864316940307617\n",
            "Epoch 8, Batch 570, G Loss: 0.6925274729728699, D Loss: 1.386458396911621\n",
            "Epoch 8, Batch 571, G Loss: 0.6924956440925598, D Loss: 1.386397361755371\n",
            "Epoch 8, Batch 572, G Loss: 0.6924765706062317, D Loss: 1.386561393737793\n",
            "Epoch 8, Batch 573, G Loss: 0.6924229264259338, D Loss: 1.3864641189575195\n",
            "Epoch 8, Batch 574, G Loss: 0.6923469305038452, D Loss: 1.3863931894302368\n",
            "Epoch 8, Batch 575, G Loss: 0.6922460794448853, D Loss: 1.386314868927002\n",
            "Epoch 8, Batch 576, G Loss: 0.692134439945221, D Loss: 1.3862746953964233\n",
            "Epoch 8, Batch 577, G Loss: 0.6920366287231445, D Loss: 1.3862096071243286\n",
            "Epoch 8, Batch 578, G Loss: 0.6919525861740112, D Loss: 1.3862543106079102\n",
            "Epoch 8, Batch 579, G Loss: 0.6918929815292358, D Loss: 1.386267066001892\n",
            "Epoch 8, Batch 580, G Loss: 0.691864550113678, D Loss: 1.3859354257583618\n",
            "Epoch 8, Batch 581, G Loss: 0.6918427348136902, D Loss: 1.3859295845031738\n",
            "Epoch 8, Batch 582, G Loss: 0.6918246150016785, D Loss: 1.3858306407928467\n",
            "Epoch 8, Batch 583, G Loss: 0.6918119192123413, D Loss: 1.3857312202453613\n",
            "Epoch 8, Batch 584, G Loss: 0.6918089985847473, D Loss: 1.3859509229660034\n",
            "Epoch 8, Batch 585, G Loss: 0.6918256878852844, D Loss: 1.385987401008606\n",
            "Epoch 8, Batch 586, G Loss: 0.6918549537658691, D Loss: 1.3860888481140137\n",
            "Epoch 8, Batch 587, G Loss: 0.6919080018997192, D Loss: 1.386233925819397\n",
            "Epoch 8, Batch 588, G Loss: 0.691996693611145, D Loss: 1.3862473964691162\n",
            "Epoch 8, Batch 589, G Loss: 0.6920972466468811, D Loss: 1.3863375186920166\n",
            "Epoch 8, Batch 590, G Loss: 0.6922181844711304, D Loss: 1.3856077194213867\n",
            "Epoch 8, Batch 591, G Loss: 0.6923472881317139, D Loss: 1.3855304718017578\n",
            "Epoch 8, Batch 592, G Loss: 0.6924508810043335, D Loss: 1.3856959342956543\n",
            "Epoch 8, Batch 593, G Loss: 0.6925621032714844, D Loss: 1.3859498500823975\n",
            "Epoch 8, Batch 594, G Loss: 0.6926795244216919, D Loss: 1.3860318660736084\n",
            "Epoch 8, Batch 595, G Loss: 0.6927856206893921, D Loss: 1.3864275217056274\n",
            "Epoch 8, Batch 596, G Loss: 0.692900538444519, D Loss: 1.3855669498443604\n",
            "Epoch 8, Batch 597, G Loss: 0.6930121779441833, D Loss: 1.3855278491973877\n",
            "Epoch 8, Batch 598, G Loss: 0.693095862865448, D Loss: 1.385786533355713\n",
            "Epoch 8, Batch 599, G Loss: 0.6931675672531128, D Loss: 1.3859338760375977\n",
            "Epoch 8, Batch 600, G Loss: 0.6932396292686462, D Loss: 1.386201024055481\n",
            "Epoch 8, Batch 601, G Loss: 0.6933151483535767, D Loss: 1.3860936164855957\n",
            "Epoch 8, Batch 602, G Loss: 0.6933870911598206, D Loss: 1.3863351345062256\n",
            "Epoch 8, Batch 603, G Loss: 0.6934463977813721, D Loss: 1.3858466148376465\n",
            "Epoch 8, Batch 604, G Loss: 0.6934896111488342, D Loss: 1.3850429058074951\n",
            "Epoch 8, Batch 605, G Loss: 0.693510115146637, D Loss: 1.3859691619873047\n",
            "Epoch 8, Batch 606, G Loss: 0.6935298442840576, D Loss: 1.3858444690704346\n",
            "Epoch 8, Batch 607, G Loss: 0.6935325264930725, D Loss: 1.3858206272125244\n",
            "Epoch 8, Batch 608, G Loss: 0.6935451030731201, D Loss: 1.3855299949645996\n",
            "Epoch 8, Batch 609, G Loss: 0.6935282945632935, D Loss: 1.3860554695129395\n",
            "Epoch 8, Batch 610, G Loss: 0.6935122609138489, D Loss: 1.3863515853881836\n",
            "Epoch 8, Batch 611, G Loss: 0.6934953927993774, D Loss: 1.3856232166290283\n",
            "Epoch 8, Batch 612, G Loss: 0.6934847235679626, D Loss: 1.3850829601287842\n",
            "Epoch 8, Batch 613, G Loss: 0.693456768989563, D Loss: 1.3851215839385986\n",
            "Epoch 8, Batch 614, G Loss: 0.6934112310409546, D Loss: 1.3853917121887207\n",
            "Epoch 8, Batch 615, G Loss: 0.693376362323761, D Loss: 1.3855631351470947\n",
            "Epoch 8, Batch 616, G Loss: 0.6933558583259583, D Loss: 1.3854625225067139\n",
            "Epoch 8, Batch 617, G Loss: 0.693328857421875, D Loss: 1.3852496147155762\n",
            "Epoch 8, Batch 618, G Loss: 0.693290114402771, D Loss: 1.3858237266540527\n",
            "Epoch 8, Batch 619, G Loss: 0.6932770013809204, D Loss: 1.3854734897613525\n",
            "Epoch 8, Batch 620, G Loss: 0.6932172179222107, D Loss: 1.385502576828003\n",
            "Epoch 8, Batch 621, G Loss: 0.693232536315918, D Loss: 1.3862576484680176\n",
            "Epoch 8, Batch 622, G Loss: 0.6932007670402527, D Loss: 1.3874061107635498\n",
            "Epoch 8, Batch 623, G Loss: 0.6932111382484436, D Loss: 1.3863927125930786\n",
            "Epoch 8, Batch 624, G Loss: 0.6932291388511658, D Loss: 1.386862874031067\n",
            "Epoch 8, Batch 625, G Loss: 0.6932651996612549, D Loss: 1.3858182430267334\n",
            "Epoch 8, Batch 626, G Loss: 0.6932776570320129, D Loss: 1.386094331741333\n",
            "Epoch 8, Batch 627, G Loss: 0.6933342218399048, D Loss: 1.3858189582824707\n",
            "Epoch 8, Batch 628, G Loss: 0.693371593952179, D Loss: 1.3858048915863037\n",
            "Epoch 8, Batch 629, G Loss: 0.693391740322113, D Loss: 1.3861072063446045\n",
            "Epoch 8, Batch 630, G Loss: 0.6934189796447754, D Loss: 1.3866922855377197\n",
            "Epoch 8, Batch 631, G Loss: 0.6934785842895508, D Loss: 1.3866965770721436\n",
            "Epoch 8, Batch 632, G Loss: 0.6934881210327148, D Loss: 1.3854212760925293\n",
            "Epoch 8, Batch 633, G Loss: 0.6935252547264099, D Loss: 1.385221242904663\n",
            "Epoch 8, Batch 634, G Loss: 0.6935747861862183, D Loss: 1.3854186534881592\n",
            "Epoch 8, Batch 635, G Loss: 0.6935625672340393, D Loss: 1.38596773147583\n",
            "Epoch 8, Batch 636, G Loss: 0.6935765147209167, D Loss: 1.3866748809814453\n",
            "Epoch 8, Batch 637, G Loss: 0.6935944557189941, D Loss: 1.3865854740142822\n",
            "Epoch 8, Batch 638, G Loss: 0.6936085820198059, D Loss: 1.387108564376831\n",
            "Epoch 8, Batch 639, G Loss: 0.693651020526886, D Loss: 1.3863427639007568\n",
            "Epoch 8, Batch 640, G Loss: 0.6936613321304321, D Loss: 1.3869622945785522\n",
            "Epoch 8, Batch 641, G Loss: 0.6936821341514587, D Loss: 1.3868000507354736\n",
            "Epoch 8, Batch 642, G Loss: 0.693724513053894, D Loss: 1.3867671489715576\n",
            "Epoch 8, Batch 643, G Loss: 0.6937229633331299, D Loss: 1.3867247104644775\n",
            "Epoch 8, Batch 644, G Loss: 0.6937476396560669, D Loss: 1.3862298727035522\n",
            "Epoch 8, Batch 645, G Loss: 0.6937744617462158, D Loss: 1.3868701457977295\n",
            "Epoch 8, Batch 646, G Loss: 0.6937776803970337, D Loss: 1.386278510093689\n",
            "Epoch 8, Batch 647, G Loss: 0.6937939524650574, D Loss: 1.3862231969833374\n",
            "Epoch 8, Batch 648, G Loss: 0.6937911510467529, D Loss: 1.3864972591400146\n",
            "Epoch 8, Batch 649, G Loss: 0.6938003301620483, D Loss: 1.3872103691101074\n",
            "Epoch 8, Batch 650, G Loss: 0.693805456161499, D Loss: 1.3863458633422852\n",
            "Epoch 8, Batch 651, G Loss: 0.6938021779060364, D Loss: 1.3868498802185059\n",
            "Epoch 8, Batch 652, G Loss: 0.6937745809555054, D Loss: 1.387843132019043\n",
            "Epoch 8, Batch 653, G Loss: 0.6937797665596008, D Loss: 1.388127088546753\n",
            "Epoch 8, Batch 654, G Loss: 0.6938159465789795, D Loss: 1.3873997926712036\n",
            "Epoch 8, Batch 655, G Loss: 0.6938549876213074, D Loss: 1.3864190578460693\n",
            "Epoch 8, Batch 656, G Loss: 0.6938738226890564, D Loss: 1.3863673210144043\n",
            "Epoch 8, Batch 657, G Loss: 0.6938773989677429, D Loss: 1.3860607147216797\n",
            "Epoch 8, Batch 658, G Loss: 0.6938786506652832, D Loss: 1.386101245880127\n",
            "Epoch 8, Batch 659, G Loss: 0.6938758492469788, D Loss: 1.3863974809646606\n",
            "Epoch 8, Batch 660, G Loss: 0.6938379406929016, D Loss: 1.3864989280700684\n",
            "Epoch 8, Batch 661, G Loss: 0.6938375234603882, D Loss: 1.3864078521728516\n",
            "Epoch 8, Batch 662, G Loss: 0.693792998790741, D Loss: 1.3868756294250488\n",
            "Epoch 8, Batch 663, G Loss: 0.6937693953514099, D Loss: 1.386712670326233\n",
            "Epoch 8, Batch 664, G Loss: 0.6937617659568787, D Loss: 1.3870432376861572\n",
            "Epoch 8, Batch 665, G Loss: 0.6937514543533325, D Loss: 1.3871676921844482\n",
            "Epoch 8, Batch 666, G Loss: 0.6937555074691772, D Loss: 1.3860280513763428\n",
            "Epoch 8, Batch 667, G Loss: 0.6937323808670044, D Loss: 1.385908842086792\n",
            "Epoch 8, Batch 668, G Loss: 0.6937107443809509, D Loss: 1.3865129947662354\n",
            "Epoch 8, Batch 669, G Loss: 0.6936792135238647, D Loss: 1.3862924575805664\n",
            "Epoch 8, Batch 670, G Loss: 0.6936551928520203, D Loss: 1.3864736557006836\n",
            "Epoch 8, Batch 671, G Loss: 0.6936228275299072, D Loss: 1.386817455291748\n",
            "Epoch 8, Batch 672, G Loss: 0.6936041116714478, D Loss: 1.3872671127319336\n",
            "Epoch 8, Batch 673, G Loss: 0.693602979183197, D Loss: 1.3871221542358398\n",
            "Epoch 8, Batch 674, G Loss: 0.693620502948761, D Loss: 1.3871593475341797\n",
            "Epoch 8, Batch 675, G Loss: 0.6936701536178589, D Loss: 1.3864402770996094\n",
            "Epoch 8, Batch 676, G Loss: 0.6937016248703003, D Loss: 1.3865265846252441\n",
            "Epoch 8, Batch 677, G Loss: 0.6937357783317566, D Loss: 1.3866952657699585\n",
            "Epoch 8, Batch 678, G Loss: 0.6937687397003174, D Loss: 1.386657476425171\n",
            "Epoch 8, Batch 679, G Loss: 0.6937894225120544, D Loss: 1.3866039514541626\n",
            "Epoch 8, Batch 680, G Loss: 0.6938222646713257, D Loss: 1.3865411281585693\n",
            "Epoch 8, Batch 681, G Loss: 0.6938461661338806, D Loss: 1.3867566585540771\n",
            "Epoch 8, Batch 682, G Loss: 0.6938773989677429, D Loss: 1.3865429162979126\n",
            "Epoch 8, Batch 683, G Loss: 0.6939125657081604, D Loss: 1.3865723609924316\n",
            "Epoch 8, Batch 684, G Loss: 0.693921685218811, D Loss: 1.3865773677825928\n",
            "Epoch 8, Batch 685, G Loss: 0.6939483880996704, D Loss: 1.3866679668426514\n",
            "Epoch 8, Batch 686, G Loss: 0.6939806342124939, D Loss: 1.3866815567016602\n",
            "Epoch 8, Batch 687, G Loss: 0.6940140724182129, D Loss: 1.3866952657699585\n",
            "Epoch 8, Batch 688, G Loss: 0.6940556764602661, D Loss: 1.386716604232788\n",
            "Epoch 8, Batch 689, G Loss: 0.6941136121749878, D Loss: 1.386468768119812\n",
            "Epoch 8, Batch 690, G Loss: 0.6941668391227722, D Loss: 1.3865363597869873\n",
            "Epoch 8, Batch 691, G Loss: 0.6942186951637268, D Loss: 1.3864481449127197\n",
            "Epoch 8, Batch 692, G Loss: 0.6942700743675232, D Loss: 1.3863766193389893\n",
            "Epoch 8, Batch 693, G Loss: 0.6943024396896362, D Loss: 1.386349081993103\n",
            "Epoch 8, Batch 694, G Loss: 0.6943231821060181, D Loss: 1.3863575458526611\n",
            "Epoch 8, Batch 695, G Loss: 0.6943443417549133, D Loss: 1.3862993717193604\n",
            "Epoch 8, Batch 696, G Loss: 0.6943530440330505, D Loss: 1.3862736225128174\n",
            "Epoch 8, Batch 697, G Loss: 0.6943560838699341, D Loss: 1.3862444162368774\n",
            "Epoch 8, Batch 698, G Loss: 0.6943554282188416, D Loss: 1.3862075805664062\n",
            "Epoch 8, Batch 699, G Loss: 0.6943520307540894, D Loss: 1.386155366897583\n",
            "Epoch 8, Batch 700, G Loss: 0.6943527460098267, D Loss: 1.386122226715088\n",
            "Epoch 8, Batch 701, G Loss: 0.6943524479866028, D Loss: 1.3860422372817993\n",
            "Epoch 8, Batch 702, G Loss: 0.6943613290786743, D Loss: 1.3860855102539062\n",
            "Epoch 8, Batch 703, G Loss: 0.6943643689155579, D Loss: 1.3861393928527832\n",
            "Epoch 8, Batch 704, G Loss: 0.6943531632423401, D Loss: 1.386061429977417\n",
            "Epoch 8, Batch 705, G Loss: 0.6943365335464478, D Loss: 1.3860979080200195\n",
            "Epoch 8, Batch 706, G Loss: 0.6943030953407288, D Loss: 1.3860207796096802\n",
            "Epoch 8, Batch 707, G Loss: 0.6942684650421143, D Loss: 1.3859843015670776\n",
            "Epoch 8, Batch 708, G Loss: 0.6942201852798462, D Loss: 1.3860273361206055\n",
            "Epoch 8, Batch 709, G Loss: 0.6941757798194885, D Loss: 1.3858675956726074\n",
            "Epoch 8, Batch 710, G Loss: 0.6941366195678711, D Loss: 1.3857896327972412\n",
            "Epoch 8, Batch 711, G Loss: 0.6941067576408386, D Loss: 1.3855456113815308\n",
            "Epoch 8, Batch 712, G Loss: 0.6940938234329224, D Loss: 1.3858939409255981\n",
            "Epoch 8, Batch 713, G Loss: 0.6940953731536865, D Loss: 1.3857308626174927\n",
            "Epoch 8, Batch 714, G Loss: 0.6940984725952148, D Loss: 1.3857554197311401\n",
            "Epoch 8, Batch 715, G Loss: 0.6941033005714417, D Loss: 1.385857105255127\n",
            "Epoch 8, Batch 716, G Loss: 0.6941040754318237, D Loss: 1.3857614994049072\n",
            "Epoch 8, Batch 717, G Loss: 0.6941108703613281, D Loss: 1.3858088254928589\n",
            "Epoch 8, Batch 718, G Loss: 0.694099485874176, D Loss: 1.3853766918182373\n",
            "Epoch 8, Batch 719, G Loss: 0.6941289901733398, D Loss: 1.3849892616271973\n",
            "Epoch 8, Batch 720, G Loss: 0.6941846013069153, D Loss: 1.3852097988128662\n",
            "Epoch 8, Batch 721, G Loss: 0.6942703723907471, D Loss: 1.3851256370544434\n",
            "Epoch 8, Batch 722, G Loss: 0.6943493485450745, D Loss: 1.3854167461395264\n",
            "Epoch 8, Batch 723, G Loss: 0.6944282650947571, D Loss: 1.385563850402832\n",
            "Epoch 8, Batch 724, G Loss: 0.6945122480392456, D Loss: 1.3854594230651855\n",
            "Epoch 8, Batch 725, G Loss: 0.6945633888244629, D Loss: 1.3860583305358887\n",
            "Epoch 8, Batch 726, G Loss: 0.69459068775177, D Loss: 1.3856935501098633\n",
            "Epoch 8, Batch 727, G Loss: 0.69460529088974, D Loss: 1.3856396675109863\n",
            "Epoch 8, Batch 728, G Loss: 0.6945849061012268, D Loss: 1.385744333267212\n",
            "Epoch 8, Batch 729, G Loss: 0.6945342421531677, D Loss: 1.3859567642211914\n",
            "Epoch 8, Batch 730, G Loss: 0.6944969892501831, D Loss: 1.3864850997924805\n",
            "Epoch 8, Batch 731, G Loss: 0.6943905353546143, D Loss: 1.3861031532287598\n",
            "Epoch 8, Batch 732, G Loss: 0.6942416429519653, D Loss: 1.3864085674285889\n",
            "Epoch 8, Batch 733, G Loss: 0.6940983533859253, D Loss: 1.386281967163086\n",
            "Epoch 8, Batch 734, G Loss: 0.6939041614532471, D Loss: 1.3860855102539062\n",
            "Epoch 8, Batch 735, G Loss: 0.6937392950057983, D Loss: 1.3859431743621826\n",
            "Epoch 8, Batch 736, G Loss: 0.6935788989067078, D Loss: 1.3860118389129639\n",
            "Epoch 8, Batch 737, G Loss: 0.693425714969635, D Loss: 1.38608980178833\n",
            "Epoch 8, Batch 738, G Loss: 0.6932563781738281, D Loss: 1.385301113128662\n",
            "Epoch 8, Batch 739, G Loss: 0.6931939125061035, D Loss: 1.3851689100265503\n",
            "Epoch 8, Batch 740, G Loss: 0.6931397318840027, D Loss: 1.3850897550582886\n",
            "Epoch 8, Batch 741, G Loss: 0.6931486129760742, D Loss: 1.3864772319793701\n",
            "Epoch 8, Batch 742, G Loss: 0.693117082118988, D Loss: 1.386448621749878\n",
            "Epoch 8, Batch 743, G Loss: 0.6931108832359314, D Loss: 1.3862450122833252\n",
            "Epoch 8, Batch 744, G Loss: 0.6930938959121704, D Loss: 1.3872839212417603\n",
            "Epoch 8, Batch 745, G Loss: 0.6930460929870605, D Loss: 1.3875672817230225\n",
            "Epoch 8, Batch 746, G Loss: 0.6929436326026917, D Loss: 1.3869986534118652\n",
            "Epoch 8, Batch 747, G Loss: 0.6928267478942871, D Loss: 1.3866848945617676\n",
            "Epoch 8, Batch 748, G Loss: 0.6927129030227661, D Loss: 1.3867864608764648\n",
            "Epoch 8, Batch 749, G Loss: 0.6926023364067078, D Loss: 1.3867771625518799\n",
            "Epoch 8, Batch 750, G Loss: 0.6925099492073059, D Loss: 1.3861690759658813\n",
            "Epoch 8, Batch 751, G Loss: 0.692426860332489, D Loss: 1.38749098777771\n",
            "Epoch 8, Batch 752, G Loss: 0.6923247575759888, D Loss: 1.3867472410202026\n",
            "Epoch 8, Batch 753, G Loss: 0.6922680735588074, D Loss: 1.387452244758606\n",
            "Epoch 8, Batch 754, G Loss: 0.6921768188476562, D Loss: 1.3872954845428467\n",
            "Epoch 8, Batch 755, G Loss: 0.6920611262321472, D Loss: 1.3872902393341064\n",
            "Epoch 8, Batch 756, G Loss: 0.6919677257537842, D Loss: 1.3871216773986816\n",
            "Epoch 8, Batch 757, G Loss: 0.6918826699256897, D Loss: 1.3871307373046875\n",
            "Epoch 8, Batch 758, G Loss: 0.6918047070503235, D Loss: 1.3868191242218018\n",
            "Epoch 8, Batch 759, G Loss: 0.6917305588722229, D Loss: 1.3871660232543945\n",
            "Epoch 8, Batch 760, G Loss: 0.6916785836219788, D Loss: 1.3867242336273193\n",
            "Epoch 8, Batch 761, G Loss: 0.6916697025299072, D Loss: 1.3868811130523682\n",
            "Epoch 8, Batch 762, G Loss: 0.6916541457176208, D Loss: 1.3870177268981934\n",
            "Epoch 8, Batch 763, G Loss: 0.691645622253418, D Loss: 1.3869528770446777\n",
            "Epoch 8, Batch 764, G Loss: 0.6916598677635193, D Loss: 1.3867218494415283\n",
            "Epoch 8, Batch 765, G Loss: 0.6917070746421814, D Loss: 1.3857288360595703\n",
            "Epoch 8, Batch 766, G Loss: 0.6917877197265625, D Loss: 1.3859167098999023\n",
            "Epoch 8, Batch 767, G Loss: 0.6919347643852234, D Loss: 1.3862264156341553\n",
            "Epoch 8, Batch 768, G Loss: 0.6920916438102722, D Loss: 1.3867014646530151\n",
            "Epoch 8, Batch 769, G Loss: 0.6922404170036316, D Loss: 1.386819839477539\n",
            "Epoch 8, Batch 770, G Loss: 0.6923761367797852, D Loss: 1.3866493701934814\n",
            "Epoch 8, Batch 771, G Loss: 0.6924927234649658, D Loss: 1.386500358581543\n",
            "Epoch 8, Batch 772, G Loss: 0.6926085948944092, D Loss: 1.3865025043487549\n",
            "Epoch 8, Batch 773, G Loss: 0.6927292346954346, D Loss: 1.3866729736328125\n",
            "Epoch 8, Batch 774, G Loss: 0.6928029656410217, D Loss: 1.3865543603897095\n",
            "Epoch 8, Batch 775, G Loss: 0.6928666830062866, D Loss: 1.3865132331848145\n",
            "Epoch 8, Batch 776, G Loss: 0.6929284930229187, D Loss: 1.3865251541137695\n",
            "Epoch 8, Batch 777, G Loss: 0.692976176738739, D Loss: 1.386242389678955\n",
            "Epoch 8, Batch 778, G Loss: 0.6930260062217712, D Loss: 1.3867926597595215\n",
            "Epoch 8, Batch 779, G Loss: 0.6930415034294128, D Loss: 1.3863773345947266\n",
            "Epoch 8, Batch 780, G Loss: 0.6930549740791321, D Loss: 1.3863308429718018\n",
            "Epoch 8, Batch 781, G Loss: 0.6930500864982605, D Loss: 1.386199951171875\n",
            "Epoch 8, Batch 782, G Loss: 0.693080723285675, D Loss: 1.3863441944122314\n",
            "Epoch 8, Batch 783, G Loss: 0.6930785179138184, D Loss: 1.3864576816558838\n",
            "Epoch 8, Batch 784, G Loss: 0.6931014657020569, D Loss: 1.3865413665771484\n",
            "Epoch 8, Batch 785, G Loss: 0.6930780410766602, D Loss: 1.3864080905914307\n",
            "Epoch 8, Batch 786, G Loss: 0.6930615901947021, D Loss: 1.3864243030548096\n",
            "Epoch 8, Batch 787, G Loss: 0.6930400133132935, D Loss: 1.386470079421997\n",
            "Epoch 8, Batch 788, G Loss: 0.6930112838745117, D Loss: 1.3863909244537354\n",
            "Epoch 8, Batch 789, G Loss: 0.6929817199707031, D Loss: 1.3864386081695557\n",
            "Epoch 8, Batch 790, G Loss: 0.6929469108581543, D Loss: 1.3865227699279785\n",
            "Epoch 8, Batch 791, G Loss: 0.6929005980491638, D Loss: 1.386399507522583\n",
            "Epoch 8, Batch 792, G Loss: 0.6928623914718628, D Loss: 1.3863779306411743\n",
            "Epoch 8, Batch 793, G Loss: 0.6928258538246155, D Loss: 1.3864725828170776\n",
            "Epoch 8, Batch 794, G Loss: 0.6927886009216309, D Loss: 1.3866162300109863\n",
            "Epoch 8, Batch 795, G Loss: 0.6927304863929749, D Loss: 1.3864772319793701\n",
            "Epoch 8, Batch 796, G Loss: 0.6926738023757935, D Loss: 1.3865197896957397\n",
            "Epoch 8, Batch 797, G Loss: 0.6926094889640808, D Loss: 1.386415719985962\n",
            "Epoch 8, Batch 798, G Loss: 0.6925584673881531, D Loss: 1.3863945007324219\n",
            "Epoch 8, Batch 799, G Loss: 0.692523181438446, D Loss: 1.3864617347717285\n",
            "Epoch 8, Batch 800, G Loss: 0.6924765706062317, D Loss: 1.3865169286727905\n",
            "Epoch 8, Batch 801, G Loss: 0.6924227476119995, D Loss: 1.3865220546722412\n",
            "Epoch 8, Batch 802, G Loss: 0.6923577189445496, D Loss: 1.3864448070526123\n",
            "Epoch 8, Batch 803, G Loss: 0.6922913193702698, D Loss: 1.3863928318023682\n",
            "Epoch 8, Batch 804, G Loss: 0.6922299265861511, D Loss: 1.3863611221313477\n",
            "Epoch 8, Batch 805, G Loss: 0.6921855211257935, D Loss: 1.3863366842269897\n",
            "Epoch 8, Batch 806, G Loss: 0.6921446919441223, D Loss: 1.3863340616226196\n",
            "Epoch 8, Batch 807, G Loss: 0.6921271681785583, D Loss: 1.386279821395874\n",
            "Epoch 8, Batch 808, G Loss: 0.6921240091323853, D Loss: 1.3862991333007812\n",
            "Epoch 8, Batch 809, G Loss: 0.6921434998512268, D Loss: 1.3863154649734497\n",
            "Epoch 8, Batch 810, G Loss: 0.6921849846839905, D Loss: 1.3862714767456055\n",
            "Epoch 8, Batch 811, G Loss: 0.6922406554222107, D Loss: 1.3863270282745361\n",
            "Epoch 8, Batch 812, G Loss: 0.6923144459724426, D Loss: 1.3862347602844238\n",
            "Epoch 8, Batch 813, G Loss: 0.6923941969871521, D Loss: 1.3862738609313965\n",
            "Epoch 8, Batch 814, G Loss: 0.6924812197685242, D Loss: 1.3864045143127441\n",
            "Epoch 8, Batch 815, G Loss: 0.6925848126411438, D Loss: 1.3862766027450562\n",
            "Epoch 8, Batch 816, G Loss: 0.6926881670951843, D Loss: 1.3863563537597656\n",
            "Epoch 8, Batch 817, G Loss: 0.6928022503852844, D Loss: 1.3863964080810547\n",
            "Epoch 8, Batch 818, G Loss: 0.6929175853729248, D Loss: 1.3862392902374268\n",
            "Epoch 8, Batch 819, G Loss: 0.6930249333381653, D Loss: 1.3862998485565186\n",
            "Epoch 8, Batch 820, G Loss: 0.6931203007698059, D Loss: 1.3859448432922363\n",
            "Epoch 8, Batch 821, G Loss: 0.6931849122047424, D Loss: 1.3861329555511475\n",
            "Epoch 8, Batch 822, G Loss: 0.6932274699211121, D Loss: 1.386132836341858\n",
            "Epoch 8, Batch 823, G Loss: 0.6932569742202759, D Loss: 1.38623046875\n",
            "Epoch 8, Batch 824, G Loss: 0.6932685971260071, D Loss: 1.3862948417663574\n",
            "Epoch 8, Batch 825, G Loss: 0.6932797431945801, D Loss: 1.3863623142242432\n",
            "Epoch 8, Batch 826, G Loss: 0.693284809589386, D Loss: 1.3861844539642334\n",
            "Epoch 8, Batch 827, G Loss: 0.6932860612869263, D Loss: 1.3860929012298584\n",
            "Epoch 8, Batch 828, G Loss: 0.6932733654975891, D Loss: 1.3859950304031372\n",
            "Epoch 8, Batch 829, G Loss: 0.6932447552680969, D Loss: 1.385908603668213\n",
            "Epoch 8, Batch 830, G Loss: 0.6932024359703064, D Loss: 1.385868787765503\n",
            "Epoch 8, Batch 831, G Loss: 0.6931266188621521, D Loss: 1.386228322982788\n",
            "Epoch 8, Batch 832, G Loss: 0.6930702328681946, D Loss: 1.3860008716583252\n",
            "Epoch 8, Batch 833, G Loss: 0.6930135488510132, D Loss: 1.3861021995544434\n",
            "Epoch 8, Batch 834, G Loss: 0.6929540634155273, D Loss: 1.3862695693969727\n",
            "Epoch 8, Batch 835, G Loss: 0.6929163932800293, D Loss: 1.3861130475997925\n",
            "Epoch 8, Batch 836, G Loss: 0.6928706169128418, D Loss: 1.386260747909546\n",
            "Epoch 8, Batch 837, G Loss: 0.6928479075431824, D Loss: 1.3861987590789795\n",
            "Epoch 8, Batch 838, G Loss: 0.6928347945213318, D Loss: 1.386444330215454\n",
            "Epoch 8, Batch 839, G Loss: 0.6928271651268005, D Loss: 1.3861875534057617\n",
            "Epoch 8, Batch 840, G Loss: 0.6928494572639465, D Loss: 1.3862398862838745\n",
            "Epoch 8, Batch 841, G Loss: 0.6928558945655823, D Loss: 1.386528730392456\n",
            "Epoch 8, Batch 842, G Loss: 0.6928890943527222, D Loss: 1.3862478733062744\n",
            "Epoch 8, Batch 843, G Loss: 0.6929240226745605, D Loss: 1.3859590291976929\n",
            "Epoch 8, Batch 844, G Loss: 0.6929689645767212, D Loss: 1.385892391204834\n",
            "Epoch 8, Batch 845, G Loss: 0.6929911375045776, D Loss: 1.386092185974121\n",
            "Epoch 8, Batch 846, G Loss: 0.6930277347564697, D Loss: 1.3860502243041992\n",
            "Epoch 8, Batch 847, G Loss: 0.693056046962738, D Loss: 1.3862018585205078\n",
            "Epoch 8, Batch 848, G Loss: 0.6930682063102722, D Loss: 1.3865282535552979\n",
            "Epoch 8, Batch 849, G Loss: 0.6931119561195374, D Loss: 1.3864940404891968\n",
            "Epoch 8, Batch 850, G Loss: 0.6931493282318115, D Loss: 1.3858284950256348\n",
            "Epoch 8, Batch 851, G Loss: 0.6931774616241455, D Loss: 1.3860173225402832\n",
            "Epoch 8, Batch 852, G Loss: 0.6931973099708557, D Loss: 1.385711431503296\n",
            "Epoch 8, Batch 853, G Loss: 0.6932132244110107, D Loss: 1.385833501815796\n",
            "Epoch 8, Batch 854, G Loss: 0.6932110786437988, D Loss: 1.385843276977539\n",
            "Epoch 8, Batch 855, G Loss: 0.6931969523429871, D Loss: 1.3859150409698486\n",
            "Epoch 8, Batch 856, G Loss: 0.6931739449501038, D Loss: 1.38590669631958\n",
            "Epoch 8, Batch 857, G Loss: 0.6931712627410889, D Loss: 1.3858697414398193\n",
            "Epoch 8, Batch 858, G Loss: 0.6931385397911072, D Loss: 1.3863650560379028\n",
            "Epoch 8, Batch 859, G Loss: 0.6931152939796448, D Loss: 1.3861287832260132\n",
            "Epoch 8, Batch 860, G Loss: 0.693107008934021, D Loss: 1.3858442306518555\n",
            "Epoch 8, Batch 861, G Loss: 0.6931057572364807, D Loss: 1.3861510753631592\n",
            "Epoch 8, Batch 862, G Loss: 0.6930941939353943, D Loss: 1.3859753608703613\n",
            "Epoch 8, Batch 863, G Loss: 0.693085253238678, D Loss: 1.3860547542572021\n",
            "Epoch 8, Batch 864, G Loss: 0.6930897235870361, D Loss: 1.3859705924987793\n",
            "Epoch 8, Batch 865, G Loss: 0.6930745840072632, D Loss: 1.3863639831542969\n",
            "Epoch 8, Batch 866, G Loss: 0.6930909156799316, D Loss: 1.3863489627838135\n",
            "Epoch 8, Batch 867, G Loss: 0.693107008934021, D Loss: 1.386232614517212\n",
            "Epoch 8, Batch 868, G Loss: 0.6931144595146179, D Loss: 1.3857097625732422\n",
            "Epoch 8, Batch 869, G Loss: 0.6931326985359192, D Loss: 1.3859370946884155\n",
            "Epoch 8, Batch 870, G Loss: 0.6931479573249817, D Loss: 1.3851349353790283\n",
            "Epoch 8, Batch 871, G Loss: 0.6931443214416504, D Loss: 1.3853743076324463\n",
            "Epoch 8, Batch 872, G Loss: 0.6931319236755371, D Loss: 1.3854005336761475\n",
            "Epoch 8, Batch 873, G Loss: 0.693103551864624, D Loss: 1.3860664367675781\n",
            "Epoch 8, Batch 874, G Loss: 0.6930876970291138, D Loss: 1.3861584663391113\n",
            "Epoch 8, Batch 875, G Loss: 0.6930806636810303, D Loss: 1.38612961769104\n",
            "Epoch 8, Batch 876, G Loss: 0.6930951476097107, D Loss: 1.386220932006836\n",
            "Epoch 8, Batch 877, G Loss: 0.6931062936782837, D Loss: 1.386317491531372\n",
            "Epoch 8, Batch 878, G Loss: 0.6931322813034058, D Loss: 1.386095643043518\n",
            "Epoch 8, Batch 879, G Loss: 0.6931407451629639, D Loss: 1.3860740661621094\n",
            "Epoch 8, Batch 880, G Loss: 0.6931732296943665, D Loss: 1.386701226234436\n",
            "Epoch 8, Batch 881, G Loss: 0.6932050585746765, D Loss: 1.3868449926376343\n",
            "Epoch 8, Batch 882, G Loss: 0.6932342052459717, D Loss: 1.3868693113327026\n",
            "Epoch 8, Batch 883, G Loss: 0.6932978630065918, D Loss: 1.3865479230880737\n",
            "Epoch 8, Batch 884, G Loss: 0.6933400630950928, D Loss: 1.3863461017608643\n",
            "Epoch 8, Batch 885, G Loss: 0.6933929920196533, D Loss: 1.3865571022033691\n",
            "Epoch 8, Batch 886, G Loss: 0.6934601664543152, D Loss: 1.3865559101104736\n",
            "Epoch 8, Batch 887, G Loss: 0.6935080885887146, D Loss: 1.3856477737426758\n",
            "Epoch 8, Batch 888, G Loss: 0.6935312151908875, D Loss: 1.3860656023025513\n",
            "Epoch 8, Batch 889, G Loss: 0.6935581564903259, D Loss: 1.386104941368103\n",
            "Epoch 8, Batch 890, G Loss: 0.69357830286026, D Loss: 1.3865773677825928\n",
            "Epoch 8, Batch 891, G Loss: 0.6936030387878418, D Loss: 1.386829137802124\n",
            "Epoch 8, Batch 892, G Loss: 0.6936129927635193, D Loss: 1.3874142169952393\n",
            "Epoch 8, Batch 893, G Loss: 0.6936330795288086, D Loss: 1.3869552612304688\n",
            "Epoch 8, Batch 894, G Loss: 0.693683385848999, D Loss: 1.386602520942688\n",
            "Epoch 8, Batch 895, G Loss: 0.6937042474746704, D Loss: 1.3863662481307983\n",
            "Epoch 8, Batch 896, G Loss: 0.693706750869751, D Loss: 1.3865057229995728\n",
            "Epoch 8, Batch 897, G Loss: 0.6937374472618103, D Loss: 1.3864232301712036\n",
            "Epoch 8, Batch 898, G Loss: 0.6937463879585266, D Loss: 1.3866288661956787\n",
            "Epoch 8, Batch 899, G Loss: 0.6937456727027893, D Loss: 1.3862388134002686\n",
            "Epoch 8, Batch 900, G Loss: 0.6937277913093567, D Loss: 1.3863718509674072\n",
            "Epoch 8, Batch 901, G Loss: 0.6937279105186462, D Loss: 1.3866842985153198\n",
            "Epoch 8, Batch 902, G Loss: 0.6937140822410583, D Loss: 1.3870066404342651\n",
            "Epoch 8, Batch 903, G Loss: 0.6936922669410706, D Loss: 1.3870208263397217\n",
            "Epoch 8, Batch 904, G Loss: 0.6937083005905151, D Loss: 1.3870456218719482\n",
            "Epoch 8, Batch 905, G Loss: 0.6937329769134521, D Loss: 1.3866817951202393\n",
            "Epoch 8, Batch 906, G Loss: 0.6937341690063477, D Loss: 1.386883020401001\n",
            "Epoch 8, Batch 907, G Loss: 0.6937533020973206, D Loss: 1.386470079421997\n",
            "Epoch 8, Batch 908, G Loss: 0.6937693953514099, D Loss: 1.3869867324829102\n",
            "Epoch 8, Batch 909, G Loss: 0.6937978863716125, D Loss: 1.3869893550872803\n",
            "Epoch 8, Batch 910, G Loss: 0.6938166618347168, D Loss: 1.3864599466323853\n",
            "Epoch 8, Batch 911, G Loss: 0.6938410997390747, D Loss: 1.3863072395324707\n",
            "Epoch 8, Batch 912, G Loss: 0.6938382387161255, D Loss: 1.386411190032959\n",
            "Epoch 8, Batch 913, G Loss: 0.693836510181427, D Loss: 1.3861463069915771\n",
            "Epoch 8, Batch 914, G Loss: 0.6938040852546692, D Loss: 1.3862015008926392\n",
            "Epoch 8, Batch 915, G Loss: 0.6937707662582397, D Loss: 1.3866896629333496\n",
            "Epoch 8, Batch 916, G Loss: 0.6937385201454163, D Loss: 1.386909008026123\n",
            "Epoch 8, Batch 917, G Loss: 0.6937258839607239, D Loss: 1.386901617050171\n",
            "Epoch 8, Batch 918, G Loss: 0.6937474012374878, D Loss: 1.386890172958374\n",
            "Epoch 8, Batch 919, G Loss: 0.6937886476516724, D Loss: 1.386671543121338\n",
            "Epoch 8, Batch 920, G Loss: 0.6938390135765076, D Loss: 1.3862667083740234\n",
            "Epoch 8, Batch 921, G Loss: 0.6938656568527222, D Loss: 1.3863842487335205\n",
            "Epoch 8, Batch 922, G Loss: 0.6938811540603638, D Loss: 1.3863614797592163\n",
            "Epoch 8, Batch 923, G Loss: 0.693885326385498, D Loss: 1.3864177465438843\n",
            "Epoch 8, Batch 924, G Loss: 0.6938896179199219, D Loss: 1.386407494544983\n",
            "Epoch 8, Batch 925, G Loss: 0.6938931345939636, D Loss: 1.3863542079925537\n",
            "Epoch 8, Batch 926, G Loss: 0.6938890814781189, D Loss: 1.3863177299499512\n",
            "Epoch 8, Batch 927, G Loss: 0.6938741207122803, D Loss: 1.3862401247024536\n",
            "Epoch 8, Batch 928, G Loss: 0.6938316822052002, D Loss: 1.3861212730407715\n",
            "Epoch 8, Batch 929, G Loss: 0.6937143802642822, D Loss: 1.3860725164413452\n",
            "Epoch 8, Batch 930, G Loss: 0.6934972405433655, D Loss: 1.3862357139587402\n",
            "Epoch 8, Batch 931, G Loss: 0.6932762265205383, D Loss: 1.386347770690918\n",
            "Epoch 8, Batch 932, G Loss: 0.6931042671203613, D Loss: 1.386307716369629\n",
            "Epoch 8, Batch 933, G Loss: 0.6929653882980347, D Loss: 1.3862736225128174\n",
            "Epoch 8, Batch 934, G Loss: 0.692847728729248, D Loss: 1.3863049745559692\n",
            "Epoch 8, Batch 935, G Loss: 0.6927647590637207, D Loss: 1.386275291442871\n",
            "Epoch 8, Batch 936, G Loss: 0.6927002668380737, D Loss: 1.3862580060958862\n",
            "Epoch 8, Batch 937, G Loss: 0.6926473379135132, D Loss: 1.3863344192504883\n",
            "Epoch 8, Batch 938, G Loss: 0.6926506757736206, D Loss: 1.3863165378570557\n",
            "Epoch 9, Batch 1, G Loss: 0.6927001476287842, D Loss: 1.3862907886505127\n",
            "Epoch 9, Batch 2, G Loss: 0.6927734613418579, D Loss: 1.386284351348877\n",
            "Epoch 9, Batch 3, G Loss: 0.6928617358207703, D Loss: 1.3862802982330322\n",
            "Epoch 9, Batch 4, G Loss: 0.6929705739021301, D Loss: 1.386265516281128\n",
            "Epoch 9, Batch 5, G Loss: 0.6930829882621765, D Loss: 1.3862578868865967\n",
            "Epoch 9, Batch 6, G Loss: 0.6932119131088257, D Loss: 1.3862494230270386\n",
            "Epoch 9, Batch 7, G Loss: 0.6933444142341614, D Loss: 1.3862262964248657\n",
            "Epoch 9, Batch 8, G Loss: 0.6934824585914612, D Loss: 1.3862104415893555\n",
            "Epoch 9, Batch 9, G Loss: 0.6936202049255371, D Loss: 1.3862148523330688\n",
            "Epoch 9, Batch 10, G Loss: 0.6937474012374878, D Loss: 1.3861632347106934\n",
            "Epoch 9, Batch 11, G Loss: 0.6938727498054504, D Loss: 1.3862059116363525\n",
            "Epoch 9, Batch 12, G Loss: 0.6939790844917297, D Loss: 1.3862683773040771\n",
            "Epoch 9, Batch 13, G Loss: 0.6940442323684692, D Loss: 1.3862113952636719\n",
            "Epoch 9, Batch 14, G Loss: 0.6940903067588806, D Loss: 1.3862242698669434\n",
            "Epoch 9, Batch 15, G Loss: 0.694105327129364, D Loss: 1.386185646057129\n",
            "Epoch 9, Batch 16, G Loss: 0.6941062808036804, D Loss: 1.3861000537872314\n",
            "Epoch 9, Batch 17, G Loss: 0.694100022315979, D Loss: 1.385981798171997\n",
            "Epoch 9, Batch 18, G Loss: 0.6940983533859253, D Loss: 1.3860197067260742\n",
            "Epoch 9, Batch 19, G Loss: 0.6941080093383789, D Loss: 1.386246681213379\n",
            "Epoch 9, Batch 20, G Loss: 0.6940808892250061, D Loss: 1.3865227699279785\n",
            "Epoch 9, Batch 21, G Loss: 0.6940032839775085, D Loss: 1.3864288330078125\n",
            "Epoch 9, Batch 22, G Loss: 0.6938897967338562, D Loss: 1.3866324424743652\n",
            "Epoch 9, Batch 23, G Loss: 0.6937210559844971, D Loss: 1.3860995769500732\n",
            "Epoch 9, Batch 24, G Loss: 0.6935740113258362, D Loss: 1.386051893234253\n",
            "Epoch 9, Batch 25, G Loss: 0.6934399008750916, D Loss: 1.3860654830932617\n",
            "Epoch 9, Batch 26, G Loss: 0.6933380365371704, D Loss: 1.3859984874725342\n",
            "Epoch 9, Batch 27, G Loss: 0.6932581663131714, D Loss: 1.386003851890564\n",
            "Epoch 9, Batch 28, G Loss: 0.6932147145271301, D Loss: 1.3860383033752441\n",
            "Epoch 9, Batch 29, G Loss: 0.6931853890419006, D Loss: 1.3860063552856445\n",
            "Epoch 9, Batch 30, G Loss: 0.6931851506233215, D Loss: 1.386193037033081\n",
            "Epoch 9, Batch 31, G Loss: 0.693184494972229, D Loss: 1.3863801956176758\n",
            "Epoch 9, Batch 32, G Loss: 0.6931728720664978, D Loss: 1.3864809274673462\n",
            "Epoch 9, Batch 33, G Loss: 0.6931355595588684, D Loss: 1.3862946033477783\n",
            "Epoch 9, Batch 34, G Loss: 0.6931073069572449, D Loss: 1.3861773014068604\n",
            "Epoch 9, Batch 35, G Loss: 0.6930895447731018, D Loss: 1.3860809803009033\n",
            "Epoch 9, Batch 36, G Loss: 0.6930818557739258, D Loss: 1.3863118886947632\n",
            "Epoch 9, Batch 37, G Loss: 0.6930699348449707, D Loss: 1.3861162662506104\n",
            "Epoch 9, Batch 38, G Loss: 0.6930713057518005, D Loss: 1.3860820531845093\n",
            "Epoch 9, Batch 39, G Loss: 0.6930987238883972, D Loss: 1.3861215114593506\n",
            "Epoch 9, Batch 40, G Loss: 0.6931230425834656, D Loss: 1.3862810134887695\n",
            "Epoch 9, Batch 41, G Loss: 0.6931423544883728, D Loss: 1.386210560798645\n",
            "Epoch 9, Batch 42, G Loss: 0.6931690573692322, D Loss: 1.3863526582717896\n",
            "Epoch 9, Batch 43, G Loss: 0.6931747198104858, D Loss: 1.3860722780227661\n",
            "Epoch 9, Batch 44, G Loss: 0.6931946277618408, D Loss: 1.3862160444259644\n",
            "Epoch 9, Batch 45, G Loss: 0.6932190656661987, D Loss: 1.3863751888275146\n",
            "Epoch 9, Batch 46, G Loss: 0.6932195425033569, D Loss: 1.3862414360046387\n",
            "Epoch 9, Batch 47, G Loss: 0.6932106018066406, D Loss: 1.385913372039795\n",
            "Epoch 9, Batch 48, G Loss: 0.6932381391525269, D Loss: 1.386143684387207\n",
            "Epoch 9, Batch 49, G Loss: 0.6932581663131714, D Loss: 1.3859598636627197\n",
            "Epoch 9, Batch 50, G Loss: 0.6933005452156067, D Loss: 1.3859766721725464\n",
            "Epoch 9, Batch 51, G Loss: 0.6933640241622925, D Loss: 1.386002779006958\n",
            "Epoch 9, Batch 52, G Loss: 0.6934168338775635, D Loss: 1.3859388828277588\n",
            "Epoch 9, Batch 53, G Loss: 0.6934898495674133, D Loss: 1.386218547821045\n",
            "Epoch 9, Batch 54, G Loss: 0.6935353875160217, D Loss: 1.3859105110168457\n",
            "Epoch 9, Batch 55, G Loss: 0.693605363368988, D Loss: 1.3859273195266724\n",
            "Epoch 9, Batch 56, G Loss: 0.6936458945274353, D Loss: 1.3862054347991943\n",
            "Epoch 9, Batch 57, G Loss: 0.6936985850334167, D Loss: 1.3864386081695557\n",
            "Epoch 9, Batch 58, G Loss: 0.6936987042427063, D Loss: 1.3866643905639648\n",
            "Epoch 9, Batch 59, G Loss: 0.6936665177345276, D Loss: 1.3861664533615112\n",
            "Epoch 9, Batch 60, G Loss: 0.6936134099960327, D Loss: 1.3861775398254395\n",
            "Epoch 9, Batch 61, G Loss: 0.6935587525367737, D Loss: 1.3860864639282227\n",
            "Epoch 9, Batch 62, G Loss: 0.6935167908668518, D Loss: 1.386177897453308\n",
            "Epoch 9, Batch 63, G Loss: 0.69344562292099, D Loss: 1.3861875534057617\n",
            "Epoch 9, Batch 64, G Loss: 0.6934038996696472, D Loss: 1.386468529701233\n",
            "Epoch 9, Batch 65, G Loss: 0.693340539932251, D Loss: 1.3862030506134033\n",
            "Epoch 9, Batch 66, G Loss: 0.6932656168937683, D Loss: 1.386126160621643\n",
            "Epoch 9, Batch 67, G Loss: 0.6932273507118225, D Loss: 1.3864102363586426\n",
            "Epoch 9, Batch 68, G Loss: 0.6931533813476562, D Loss: 1.3862208127975464\n",
            "Epoch 9, Batch 69, G Loss: 0.6930981278419495, D Loss: 1.3860069513320923\n",
            "Epoch 9, Batch 70, G Loss: 0.6930725574493408, D Loss: 1.386242151260376\n",
            "Epoch 9, Batch 71, G Loss: 0.6930537223815918, D Loss: 1.3868119716644287\n",
            "Epoch 9, Batch 72, G Loss: 0.693001389503479, D Loss: 1.3864028453826904\n",
            "Epoch 9, Batch 73, G Loss: 0.692934513092041, D Loss: 1.3867367506027222\n",
            "Epoch 9, Batch 74, G Loss: 0.692882239818573, D Loss: 1.3869667053222656\n",
            "Epoch 9, Batch 75, G Loss: 0.6927726864814758, D Loss: 1.386494517326355\n",
            "Epoch 9, Batch 76, G Loss: 0.6926733255386353, D Loss: 1.386662483215332\n",
            "Epoch 9, Batch 77, G Loss: 0.6925814747810364, D Loss: 1.3866031169891357\n",
            "Epoch 9, Batch 78, G Loss: 0.6924888491630554, D Loss: 1.386198878288269\n",
            "Epoch 9, Batch 79, G Loss: 0.6924379467964172, D Loss: 1.3864307403564453\n",
            "Epoch 9, Batch 80, G Loss: 0.6923991441726685, D Loss: 1.3866034746170044\n",
            "Epoch 9, Batch 81, G Loss: 0.6923591494560242, D Loss: 1.3865771293640137\n",
            "Epoch 9, Batch 82, G Loss: 0.69233238697052, D Loss: 1.38653564453125\n",
            "Epoch 9, Batch 83, G Loss: 0.6923125386238098, D Loss: 1.386466145515442\n",
            "Epoch 9, Batch 84, G Loss: 0.6922991871833801, D Loss: 1.3867952823638916\n",
            "Epoch 9, Batch 85, G Loss: 0.6922744512557983, D Loss: 1.3867363929748535\n",
            "Epoch 9, Batch 86, G Loss: 0.6922400593757629, D Loss: 1.386714220046997\n",
            "Epoch 9, Batch 87, G Loss: 0.6922124624252319, D Loss: 1.3865437507629395\n",
            "Epoch 9, Batch 88, G Loss: 0.6921938061714172, D Loss: 1.3864936828613281\n",
            "Epoch 9, Batch 89, G Loss: 0.6921812295913696, D Loss: 1.3864178657531738\n",
            "Epoch 9, Batch 90, G Loss: 0.6921805739402771, D Loss: 1.3864808082580566\n",
            "Epoch 9, Batch 91, G Loss: 0.6921960711479187, D Loss: 1.3863357305526733\n",
            "Epoch 9, Batch 92, G Loss: 0.6922380328178406, D Loss: 1.3863067626953125\n",
            "Epoch 9, Batch 93, G Loss: 0.6922932863235474, D Loss: 1.3863524198532104\n",
            "Epoch 9, Batch 94, G Loss: 0.6923659443855286, D Loss: 1.386631965637207\n",
            "Epoch 9, Batch 95, G Loss: 0.6924089789390564, D Loss: 1.386589527130127\n",
            "Epoch 9, Batch 96, G Loss: 0.6924186944961548, D Loss: 1.3865480422973633\n",
            "Epoch 9, Batch 97, G Loss: 0.6924085021018982, D Loss: 1.3864582777023315\n",
            "Epoch 9, Batch 98, G Loss: 0.6923797726631165, D Loss: 1.386379361152649\n",
            "Epoch 9, Batch 99, G Loss: 0.6923569440841675, D Loss: 1.3863542079925537\n",
            "Epoch 9, Batch 100, G Loss: 0.6923386454582214, D Loss: 1.3863284587860107\n",
            "Epoch 9, Batch 101, G Loss: 0.692322850227356, D Loss: 1.386294960975647\n",
            "Epoch 9, Batch 102, G Loss: 0.6923179626464844, D Loss: 1.3862897157669067\n",
            "Epoch 9, Batch 103, G Loss: 0.6923340559005737, D Loss: 1.3863328695297241\n",
            "Epoch 9, Batch 104, G Loss: 0.6923778057098389, D Loss: 1.3862451314926147\n",
            "Epoch 9, Batch 105, G Loss: 0.6924271583557129, D Loss: 1.3864178657531738\n",
            "Epoch 9, Batch 106, G Loss: 0.6925116777420044, D Loss: 1.3863978385925293\n",
            "Epoch 9, Batch 107, G Loss: 0.6926213502883911, D Loss: 1.38645601272583\n",
            "Epoch 9, Batch 108, G Loss: 0.6927586793899536, D Loss: 1.3861992359161377\n",
            "Epoch 9, Batch 109, G Loss: 0.6928808093070984, D Loss: 1.3861292600631714\n",
            "Epoch 9, Batch 110, G Loss: 0.6929786801338196, D Loss: 1.3862441778182983\n",
            "Epoch 9, Batch 111, G Loss: 0.6930666565895081, D Loss: 1.386376142501831\n",
            "Epoch 9, Batch 112, G Loss: 0.6931559443473816, D Loss: 1.3863670825958252\n",
            "Epoch 9, Batch 113, G Loss: 0.6932438611984253, D Loss: 1.3864121437072754\n",
            "Epoch 9, Batch 114, G Loss: 0.6933329105377197, D Loss: 1.386356234550476\n",
            "Epoch 9, Batch 115, G Loss: 0.6934107542037964, D Loss: 1.3865606784820557\n",
            "Epoch 9, Batch 116, G Loss: 0.6934977769851685, D Loss: 1.3864772319793701\n",
            "Epoch 9, Batch 117, G Loss: 0.6935868263244629, D Loss: 1.3863592147827148\n",
            "Epoch 9, Batch 118, G Loss: 0.6936589479446411, D Loss: 1.3864071369171143\n",
            "Epoch 9, Batch 119, G Loss: 0.6937174797058105, D Loss: 1.3863879442214966\n",
            "Epoch 9, Batch 120, G Loss: 0.6937609314918518, D Loss: 1.386252760887146\n",
            "Epoch 9, Batch 121, G Loss: 0.6937769651412964, D Loss: 1.3862812519073486\n",
            "Epoch 9, Batch 122, G Loss: 0.6937742233276367, D Loss: 1.38627028465271\n",
            "Epoch 9, Batch 123, G Loss: 0.6937499046325684, D Loss: 1.3861932754516602\n",
            "Epoch 9, Batch 124, G Loss: 0.6936998963356018, D Loss: 1.3860538005828857\n",
            "Epoch 9, Batch 125, G Loss: 0.6936111450195312, D Loss: 1.3861148357391357\n",
            "Epoch 9, Batch 126, G Loss: 0.6934983134269714, D Loss: 1.3860993385314941\n",
            "Epoch 9, Batch 127, G Loss: 0.6933623552322388, D Loss: 1.3861819505691528\n",
            "Epoch 9, Batch 128, G Loss: 0.6932289004325867, D Loss: 1.386413335800171\n",
            "Epoch 9, Batch 129, G Loss: 0.6931123733520508, D Loss: 1.3863720893859863\n",
            "Epoch 9, Batch 130, G Loss: 0.6930146217346191, D Loss: 1.3861119747161865\n",
            "Epoch 9, Batch 131, G Loss: 0.6929216980934143, D Loss: 1.386193871498108\n",
            "Epoch 9, Batch 132, G Loss: 0.6928379535675049, D Loss: 1.3862559795379639\n",
            "Epoch 9, Batch 133, G Loss: 0.6927661299705505, D Loss: 1.3857901096343994\n",
            "Epoch 9, Batch 134, G Loss: 0.6926741600036621, D Loss: 1.3859267234802246\n",
            "Epoch 9, Batch 135, G Loss: 0.6925801038742065, D Loss: 1.3859713077545166\n",
            "Epoch 9, Batch 136, G Loss: 0.6924871206283569, D Loss: 1.3858623504638672\n",
            "Epoch 9, Batch 137, G Loss: 0.6924003958702087, D Loss: 1.386139988899231\n",
            "Epoch 9, Batch 138, G Loss: 0.6923390030860901, D Loss: 1.3861924409866333\n",
            "Epoch 9, Batch 139, G Loss: 0.6923021078109741, D Loss: 1.3863751888275146\n",
            "Epoch 9, Batch 140, G Loss: 0.6922966837882996, D Loss: 1.3861351013183594\n",
            "Epoch 9, Batch 141, G Loss: 0.6923184394836426, D Loss: 1.386321783065796\n",
            "Epoch 9, Batch 142, G Loss: 0.6923599243164062, D Loss: 1.3862121105194092\n",
            "Epoch 9, Batch 143, G Loss: 0.692425012588501, D Loss: 1.3865580558776855\n",
            "Epoch 9, Batch 144, G Loss: 0.6925199627876282, D Loss: 1.3863497972488403\n",
            "Epoch 9, Batch 145, G Loss: 0.692629337310791, D Loss: 1.3867204189300537\n",
            "Epoch 9, Batch 146, G Loss: 0.6927576065063477, D Loss: 1.3861472606658936\n",
            "Epoch 9, Batch 147, G Loss: 0.692888081073761, D Loss: 1.3860583305358887\n",
            "Epoch 9, Batch 148, G Loss: 0.6929999589920044, D Loss: 1.3860645294189453\n",
            "Epoch 9, Batch 149, G Loss: 0.6930996775627136, D Loss: 1.3859021663665771\n",
            "Epoch 9, Batch 150, G Loss: 0.6931999325752258, D Loss: 1.386510968208313\n",
            "Epoch 9, Batch 151, G Loss: 0.6932854652404785, D Loss: 1.386300802230835\n",
            "Epoch 9, Batch 152, G Loss: 0.693359375, D Loss: 1.3860012292861938\n",
            "Epoch 9, Batch 153, G Loss: 0.693415641784668, D Loss: 1.3861160278320312\n",
            "Epoch 9, Batch 154, G Loss: 0.6934607625007629, D Loss: 1.3862922191619873\n",
            "Epoch 9, Batch 155, G Loss: 0.6934921741485596, D Loss: 1.386286973953247\n",
            "Epoch 9, Batch 156, G Loss: 0.6935121417045593, D Loss: 1.3863592147827148\n",
            "Epoch 9, Batch 157, G Loss: 0.6935184001922607, D Loss: 1.3859946727752686\n",
            "Epoch 9, Batch 158, G Loss: 0.693518340587616, D Loss: 1.3863918781280518\n",
            "Epoch 9, Batch 159, G Loss: 0.6935189962387085, D Loss: 1.386622428894043\n",
            "Epoch 9, Batch 160, G Loss: 0.6935052871704102, D Loss: 1.3853209018707275\n",
            "Epoch 9, Batch 161, G Loss: 0.6934659481048584, D Loss: 1.385188102722168\n",
            "Epoch 9, Batch 162, G Loss: 0.6933982968330383, D Loss: 1.3857464790344238\n",
            "Epoch 9, Batch 163, G Loss: 0.6932988166809082, D Loss: 1.3859338760375977\n",
            "Epoch 9, Batch 164, G Loss: 0.6931993365287781, D Loss: 1.3857522010803223\n",
            "Epoch 9, Batch 165, G Loss: 0.6931126117706299, D Loss: 1.3860000371932983\n",
            "Epoch 9, Batch 166, G Loss: 0.6930272579193115, D Loss: 1.3862287998199463\n",
            "Epoch 9, Batch 167, G Loss: 0.6929438710212708, D Loss: 1.386692762374878\n",
            "Epoch 9, Batch 168, G Loss: 0.6929024457931519, D Loss: 1.3864028453826904\n",
            "Epoch 9, Batch 169, G Loss: 0.6928853988647461, D Loss: 1.3861420154571533\n",
            "Epoch 9, Batch 170, G Loss: 0.6928687691688538, D Loss: 1.3861124515533447\n",
            "Epoch 9, Batch 171, G Loss: 0.6928679943084717, D Loss: 1.3862175941467285\n",
            "Epoch 9, Batch 172, G Loss: 0.6928781270980835, D Loss: 1.3859070539474487\n",
            "Epoch 9, Batch 173, G Loss: 0.6928989887237549, D Loss: 1.3858215808868408\n",
            "Epoch 9, Batch 174, G Loss: 0.6929092407226562, D Loss: 1.3863743543624878\n",
            "Epoch 9, Batch 175, G Loss: 0.692943811416626, D Loss: 1.3862385749816895\n",
            "Epoch 9, Batch 176, G Loss: 0.6929885745048523, D Loss: 1.3863348960876465\n",
            "Epoch 9, Batch 177, G Loss: 0.6930135488510132, D Loss: 1.386551856994629\n",
            "Epoch 9, Batch 178, G Loss: 0.6930838823318481, D Loss: 1.386307954788208\n",
            "Epoch 9, Batch 179, G Loss: 0.693140983581543, D Loss: 1.386484146118164\n",
            "Epoch 9, Batch 180, G Loss: 0.6932106018066406, D Loss: 1.3864858150482178\n",
            "Epoch 9, Batch 181, G Loss: 0.6932579874992371, D Loss: 1.3865445852279663\n",
            "Epoch 9, Batch 182, G Loss: 0.6933444142341614, D Loss: 1.3865036964416504\n",
            "Epoch 9, Batch 183, G Loss: 0.69341641664505, D Loss: 1.3864386081695557\n",
            "Epoch 9, Batch 184, G Loss: 0.6934677362442017, D Loss: 1.3866136074066162\n",
            "Epoch 9, Batch 185, G Loss: 0.6935286521911621, D Loss: 1.3864858150482178\n",
            "Epoch 9, Batch 186, G Loss: 0.6935821771621704, D Loss: 1.386528730392456\n",
            "Epoch 9, Batch 187, G Loss: 0.693638026714325, D Loss: 1.3866534233093262\n",
            "Epoch 9, Batch 188, G Loss: 0.6936730146408081, D Loss: 1.3859375715255737\n",
            "Epoch 9, Batch 189, G Loss: 0.6936955451965332, D Loss: 1.3862926959991455\n",
            "Epoch 9, Batch 190, G Loss: 0.6937047243118286, D Loss: 1.3863141536712646\n",
            "Epoch 9, Batch 191, G Loss: 0.6937136054039001, D Loss: 1.3861764669418335\n",
            "Epoch 9, Batch 192, G Loss: 0.6936812996864319, D Loss: 1.3861223459243774\n",
            "Epoch 9, Batch 193, G Loss: 0.6936611533164978, D Loss: 1.3863184452056885\n",
            "Epoch 9, Batch 194, G Loss: 0.6936100721359253, D Loss: 1.386314868927002\n",
            "Epoch 9, Batch 195, G Loss: 0.6935725212097168, D Loss: 1.3861205577850342\n",
            "Epoch 9, Batch 196, G Loss: 0.6935173869132996, D Loss: 1.386242389678955\n",
            "Epoch 9, Batch 197, G Loss: 0.6934794187545776, D Loss: 1.3861920833587646\n",
            "Epoch 9, Batch 198, G Loss: 0.6934189200401306, D Loss: 1.3862030506134033\n",
            "Epoch 9, Batch 199, G Loss: 0.6933755874633789, D Loss: 1.3863954544067383\n",
            "Epoch 9, Batch 200, G Loss: 0.6933209300041199, D Loss: 1.3867496252059937\n",
            "Epoch 9, Batch 201, G Loss: 0.6932989358901978, D Loss: 1.3869893550872803\n",
            "Epoch 9, Batch 202, G Loss: 0.6933040022850037, D Loss: 1.3864307403564453\n",
            "Epoch 9, Batch 203, G Loss: 0.6933115124702454, D Loss: 1.385986089706421\n",
            "Epoch 9, Batch 204, G Loss: 0.6933104991912842, D Loss: 1.3860502243041992\n",
            "Epoch 9, Batch 205, G Loss: 0.6933019161224365, D Loss: 1.3857288360595703\n",
            "Epoch 9, Batch 206, G Loss: 0.6932716965675354, D Loss: 1.3862905502319336\n",
            "Epoch 9, Batch 207, G Loss: 0.6932451725006104, D Loss: 1.3861817121505737\n",
            "Epoch 9, Batch 208, G Loss: 0.6932230591773987, D Loss: 1.3863115310668945\n",
            "Epoch 9, Batch 209, G Loss: 0.6932069659233093, D Loss: 1.3862977027893066\n",
            "Epoch 9, Batch 210, G Loss: 0.6931894421577454, D Loss: 1.386407732963562\n",
            "Epoch 9, Batch 211, G Loss: 0.6931968927383423, D Loss: 1.3862453699111938\n",
            "Epoch 9, Batch 212, G Loss: 0.6932005882263184, D Loss: 1.3860156536102295\n",
            "Epoch 9, Batch 213, G Loss: 0.6931997537612915, D Loss: 1.3863146305084229\n",
            "Epoch 9, Batch 214, G Loss: 0.6932013630867004, D Loss: 1.3868180513381958\n",
            "Epoch 9, Batch 215, G Loss: 0.6932328939437866, D Loss: 1.3870043754577637\n",
            "Epoch 9, Batch 216, G Loss: 0.6932970881462097, D Loss: 1.3867497444152832\n",
            "Epoch 9, Batch 217, G Loss: 0.6933754086494446, D Loss: 1.3865374326705933\n",
            "Epoch 9, Batch 218, G Loss: 0.6934664845466614, D Loss: 1.3865172863006592\n",
            "Epoch 9, Batch 219, G Loss: 0.6935392618179321, D Loss: 1.3866944313049316\n",
            "Epoch 9, Batch 220, G Loss: 0.693626880645752, D Loss: 1.3861459493637085\n",
            "Epoch 9, Batch 221, G Loss: 0.6936927437782288, D Loss: 1.3861870765686035\n",
            "Epoch 9, Batch 222, G Loss: 0.6937348246574402, D Loss: 1.386091947555542\n",
            "Epoch 9, Batch 223, G Loss: 0.6937385201454163, D Loss: 1.3862614631652832\n",
            "Epoch 9, Batch 224, G Loss: 0.6937408447265625, D Loss: 1.3862507343292236\n",
            "Epoch 9, Batch 225, G Loss: 0.6937157511711121, D Loss: 1.3863558769226074\n",
            "Epoch 9, Batch 226, G Loss: 0.6936910152435303, D Loss: 1.3865025043487549\n",
            "Epoch 9, Batch 227, G Loss: 0.6936693787574768, D Loss: 1.3865749835968018\n",
            "Epoch 9, Batch 228, G Loss: 0.6936659812927246, D Loss: 1.3866040706634521\n",
            "Epoch 9, Batch 229, G Loss: 0.6936840415000916, D Loss: 1.3864893913269043\n",
            "Epoch 9, Batch 230, G Loss: 0.6937016248703003, D Loss: 1.386350154876709\n",
            "Epoch 9, Batch 231, G Loss: 0.6937167644500732, D Loss: 1.386451244354248\n",
            "Epoch 9, Batch 232, G Loss: 0.693736732006073, D Loss: 1.3864063024520874\n",
            "Epoch 9, Batch 233, G Loss: 0.6937575340270996, D Loss: 1.3864048719406128\n",
            "Epoch 9, Batch 234, G Loss: 0.6937836408615112, D Loss: 1.3863879442214966\n",
            "Epoch 9, Batch 235, G Loss: 0.6938115358352661, D Loss: 1.3864374160766602\n",
            "Epoch 9, Batch 236, G Loss: 0.6938524842262268, D Loss: 1.3863190412521362\n",
            "Epoch 9, Batch 237, G Loss: 0.6938878297805786, D Loss: 1.3863226175308228\n",
            "Epoch 9, Batch 238, G Loss: 0.693928062915802, D Loss: 1.3863110542297363\n",
            "Epoch 9, Batch 239, G Loss: 0.6939789056777954, D Loss: 1.3862597942352295\n",
            "Epoch 9, Batch 240, G Loss: 0.6940300464630127, D Loss: 1.3862230777740479\n",
            "Epoch 9, Batch 241, G Loss: 0.6940851807594299, D Loss: 1.3861780166625977\n",
            "Epoch 9, Batch 242, G Loss: 0.6941480040550232, D Loss: 1.3861417770385742\n",
            "Epoch 9, Batch 243, G Loss: 0.6942090392112732, D Loss: 1.3861421346664429\n",
            "Epoch 9, Batch 244, G Loss: 0.6942583918571472, D Loss: 1.3860893249511719\n",
            "Epoch 9, Batch 245, G Loss: 0.6943008899688721, D Loss: 1.3860936164855957\n",
            "Epoch 9, Batch 246, G Loss: 0.6943214535713196, D Loss: 1.386041522026062\n",
            "Epoch 9, Batch 247, G Loss: 0.6943351030349731, D Loss: 1.3860348463058472\n",
            "Epoch 9, Batch 248, G Loss: 0.6943354606628418, D Loss: 1.3859236240386963\n",
            "Epoch 9, Batch 249, G Loss: 0.6943262815475464, D Loss: 1.3861513137817383\n",
            "Epoch 9, Batch 250, G Loss: 0.6942931413650513, D Loss: 1.3859405517578125\n",
            "Epoch 9, Batch 251, G Loss: 0.6942556500434875, D Loss: 1.3860926628112793\n",
            "Epoch 9, Batch 252, G Loss: 0.6941937804222107, D Loss: 1.3862452507019043\n",
            "Epoch 9, Batch 253, G Loss: 0.6941123008728027, D Loss: 1.3861887454986572\n",
            "Epoch 9, Batch 254, G Loss: 0.6940001845359802, D Loss: 1.3860925436019897\n",
            "Epoch 9, Batch 255, G Loss: 0.6938801407814026, D Loss: 1.3859469890594482\n",
            "Epoch 9, Batch 256, G Loss: 0.6937698125839233, D Loss: 1.3859713077545166\n",
            "Epoch 9, Batch 257, G Loss: 0.6936703324317932, D Loss: 1.3858747482299805\n",
            "Epoch 9, Batch 258, G Loss: 0.6935940980911255, D Loss: 1.3858639001846313\n",
            "Epoch 9, Batch 259, G Loss: 0.6935269236564636, D Loss: 1.3858956098556519\n",
            "Epoch 9, Batch 260, G Loss: 0.6934912800788879, D Loss: 1.3857324123382568\n",
            "Epoch 9, Batch 261, G Loss: 0.6934518814086914, D Loss: 1.385655403137207\n",
            "Epoch 9, Batch 262, G Loss: 0.6934685707092285, D Loss: 1.3857908248901367\n",
            "Epoch 9, Batch 263, G Loss: 0.6934868693351746, D Loss: 1.3858366012573242\n",
            "Epoch 9, Batch 264, G Loss: 0.6935166120529175, D Loss: 1.3855758905410767\n",
            "Epoch 9, Batch 265, G Loss: 0.6935728192329407, D Loss: 1.3856606483459473\n",
            "Epoch 9, Batch 266, G Loss: 0.693638265132904, D Loss: 1.385892391204834\n",
            "Epoch 9, Batch 267, G Loss: 0.6936960816383362, D Loss: 1.3856282234191895\n",
            "Epoch 9, Batch 268, G Loss: 0.6937711834907532, D Loss: 1.3861719369888306\n",
            "Epoch 9, Batch 269, G Loss: 0.6938173770904541, D Loss: 1.386345386505127\n",
            "Epoch 9, Batch 270, G Loss: 0.6938217878341675, D Loss: 1.386163353919983\n",
            "Epoch 9, Batch 271, G Loss: 0.6938219666481018, D Loss: 1.3866219520568848\n",
            "Epoch 9, Batch 272, G Loss: 0.6937522888183594, D Loss: 1.3857016563415527\n",
            "Epoch 9, Batch 273, G Loss: 0.6937073469161987, D Loss: 1.385725975036621\n",
            "Epoch 9, Batch 274, G Loss: 0.6936715841293335, D Loss: 1.385871171951294\n",
            "Epoch 9, Batch 275, G Loss: 0.6936352849006653, D Loss: 1.3854345083236694\n",
            "Epoch 9, Batch 276, G Loss: 0.6936348676681519, D Loss: 1.385480284690857\n",
            "Epoch 9, Batch 277, G Loss: 0.6936495900154114, D Loss: 1.3855540752410889\n",
            "Epoch 9, Batch 278, G Loss: 0.6936842203140259, D Loss: 1.3855605125427246\n",
            "Epoch 9, Batch 279, G Loss: 0.6937226057052612, D Loss: 1.3854343891143799\n",
            "Epoch 9, Batch 280, G Loss: 0.6937975287437439, D Loss: 1.3854477405548096\n",
            "Epoch 9, Batch 281, G Loss: 0.6938281655311584, D Loss: 1.3862876892089844\n",
            "Epoch 9, Batch 282, G Loss: 0.693877100944519, D Loss: 1.3859899044036865\n",
            "Epoch 9, Batch 283, G Loss: 0.6939018368721008, D Loss: 1.3858094215393066\n",
            "Epoch 9, Batch 284, G Loss: 0.6939191222190857, D Loss: 1.386295199394226\n",
            "Epoch 9, Batch 285, G Loss: 0.6938982605934143, D Loss: 1.3864420652389526\n",
            "Epoch 9, Batch 286, G Loss: 0.6938193440437317, D Loss: 1.3861032724380493\n",
            "Epoch 9, Batch 287, G Loss: 0.6937799453735352, D Loss: 1.385697841644287\n",
            "Epoch 9, Batch 288, G Loss: 0.6936922669410706, D Loss: 1.386204481124878\n",
            "Epoch 9, Batch 289, G Loss: 0.6936492919921875, D Loss: 1.3858946561813354\n",
            "Epoch 9, Batch 290, G Loss: 0.6935730576515198, D Loss: 1.3863744735717773\n",
            "Epoch 9, Batch 291, G Loss: 0.6935069561004639, D Loss: 1.3863029479980469\n",
            "Epoch 9, Batch 292, G Loss: 0.6934100389480591, D Loss: 1.3864997625350952\n",
            "Epoch 9, Batch 293, G Loss: 0.6933302879333496, D Loss: 1.3861892223358154\n",
            "Epoch 9, Batch 294, G Loss: 0.6932388544082642, D Loss: 1.3862371444702148\n",
            "Epoch 9, Batch 295, G Loss: 0.6931486129760742, D Loss: 1.3861823081970215\n",
            "Epoch 9, Batch 296, G Loss: 0.6930906176567078, D Loss: 1.386554479598999\n",
            "Epoch 9, Batch 297, G Loss: 0.6930235028266907, D Loss: 1.3862977027893066\n",
            "Epoch 9, Batch 298, G Loss: 0.6929567456245422, D Loss: 1.3857667446136475\n",
            "Epoch 9, Batch 299, G Loss: 0.6929207444190979, D Loss: 1.3861031532287598\n",
            "Epoch 9, Batch 300, G Loss: 0.6929001212120056, D Loss: 1.3861422538757324\n",
            "Epoch 9, Batch 301, G Loss: 0.6929154396057129, D Loss: 1.388005256652832\n",
            "Epoch 9, Batch 302, G Loss: 0.6928618550300598, D Loss: 1.3869655132293701\n",
            "Epoch 9, Batch 303, G Loss: 0.6927741169929504, D Loss: 1.387831211090088\n",
            "Epoch 9, Batch 304, G Loss: 0.6926523447036743, D Loss: 1.387485384941101\n",
            "Epoch 9, Batch 305, G Loss: 0.692527174949646, D Loss: 1.387554407119751\n",
            "Epoch 9, Batch 306, G Loss: 0.6923676133155823, D Loss: 1.3876972198486328\n",
            "Epoch 9, Batch 307, G Loss: 0.6922004818916321, D Loss: 1.3874094486236572\n",
            "Epoch 9, Batch 308, G Loss: 0.6920191049575806, D Loss: 1.387321949005127\n",
            "Epoch 9, Batch 309, G Loss: 0.6918447017669678, D Loss: 1.3874578475952148\n",
            "Epoch 9, Batch 310, G Loss: 0.6917080283164978, D Loss: 1.3873543739318848\n",
            "Epoch 9, Batch 311, G Loss: 0.691565990447998, D Loss: 1.3870328664779663\n",
            "Epoch 9, Batch 312, G Loss: 0.6914505362510681, D Loss: 1.3874056339263916\n",
            "Epoch 9, Batch 313, G Loss: 0.6913709044456482, D Loss: 1.3871402740478516\n",
            "Epoch 9, Batch 314, G Loss: 0.6913055777549744, D Loss: 1.3868112564086914\n",
            "Epoch 9, Batch 315, G Loss: 0.691277801990509, D Loss: 1.386955738067627\n",
            "Epoch 9, Batch 316, G Loss: 0.691281795501709, D Loss: 1.3867639303207397\n",
            "Epoch 9, Batch 317, G Loss: 0.691328227519989, D Loss: 1.3865870237350464\n",
            "Epoch 9, Batch 318, G Loss: 0.6914160847663879, D Loss: 1.3864761590957642\n",
            "Epoch 9, Batch 319, G Loss: 0.6915499567985535, D Loss: 1.3868272304534912\n",
            "Epoch 9, Batch 320, G Loss: 0.6916787028312683, D Loss: 1.386526107788086\n",
            "Epoch 9, Batch 321, G Loss: 0.6918408870697021, D Loss: 1.3867924213409424\n",
            "Epoch 9, Batch 322, G Loss: 0.6919822692871094, D Loss: 1.3866493701934814\n",
            "Epoch 9, Batch 323, G Loss: 0.6921320557594299, D Loss: 1.3868225812911987\n",
            "Epoch 9, Batch 324, G Loss: 0.6922625303268433, D Loss: 1.3866589069366455\n",
            "Epoch 9, Batch 325, G Loss: 0.6923816204071045, D Loss: 1.386458158493042\n",
            "Epoch 9, Batch 326, G Loss: 0.692496657371521, D Loss: 1.3865647315979004\n",
            "Epoch 9, Batch 327, G Loss: 0.69260174036026, D Loss: 1.3864678144454956\n",
            "Epoch 9, Batch 328, G Loss: 0.692694902420044, D Loss: 1.3866081237792969\n",
            "Epoch 9, Batch 329, G Loss: 0.6927604079246521, D Loss: 1.3864350318908691\n",
            "Epoch 9, Batch 330, G Loss: 0.6928151249885559, D Loss: 1.3863248825073242\n",
            "Epoch 9, Batch 331, G Loss: 0.6928796768188477, D Loss: 1.3863946199417114\n",
            "Epoch 9, Batch 332, G Loss: 0.6929338574409485, D Loss: 1.3863924741744995\n",
            "Epoch 9, Batch 333, G Loss: 0.6929733157157898, D Loss: 1.3864080905914307\n",
            "Epoch 9, Batch 334, G Loss: 0.6929873824119568, D Loss: 1.3863525390625\n",
            "Epoch 9, Batch 335, G Loss: 0.6929913759231567, D Loss: 1.3863540887832642\n",
            "Epoch 9, Batch 336, G Loss: 0.692974328994751, D Loss: 1.3863376379013062\n",
            "Epoch 9, Batch 337, G Loss: 0.6929414868354797, D Loss: 1.3862814903259277\n",
            "Epoch 9, Batch 338, G Loss: 0.6928859353065491, D Loss: 1.386350154876709\n",
            "Epoch 9, Batch 339, G Loss: 0.6928505897521973, D Loss: 1.3863143920898438\n",
            "Epoch 9, Batch 340, G Loss: 0.6928267478942871, D Loss: 1.3863775730133057\n",
            "Epoch 9, Batch 341, G Loss: 0.6928240060806274, D Loss: 1.3860759735107422\n",
            "Epoch 9, Batch 342, G Loss: 0.6927893757820129, D Loss: 1.3860819339752197\n",
            "Epoch 9, Batch 343, G Loss: 0.6927398443222046, D Loss: 1.3859894275665283\n",
            "Epoch 9, Batch 344, G Loss: 0.6926663517951965, D Loss: 1.3859848976135254\n",
            "Epoch 9, Batch 345, G Loss: 0.6925899982452393, D Loss: 1.3861169815063477\n",
            "Epoch 9, Batch 346, G Loss: 0.6925188899040222, D Loss: 1.3861889839172363\n",
            "Epoch 9, Batch 347, G Loss: 0.6924746632575989, D Loss: 1.3861439228057861\n",
            "Epoch 9, Batch 348, G Loss: 0.6924484372138977, D Loss: 1.3860833644866943\n",
            "Epoch 9, Batch 349, G Loss: 0.692435622215271, D Loss: 1.386218547821045\n",
            "Epoch 9, Batch 350, G Loss: 0.6924453973770142, D Loss: 1.3860641717910767\n",
            "Epoch 9, Batch 351, G Loss: 0.6924647688865662, D Loss: 1.3860790729522705\n",
            "Epoch 9, Batch 352, G Loss: 0.6924974322319031, D Loss: 1.3861572742462158\n",
            "Epoch 9, Batch 353, G Loss: 0.692547082901001, D Loss: 1.3859243392944336\n",
            "Epoch 9, Batch 354, G Loss: 0.6926100850105286, D Loss: 1.3858141899108887\n",
            "Epoch 9, Batch 355, G Loss: 0.6926636695861816, D Loss: 1.3853576183319092\n",
            "Epoch 9, Batch 356, G Loss: 0.6926926374435425, D Loss: 1.3854377269744873\n",
            "Epoch 9, Batch 357, G Loss: 0.6927247047424316, D Loss: 1.3857789039611816\n",
            "Epoch 9, Batch 358, G Loss: 0.6927704215049744, D Loss: 1.3855392932891846\n",
            "Epoch 9, Batch 359, G Loss: 0.6927925944328308, D Loss: 1.385408878326416\n",
            "Epoch 9, Batch 360, G Loss: 0.6928175091743469, D Loss: 1.3855347633361816\n",
            "Epoch 9, Batch 361, G Loss: 0.692838728427887, D Loss: 1.385093092918396\n",
            "Epoch 9, Batch 362, G Loss: 0.6928585767745972, D Loss: 1.3855937719345093\n",
            "Epoch 9, Batch 363, G Loss: 0.6928840279579163, D Loss: 1.3858448266983032\n",
            "Epoch 9, Batch 364, G Loss: 0.6929029226303101, D Loss: 1.3857448101043701\n",
            "Epoch 9, Batch 365, G Loss: 0.6929255127906799, D Loss: 1.385615348815918\n",
            "Epoch 9, Batch 366, G Loss: 0.6929718255996704, D Loss: 1.3856117725372314\n",
            "Epoch 9, Batch 367, G Loss: 0.6930229663848877, D Loss: 1.3859035968780518\n",
            "Epoch 9, Batch 368, G Loss: 0.6930662393569946, D Loss: 1.386073112487793\n",
            "Epoch 9, Batch 369, G Loss: 0.6930912733078003, D Loss: 1.3861737251281738\n",
            "Epoch 9, Batch 370, G Loss: 0.6931479573249817, D Loss: 1.3865344524383545\n",
            "Epoch 9, Batch 371, G Loss: 0.6931851506233215, D Loss: 1.386491060256958\n",
            "Epoch 9, Batch 372, G Loss: 0.6932599544525146, D Loss: 1.3856477737426758\n",
            "Epoch 9, Batch 373, G Loss: 0.6933329105377197, D Loss: 1.3856971263885498\n",
            "Epoch 9, Batch 374, G Loss: 0.6933692097663879, D Loss: 1.3860766887664795\n",
            "Epoch 9, Batch 375, G Loss: 0.6934008002281189, D Loss: 1.3858733177185059\n",
            "Epoch 9, Batch 376, G Loss: 0.6934609413146973, D Loss: 1.385770559310913\n",
            "Epoch 9, Batch 377, G Loss: 0.6934799551963806, D Loss: 1.3858020305633545\n",
            "Epoch 9, Batch 378, G Loss: 0.6935068368911743, D Loss: 1.3861730098724365\n",
            "Epoch 9, Batch 379, G Loss: 0.6935354471206665, D Loss: 1.3861067295074463\n",
            "Epoch 9, Batch 380, G Loss: 0.6935592889785767, D Loss: 1.3860160112380981\n",
            "Epoch 9, Batch 381, G Loss: 0.6935638189315796, D Loss: 1.3866503238677979\n",
            "Epoch 9, Batch 382, G Loss: 0.6936163902282715, D Loss: 1.3853578567504883\n",
            "Epoch 9, Batch 383, G Loss: 0.6935882568359375, D Loss: 1.385779857635498\n",
            "Epoch 9, Batch 384, G Loss: 0.6936007142066956, D Loss: 1.386256456375122\n",
            "Epoch 9, Batch 385, G Loss: 0.6935872435569763, D Loss: 1.3857579231262207\n",
            "Epoch 9, Batch 386, G Loss: 0.6935986280441284, D Loss: 1.3853787183761597\n",
            "Epoch 9, Batch 387, G Loss: 0.6935757994651794, D Loss: 1.3859745264053345\n",
            "Epoch 9, Batch 388, G Loss: 0.6935563087463379, D Loss: 1.3861730098724365\n",
            "Epoch 9, Batch 389, G Loss: 0.6935447454452515, D Loss: 1.3863916397094727\n",
            "Epoch 9, Batch 390, G Loss: 0.6935418844223022, D Loss: 1.3875483274459839\n",
            "Epoch 9, Batch 391, G Loss: 0.693515956401825, D Loss: 1.3864986896514893\n",
            "Epoch 9, Batch 392, G Loss: 0.6935197710990906, D Loss: 1.3866711854934692\n",
            "Epoch 9, Batch 393, G Loss: 0.6935409307479858, D Loss: 1.3865206241607666\n",
            "Epoch 9, Batch 394, G Loss: 0.6935132145881653, D Loss: 1.3863983154296875\n",
            "Epoch 9, Batch 395, G Loss: 0.6935006976127625, D Loss: 1.3866705894470215\n",
            "Epoch 9, Batch 396, G Loss: 0.6935312747955322, D Loss: 1.3843451738357544\n",
            "Epoch 9, Batch 397, G Loss: 0.6935145854949951, D Loss: 1.3848985433578491\n",
            "Epoch 9, Batch 398, G Loss: 0.6935188174247742, D Loss: 1.3855856657028198\n",
            "Epoch 9, Batch 399, G Loss: 0.693498432636261, D Loss: 1.385560393333435\n",
            "Epoch 9, Batch 400, G Loss: 0.6934857964515686, D Loss: 1.3868931531906128\n",
            "Epoch 9, Batch 401, G Loss: 0.693480908870697, D Loss: 1.3863410949707031\n",
            "Epoch 9, Batch 402, G Loss: 0.6934714317321777, D Loss: 1.3861204385757446\n",
            "Epoch 9, Batch 403, G Loss: 0.693474292755127, D Loss: 1.3870174884796143\n",
            "Epoch 9, Batch 404, G Loss: 0.6934924721717834, D Loss: 1.3876426219940186\n",
            "Epoch 9, Batch 405, G Loss: 0.693511962890625, D Loss: 1.3876584768295288\n",
            "Epoch 9, Batch 406, G Loss: 0.693540632724762, D Loss: 1.3875641822814941\n",
            "Epoch 9, Batch 407, G Loss: 0.6935745477676392, D Loss: 1.3859825134277344\n",
            "Epoch 9, Batch 408, G Loss: 0.6936051249504089, D Loss: 1.3864765167236328\n",
            "Epoch 9, Batch 409, G Loss: 0.6936310529708862, D Loss: 1.3867859840393066\n",
            "Epoch 9, Batch 410, G Loss: 0.6936489939689636, D Loss: 1.3865623474121094\n",
            "Epoch 9, Batch 411, G Loss: 0.6936659812927246, D Loss: 1.3871128559112549\n",
            "Epoch 9, Batch 412, G Loss: 0.6937025189399719, D Loss: 1.387068271636963\n",
            "Epoch 9, Batch 413, G Loss: 0.6937087774276733, D Loss: 1.3870737552642822\n",
            "Epoch 9, Batch 414, G Loss: 0.6937275528907776, D Loss: 1.38728928565979\n",
            "Epoch 9, Batch 415, G Loss: 0.6937428712844849, D Loss: 1.3874253034591675\n",
            "Epoch 9, Batch 416, G Loss: 0.6937732100486755, D Loss: 1.3874680995941162\n",
            "Epoch 9, Batch 417, G Loss: 0.6937866806983948, D Loss: 1.386594295501709\n",
            "Epoch 9, Batch 418, G Loss: 0.6938158869743347, D Loss: 1.386589527130127\n",
            "Epoch 9, Batch 419, G Loss: 0.6938201189041138, D Loss: 1.3863303661346436\n",
            "Epoch 9, Batch 420, G Loss: 0.6938095688819885, D Loss: 1.3864614963531494\n",
            "Epoch 9, Batch 421, G Loss: 0.6938004493713379, D Loss: 1.386818528175354\n",
            "Epoch 9, Batch 422, G Loss: 0.6937854290008545, D Loss: 1.3870718479156494\n",
            "Epoch 9, Batch 423, G Loss: 0.6937682628631592, D Loss: 1.386875867843628\n",
            "Epoch 9, Batch 424, G Loss: 0.6937555074691772, D Loss: 1.3860526084899902\n",
            "Epoch 9, Batch 425, G Loss: 0.6937246322631836, D Loss: 1.3862152099609375\n",
            "Epoch 9, Batch 426, G Loss: 0.6936941146850586, D Loss: 1.386412262916565\n",
            "Epoch 9, Batch 427, G Loss: 0.6936502456665039, D Loss: 1.3863731622695923\n",
            "Epoch 9, Batch 428, G Loss: 0.6936094164848328, D Loss: 1.3866496086120605\n",
            "Epoch 9, Batch 429, G Loss: 0.6935732960700989, D Loss: 1.3864473104476929\n",
            "Epoch 9, Batch 430, G Loss: 0.6935214400291443, D Loss: 1.386954665184021\n",
            "Epoch 9, Batch 431, G Loss: 0.6935065388679504, D Loss: 1.3869192600250244\n",
            "Epoch 9, Batch 432, G Loss: 0.6935116648674011, D Loss: 1.3866808414459229\n",
            "Epoch 9, Batch 433, G Loss: 0.6935085654258728, D Loss: 1.3869383335113525\n",
            "Epoch 9, Batch 434, G Loss: 0.6935251951217651, D Loss: 1.3868870735168457\n",
            "Epoch 9, Batch 435, G Loss: 0.6935595273971558, D Loss: 1.3869881629943848\n",
            "Epoch 9, Batch 436, G Loss: 0.6936010122299194, D Loss: 1.3866769075393677\n",
            "Epoch 9, Batch 437, G Loss: 0.6936503052711487, D Loss: 1.3862637281417847\n",
            "Epoch 9, Batch 438, G Loss: 0.6936842203140259, D Loss: 1.3863716125488281\n",
            "Epoch 9, Batch 439, G Loss: 0.6937015056610107, D Loss: 1.3863391876220703\n",
            "Epoch 9, Batch 440, G Loss: 0.6937100291252136, D Loss: 1.386326789855957\n",
            "Epoch 9, Batch 441, G Loss: 0.6937026977539062, D Loss: 1.3862128257751465\n",
            "Epoch 9, Batch 442, G Loss: 0.6936752200126648, D Loss: 1.386266827583313\n",
            "Epoch 9, Batch 443, G Loss: 0.6936307549476624, D Loss: 1.3863496780395508\n",
            "Epoch 9, Batch 444, G Loss: 0.6935877799987793, D Loss: 1.3862149715423584\n",
            "Epoch 9, Batch 445, G Loss: 0.69353187084198, D Loss: 1.3862478733062744\n",
            "Epoch 9, Batch 446, G Loss: 0.6934633255004883, D Loss: 1.3863319158554077\n",
            "Epoch 9, Batch 447, G Loss: 0.6934012770652771, D Loss: 1.3863080739974976\n",
            "Epoch 9, Batch 448, G Loss: 0.6933422684669495, D Loss: 1.3864120244979858\n",
            "Epoch 9, Batch 449, G Loss: 0.6932966113090515, D Loss: 1.3864214420318604\n",
            "Epoch 9, Batch 450, G Loss: 0.6932708024978638, D Loss: 1.3863921165466309\n",
            "Epoch 9, Batch 451, G Loss: 0.6932669878005981, D Loss: 1.3864068984985352\n",
            "Epoch 9, Batch 452, G Loss: 0.6932752728462219, D Loss: 1.3862648010253906\n",
            "Epoch 9, Batch 453, G Loss: 0.6932796835899353, D Loss: 1.3863210678100586\n",
            "Epoch 9, Batch 454, G Loss: 0.6932942867279053, D Loss: 1.3863104581832886\n",
            "Epoch 9, Batch 455, G Loss: 0.6933115720748901, D Loss: 1.3864327669143677\n",
            "Epoch 9, Batch 456, G Loss: 0.6933627724647522, D Loss: 1.3863816261291504\n",
            "Epoch 9, Batch 457, G Loss: 0.6934397220611572, D Loss: 1.3863403797149658\n",
            "Epoch 9, Batch 458, G Loss: 0.6935314536094666, D Loss: 1.3862553834915161\n",
            "Epoch 9, Batch 459, G Loss: 0.6935933828353882, D Loss: 1.3862600326538086\n",
            "Epoch 9, Batch 460, G Loss: 0.6936293840408325, D Loss: 1.3862600326538086\n",
            "Epoch 9, Batch 461, G Loss: 0.6936373114585876, D Loss: 1.386260747909546\n",
            "Epoch 9, Batch 462, G Loss: 0.693626880645752, D Loss: 1.3862419128417969\n",
            "Epoch 9, Batch 463, G Loss: 0.6936345100402832, D Loss: 1.3862119913101196\n",
            "Epoch 9, Batch 464, G Loss: 0.6936642527580261, D Loss: 1.3861991167068481\n",
            "Epoch 9, Batch 465, G Loss: 0.693702220916748, D Loss: 1.3861459493637085\n",
            "Epoch 9, Batch 466, G Loss: 0.6937677264213562, D Loss: 1.3861057758331299\n",
            "Epoch 9, Batch 467, G Loss: 0.6938498616218567, D Loss: 1.38606595993042\n",
            "Epoch 9, Batch 468, G Loss: 0.6939446330070496, D Loss: 1.3860352039337158\n",
            "Epoch 9, Batch 469, G Loss: 0.6940531730651855, D Loss: 1.385894775390625\n",
            "Epoch 9, Batch 470, G Loss: 0.6941749453544617, D Loss: 1.3860821723937988\n",
            "Epoch 9, Batch 471, G Loss: 0.694273054599762, D Loss: 1.3860143423080444\n",
            "Epoch 9, Batch 472, G Loss: 0.6943563222885132, D Loss: 1.3859777450561523\n",
            "Epoch 9, Batch 473, G Loss: 0.6944111585617065, D Loss: 1.3859314918518066\n",
            "Epoch 9, Batch 474, G Loss: 0.6944618821144104, D Loss: 1.385894775390625\n",
            "Epoch 9, Batch 475, G Loss: 0.6944798231124878, D Loss: 1.3860976696014404\n",
            "Epoch 9, Batch 476, G Loss: 0.6944686770439148, D Loss: 1.386131763458252\n",
            "Epoch 9, Batch 477, G Loss: 0.6944230794906616, D Loss: 1.3860714435577393\n",
            "Epoch 9, Batch 478, G Loss: 0.6943492293357849, D Loss: 1.3858630657196045\n",
            "Epoch 9, Batch 479, G Loss: 0.6942675113677979, D Loss: 1.3852202892303467\n",
            "Epoch 9, Batch 480, G Loss: 0.6942326426506042, D Loss: 1.38535475730896\n",
            "Epoch 9, Batch 481, G Loss: 0.6942138671875, D Loss: 1.385291337966919\n",
            "Epoch 9, Batch 482, G Loss: 0.6942492127418518, D Loss: 1.3859609365463257\n",
            "Epoch 9, Batch 483, G Loss: 0.694240152835846, D Loss: 1.3858206272125244\n",
            "Epoch 9, Batch 484, G Loss: 0.6942152976989746, D Loss: 1.3855544328689575\n",
            "Epoch 9, Batch 485, G Loss: 0.6942028403282166, D Loss: 1.3855876922607422\n",
            "Epoch 9, Batch 486, G Loss: 0.6941937208175659, D Loss: 1.3856300115585327\n",
            "Epoch 9, Batch 487, G Loss: 0.6941666007041931, D Loss: 1.3855375051498413\n",
            "Epoch 9, Batch 488, G Loss: 0.6941621899604797, D Loss: 1.3855175971984863\n",
            "Epoch 9, Batch 489, G Loss: 0.6941416263580322, D Loss: 1.3855507373809814\n",
            "Epoch 9, Batch 490, G Loss: 0.6941401958465576, D Loss: 1.385522484779358\n",
            "Epoch 9, Batch 491, G Loss: 0.694146454334259, D Loss: 1.385331153869629\n",
            "Epoch 9, Batch 492, G Loss: 0.694139301776886, D Loss: 1.385563850402832\n",
            "Epoch 9, Batch 493, G Loss: 0.6941330432891846, D Loss: 1.3850758075714111\n",
            "Epoch 9, Batch 494, G Loss: 0.6941632032394409, D Loss: 1.3850983381271362\n",
            "Epoch 9, Batch 495, G Loss: 0.6941885948181152, D Loss: 1.3851250410079956\n",
            "Epoch 9, Batch 496, G Loss: 0.6942270994186401, D Loss: 1.385249137878418\n",
            "Epoch 9, Batch 497, G Loss: 0.6942772269248962, D Loss: 1.3855705261230469\n",
            "Epoch 9, Batch 498, G Loss: 0.6943067312240601, D Loss: 1.3856571912765503\n",
            "Epoch 9, Batch 499, G Loss: 0.6943234205245972, D Loss: 1.3854036331176758\n",
            "Epoch 9, Batch 500, G Loss: 0.6943158507347107, D Loss: 1.3860560655593872\n",
            "Epoch 9, Batch 501, G Loss: 0.6942837834358215, D Loss: 1.386480450630188\n",
            "Epoch 9, Batch 502, G Loss: 0.694220781326294, D Loss: 1.386467695236206\n",
            "Epoch 9, Batch 503, G Loss: 0.6940776109695435, D Loss: 1.3861844539642334\n",
            "Epoch 9, Batch 504, G Loss: 0.6939291954040527, D Loss: 1.3859248161315918\n",
            "Epoch 9, Batch 505, G Loss: 0.6938168406486511, D Loss: 1.3868166208267212\n",
            "Epoch 9, Batch 506, G Loss: 0.6936295032501221, D Loss: 1.3867974281311035\n",
            "Epoch 9, Batch 507, G Loss: 0.6934424638748169, D Loss: 1.3874804973602295\n",
            "Epoch 9, Batch 508, G Loss: 0.6932024359703064, D Loss: 1.3869078159332275\n",
            "Epoch 9, Batch 509, G Loss: 0.6929425597190857, D Loss: 1.385967493057251\n",
            "Epoch 9, Batch 510, G Loss: 0.6927619576454163, D Loss: 1.3861644268035889\n",
            "Epoch 9, Batch 511, G Loss: 0.6925989389419556, D Loss: 1.3858873844146729\n",
            "Epoch 9, Batch 512, G Loss: 0.6925132870674133, D Loss: 1.3853589296340942\n",
            "Epoch 9, Batch 513, G Loss: 0.6924862265586853, D Loss: 1.3860300779342651\n",
            "Epoch 9, Batch 514, G Loss: 0.6925133466720581, D Loss: 1.3859859704971313\n",
            "Epoch 9, Batch 515, G Loss: 0.692559003829956, D Loss: 1.386695384979248\n",
            "Epoch 9, Batch 516, G Loss: 0.6926231384277344, D Loss: 1.3871568441390991\n",
            "Epoch 9, Batch 517, G Loss: 0.6926536560058594, D Loss: 1.3872942924499512\n",
            "Epoch 9, Batch 518, G Loss: 0.6926426887512207, D Loss: 1.3866653442382812\n",
            "Epoch 9, Batch 519, G Loss: 0.692655622959137, D Loss: 1.3869283199310303\n",
            "Epoch 9, Batch 520, G Loss: 0.6926699280738831, D Loss: 1.3868402242660522\n",
            "Epoch 9, Batch 521, G Loss: 0.6926589012145996, D Loss: 1.3867905139923096\n",
            "Epoch 9, Batch 522, G Loss: 0.6926605701446533, D Loss: 1.386228322982788\n",
            "Epoch 9, Batch 523, G Loss: 0.6926592588424683, D Loss: 1.3859903812408447\n",
            "Epoch 9, Batch 524, G Loss: 0.6927348375320435, D Loss: 1.3860929012298584\n",
            "Epoch 9, Batch 525, G Loss: 0.6927720904350281, D Loss: 1.3857228755950928\n",
            "Epoch 9, Batch 526, G Loss: 0.6928989887237549, D Loss: 1.3857477903366089\n",
            "Epoch 9, Batch 527, G Loss: 0.6930331587791443, D Loss: 1.3863441944122314\n",
            "Epoch 9, Batch 528, G Loss: 0.6931491494178772, D Loss: 1.3870747089385986\n",
            "Epoch 9, Batch 529, G Loss: 0.6932240724563599, D Loss: 1.3876585960388184\n",
            "Epoch 9, Batch 530, G Loss: 0.6932322978973389, D Loss: 1.3870227336883545\n",
            "Epoch 9, Batch 531, G Loss: 0.6932075619697571, D Loss: 1.3867868185043335\n",
            "Epoch 9, Batch 532, G Loss: 0.6931543350219727, D Loss: 1.3859007358551025\n",
            "Epoch 9, Batch 533, G Loss: 0.6931226253509521, D Loss: 1.3867213726043701\n",
            "Epoch 9, Batch 534, G Loss: 0.6930841207504272, D Loss: 1.3868980407714844\n",
            "Epoch 9, Batch 535, G Loss: 0.6930481195449829, D Loss: 1.3865413665771484\n",
            "Epoch 9, Batch 536, G Loss: 0.6929895877838135, D Loss: 1.3863396644592285\n",
            "Epoch 9, Batch 537, G Loss: 0.6929507851600647, D Loss: 1.3870052099227905\n",
            "Epoch 9, Batch 538, G Loss: 0.6928762197494507, D Loss: 1.387237310409546\n",
            "Epoch 9, Batch 539, G Loss: 0.6927810907363892, D Loss: 1.3870387077331543\n",
            "Epoch 9, Batch 540, G Loss: 0.6926754713058472, D Loss: 1.387070655822754\n",
            "Epoch 9, Batch 541, G Loss: 0.6925730109214783, D Loss: 1.3870372772216797\n",
            "Epoch 9, Batch 542, G Loss: 0.6924760937690735, D Loss: 1.3869106769561768\n",
            "Epoch 9, Batch 543, G Loss: 0.6923558115959167, D Loss: 1.3869630098342896\n",
            "Epoch 9, Batch 544, G Loss: 0.6922703385353088, D Loss: 1.3869600296020508\n",
            "Epoch 9, Batch 545, G Loss: 0.6921935081481934, D Loss: 1.3869000673294067\n",
            "Epoch 9, Batch 546, G Loss: 0.6921108961105347, D Loss: 1.3868529796600342\n",
            "Epoch 9, Batch 547, G Loss: 0.6920491456985474, D Loss: 1.3868569135665894\n",
            "Epoch 9, Batch 548, G Loss: 0.6920181512832642, D Loss: 1.3872277736663818\n",
            "Epoch 9, Batch 549, G Loss: 0.6919816732406616, D Loss: 1.3867663145065308\n",
            "Epoch 9, Batch 550, G Loss: 0.6919568777084351, D Loss: 1.3864169120788574\n",
            "Epoch 9, Batch 551, G Loss: 0.6919809579849243, D Loss: 1.3864754438400269\n",
            "Epoch 9, Batch 552, G Loss: 0.6920254826545715, D Loss: 1.3864705562591553\n",
            "Epoch 9, Batch 553, G Loss: 0.6921084523200989, D Loss: 1.3868191242218018\n",
            "Epoch 9, Batch 554, G Loss: 0.6921869516372681, D Loss: 1.3868615627288818\n",
            "Epoch 9, Batch 555, G Loss: 0.6922395825386047, D Loss: 1.3869303464889526\n",
            "Epoch 9, Batch 556, G Loss: 0.6922851800918579, D Loss: 1.3865694999694824\n",
            "Epoch 9, Batch 557, G Loss: 0.6923449635505676, D Loss: 1.3864796161651611\n",
            "Epoch 9, Batch 558, G Loss: 0.692399263381958, D Loss: 1.3866993188858032\n",
            "Epoch 9, Batch 559, G Loss: 0.6924746036529541, D Loss: 1.3865382671356201\n",
            "Epoch 9, Batch 560, G Loss: 0.6925216913223267, D Loss: 1.3865541219711304\n",
            "Epoch 9, Batch 561, G Loss: 0.692577064037323, D Loss: 1.386391520500183\n",
            "Epoch 9, Batch 562, G Loss: 0.6926283836364746, D Loss: 1.3863027095794678\n",
            "Epoch 9, Batch 563, G Loss: 0.6927073001861572, D Loss: 1.386284351348877\n",
            "Epoch 9, Batch 564, G Loss: 0.6927902102470398, D Loss: 1.3862082958221436\n",
            "Epoch 9, Batch 565, G Loss: 0.6928988099098206, D Loss: 1.3865514993667603\n",
            "Epoch 9, Batch 566, G Loss: 0.6929672956466675, D Loss: 1.3867790699005127\n",
            "Epoch 9, Batch 567, G Loss: 0.6929760575294495, D Loss: 1.3865938186645508\n",
            "Epoch 9, Batch 568, G Loss: 0.6929451823234558, D Loss: 1.386561632156372\n",
            "Epoch 9, Batch 569, G Loss: 0.6928808093070984, D Loss: 1.386374592781067\n",
            "Epoch 9, Batch 570, G Loss: 0.6928106546401978, D Loss: 1.3863804340362549\n",
            "Epoch 9, Batch 571, G Loss: 0.6927457451820374, D Loss: 1.386343240737915\n",
            "Epoch 9, Batch 572, G Loss: 0.6926895380020142, D Loss: 1.3863446712493896\n",
            "Epoch 9, Batch 573, G Loss: 0.6926025152206421, D Loss: 1.3862853050231934\n",
            "Epoch 9, Batch 574, G Loss: 0.6925015449523926, D Loss: 1.3861737251281738\n",
            "Epoch 9, Batch 575, G Loss: 0.6923825144767761, D Loss: 1.386121153831482\n",
            "Epoch 9, Batch 576, G Loss: 0.6922595500946045, D Loss: 1.386174201965332\n",
            "Epoch 9, Batch 577, G Loss: 0.6921688914299011, D Loss: 1.386113166809082\n",
            "Epoch 9, Batch 578, G Loss: 0.6921036243438721, D Loss: 1.3861987590789795\n",
            "Epoch 9, Batch 579, G Loss: 0.6920694708824158, D Loss: 1.3862316608428955\n",
            "Epoch 9, Batch 580, G Loss: 0.6920704245567322, D Loss: 1.385784387588501\n",
            "Epoch 9, Batch 581, G Loss: 0.6920799016952515, D Loss: 1.3858051300048828\n",
            "Epoch 9, Batch 582, G Loss: 0.6920995116233826, D Loss: 1.3857038021087646\n",
            "Epoch 9, Batch 583, G Loss: 0.6921394467353821, D Loss: 1.3856043815612793\n",
            "Epoch 9, Batch 584, G Loss: 0.6921769976615906, D Loss: 1.3858970403671265\n",
            "Epoch 9, Batch 585, G Loss: 0.6922481060028076, D Loss: 1.3859448432922363\n",
            "Epoch 9, Batch 586, G Loss: 0.6923322081565857, D Loss: 1.386063814163208\n",
            "Epoch 9, Batch 587, G Loss: 0.6924313902854919, D Loss: 1.386232852935791\n",
            "Epoch 9, Batch 588, G Loss: 0.6925578713417053, D Loss: 1.3862484693527222\n",
            "Epoch 9, Batch 589, G Loss: 0.6926791071891785, D Loss: 1.3863563537597656\n",
            "Epoch 9, Batch 590, G Loss: 0.6928138136863708, D Loss: 1.3855512142181396\n",
            "Epoch 9, Batch 591, G Loss: 0.6929484605789185, D Loss: 1.3854773044586182\n",
            "Epoch 9, Batch 592, G Loss: 0.6930580735206604, D Loss: 1.3856594562530518\n",
            "Epoch 9, Batch 593, G Loss: 0.6931660771369934, D Loss: 1.385937213897705\n",
            "Epoch 9, Batch 594, G Loss: 0.6932603716850281, D Loss: 1.3860352039337158\n",
            "Epoch 9, Batch 595, G Loss: 0.6933316588401794, D Loss: 1.3864736557006836\n",
            "Epoch 9, Batch 596, G Loss: 0.6934260129928589, D Loss: 1.385537028312683\n",
            "Epoch 9, Batch 597, G Loss: 0.6934779286384583, D Loss: 1.385516881942749\n",
            "Epoch 9, Batch 598, G Loss: 0.6935213208198547, D Loss: 1.3857917785644531\n",
            "Epoch 9, Batch 599, G Loss: 0.6935587525367737, D Loss: 1.3859354257583618\n",
            "Epoch 9, Batch 600, G Loss: 0.6935742497444153, D Loss: 1.386223316192627\n",
            "Epoch 9, Batch 601, G Loss: 0.6935866475105286, D Loss: 1.3861137628555298\n",
            "Epoch 9, Batch 602, G Loss: 0.6935811042785645, D Loss: 1.386383295059204\n",
            "Epoch 9, Batch 603, G Loss: 0.6935811042785645, D Loss: 1.385867714881897\n",
            "Epoch 9, Batch 604, G Loss: 0.6935480833053589, D Loss: 1.3850475549697876\n",
            "Epoch 9, Batch 605, G Loss: 0.6935444474220276, D Loss: 1.3859955072402954\n",
            "Epoch 9, Batch 606, G Loss: 0.6935109496116638, D Loss: 1.3858771324157715\n",
            "Epoch 9, Batch 607, G Loss: 0.6934781074523926, D Loss: 1.3858532905578613\n",
            "Epoch 9, Batch 608, G Loss: 0.6934453248977661, D Loss: 1.385570764541626\n",
            "Epoch 9, Batch 609, G Loss: 0.6934180855751038, D Loss: 1.3861082792282104\n",
            "Epoch 9, Batch 610, G Loss: 0.6934037208557129, D Loss: 1.3863999843597412\n",
            "Epoch 9, Batch 611, G Loss: 0.6933867931365967, D Loss: 1.385638952255249\n",
            "Epoch 9, Batch 612, G Loss: 0.6933707594871521, D Loss: 1.3850888013839722\n",
            "Epoch 9, Batch 613, G Loss: 0.693333089351654, D Loss: 1.3851509094238281\n",
            "Epoch 9, Batch 614, G Loss: 0.6933237314224243, D Loss: 1.3854115009307861\n",
            "Epoch 9, Batch 615, G Loss: 0.6933109760284424, D Loss: 1.3855915069580078\n",
            "Epoch 9, Batch 616, G Loss: 0.6932969093322754, D Loss: 1.385514497756958\n",
            "Epoch 9, Batch 617, G Loss: 0.6932886838912964, D Loss: 1.3853124380111694\n",
            "Epoch 9, Batch 618, G Loss: 0.6933128237724304, D Loss: 1.3858742713928223\n",
            "Epoch 9, Batch 619, G Loss: 0.6933169364929199, D Loss: 1.3855420351028442\n",
            "Epoch 9, Batch 620, G Loss: 0.6933263540267944, D Loss: 1.3855431079864502\n",
            "Epoch 9, Batch 621, G Loss: 0.6933442950248718, D Loss: 1.3863396644592285\n",
            "Epoch 9, Batch 622, G Loss: 0.6933748126029968, D Loss: 1.3874691724777222\n",
            "Epoch 9, Batch 623, G Loss: 0.6933864951133728, D Loss: 1.3864599466323853\n",
            "Epoch 9, Batch 624, G Loss: 0.6934475898742676, D Loss: 1.3869030475616455\n",
            "Epoch 9, Batch 625, G Loss: 0.693450391292572, D Loss: 1.3858859539031982\n",
            "Epoch 9, Batch 626, G Loss: 0.6934902667999268, D Loss: 1.3861382007598877\n",
            "Epoch 9, Batch 627, G Loss: 0.6935356259346008, D Loss: 1.3858706951141357\n",
            "Epoch 9, Batch 628, G Loss: 0.6935592889785767, D Loss: 1.3858673572540283\n",
            "Epoch 9, Batch 629, G Loss: 0.6935954093933105, D Loss: 1.3861448764801025\n",
            "Epoch 9, Batch 630, G Loss: 0.6936004161834717, D Loss: 1.3867416381835938\n",
            "Epoch 9, Batch 631, G Loss: 0.6936335563659668, D Loss: 1.3867539167404175\n",
            "Epoch 9, Batch 632, G Loss: 0.6936456561088562, D Loss: 1.3854539394378662\n",
            "Epoch 9, Batch 633, G Loss: 0.6936467289924622, D Loss: 1.3852685689926147\n",
            "Epoch 9, Batch 634, G Loss: 0.6936591863632202, D Loss: 1.3854854106903076\n",
            "Epoch 9, Batch 635, G Loss: 0.6936767101287842, D Loss: 1.385984182357788\n",
            "Epoch 9, Batch 636, G Loss: 0.6936531662940979, D Loss: 1.3867065906524658\n",
            "Epoch 9, Batch 637, G Loss: 0.6936497688293457, D Loss: 1.3866198062896729\n",
            "Epoch 9, Batch 638, G Loss: 0.6936625838279724, D Loss: 1.3871172666549683\n",
            "Epoch 9, Batch 639, G Loss: 0.6936597228050232, D Loss: 1.3863780498504639\n",
            "Epoch 9, Batch 640, G Loss: 0.6936701536178589, D Loss: 1.386963129043579\n",
            "Epoch 9, Batch 641, G Loss: 0.6936711668968201, D Loss: 1.3867931365966797\n",
            "Epoch 9, Batch 642, G Loss: 0.6936519145965576, D Loss: 1.3867921829223633\n",
            "Epoch 9, Batch 643, G Loss: 0.6936509013175964, D Loss: 1.3867194652557373\n",
            "Epoch 9, Batch 644, G Loss: 0.6936584711074829, D Loss: 1.3862202167510986\n",
            "Epoch 9, Batch 645, G Loss: 0.6936559081077576, D Loss: 1.3868529796600342\n",
            "Epoch 9, Batch 646, G Loss: 0.6936526894569397, D Loss: 1.38625168800354\n",
            "Epoch 9, Batch 647, G Loss: 0.6936119198799133, D Loss: 1.3862336874008179\n",
            "Epoch 9, Batch 648, G Loss: 0.6936163902282715, D Loss: 1.3864762783050537\n",
            "Epoch 9, Batch 649, G Loss: 0.6936025619506836, D Loss: 1.387184739112854\n",
            "Epoch 9, Batch 650, G Loss: 0.6935810446739197, D Loss: 1.386349081993103\n",
            "Epoch 9, Batch 651, G Loss: 0.6935924887657166, D Loss: 1.3868175745010376\n",
            "Epoch 9, Batch 652, G Loss: 0.6935727596282959, D Loss: 1.387771725654602\n",
            "Epoch 9, Batch 653, G Loss: 0.6935791969299316, D Loss: 1.3880348205566406\n",
            "Epoch 9, Batch 654, G Loss: 0.6935843825340271, D Loss: 1.3873389959335327\n",
            "Epoch 9, Batch 655, G Loss: 0.6936118602752686, D Loss: 1.386375904083252\n",
            "Epoch 9, Batch 656, G Loss: 0.6936292052268982, D Loss: 1.3863155841827393\n",
            "Epoch 9, Batch 657, G Loss: 0.6936225295066833, D Loss: 1.3860186338424683\n",
            "Epoch 9, Batch 658, G Loss: 0.6936286091804504, D Loss: 1.3860485553741455\n",
            "Epoch 9, Batch 659, G Loss: 0.6936137676239014, D Loss: 1.3863482475280762\n",
            "Epoch 9, Batch 660, G Loss: 0.6935924887657166, D Loss: 1.3864328861236572\n",
            "Epoch 9, Batch 661, G Loss: 0.6935877203941345, D Loss: 1.3863509893417358\n",
            "Epoch 9, Batch 662, G Loss: 0.6935650706291199, D Loss: 1.386791467666626\n",
            "Epoch 9, Batch 663, G Loss: 0.6935354471206665, D Loss: 1.3866424560546875\n",
            "Epoch 9, Batch 664, G Loss: 0.6935358047485352, D Loss: 1.3869619369506836\n",
            "Epoch 9, Batch 665, G Loss: 0.6935181617736816, D Loss: 1.387094259262085\n",
            "Epoch 9, Batch 666, G Loss: 0.6935219764709473, D Loss: 1.3859784603118896\n",
            "Epoch 9, Batch 667, G Loss: 0.6935061812400818, D Loss: 1.3858572244644165\n",
            "Epoch 9, Batch 668, G Loss: 0.6934903264045715, D Loss: 1.386452078819275\n",
            "Epoch 9, Batch 669, G Loss: 0.6934744119644165, D Loss: 1.3862276077270508\n",
            "Epoch 9, Batch 670, G Loss: 0.6934546828269958, D Loss: 1.3864104747772217\n",
            "Epoch 9, Batch 671, G Loss: 0.6934254169464111, D Loss: 1.3867583274841309\n",
            "Epoch 9, Batch 672, G Loss: 0.693424642086029, D Loss: 1.387195348739624\n",
            "Epoch 9, Batch 673, G Loss: 0.693425714969635, D Loss: 1.3870534896850586\n",
            "Epoch 9, Batch 674, G Loss: 0.6934437155723572, D Loss: 1.3870949745178223\n",
            "Epoch 9, Batch 675, G Loss: 0.6934780478477478, D Loss: 1.386387825012207\n",
            "Epoch 9, Batch 676, G Loss: 0.6935121417045593, D Loss: 1.3864705562591553\n",
            "Epoch 9, Batch 677, G Loss: 0.6935350298881531, D Loss: 1.3866500854492188\n",
            "Epoch 9, Batch 678, G Loss: 0.6935590505599976, D Loss: 1.386618971824646\n",
            "Epoch 9, Batch 679, G Loss: 0.693581223487854, D Loss: 1.3865594863891602\n",
            "Epoch 9, Batch 680, G Loss: 0.6936073303222656, D Loss: 1.386495590209961\n",
            "Epoch 9, Batch 681, G Loss: 0.6936178207397461, D Loss: 1.386728048324585\n",
            "Epoch 9, Batch 682, G Loss: 0.6936367750167847, D Loss: 1.3865087032318115\n",
            "Epoch 9, Batch 683, G Loss: 0.6936440467834473, D Loss: 1.3865598440170288\n",
            "Epoch 9, Batch 684, G Loss: 0.6936609148979187, D Loss: 1.3865493535995483\n",
            "Epoch 9, Batch 685, G Loss: 0.6936683654785156, D Loss: 1.3866593837738037\n",
            "Epoch 9, Batch 686, G Loss: 0.6936779618263245, D Loss: 1.3866914510726929\n",
            "Epoch 9, Batch 687, G Loss: 0.6936938762664795, D Loss: 1.386718511581421\n",
            "Epoch 9, Batch 688, G Loss: 0.693724513053894, D Loss: 1.3867497444152832\n",
            "Epoch 9, Batch 689, G Loss: 0.6937639117240906, D Loss: 1.3864765167236328\n",
            "Epoch 9, Batch 690, G Loss: 0.6937980651855469, D Loss: 1.386573076248169\n",
            "Epoch 9, Batch 691, G Loss: 0.693830668926239, D Loss: 1.3864820003509521\n",
            "Epoch 9, Batch 692, G Loss: 0.6938601732254028, D Loss: 1.3864078521728516\n",
            "Epoch 9, Batch 693, G Loss: 0.693879246711731, D Loss: 1.3863797187805176\n",
            "Epoch 9, Batch 694, G Loss: 0.6938868761062622, D Loss: 1.3864216804504395\n",
            "Epoch 9, Batch 695, G Loss: 0.6938962340354919, D Loss: 1.386337161064148\n",
            "Epoch 9, Batch 696, G Loss: 0.6938918828964233, D Loss: 1.386339783668518\n",
            "Epoch 9, Batch 697, G Loss: 0.693886399269104, D Loss: 1.3863108158111572\n",
            "Epoch 9, Batch 698, G Loss: 0.6938774585723877, D Loss: 1.3862848281860352\n",
            "Epoch 9, Batch 699, G Loss: 0.6938686966896057, D Loss: 1.3862515687942505\n",
            "Epoch 9, Batch 700, G Loss: 0.6938648819923401, D Loss: 1.3862208127975464\n",
            "Epoch 9, Batch 701, G Loss: 0.6938654184341431, D Loss: 1.386164665222168\n",
            "Epoch 9, Batch 702, G Loss: 0.6938744187355042, D Loss: 1.3861794471740723\n",
            "Epoch 9, Batch 703, G Loss: 0.6938837766647339, D Loss: 1.3862056732177734\n",
            "Epoch 9, Batch 704, G Loss: 0.6938764452934265, D Loss: 1.3861517906188965\n",
            "Epoch 9, Batch 705, G Loss: 0.6938652396202087, D Loss: 1.3861759901046753\n",
            "Epoch 9, Batch 706, G Loss: 0.6938446164131165, D Loss: 1.3861150741577148\n",
            "Epoch 9, Batch 707, G Loss: 0.6938132643699646, D Loss: 1.3860934972763062\n",
            "Epoch 9, Batch 708, G Loss: 0.6937792897224426, D Loss: 1.3861236572265625\n",
            "Epoch 9, Batch 709, G Loss: 0.6937552690505981, D Loss: 1.3859926462173462\n",
            "Epoch 9, Batch 710, G Loss: 0.6937276124954224, D Loss: 1.3859336376190186\n",
            "Epoch 9, Batch 711, G Loss: 0.6937161087989807, D Loss: 1.385737419128418\n",
            "Epoch 9, Batch 712, G Loss: 0.6937288641929626, D Loss: 1.3860054016113281\n",
            "Epoch 9, Batch 713, G Loss: 0.6937467455863953, D Loss: 1.3858740329742432\n",
            "Epoch 9, Batch 714, G Loss: 0.6937617063522339, D Loss: 1.3858990669250488\n",
            "Epoch 9, Batch 715, G Loss: 0.6937988996505737, D Loss: 1.3859667778015137\n",
            "Epoch 9, Batch 716, G Loss: 0.693815290927887, D Loss: 1.3858888149261475\n",
            "Epoch 9, Batch 717, G Loss: 0.693839967250824, D Loss: 1.3859221935272217\n",
            "Epoch 9, Batch 718, G Loss: 0.6938385963439941, D Loss: 1.3855650424957275\n",
            "Epoch 9, Batch 719, G Loss: 0.6938671469688416, D Loss: 1.3852496147155762\n",
            "Epoch 9, Batch 720, G Loss: 0.693951427936554, D Loss: 1.3854119777679443\n",
            "Epoch 9, Batch 721, G Loss: 0.6940229535102844, D Loss: 1.3853559494018555\n",
            "Epoch 9, Batch 722, G Loss: 0.6941230893135071, D Loss: 1.3855855464935303\n",
            "Epoch 9, Batch 723, G Loss: 0.6942168474197388, D Loss: 1.3856966495513916\n",
            "Epoch 9, Batch 724, G Loss: 0.694300651550293, D Loss: 1.38560152053833\n",
            "Epoch 9, Batch 725, G Loss: 0.6943605542182922, D Loss: 1.386101245880127\n",
            "Epoch 9, Batch 726, G Loss: 0.6943714618682861, D Loss: 1.3857882022857666\n",
            "Epoch 9, Batch 727, G Loss: 0.6943634152412415, D Loss: 1.385744333267212\n",
            "Epoch 9, Batch 728, G Loss: 0.6943144798278809, D Loss: 1.3858437538146973\n",
            "Epoch 9, Batch 729, G Loss: 0.6942751407623291, D Loss: 1.3859989643096924\n",
            "Epoch 9, Batch 730, G Loss: 0.694182276725769, D Loss: 1.3864898681640625\n",
            "Epoch 9, Batch 731, G Loss: 0.6940587759017944, D Loss: 1.3861503601074219\n",
            "Epoch 9, Batch 732, G Loss: 0.693922221660614, D Loss: 1.3863900899887085\n",
            "Epoch 9, Batch 733, G Loss: 0.6937459707260132, D Loss: 1.3862935304641724\n",
            "Epoch 9, Batch 734, G Loss: 0.6935820579528809, D Loss: 1.3860862255096436\n",
            "Epoch 9, Batch 735, G Loss: 0.6934057474136353, D Loss: 1.3859703540802002\n",
            "Epoch 9, Batch 736, G Loss: 0.6932558417320251, D Loss: 1.3860375881195068\n",
            "Epoch 9, Batch 737, G Loss: 0.6931388974189758, D Loss: 1.3860994577407837\n",
            "Epoch 9, Batch 738, G Loss: 0.6930341124534607, D Loss: 1.3853778839111328\n",
            "Epoch 9, Batch 739, G Loss: 0.6930018663406372, D Loss: 1.3852739334106445\n",
            "Epoch 9, Batch 740, G Loss: 0.6930264830589294, D Loss: 1.3851826190948486\n",
            "Epoch 9, Batch 741, G Loss: 0.693119466304779, D Loss: 1.3864049911499023\n",
            "Epoch 9, Batch 742, G Loss: 0.6931648254394531, D Loss: 1.3863749504089355\n",
            "Epoch 9, Batch 743, G Loss: 0.6931986808776855, D Loss: 1.386213779449463\n",
            "Epoch 9, Batch 744, G Loss: 0.6932526230812073, D Loss: 1.3871374130249023\n",
            "Epoch 9, Batch 745, G Loss: 0.6932429075241089, D Loss: 1.3874019384384155\n",
            "Epoch 9, Batch 746, G Loss: 0.6931885480880737, D Loss: 1.3868775367736816\n",
            "Epoch 9, Batch 747, G Loss: 0.6931066513061523, D Loss: 1.3865841627120972\n",
            "Epoch 9, Batch 748, G Loss: 0.6930272579193115, D Loss: 1.3866617679595947\n",
            "Epoch 9, Batch 749, G Loss: 0.6929212808609009, D Loss: 1.3866603374481201\n",
            "Epoch 9, Batch 750, G Loss: 0.692828893661499, D Loss: 1.3861151933670044\n",
            "Epoch 9, Batch 751, G Loss: 0.6927707195281982, D Loss: 1.3872976303100586\n",
            "Epoch 9, Batch 752, G Loss: 0.6926875710487366, D Loss: 1.386604905128479\n",
            "Epoch 9, Batch 753, G Loss: 0.6926167011260986, D Loss: 1.3872644901275635\n",
            "Epoch 9, Batch 754, G Loss: 0.6925326585769653, D Loss: 1.387117624282837\n",
            "Epoch 9, Batch 755, G Loss: 0.6924504041671753, D Loss: 1.3870837688446045\n",
            "Epoch 9, Batch 756, G Loss: 0.6923630237579346, D Loss: 1.3869261741638184\n",
            "Epoch 9, Batch 757, G Loss: 0.6922633647918701, D Loss: 1.3869528770446777\n",
            "Epoch 9, Batch 758, G Loss: 0.6921929121017456, D Loss: 1.3866627216339111\n",
            "Epoch 9, Batch 759, G Loss: 0.692145049571991, D Loss: 1.3869686126708984\n",
            "Epoch 9, Batch 760, G Loss: 0.6920910477638245, D Loss: 1.3865762948989868\n",
            "Epoch 9, Batch 761, G Loss: 0.6920919418334961, D Loss: 1.3867299556732178\n",
            "Epoch 9, Batch 762, G Loss: 0.6921051740646362, D Loss: 1.3868544101715088\n",
            "Epoch 9, Batch 763, G Loss: 0.6921224594116211, D Loss: 1.3867944478988647\n",
            "Epoch 9, Batch 764, G Loss: 0.692158579826355, D Loss: 1.3865829706192017\n",
            "Epoch 9, Batch 765, G Loss: 0.6922210454940796, D Loss: 1.3856667280197144\n",
            "Epoch 9, Batch 766, G Loss: 0.6923297047615051, D Loss: 1.3858425617218018\n",
            "Epoch 9, Batch 767, G Loss: 0.692511260509491, D Loss: 1.3861252069473267\n",
            "Epoch 9, Batch 768, G Loss: 0.692686915397644, D Loss: 1.3865808248519897\n",
            "Epoch 9, Batch 769, G Loss: 0.6928636431694031, D Loss: 1.3866817951202393\n",
            "Epoch 9, Batch 770, G Loss: 0.692997932434082, D Loss: 1.3865222930908203\n",
            "Epoch 9, Batch 771, G Loss: 0.693103551864624, D Loss: 1.3863849639892578\n",
            "Epoch 9, Batch 772, G Loss: 0.6931952834129333, D Loss: 1.3863896131515503\n",
            "Epoch 9, Batch 773, G Loss: 0.6932830214500427, D Loss: 1.3865669965744019\n",
            "Epoch 9, Batch 774, G Loss: 0.69332355260849, D Loss: 1.3864457607269287\n",
            "Epoch 9, Batch 775, G Loss: 0.6933445930480957, D Loss: 1.3864054679870605\n",
            "Epoch 9, Batch 776, G Loss: 0.6933521628379822, D Loss: 1.3864212036132812\n",
            "Epoch 9, Batch 777, G Loss: 0.693339467048645, D Loss: 1.386141300201416\n",
            "Epoch 9, Batch 778, G Loss: 0.6933291554450989, D Loss: 1.386712908744812\n",
            "Epoch 9, Batch 779, G Loss: 0.6932817697525024, D Loss: 1.3862895965576172\n",
            "Epoch 9, Batch 780, G Loss: 0.6932368278503418, D Loss: 1.3862435817718506\n",
            "Epoch 9, Batch 781, G Loss: 0.6931910514831543, D Loss: 1.3860957622528076\n",
            "Epoch 9, Batch 782, G Loss: 0.6931754350662231, D Loss: 1.3862559795379639\n",
            "Epoch 9, Batch 783, G Loss: 0.6931454539299011, D Loss: 1.3863694667816162\n",
            "Epoch 9, Batch 784, G Loss: 0.6931254863739014, D Loss: 1.3864812850952148\n",
            "Epoch 9, Batch 785, G Loss: 0.693091094493866, D Loss: 1.3863282203674316\n",
            "Epoch 9, Batch 786, G Loss: 0.6930547952651978, D Loss: 1.3863528966903687\n",
            "Epoch 9, Batch 787, G Loss: 0.6930282711982727, D Loss: 1.3864059448242188\n",
            "Epoch 9, Batch 788, G Loss: 0.6929952502250671, D Loss: 1.3863189220428467\n",
            "Epoch 9, Batch 789, G Loss: 0.6929677724838257, D Loss: 1.3863768577575684\n",
            "Epoch 9, Batch 790, G Loss: 0.6929414868354797, D Loss: 1.3864856958389282\n",
            "Epoch 9, Batch 791, G Loss: 0.6929076910018921, D Loss: 1.3863346576690674\n",
            "Epoch 9, Batch 792, G Loss: 0.6928876638412476, D Loss: 1.3863112926483154\n",
            "Epoch 9, Batch 793, G Loss: 0.6928790211677551, D Loss: 1.3864421844482422\n",
            "Epoch 9, Batch 794, G Loss: 0.6928563117980957, D Loss: 1.3866477012634277\n",
            "Epoch 9, Batch 795, G Loss: 0.6928349733352661, D Loss: 1.3864496946334839\n",
            "Epoch 9, Batch 796, G Loss: 0.6928070783615112, D Loss: 1.3865268230438232\n",
            "Epoch 9, Batch 797, G Loss: 0.6927633285522461, D Loss: 1.3863935470581055\n",
            "Epoch 9, Batch 798, G Loss: 0.6927371621131897, D Loss: 1.3863656520843506\n",
            "Epoch 9, Batch 799, G Loss: 0.69272381067276, D Loss: 1.3865058422088623\n",
            "Epoch 9, Batch 800, G Loss: 0.6927024126052856, D Loss: 1.3866052627563477\n",
            "Epoch 9, Batch 801, G Loss: 0.692668080329895, D Loss: 1.3866610527038574\n",
            "Epoch 9, Batch 802, G Loss: 0.6926113963127136, D Loss: 1.3865392208099365\n",
            "Epoch 9, Batch 803, G Loss: 0.6925548911094666, D Loss: 1.3864738941192627\n",
            "Epoch 9, Batch 804, G Loss: 0.6925016045570374, D Loss: 1.3864095211029053\n",
            "Epoch 9, Batch 805, G Loss: 0.6924586892127991, D Loss: 1.3864648342132568\n",
            "Epoch 9, Batch 806, G Loss: 0.6924235820770264, D Loss: 1.3863468170166016\n",
            "Epoch 9, Batch 807, G Loss: 0.6924111247062683, D Loss: 1.3863940238952637\n",
            "Epoch 9, Batch 808, G Loss: 0.6924040913581848, D Loss: 1.3863232135772705\n",
            "Epoch 9, Batch 809, G Loss: 0.6924281120300293, D Loss: 1.3863098621368408\n",
            "Epoch 9, Batch 810, G Loss: 0.6924720406532288, D Loss: 1.3863189220428467\n",
            "Epoch 9, Batch 811, G Loss: 0.6925325989723206, D Loss: 1.3863122463226318\n",
            "Epoch 9, Batch 812, G Loss: 0.6926127672195435, D Loss: 1.3863136768341064\n",
            "Epoch 9, Batch 813, G Loss: 0.6926945447921753, D Loss: 1.3863024711608887\n",
            "Epoch 9, Batch 814, G Loss: 0.6927832961082458, D Loss: 1.3863259553909302\n",
            "Epoch 9, Batch 815, G Loss: 0.6928896307945251, D Loss: 1.3863023519515991\n",
            "Epoch 9, Batch 816, G Loss: 0.6929947137832642, D Loss: 1.3863258361816406\n",
            "Epoch 9, Batch 817, G Loss: 0.6931002736091614, D Loss: 1.3863375186920166\n",
            "Epoch 9, Batch 818, G Loss: 0.6932075023651123, D Loss: 1.3862967491149902\n",
            "Epoch 9, Batch 819, G Loss: 0.6932966113090515, D Loss: 1.3863153457641602\n",
            "Epoch 9, Batch 820, G Loss: 0.6933714151382446, D Loss: 1.3862162828445435\n",
            "Epoch 9, Batch 821, G Loss: 0.6933980584144592, D Loss: 1.3862640857696533\n",
            "Epoch 9, Batch 822, G Loss: 0.6933984756469727, D Loss: 1.3862603902816772\n",
            "Epoch 9, Batch 823, G Loss: 0.6933758854866028, D Loss: 1.3862931728363037\n",
            "Epoch 9, Batch 824, G Loss: 0.6933447122573853, D Loss: 1.3863120079040527\n",
            "Epoch 9, Batch 825, G Loss: 0.6933097243309021, D Loss: 1.3863400220870972\n",
            "Epoch 9, Batch 826, G Loss: 0.6932781934738159, D Loss: 1.3862674236297607\n",
            "Epoch 9, Batch 827, G Loss: 0.6932385563850403, D Loss: 1.386228084564209\n",
            "Epoch 9, Batch 828, G Loss: 0.6931855082511902, D Loss: 1.3861844539642334\n",
            "Epoch 9, Batch 829, G Loss: 0.6931203603744507, D Loss: 1.3861439228057861\n",
            "Epoch 9, Batch 830, G Loss: 0.6930445432662964, D Loss: 1.3861188888549805\n",
            "Epoch 9, Batch 831, G Loss: 0.6929578185081482, D Loss: 1.3862788677215576\n",
            "Epoch 9, Batch 832, G Loss: 0.6928810477256775, D Loss: 1.3861663341522217\n",
            "Epoch 9, Batch 833, G Loss: 0.6928123235702515, D Loss: 1.3862136602401733\n",
            "Epoch 9, Batch 834, G Loss: 0.6927549242973328, D Loss: 1.3863024711608887\n",
            "Epoch 9, Batch 835, G Loss: 0.6927194595336914, D Loss: 1.386222243309021\n",
            "Epoch 9, Batch 836, G Loss: 0.6927039623260498, D Loss: 1.3862935304641724\n",
            "Epoch 9, Batch 837, G Loss: 0.6927013993263245, D Loss: 1.3862603902816772\n",
            "Epoch 9, Batch 838, G Loss: 0.6927180290222168, D Loss: 1.3863993883132935\n",
            "Epoch 9, Batch 839, G Loss: 0.6927528977394104, D Loss: 1.3862494230270386\n",
            "Epoch 9, Batch 840, G Loss: 0.6928004026412964, D Loss: 1.3862847089767456\n",
            "Epoch 9, Batch 841, G Loss: 0.6928613781929016, D Loss: 1.3864455223083496\n",
            "Epoch 9, Batch 842, G Loss: 0.6929313540458679, D Loss: 1.3862829208374023\n",
            "Epoch 9, Batch 843, G Loss: 0.6930109858512878, D Loss: 1.386107087135315\n",
            "Epoch 9, Batch 844, G Loss: 0.6930747628211975, D Loss: 1.3860682249069214\n",
            "Epoch 9, Batch 845, G Loss: 0.6931267380714417, D Loss: 1.3861825466156006\n",
            "Epoch 9, Batch 846, G Loss: 0.6931635737419128, D Loss: 1.3861666917800903\n",
            "Epoch 9, Batch 847, G Loss: 0.6932035684585571, D Loss: 1.3862513303756714\n",
            "Epoch 9, Batch 848, G Loss: 0.6932250261306763, D Loss: 1.3864474296569824\n",
            "Epoch 9, Batch 849, G Loss: 0.6932611465454102, D Loss: 1.3864223957061768\n",
            "Epoch 9, Batch 850, G Loss: 0.6932884454727173, D Loss: 1.3860180377960205\n",
            "Epoch 9, Batch 851, G Loss: 0.6932988166809082, D Loss: 1.3861334323883057\n",
            "Epoch 9, Batch 852, G Loss: 0.6933041214942932, D Loss: 1.3859319686889648\n",
            "Epoch 9, Batch 853, G Loss: 0.6932807564735413, D Loss: 1.3860118389129639\n",
            "Epoch 9, Batch 854, G Loss: 0.6932472586631775, D Loss: 1.3860132694244385\n",
            "Epoch 9, Batch 855, G Loss: 0.693205714225769, D Loss: 1.3860511779785156\n",
            "Epoch 9, Batch 856, G Loss: 0.6931610703468323, D Loss: 1.3860290050506592\n",
            "Epoch 9, Batch 857, G Loss: 0.6930966973304749, D Loss: 1.3860235214233398\n",
            "Epoch 9, Batch 858, G Loss: 0.6930461525917053, D Loss: 1.3863379955291748\n",
            "Epoch 9, Batch 859, G Loss: 0.6930060982704163, D Loss: 1.3861734867095947\n",
            "Epoch 9, Batch 860, G Loss: 0.6929740309715271, D Loss: 1.3859789371490479\n",
            "Epoch 9, Batch 861, G Loss: 0.692961573600769, D Loss: 1.3861793279647827\n",
            "Epoch 9, Batch 862, G Loss: 0.6929425597190857, D Loss: 1.3860523700714111\n",
            "Epoch 9, Batch 863, G Loss: 0.6929267048835754, D Loss: 1.3861067295074463\n",
            "Epoch 9, Batch 864, G Loss: 0.6929165720939636, D Loss: 1.3860557079315186\n",
            "Epoch 9, Batch 865, G Loss: 0.6929243206977844, D Loss: 1.3863182067871094\n",
            "Epoch 9, Batch 866, G Loss: 0.6929470896720886, D Loss: 1.3863056898117065\n",
            "Epoch 9, Batch 867, G Loss: 0.6929643750190735, D Loss: 1.3862298727035522\n",
            "Epoch 9, Batch 868, G Loss: 0.6929993033409119, D Loss: 1.385838508605957\n",
            "Epoch 9, Batch 869, G Loss: 0.6930343508720398, D Loss: 1.3859984874725342\n",
            "Epoch 9, Batch 870, G Loss: 0.6930643916130066, D Loss: 1.3853988647460938\n",
            "Epoch 9, Batch 871, G Loss: 0.6930680274963379, D Loss: 1.3855745792388916\n",
            "Epoch 9, Batch 872, G Loss: 0.6930829882621765, D Loss: 1.3855698108673096\n",
            "Epoch 9, Batch 873, G Loss: 0.6930649280548096, D Loss: 1.3860727548599243\n",
            "Epoch 9, Batch 874, G Loss: 0.693065881729126, D Loss: 1.3861339092254639\n",
            "Epoch 9, Batch 875, G Loss: 0.6930743455886841, D Loss: 1.386101245880127\n",
            "Epoch 9, Batch 876, G Loss: 0.6930753588676453, D Loss: 1.3861887454986572\n",
            "Epoch 9, Batch 877, G Loss: 0.6930981278419495, D Loss: 1.3862555027008057\n",
            "Epoch 9, Batch 878, G Loss: 0.6930997371673584, D Loss: 1.3861024379730225\n",
            "Epoch 9, Batch 879, G Loss: 0.6931449770927429, D Loss: 1.3860520124435425\n",
            "Epoch 9, Batch 880, G Loss: 0.6931753158569336, D Loss: 1.3865609169006348\n",
            "Epoch 9, Batch 881, G Loss: 0.6932104229927063, D Loss: 1.3866760730743408\n",
            "Epoch 9, Batch 882, G Loss: 0.693250834941864, D Loss: 1.3866841793060303\n",
            "Epoch 9, Batch 883, G Loss: 0.693302571773529, D Loss: 1.3864234685897827\n",
            "Epoch 9, Batch 884, G Loss: 0.6933397650718689, D Loss: 1.3862518072128296\n",
            "Epoch 9, Batch 885, G Loss: 0.6933908462524414, D Loss: 1.3864152431488037\n",
            "Epoch 9, Batch 886, G Loss: 0.6934189200401306, D Loss: 1.3864352703094482\n",
            "Epoch 9, Batch 887, G Loss: 0.6934733986854553, D Loss: 1.3856475353240967\n",
            "Epoch 9, Batch 888, G Loss: 0.6934823989868164, D Loss: 1.3859909772872925\n",
            "Epoch 9, Batch 889, G Loss: 0.6934828758239746, D Loss: 1.3860247135162354\n",
            "Epoch 9, Batch 890, G Loss: 0.6934767961502075, D Loss: 1.3864271640777588\n",
            "Epoch 9, Batch 891, G Loss: 0.6934664845466614, D Loss: 1.3866521120071411\n",
            "Epoch 9, Batch 892, G Loss: 0.6934663653373718, D Loss: 1.387143611907959\n",
            "Epoch 9, Batch 893, G Loss: 0.6934755444526672, D Loss: 1.3867344856262207\n",
            "Epoch 9, Batch 894, G Loss: 0.6934685707092285, D Loss: 1.3864580392837524\n",
            "Epoch 9, Batch 895, G Loss: 0.6934746503829956, D Loss: 1.3862426280975342\n",
            "Epoch 9, Batch 896, G Loss: 0.6934900283813477, D Loss: 1.38633131980896\n",
            "Epoch 9, Batch 897, G Loss: 0.6934728622436523, D Loss: 1.3862838745117188\n",
            "Epoch 9, Batch 898, G Loss: 0.6934670209884644, D Loss: 1.3864622116088867\n",
            "Epoch 9, Batch 899, G Loss: 0.693452775478363, D Loss: 1.3861140012741089\n",
            "Epoch 9, Batch 900, G Loss: 0.6934230327606201, D Loss: 1.3862329721450806\n",
            "Epoch 9, Batch 901, G Loss: 0.6934051513671875, D Loss: 1.3865244388580322\n",
            "Epoch 9, Batch 902, G Loss: 0.6933836936950684, D Loss: 1.3868235349655151\n",
            "Epoch 9, Batch 903, G Loss: 0.6933733224868774, D Loss: 1.386826992034912\n",
            "Epoch 9, Batch 904, G Loss: 0.6933799386024475, D Loss: 1.3868622779846191\n",
            "Epoch 9, Batch 905, G Loss: 0.6933810114860535, D Loss: 1.3865514993667603\n",
            "Epoch 9, Batch 906, G Loss: 0.69340980052948, D Loss: 1.3867183923721313\n",
            "Epoch 9, Batch 907, G Loss: 0.6934300661087036, D Loss: 1.3863344192504883\n",
            "Epoch 9, Batch 908, G Loss: 0.6934449076652527, D Loss: 1.3868374824523926\n",
            "Epoch 9, Batch 909, G Loss: 0.6934674978256226, D Loss: 1.3868582248687744\n",
            "Epoch 9, Batch 910, G Loss: 0.6935037970542908, D Loss: 1.3863348960876465\n",
            "Epoch 9, Batch 911, G Loss: 0.6935158371925354, D Loss: 1.386202335357666\n",
            "Epoch 9, Batch 912, G Loss: 0.6935267448425293, D Loss: 1.3862967491149902\n",
            "Epoch 9, Batch 913, G Loss: 0.6935259699821472, D Loss: 1.3860321044921875\n",
            "Epoch 9, Batch 914, G Loss: 0.693488597869873, D Loss: 1.3860962390899658\n",
            "Epoch 9, Batch 915, G Loss: 0.6934524774551392, D Loss: 1.3866205215454102\n",
            "Epoch 9, Batch 916, G Loss: 0.6934183835983276, D Loss: 1.3868708610534668\n",
            "Epoch 9, Batch 917, G Loss: 0.6934157609939575, D Loss: 1.386871337890625\n",
            "Epoch 9, Batch 918, G Loss: 0.6934268474578857, D Loss: 1.3868898153305054\n",
            "Epoch 9, Batch 919, G Loss: 0.6934624314308167, D Loss: 1.386663794517517\n",
            "Epoch 9, Batch 920, G Loss: 0.6935083270072937, D Loss: 1.3861913681030273\n",
            "Epoch 9, Batch 921, G Loss: 0.6935327649116516, D Loss: 1.386338710784912\n",
            "Epoch 9, Batch 922, G Loss: 0.6935384273529053, D Loss: 1.386324405670166\n",
            "Epoch 9, Batch 923, G Loss: 0.6935373544692993, D Loss: 1.3864167928695679\n",
            "Epoch 9, Batch 924, G Loss: 0.6935350298881531, D Loss: 1.3864233493804932\n",
            "Epoch 9, Batch 925, G Loss: 0.6935297846794128, D Loss: 1.3863532543182373\n",
            "Epoch 9, Batch 926, G Loss: 0.69351726770401, D Loss: 1.3863064050674438\n",
            "Epoch 9, Batch 927, G Loss: 0.6934974789619446, D Loss: 1.3861653804779053\n",
            "Epoch 9, Batch 928, G Loss: 0.693449079990387, D Loss: 1.3858318328857422\n",
            "Epoch 9, Batch 929, G Loss: 0.6933375597000122, D Loss: 1.3856024742126465\n",
            "Epoch 9, Batch 930, G Loss: 0.6931460499763489, D Loss: 1.3861215114593506\n",
            "Epoch 9, Batch 931, G Loss: 0.6929521560668945, D Loss: 1.3864712715148926\n",
            "Epoch 9, Batch 932, G Loss: 0.6928095817565918, D Loss: 1.3863551616668701\n",
            "Epoch 9, Batch 933, G Loss: 0.6926990151405334, D Loss: 1.3862441778182983\n",
            "Epoch 9, Batch 934, G Loss: 0.692620038986206, D Loss: 1.3863532543182373\n",
            "Epoch 9, Batch 935, G Loss: 0.6925743222236633, D Loss: 1.3862344026565552\n",
            "Epoch 9, Batch 936, G Loss: 0.6925557851791382, D Loss: 1.3861708641052246\n",
            "Epoch 9, Batch 937, G Loss: 0.6925506591796875, D Loss: 1.3865492343902588\n",
            "Epoch 9, Batch 938, G Loss: 0.6925984621047974, D Loss: 1.38651442527771\n",
            "Epoch 10, Batch 1, G Loss: 0.6926910877227783, D Loss: 1.3863835334777832\n",
            "Epoch 10, Batch 2, G Loss: 0.6927978992462158, D Loss: 1.3863718509674072\n",
            "Epoch 10, Batch 3, G Loss: 0.6929232478141785, D Loss: 1.3864097595214844\n",
            "Epoch 10, Batch 4, G Loss: 0.6930583119392395, D Loss: 1.3863065242767334\n",
            "Epoch 10, Batch 5, G Loss: 0.6931913495063782, D Loss: 1.386423110961914\n",
            "Epoch 10, Batch 6, G Loss: 0.693321704864502, D Loss: 1.3863842487335205\n",
            "Epoch 10, Batch 7, G Loss: 0.6934494972229004, D Loss: 1.3864095211029053\n",
            "Epoch 10, Batch 8, G Loss: 0.6935706734657288, D Loss: 1.3863931894302368\n",
            "Epoch 10, Batch 9, G Loss: 0.6936782598495483, D Loss: 1.3863415718078613\n",
            "Epoch 10, Batch 10, G Loss: 0.6937667727470398, D Loss: 1.3864049911499023\n",
            "Epoch 10, Batch 11, G Loss: 0.6938415169715881, D Loss: 1.386324167251587\n",
            "Epoch 10, Batch 12, G Loss: 0.6938903331756592, D Loss: 1.3862464427947998\n",
            "Epoch 10, Batch 13, G Loss: 0.6939007639884949, D Loss: 1.3862955570220947\n",
            "Epoch 10, Batch 14, G Loss: 0.6938840746879578, D Loss: 1.386282205581665\n",
            "Epoch 10, Batch 15, G Loss: 0.6938422322273254, D Loss: 1.3863011598587036\n",
            "Epoch 10, Batch 16, G Loss: 0.6937839984893799, D Loss: 1.3863253593444824\n",
            "Epoch 10, Batch 17, G Loss: 0.6937252879142761, D Loss: 1.3863441944122314\n",
            "Epoch 10, Batch 18, G Loss: 0.6936788558959961, D Loss: 1.386315107345581\n",
            "Epoch 10, Batch 19, G Loss: 0.6936405897140503, D Loss: 1.3862754106521606\n",
            "Epoch 10, Batch 20, G Loss: 0.693584680557251, D Loss: 1.386265754699707\n",
            "Epoch 10, Batch 21, G Loss: 0.6934895515441895, D Loss: 1.3862791061401367\n",
            "Epoch 10, Batch 22, G Loss: 0.6933721303939819, D Loss: 1.386279582977295\n",
            "Epoch 10, Batch 23, G Loss: 0.6932200193405151, D Loss: 1.3862738609313965\n",
            "Epoch 10, Batch 24, G Loss: 0.6930930614471436, D Loss: 1.3862738609313965\n",
            "Epoch 10, Batch 25, G Loss: 0.6929982900619507, D Loss: 1.3862683773040771\n",
            "Epoch 10, Batch 26, G Loss: 0.6929344534873962, D Loss: 1.386263370513916\n",
            "Epoch 10, Batch 27, G Loss: 0.6929072141647339, D Loss: 1.3862583637237549\n",
            "Epoch 10, Batch 28, G Loss: 0.6929135322570801, D Loss: 1.3862522840499878\n",
            "Epoch 10, Batch 29, G Loss: 0.6929453611373901, D Loss: 1.3862462043762207\n",
            "Epoch 10, Batch 30, G Loss: 0.6930020451545715, D Loss: 1.3862695693969727\n",
            "Epoch 10, Batch 31, G Loss: 0.6930630207061768, D Loss: 1.386300802230835\n",
            "Epoch 10, Batch 32, G Loss: 0.6931107044219971, D Loss: 1.3863260746002197\n",
            "Epoch 10, Batch 33, G Loss: 0.6931390762329102, D Loss: 1.386284351348877\n",
            "Epoch 10, Batch 34, G Loss: 0.6931617259979248, D Loss: 1.3862574100494385\n",
            "Epoch 10, Batch 35, G Loss: 0.6931884288787842, D Loss: 1.3862327337265015\n",
            "Epoch 10, Batch 36, G Loss: 0.6932218670845032, D Loss: 1.38628089427948\n",
            "Epoch 10, Batch 37, G Loss: 0.6932461857795715, D Loss: 1.386231541633606\n",
            "Epoch 10, Batch 38, G Loss: 0.6932726502418518, D Loss: 1.386218786239624\n",
            "Epoch 10, Batch 39, G Loss: 0.693305492401123, D Loss: 1.3862297534942627\n",
            "Epoch 10, Batch 40, G Loss: 0.6933392882347107, D Loss: 1.3862683773040771\n",
            "Epoch 10, Batch 41, G Loss: 0.6933568716049194, D Loss: 1.3862515687942505\n",
            "Epoch 10, Batch 42, G Loss: 0.69336998462677, D Loss: 1.3862919807434082\n",
            "Epoch 10, Batch 43, G Loss: 0.6933670043945312, D Loss: 1.3862066268920898\n",
            "Epoch 10, Batch 44, G Loss: 0.6933696269989014, D Loss: 1.3862478733062744\n",
            "Epoch 10, Batch 45, G Loss: 0.6933693289756775, D Loss: 1.386297345161438\n",
            "Epoch 10, Batch 46, G Loss: 0.6933519244194031, D Loss: 1.3862484693527222\n",
            "Epoch 10, Batch 47, G Loss: 0.6933301687240601, D Loss: 1.3861284255981445\n",
            "Epoch 10, Batch 48, G Loss: 0.6933286190032959, D Loss: 1.3862075805664062\n",
            "Epoch 10, Batch 49, G Loss: 0.6933314800262451, D Loss: 1.3861336708068848\n",
            "Epoch 10, Batch 50, G Loss: 0.6933484673500061, D Loss: 1.3861360549926758\n",
            "Epoch 10, Batch 51, G Loss: 0.6933837532997131, D Loss: 1.3861443996429443\n",
            "Epoch 10, Batch 52, G Loss: 0.6934207081794739, D Loss: 1.3861112594604492\n",
            "Epoch 10, Batch 53, G Loss: 0.6934760212898254, D Loss: 1.3862228393554688\n",
            "Epoch 10, Batch 54, G Loss: 0.6935086846351624, D Loss: 1.3860771656036377\n",
            "Epoch 10, Batch 55, G Loss: 0.6935582160949707, D Loss: 1.3860788345336914\n",
            "Epoch 10, Batch 56, G Loss: 0.6936086416244507, D Loss: 1.386197566986084\n",
            "Epoch 10, Batch 57, G Loss: 0.6936467289924622, D Loss: 1.3863177299499512\n",
            "Epoch 10, Batch 58, G Loss: 0.693651556968689, D Loss: 1.38643217086792\n",
            "Epoch 10, Batch 59, G Loss: 0.6936002969741821, D Loss: 1.3861725330352783\n",
            "Epoch 10, Batch 60, G Loss: 0.6935577392578125, D Loss: 1.386167049407959\n",
            "Epoch 10, Batch 61, G Loss: 0.69351726770401, D Loss: 1.3861002922058105\n",
            "Epoch 10, Batch 62, G Loss: 0.6934658885002136, D Loss: 1.386162281036377\n",
            "Epoch 10, Batch 63, G Loss: 0.6934296488761902, D Loss: 1.3861472606658936\n",
            "Epoch 10, Batch 64, G Loss: 0.693389892578125, D Loss: 1.3863269090652466\n",
            "Epoch 10, Batch 65, G Loss: 0.6933391094207764, D Loss: 1.386159896850586\n",
            "Epoch 10, Batch 66, G Loss: 0.6932967901229858, D Loss: 1.386096715927124\n",
            "Epoch 10, Batch 67, G Loss: 0.6932641863822937, D Loss: 1.3862926959991455\n",
            "Epoch 10, Batch 68, G Loss: 0.6932395696640015, D Loss: 1.3861433267593384\n",
            "Epoch 10, Batch 69, G Loss: 0.6932098865509033, D Loss: 1.3859941959381104\n",
            "Epoch 10, Batch 70, G Loss: 0.6932052373886108, D Loss: 1.3861610889434814\n",
            "Epoch 10, Batch 71, G Loss: 0.6932213306427002, D Loss: 1.3865618705749512\n",
            "Epoch 10, Batch 72, G Loss: 0.6931909322738647, D Loss: 1.3862724304199219\n",
            "Epoch 10, Batch 73, G Loss: 0.693164587020874, D Loss: 1.3865115642547607\n",
            "Epoch 10, Batch 74, G Loss: 0.6931276917457581, D Loss: 1.3866944313049316\n",
            "Epoch 10, Batch 75, G Loss: 0.6930394172668457, D Loss: 1.3863402605056763\n",
            "Epoch 10, Batch 76, G Loss: 0.6929666996002197, D Loss: 1.3864669799804688\n",
            "Epoch 10, Batch 77, G Loss: 0.6928911805152893, D Loss: 1.3864243030548096\n",
            "Epoch 10, Batch 78, G Loss: 0.6928237676620483, D Loss: 1.3860974311828613\n",
            "Epoch 10, Batch 79, G Loss: 0.6927826404571533, D Loss: 1.3862957954406738\n",
            "Epoch 10, Batch 80, G Loss: 0.692768931388855, D Loss: 1.3864378929138184\n",
            "Epoch 10, Batch 81, G Loss: 0.6927551031112671, D Loss: 1.3864164352416992\n",
            "Epoch 10, Batch 82, G Loss: 0.6927440166473389, D Loss: 1.3863892555236816\n",
            "Epoch 10, Batch 83, G Loss: 0.6927404403686523, D Loss: 1.3863348960876465\n",
            "Epoch 10, Batch 84, G Loss: 0.692750096321106, D Loss: 1.386622667312622\n",
            "Epoch 10, Batch 85, G Loss: 0.692742645740509, D Loss: 1.3865716457366943\n",
            "Epoch 10, Batch 86, G Loss: 0.6927165389060974, D Loss: 1.3865585327148438\n",
            "Epoch 10, Batch 87, G Loss: 0.6926801800727844, D Loss: 1.3864221572875977\n",
            "Epoch 10, Batch 88, G Loss: 0.6926618218421936, D Loss: 1.3863823413848877\n",
            "Epoch 10, Batch 89, G Loss: 0.6926469802856445, D Loss: 1.3863186836242676\n",
            "Epoch 10, Batch 90, G Loss: 0.6926425099372864, D Loss: 1.3863908052444458\n",
            "Epoch 10, Batch 91, G Loss: 0.6926603317260742, D Loss: 1.386242389678955\n",
            "Epoch 10, Batch 92, G Loss: 0.6926925182342529, D Loss: 1.3862249851226807\n",
            "Epoch 10, Batch 93, G Loss: 0.6927545666694641, D Loss: 1.3862733840942383\n",
            "Epoch 10, Batch 94, G Loss: 0.6928284168243408, D Loss: 1.386608362197876\n",
            "Epoch 10, Batch 95, G Loss: 0.6928573846817017, D Loss: 1.386592149734497\n",
            "Epoch 10, Batch 96, G Loss: 0.6928480267524719, D Loss: 1.386568307876587\n",
            "Epoch 10, Batch 97, G Loss: 0.6928223371505737, D Loss: 1.3864814043045044\n",
            "Epoch 10, Batch 98, G Loss: 0.6927728652954102, D Loss: 1.3863751888275146\n",
            "Epoch 10, Batch 99, G Loss: 0.6927263140678406, D Loss: 1.3863792419433594\n",
            "Epoch 10, Batch 100, G Loss: 0.6926862597465515, D Loss: 1.3863575458526611\n",
            "Epoch 10, Batch 101, G Loss: 0.6926490664482117, D Loss: 1.3863208293914795\n",
            "Epoch 10, Batch 102, G Loss: 0.6926285624504089, D Loss: 1.3862829208374023\n",
            "Epoch 10, Batch 103, G Loss: 0.6926313042640686, D Loss: 1.3862812519073486\n",
            "Epoch 10, Batch 104, G Loss: 0.6926701664924622, D Loss: 1.3862934112548828\n",
            "Epoch 10, Batch 105, G Loss: 0.6927156448364258, D Loss: 1.3862849473953247\n",
            "Epoch 10, Batch 106, G Loss: 0.6928057074546814, D Loss: 1.3862899541854858\n",
            "Epoch 10, Batch 107, G Loss: 0.6929256916046143, D Loss: 1.3862948417663574\n",
            "Epoch 10, Batch 108, G Loss: 0.6930750608444214, D Loss: 1.3862882852554321\n",
            "Epoch 10, Batch 109, G Loss: 0.6932017803192139, D Loss: 1.3862781524658203\n",
            "Epoch 10, Batch 110, G Loss: 0.6932947039604187, D Loss: 1.3862898349761963\n",
            "Epoch 10, Batch 111, G Loss: 0.6933693885803223, D Loss: 1.3863009214401245\n",
            "Epoch 10, Batch 112, G Loss: 0.6934418678283691, D Loss: 1.3862988948822021\n",
            "Epoch 10, Batch 113, G Loss: 0.6935096383094788, D Loss: 1.3863017559051514\n",
            "Epoch 10, Batch 114, G Loss: 0.693574070930481, D Loss: 1.3862940073013306\n",
            "Epoch 10, Batch 115, G Loss: 0.6936269998550415, D Loss: 1.3863061666488647\n",
            "Epoch 10, Batch 116, G Loss: 0.6936905384063721, D Loss: 1.3862969875335693\n",
            "Epoch 10, Batch 117, G Loss: 0.693748950958252, D Loss: 1.3862860202789307\n",
            "Epoch 10, Batch 118, G Loss: 0.6937888264656067, D Loss: 1.386284351348877\n",
            "Epoch 10, Batch 119, G Loss: 0.6938158869743347, D Loss: 1.3862746953964233\n",
            "Epoch 10, Batch 120, G Loss: 0.6938267946243286, D Loss: 1.3862897157669067\n",
            "Epoch 10, Batch 121, G Loss: 0.6938057541847229, D Loss: 1.3862824440002441\n",
            "Epoch 10, Batch 122, G Loss: 0.6937627792358398, D Loss: 1.3862850666046143\n",
            "Epoch 10, Batch 123, G Loss: 0.6936960816383362, D Loss: 1.3863221406936646\n",
            "Epoch 10, Batch 124, G Loss: 0.693601667881012, D Loss: 1.3863861560821533\n",
            "Epoch 10, Batch 125, G Loss: 0.6934677362442017, D Loss: 1.3863716125488281\n",
            "Epoch 10, Batch 126, G Loss: 0.6933054327964783, D Loss: 1.386361837387085\n",
            "Epoch 10, Batch 127, G Loss: 0.693134069442749, D Loss: 1.3863303661346436\n",
            "Epoch 10, Batch 128, G Loss: 0.6929596662521362, D Loss: 1.3862416744232178\n",
            "Epoch 10, Batch 129, G Loss: 0.6928346753120422, D Loss: 1.3862521648406982\n",
            "Epoch 10, Batch 130, G Loss: 0.6927465200424194, D Loss: 1.386336088180542\n",
            "Epoch 10, Batch 131, G Loss: 0.6926645636558533, D Loss: 1.3863160610198975\n",
            "Epoch 10, Batch 132, G Loss: 0.6926047801971436, D Loss: 1.3862968683242798\n",
            "Epoch 10, Batch 133, G Loss: 0.6925704479217529, D Loss: 1.3864296674728394\n",
            "Epoch 10, Batch 134, G Loss: 0.6925117373466492, D Loss: 1.3863756656646729\n",
            "Epoch 10, Batch 135, G Loss: 0.6924474835395813, D Loss: 1.3863482475280762\n",
            "Epoch 10, Batch 136, G Loss: 0.6923940181732178, D Loss: 1.3863285779953003\n",
            "Epoch 10, Batch 137, G Loss: 0.6923406720161438, D Loss: 1.3862991333007812\n",
            "Epoch 10, Batch 138, G Loss: 0.6923171281814575, D Loss: 1.3862907886505127\n",
            "Epoch 10, Batch 139, G Loss: 0.6923260688781738, D Loss: 1.3863060474395752\n",
            "Epoch 10, Batch 140, G Loss: 0.692377507686615, D Loss: 1.3862779140472412\n",
            "Epoch 10, Batch 141, G Loss: 0.6924483776092529, D Loss: 1.3863047361373901\n",
            "Epoch 10, Batch 142, G Loss: 0.6925454139709473, D Loss: 1.3862807750701904\n",
            "Epoch 10, Batch 143, G Loss: 0.6926566362380981, D Loss: 1.3863507509231567\n",
            "Epoch 10, Batch 144, G Loss: 0.6927929520606995, D Loss: 1.3863043785095215\n",
            "Epoch 10, Batch 145, G Loss: 0.6929389238357544, D Loss: 1.3863953351974487\n",
            "Epoch 10, Batch 146, G Loss: 0.6931049823760986, D Loss: 1.3862652778625488\n",
            "Epoch 10, Batch 147, G Loss: 0.6932511329650879, D Loss: 1.3862391710281372\n",
            "Epoch 10, Batch 148, G Loss: 0.6933668255805969, D Loss: 1.3862390518188477\n",
            "Epoch 10, Batch 149, G Loss: 0.6934518814086914, D Loss: 1.3861987590789795\n",
            "Epoch 10, Batch 150, G Loss: 0.693495512008667, D Loss: 1.3863592147827148\n",
            "Epoch 10, Batch 151, G Loss: 0.6935335397720337, D Loss: 1.3863033056259155\n",
            "Epoch 10, Batch 152, G Loss: 0.6935519576072693, D Loss: 1.386218547821045\n",
            "Epoch 10, Batch 153, G Loss: 0.6935378909111023, D Loss: 1.3862441778182983\n",
            "Epoch 10, Batch 154, G Loss: 0.6935034990310669, D Loss: 1.386286735534668\n",
            "Epoch 10, Batch 155, G Loss: 0.6934576034545898, D Loss: 1.386289119720459\n",
            "Epoch 10, Batch 156, G Loss: 0.6934050917625427, D Loss: 1.3863065242767334\n",
            "Epoch 10, Batch 157, G Loss: 0.6933503150939941, D Loss: 1.386197566986084\n",
            "Epoch 10, Batch 158, G Loss: 0.6932788491249084, D Loss: 1.386319637298584\n",
            "Epoch 10, Batch 159, G Loss: 0.6932134628295898, D Loss: 1.386396884918213\n",
            "Epoch 10, Batch 160, G Loss: 0.693168044090271, D Loss: 1.3859763145446777\n",
            "Epoch 10, Batch 161, G Loss: 0.6930812001228333, D Loss: 1.38592529296875\n",
            "Epoch 10, Batch 162, G Loss: 0.6929579973220825, D Loss: 1.3860926628112793\n",
            "Epoch 10, Batch 163, G Loss: 0.6928309202194214, D Loss: 1.3861415386199951\n",
            "Epoch 10, Batch 164, G Loss: 0.6927182674407959, D Loss: 1.3860588073730469\n",
            "Epoch 10, Batch 165, G Loss: 0.6926121711730957, D Loss: 1.3861544132232666\n",
            "Epoch 10, Batch 166, G Loss: 0.6925325989723206, D Loss: 1.3862435817718506\n",
            "Epoch 10, Batch 167, G Loss: 0.6924809217453003, D Loss: 1.3864561319351196\n",
            "Epoch 10, Batch 168, G Loss: 0.6924819946289062, D Loss: 1.386319637298584\n",
            "Epoch 10, Batch 169, G Loss: 0.692521870136261, D Loss: 1.3861913681030273\n",
            "Epoch 10, Batch 170, G Loss: 0.6925771236419678, D Loss: 1.3861714601516724\n",
            "Epoch 10, Batch 171, G Loss: 0.6926453113555908, D Loss: 1.386224389076233\n",
            "Epoch 10, Batch 172, G Loss: 0.692730724811554, D Loss: 1.3860503435134888\n",
            "Epoch 10, Batch 173, G Loss: 0.692816436290741, D Loss: 1.3859915733337402\n",
            "Epoch 10, Batch 174, G Loss: 0.6928860545158386, D Loss: 1.3862942457199097\n",
            "Epoch 10, Batch 175, G Loss: 0.6929662227630615, D Loss: 1.386216402053833\n",
            "Epoch 10, Batch 176, G Loss: 0.6930498480796814, D Loss: 1.386271595954895\n",
            "Epoch 10, Batch 177, G Loss: 0.6931345462799072, D Loss: 1.3863801956176758\n",
            "Epoch 10, Batch 178, G Loss: 0.6932116150856018, D Loss: 1.3862454891204834\n",
            "Epoch 10, Batch 179, G Loss: 0.6932894587516785, D Loss: 1.3863415718078613\n",
            "Epoch 10, Batch 180, G Loss: 0.6933498978614807, D Loss: 1.3863487243652344\n",
            "Epoch 10, Batch 181, G Loss: 0.6934101581573486, D Loss: 1.3863673210144043\n",
            "Epoch 10, Batch 182, G Loss: 0.6934496760368347, D Loss: 1.3863580226898193\n",
            "Epoch 10, Batch 183, G Loss: 0.6934934258460999, D Loss: 1.3863117694854736\n",
            "Epoch 10, Batch 184, G Loss: 0.693513035774231, D Loss: 1.3864179849624634\n",
            "Epoch 10, Batch 185, G Loss: 0.6935233473777771, D Loss: 1.3863352537155151\n",
            "Epoch 10, Batch 186, G Loss: 0.6935323476791382, D Loss: 1.3863551616668701\n",
            "Epoch 10, Batch 187, G Loss: 0.6935178637504578, D Loss: 1.3864507675170898\n",
            "Epoch 10, Batch 188, G Loss: 0.693504810333252, D Loss: 1.3859654664993286\n",
            "Epoch 10, Batch 189, G Loss: 0.6934736371040344, D Loss: 1.386198878288269\n",
            "Epoch 10, Batch 190, G Loss: 0.6934266090393066, D Loss: 1.3862121105194092\n",
            "Epoch 10, Batch 191, G Loss: 0.69336998462677, D Loss: 1.3861255645751953\n",
            "Epoch 10, Batch 192, G Loss: 0.6933134198188782, D Loss: 1.3860676288604736\n",
            "Epoch 10, Batch 193, G Loss: 0.693238377571106, D Loss: 1.3862192630767822\n",
            "Epoch 10, Batch 194, G Loss: 0.6931782960891724, D Loss: 1.3862022161483765\n",
            "Epoch 10, Batch 195, G Loss: 0.6931177377700806, D Loss: 1.386056661605835\n",
            "Epoch 10, Batch 196, G Loss: 0.6930575370788574, D Loss: 1.3861428499221802\n",
            "Epoch 10, Batch 197, G Loss: 0.6930092573165894, D Loss: 1.386108160018921\n",
            "Epoch 10, Batch 198, G Loss: 0.6929718852043152, D Loss: 1.3861033916473389\n",
            "Epoch 10, Batch 199, G Loss: 0.6929419636726379, D Loss: 1.3862578868865967\n",
            "Epoch 10, Batch 200, G Loss: 0.6929143667221069, D Loss: 1.386549472808838\n",
            "Epoch 10, Batch 201, G Loss: 0.6929273009300232, D Loss: 1.3867530822753906\n",
            "Epoch 10, Batch 202, G Loss: 0.6929710507392883, D Loss: 1.3862870931625366\n",
            "Epoch 10, Batch 203, G Loss: 0.6930129528045654, D Loss: 1.3859115839004517\n",
            "Epoch 10, Batch 204, G Loss: 0.6930473446846008, D Loss: 1.3859679698944092\n",
            "Epoch 10, Batch 205, G Loss: 0.6930881142616272, D Loss: 1.3856664896011353\n",
            "Epoch 10, Batch 206, G Loss: 0.6930999755859375, D Loss: 1.3861668109893799\n",
            "Epoch 10, Batch 207, G Loss: 0.693108081817627, D Loss: 1.3860654830932617\n",
            "Epoch 10, Batch 208, G Loss: 0.6931250691413879, D Loss: 1.3861794471740723\n",
            "Epoch 10, Batch 209, G Loss: 0.6931323409080505, D Loss: 1.386168360710144\n",
            "Epoch 10, Batch 210, G Loss: 0.6931378245353699, D Loss: 1.3862755298614502\n",
            "Epoch 10, Batch 211, G Loss: 0.6931513547897339, D Loss: 1.3861193656921387\n",
            "Epoch 10, Batch 212, G Loss: 0.6931760907173157, D Loss: 1.385864496231079\n",
            "Epoch 10, Batch 213, G Loss: 0.6931729912757874, D Loss: 1.386183261871338\n",
            "Epoch 10, Batch 214, G Loss: 0.6931800246238708, D Loss: 1.3867273330688477\n",
            "Epoch 10, Batch 215, G Loss: 0.6932008862495422, D Loss: 1.3869532346725464\n",
            "Epoch 10, Batch 216, G Loss: 0.6932536363601685, D Loss: 1.3866808414459229\n",
            "Epoch 10, Batch 217, G Loss: 0.6933217644691467, D Loss: 1.3864400386810303\n",
            "Epoch 10, Batch 218, G Loss: 0.6933755874633789, D Loss: 1.386441946029663\n",
            "Epoch 10, Batch 219, G Loss: 0.6934356093406677, D Loss: 1.3866539001464844\n",
            "Epoch 10, Batch 220, G Loss: 0.6934770345687866, D Loss: 1.3859822750091553\n",
            "Epoch 10, Batch 221, G Loss: 0.6935194134712219, D Loss: 1.3860251903533936\n",
            "Epoch 10, Batch 222, G Loss: 0.6935170888900757, D Loss: 1.3859126567840576\n",
            "Epoch 10, Batch 223, G Loss: 0.6934952735900879, D Loss: 1.3861324787139893\n",
            "Epoch 10, Batch 224, G Loss: 0.6934616565704346, D Loss: 1.386138916015625\n",
            "Epoch 10, Batch 225, G Loss: 0.6934210062026978, D Loss: 1.3862888813018799\n",
            "Epoch 10, Batch 226, G Loss: 0.6933740377426147, D Loss: 1.3865251541137695\n",
            "Epoch 10, Batch 227, G Loss: 0.6933357119560242, D Loss: 1.3866589069366455\n",
            "Epoch 10, Batch 228, G Loss: 0.6933138966560364, D Loss: 1.3867303133010864\n",
            "Epoch 10, Batch 229, G Loss: 0.6933130025863647, D Loss: 1.3865673542022705\n",
            "Epoch 10, Batch 230, G Loss: 0.6933151483535767, D Loss: 1.386338233947754\n",
            "Epoch 10, Batch 231, G Loss: 0.6933255791664124, D Loss: 1.3865466117858887\n",
            "Epoch 10, Batch 232, G Loss: 0.6933432817459106, D Loss: 1.386481523513794\n",
            "Epoch 10, Batch 233, G Loss: 0.6933619379997253, D Loss: 1.3865153789520264\n",
            "Epoch 10, Batch 234, G Loss: 0.6933842897415161, D Loss: 1.3865211009979248\n",
            "Epoch 10, Batch 235, G Loss: 0.693415641784668, D Loss: 1.386704921722412\n",
            "Epoch 10, Batch 236, G Loss: 0.6934523582458496, D Loss: 1.3864411115646362\n",
            "Epoch 10, Batch 237, G Loss: 0.6934928297996521, D Loss: 1.386565923690796\n",
            "Epoch 10, Batch 238, G Loss: 0.6935407519340515, D Loss: 1.3866255283355713\n",
            "Epoch 10, Batch 239, G Loss: 0.6935823559761047, D Loss: 1.3865187168121338\n",
            "Epoch 10, Batch 240, G Loss: 0.6936310529708862, D Loss: 1.3865495920181274\n",
            "Epoch 10, Batch 241, G Loss: 0.6936776041984558, D Loss: 1.3865540027618408\n",
            "Epoch 10, Batch 242, G Loss: 0.6937319040298462, D Loss: 1.3865084648132324\n",
            "Epoch 10, Batch 243, G Loss: 0.693779468536377, D Loss: 1.3864024877548218\n",
            "Epoch 10, Batch 244, G Loss: 0.6938145160675049, D Loss: 1.386398434638977\n",
            "Epoch 10, Batch 245, G Loss: 0.6938405632972717, D Loss: 1.3863471746444702\n",
            "Epoch 10, Batch 246, G Loss: 0.6938499212265015, D Loss: 1.3863301277160645\n",
            "Epoch 10, Batch 247, G Loss: 0.6938463449478149, D Loss: 1.3863070011138916\n",
            "Epoch 10, Batch 248, G Loss: 0.6938298940658569, D Loss: 1.3863041400909424\n",
            "Epoch 10, Batch 249, G Loss: 0.6938120722770691, D Loss: 1.3862782716751099\n",
            "Epoch 10, Batch 250, G Loss: 0.6937707662582397, D Loss: 1.3862687349319458\n",
            "Epoch 10, Batch 251, G Loss: 0.693727433681488, D Loss: 1.3862645626068115\n",
            "Epoch 10, Batch 252, G Loss: 0.6936721205711365, D Loss: 1.3862788677215576\n",
            "Epoch 10, Batch 253, G Loss: 0.6935922503471375, D Loss: 1.38627028465271\n",
            "Epoch 10, Batch 254, G Loss: 0.6934974789619446, D Loss: 1.3862543106079102\n",
            "Epoch 10, Batch 255, G Loss: 0.6934059262275696, D Loss: 1.3862076997756958\n",
            "Epoch 10, Batch 256, G Loss: 0.6933268904685974, D Loss: 1.3862054347991943\n",
            "Epoch 10, Batch 257, G Loss: 0.6932597160339355, D Loss: 1.386169672012329\n",
            "Epoch 10, Batch 258, G Loss: 0.6932179927825928, D Loss: 1.3861593008041382\n",
            "Epoch 10, Batch 259, G Loss: 0.6931923031806946, D Loss: 1.386160135269165\n",
            "Epoch 10, Batch 260, G Loss: 0.69318687915802, D Loss: 1.3860952854156494\n",
            "Epoch 10, Batch 261, G Loss: 0.693208634853363, D Loss: 1.3860511779785156\n",
            "Epoch 10, Batch 262, G Loss: 0.6932520866394043, D Loss: 1.386096715927124\n",
            "Epoch 10, Batch 263, G Loss: 0.6933130025863647, D Loss: 1.386099100112915\n",
            "Epoch 10, Batch 264, G Loss: 0.6933768391609192, D Loss: 1.3859784603118896\n",
            "Epoch 10, Batch 265, G Loss: 0.6934643983840942, D Loss: 1.386002540588379\n",
            "Epoch 10, Batch 266, G Loss: 0.6935454607009888, D Loss: 1.3861076831817627\n",
            "Epoch 10, Batch 267, G Loss: 0.693622350692749, D Loss: 1.3859679698944092\n",
            "Epoch 10, Batch 268, G Loss: 0.6937040090560913, D Loss: 1.3862276077270508\n",
            "Epoch 10, Batch 269, G Loss: 0.6937545537948608, D Loss: 1.3863089084625244\n",
            "Epoch 10, Batch 270, G Loss: 0.6937659978866577, D Loss: 1.3862049579620361\n",
            "Epoch 10, Batch 271, G Loss: 0.6937466263771057, D Loss: 1.3864530324935913\n",
            "Epoch 10, Batch 272, G Loss: 0.693681538105011, D Loss: 1.3859429359436035\n",
            "Epoch 10, Batch 273, G Loss: 0.693627655506134, D Loss: 1.3859457969665527\n",
            "Epoch 10, Batch 274, G Loss: 0.6935819387435913, D Loss: 1.3860163688659668\n",
            "Epoch 10, Batch 275, G Loss: 0.6935439705848694, D Loss: 1.385749340057373\n",
            "Epoch 10, Batch 276, G Loss: 0.6935298442840576, D Loss: 1.3857635259628296\n",
            "Epoch 10, Batch 277, G Loss: 0.6935282349586487, D Loss: 1.3858028650283813\n",
            "Epoch 10, Batch 278, G Loss: 0.6935561895370483, D Loss: 1.385793685913086\n",
            "Epoch 10, Batch 279, G Loss: 0.6935926079750061, D Loss: 1.3857038021087646\n",
            "Epoch 10, Batch 280, G Loss: 0.6936577558517456, D Loss: 1.3856979608535767\n",
            "Epoch 10, Batch 281, G Loss: 0.6937301754951477, D Loss: 1.38619065284729\n",
            "Epoch 10, Batch 282, G Loss: 0.6937617063522339, D Loss: 1.38600754737854\n",
            "Epoch 10, Batch 283, G Loss: 0.6937863230705261, D Loss: 1.3858838081359863\n",
            "Epoch 10, Batch 284, G Loss: 0.6938040852546692, D Loss: 1.3862015008926392\n",
            "Epoch 10, Batch 285, G Loss: 0.6937940716743469, D Loss: 1.3862937688827515\n",
            "Epoch 10, Batch 286, G Loss: 0.6937516927719116, D Loss: 1.3860373497009277\n",
            "Epoch 10, Batch 287, G Loss: 0.6936976909637451, D Loss: 1.3857659101486206\n",
            "Epoch 10, Batch 288, G Loss: 0.6936579942703247, D Loss: 1.3860883712768555\n",
            "Epoch 10, Batch 289, G Loss: 0.6936098337173462, D Loss: 1.3858754634857178\n",
            "Epoch 10, Batch 290, G Loss: 0.6935720443725586, D Loss: 1.3861979246139526\n",
            "Epoch 10, Batch 291, G Loss: 0.6935181617736816, D Loss: 1.3861467838287354\n",
            "Epoch 10, Batch 292, G Loss: 0.6934714913368225, D Loss: 1.3862640857696533\n",
            "Epoch 10, Batch 293, G Loss: 0.69339519739151, D Loss: 1.3860454559326172\n",
            "Epoch 10, Batch 294, G Loss: 0.6933454275131226, D Loss: 1.3860628604888916\n",
            "Epoch 10, Batch 295, G Loss: 0.6932899951934814, D Loss: 1.3860130310058594\n",
            "Epoch 10, Batch 296, G Loss: 0.6932469606399536, D Loss: 1.386307716369629\n",
            "Epoch 10, Batch 297, G Loss: 0.6932069659233093, D Loss: 1.3861093521118164\n",
            "Epoch 10, Batch 298, G Loss: 0.6931865215301514, D Loss: 1.3856761455535889\n",
            "Epoch 10, Batch 299, G Loss: 0.6931991577148438, D Loss: 1.3859233856201172\n",
            "Epoch 10, Batch 300, G Loss: 0.6932119131088257, D Loss: 1.3859522342681885\n",
            "Epoch 10, Batch 301, G Loss: 0.6932802796363831, D Loss: 1.3874354362487793\n",
            "Epoch 10, Batch 302, G Loss: 0.6932419538497925, D Loss: 1.386601209640503\n",
            "Epoch 10, Batch 303, G Loss: 0.6931813955307007, D Loss: 1.3873109817504883\n",
            "Epoch 10, Batch 304, G Loss: 0.693068265914917, D Loss: 1.3870306015014648\n",
            "Epoch 10, Batch 305, G Loss: 0.6929216980934143, D Loss: 1.3871171474456787\n",
            "Epoch 10, Batch 306, G Loss: 0.6927492022514343, D Loss: 1.3872581720352173\n",
            "Epoch 10, Batch 307, G Loss: 0.6925842761993408, D Loss: 1.3870182037353516\n",
            "Epoch 10, Batch 308, G Loss: 0.6924247145652771, D Loss: 1.3869364261627197\n",
            "Epoch 10, Batch 309, G Loss: 0.692253828048706, D Loss: 1.3870694637298584\n",
            "Epoch 10, Batch 310, G Loss: 0.6921162605285645, D Loss: 1.3870012760162354\n",
            "Epoch 10, Batch 311, G Loss: 0.6919999122619629, D Loss: 1.3867230415344238\n",
            "Epoch 10, Batch 312, G Loss: 0.691912829875946, D Loss: 1.387059211730957\n",
            "Epoch 10, Batch 313, G Loss: 0.6918590068817139, D Loss: 1.3868392705917358\n",
            "Epoch 10, Batch 314, G Loss: 0.6918200850486755, D Loss: 1.3865618705749512\n",
            "Epoch 10, Batch 315, G Loss: 0.6918306946754456, D Loss: 1.3867065906524658\n",
            "Epoch 10, Batch 316, G Loss: 0.6918797492980957, D Loss: 1.3865413665771484\n",
            "Epoch 10, Batch 317, G Loss: 0.6919835209846497, D Loss: 1.3863747119903564\n",
            "Epoch 10, Batch 318, G Loss: 0.6921008229255676, D Loss: 1.3862946033477783\n",
            "Epoch 10, Batch 319, G Loss: 0.6922696828842163, D Loss: 1.386641502380371\n",
            "Epoch 10, Batch 320, G Loss: 0.6924260854721069, D Loss: 1.3863646984100342\n",
            "Epoch 10, Batch 321, G Loss: 0.6926159262657166, D Loss: 1.3866350650787354\n",
            "Epoch 10, Batch 322, G Loss: 0.6927633285522461, D Loss: 1.3865032196044922\n",
            "Epoch 10, Batch 323, G Loss: 0.6929123401641846, D Loss: 1.3866887092590332\n",
            "Epoch 10, Batch 324, G Loss: 0.6930059194564819, D Loss: 1.386545181274414\n",
            "Epoch 10, Batch 325, G Loss: 0.6930770874023438, D Loss: 1.3863518238067627\n",
            "Epoch 10, Batch 326, G Loss: 0.6931454539299011, D Loss: 1.3864811658859253\n",
            "Epoch 10, Batch 327, G Loss: 0.6931906938552856, D Loss: 1.3863823413848877\n",
            "Epoch 10, Batch 328, G Loss: 0.6932128667831421, D Loss: 1.3865814208984375\n",
            "Epoch 10, Batch 329, G Loss: 0.6932008862495422, D Loss: 1.3863725662231445\n",
            "Epoch 10, Batch 330, G Loss: 0.6931800842285156, D Loss: 1.386223316192627\n",
            "Epoch 10, Batch 331, G Loss: 0.6931671500205994, D Loss: 1.386350154876709\n",
            "Epoch 10, Batch 332, G Loss: 0.6931491494178772, D Loss: 1.3863729238510132\n",
            "Epoch 10, Batch 333, G Loss: 0.6931282877922058, D Loss: 1.3864398002624512\n",
            "Epoch 10, Batch 334, G Loss: 0.6930786371231079, D Loss: 1.3863580226898193\n",
            "Epoch 10, Batch 335, G Loss: 0.6930338144302368, D Loss: 1.3864094018936157\n",
            "Epoch 10, Batch 336, G Loss: 0.6929672360420227, D Loss: 1.3864130973815918\n",
            "Epoch 10, Batch 337, G Loss: 0.6928985118865967, D Loss: 1.3864268064498901\n",
            "Epoch 10, Batch 338, G Loss: 0.692814290523529, D Loss: 1.386267900466919\n",
            "Epoch 10, Batch 339, G Loss: 0.6927680969238281, D Loss: 1.386305809020996\n",
            "Epoch 10, Batch 340, G Loss: 0.6927464008331299, D Loss: 1.386275291442871\n",
            "Epoch 10, Batch 341, G Loss: 0.6927642226219177, D Loss: 1.3863799571990967\n",
            "Epoch 10, Batch 342, G Loss: 0.6927488446235657, D Loss: 1.3863468170166016\n",
            "Epoch 10, Batch 343, G Loss: 0.6927183270454407, D Loss: 1.386315107345581\n",
            "Epoch 10, Batch 344, G Loss: 0.6926655769348145, D Loss: 1.3862780332565308\n",
            "Epoch 10, Batch 345, G Loss: 0.6926053762435913, D Loss: 1.3862744569778442\n",
            "Epoch 10, Batch 346, G Loss: 0.6925626993179321, D Loss: 1.386285424232483\n",
            "Epoch 10, Batch 347, G Loss: 0.6925449967384338, D Loss: 1.3862687349319458\n",
            "Epoch 10, Batch 348, G Loss: 0.6925517320632935, D Loss: 1.3862481117248535\n",
            "Epoch 10, Batch 349, G Loss: 0.6925731301307678, D Loss: 1.3862987756729126\n",
            "Epoch 10, Batch 350, G Loss: 0.6926200985908508, D Loss: 1.3862359523773193\n",
            "Epoch 10, Batch 351, G Loss: 0.6926792860031128, D Loss: 1.3862390518188477\n",
            "Epoch 10, Batch 352, G Loss: 0.6927455067634583, D Loss: 1.3862767219543457\n",
            "Epoch 10, Batch 353, G Loss: 0.6928281784057617, D Loss: 1.3861584663391113\n",
            "Epoch 10, Batch 354, G Loss: 0.6928969621658325, D Loss: 1.3861122131347656\n",
            "Epoch 10, Batch 355, G Loss: 0.6929550766944885, D Loss: 1.385878086090088\n",
            "Epoch 10, Batch 356, G Loss: 0.6929872632026672, D Loss: 1.3859035968780518\n",
            "Epoch 10, Batch 357, G Loss: 0.693005383014679, D Loss: 1.386073112487793\n",
            "Epoch 10, Batch 358, G Loss: 0.6930099129676819, D Loss: 1.3859422206878662\n",
            "Epoch 10, Batch 359, G Loss: 0.6930073499679565, D Loss: 1.3858516216278076\n",
            "Epoch 10, Batch 360, G Loss: 0.6929899454116821, D Loss: 1.385917067527771\n",
            "Epoch 10, Batch 361, G Loss: 0.6929662823677063, D Loss: 1.385643720626831\n",
            "Epoch 10, Batch 362, G Loss: 0.6929362416267395, D Loss: 1.3859400749206543\n",
            "Epoch 10, Batch 363, G Loss: 0.6928980350494385, D Loss: 1.3861074447631836\n",
            "Epoch 10, Batch 364, G Loss: 0.6928958296775818, D Loss: 1.3860251903533936\n",
            "Epoch 10, Batch 365, G Loss: 0.6928818225860596, D Loss: 1.3859405517578125\n",
            "Epoch 10, Batch 366, G Loss: 0.6928937435150146, D Loss: 1.3859366178512573\n",
            "Epoch 10, Batch 367, G Loss: 0.6929100155830383, D Loss: 1.386135220527649\n",
            "Epoch 10, Batch 368, G Loss: 0.6929313540458679, D Loss: 1.3862550258636475\n",
            "Epoch 10, Batch 369, G Loss: 0.6929789781570435, D Loss: 1.386297583580017\n",
            "Epoch 10, Batch 370, G Loss: 0.6930286884307861, D Loss: 1.3865461349487305\n",
            "Epoch 10, Batch 371, G Loss: 0.6930757761001587, D Loss: 1.386518955230713\n",
            "Epoch 10, Batch 372, G Loss: 0.6931482553482056, D Loss: 1.3859491348266602\n",
            "Epoch 10, Batch 373, G Loss: 0.6932258009910583, D Loss: 1.3859806060791016\n",
            "Epoch 10, Batch 374, G Loss: 0.6932699084281921, D Loss: 1.3862433433532715\n",
            "Epoch 10, Batch 375, G Loss: 0.6933361887931824, D Loss: 1.3860797882080078\n",
            "Epoch 10, Batch 376, G Loss: 0.6933820843696594, D Loss: 1.3860201835632324\n",
            "Epoch 10, Batch 377, G Loss: 0.6934062242507935, D Loss: 1.3860373497009277\n",
            "Epoch 10, Batch 378, G Loss: 0.6934294700622559, D Loss: 1.3862922191619873\n",
            "Epoch 10, Batch 379, G Loss: 0.6934410929679871, D Loss: 1.3862522840499878\n",
            "Epoch 10, Batch 380, G Loss: 0.6934699416160583, D Loss: 1.3861711025238037\n",
            "Epoch 10, Batch 381, G Loss: 0.6934697031974792, D Loss: 1.3866033554077148\n",
            "Epoch 10, Batch 382, G Loss: 0.693475067615509, D Loss: 1.3857288360595703\n",
            "Epoch 10, Batch 383, G Loss: 0.6934534907341003, D Loss: 1.386005163192749\n",
            "Epoch 10, Batch 384, G Loss: 0.6934410333633423, D Loss: 1.3863346576690674\n",
            "Epoch 10, Batch 385, G Loss: 0.6934278011322021, D Loss: 1.3859732151031494\n",
            "Epoch 10, Batch 386, G Loss: 0.693402886390686, D Loss: 1.3857206106185913\n",
            "Epoch 10, Batch 387, G Loss: 0.6933695077896118, D Loss: 1.386124849319458\n",
            "Epoch 10, Batch 388, G Loss: 0.6933210492134094, D Loss: 1.3862738609313965\n",
            "Epoch 10, Batch 389, G Loss: 0.6933171153068542, D Loss: 1.3864024877548218\n",
            "Epoch 10, Batch 390, G Loss: 0.6932835578918457, D Loss: 1.3872230052947998\n",
            "Epoch 10, Batch 391, G Loss: 0.6932846307754517, D Loss: 1.3864688873291016\n",
            "Epoch 10, Batch 392, G Loss: 0.6932992935180664, D Loss: 1.38657546043396\n",
            "Epoch 10, Batch 393, G Loss: 0.6932899355888367, D Loss: 1.3864986896514893\n",
            "Epoch 10, Batch 394, G Loss: 0.6933122873306274, D Loss: 1.3863792419433594\n",
            "Epoch 10, Batch 395, G Loss: 0.6933268308639526, D Loss: 1.3865491151809692\n",
            "Epoch 10, Batch 396, G Loss: 0.6933397054672241, D Loss: 1.3849797248840332\n",
            "Epoch 10, Batch 397, G Loss: 0.6933528184890747, D Loss: 1.3853317499160767\n",
            "Epoch 10, Batch 398, G Loss: 0.6933326125144958, D Loss: 1.3858146667480469\n",
            "Epoch 10, Batch 399, G Loss: 0.6933223009109497, D Loss: 1.38578200340271\n",
            "Epoch 10, Batch 400, G Loss: 0.6933061480522156, D Loss: 1.3866901397705078\n",
            "Epoch 10, Batch 401, G Loss: 0.6932888627052307, D Loss: 1.386317491531372\n",
            "Epoch 10, Batch 402, G Loss: 0.6932888627052307, D Loss: 1.3861515522003174\n",
            "Epoch 10, Batch 403, G Loss: 0.6932863593101501, D Loss: 1.386770486831665\n",
            "Epoch 10, Batch 404, G Loss: 0.6933031678199768, D Loss: 1.3871928453445435\n",
            "Epoch 10, Batch 405, G Loss: 0.6933116316795349, D Loss: 1.3872102499008179\n",
            "Epoch 10, Batch 406, G Loss: 0.6933495402336121, D Loss: 1.3871331214904785\n",
            "Epoch 10, Batch 407, G Loss: 0.6933760643005371, D Loss: 1.3860529661178589\n",
            "Epoch 10, Batch 408, G Loss: 0.6934183835983276, D Loss: 1.386378526687622\n",
            "Epoch 10, Batch 409, G Loss: 0.693450927734375, D Loss: 1.3865811824798584\n",
            "Epoch 10, Batch 410, G Loss: 0.6934775710105896, D Loss: 1.3864173889160156\n",
            "Epoch 10, Batch 411, G Loss: 0.6934882998466492, D Loss: 1.3867913484573364\n",
            "Epoch 10, Batch 412, G Loss: 0.6935092210769653, D Loss: 1.3867642879486084\n",
            "Epoch 10, Batch 413, G Loss: 0.6935216784477234, D Loss: 1.3867568969726562\n",
            "Epoch 10, Batch 414, G Loss: 0.6935371160507202, D Loss: 1.3868932723999023\n",
            "Epoch 10, Batch 415, G Loss: 0.6935501098632812, D Loss: 1.386974573135376\n",
            "Epoch 10, Batch 416, G Loss: 0.6935800313949585, D Loss: 1.386986494064331\n",
            "Epoch 10, Batch 417, G Loss: 0.6935890316963196, D Loss: 1.386413812637329\n",
            "Epoch 10, Batch 418, G Loss: 0.6935991644859314, D Loss: 1.386412501335144\n",
            "Epoch 10, Batch 419, G Loss: 0.693599283695221, D Loss: 1.3862383365631104\n",
            "Epoch 10, Batch 420, G Loss: 0.6935704946517944, D Loss: 1.3863245248794556\n",
            "Epoch 10, Batch 421, G Loss: 0.693544328212738, D Loss: 1.3865420818328857\n",
            "Epoch 10, Batch 422, G Loss: 0.6935125589370728, D Loss: 1.3866944313049316\n",
            "Epoch 10, Batch 423, G Loss: 0.6934816241264343, D Loss: 1.3865716457366943\n",
            "Epoch 10, Batch 424, G Loss: 0.693455159664154, D Loss: 1.386077642440796\n",
            "Epoch 10, Batch 425, G Loss: 0.693408727645874, D Loss: 1.386179804801941\n",
            "Epoch 10, Batch 426, G Loss: 0.6933687925338745, D Loss: 1.386290431022644\n",
            "Epoch 10, Batch 427, G Loss: 0.6933168768882751, D Loss: 1.3862667083740234\n",
            "Epoch 10, Batch 428, G Loss: 0.6932612657546997, D Loss: 1.3864332437515259\n",
            "Epoch 10, Batch 429, G Loss: 0.6932209730148315, D Loss: 1.3863177299499512\n",
            "Epoch 10, Batch 430, G Loss: 0.6931881904602051, D Loss: 1.3865993022918701\n",
            "Epoch 10, Batch 431, G Loss: 0.693174421787262, D Loss: 1.3865835666656494\n",
            "Epoch 10, Batch 432, G Loss: 0.6931793689727783, D Loss: 1.386457920074463\n",
            "Epoch 10, Batch 433, G Loss: 0.6932038068771362, D Loss: 1.3865957260131836\n",
            "Epoch 10, Batch 434, G Loss: 0.6932467818260193, D Loss: 1.3865625858306885\n",
            "Epoch 10, Batch 435, G Loss: 0.6933075189590454, D Loss: 1.3866113424301147\n",
            "Epoch 10, Batch 436, G Loss: 0.6933772563934326, D Loss: 1.386444091796875\n",
            "Epoch 10, Batch 437, G Loss: 0.6934481263160706, D Loss: 1.386236310005188\n",
            "Epoch 10, Batch 438, G Loss: 0.6934967041015625, D Loss: 1.3862929344177246\n",
            "Epoch 10, Batch 439, G Loss: 0.6935256719589233, D Loss: 1.3862786293029785\n",
            "Epoch 10, Batch 440, G Loss: 0.6935346722602844, D Loss: 1.3862740993499756\n",
            "Epoch 10, Batch 441, G Loss: 0.6935189962387085, D Loss: 1.3862295150756836\n",
            "Epoch 10, Batch 442, G Loss: 0.6934783458709717, D Loss: 1.3862537145614624\n",
            "Epoch 10, Batch 443, G Loss: 0.6934170126914978, D Loss: 1.386289119720459\n",
            "Epoch 10, Batch 444, G Loss: 0.6933475136756897, D Loss: 1.3862378597259521\n",
            "Epoch 10, Batch 445, G Loss: 0.693265974521637, D Loss: 1.38625168800354\n",
            "Epoch 10, Batch 446, G Loss: 0.6931747794151306, D Loss: 1.3862873315811157\n",
            "Epoch 10, Batch 447, G Loss: 0.6930915713310242, D Loss: 1.3862791061401367\n",
            "Epoch 10, Batch 448, G Loss: 0.693016529083252, D Loss: 1.3863253593444824\n",
            "Epoch 10, Batch 449, G Loss: 0.6929682493209839, D Loss: 1.3863296508789062\n",
            "Epoch 10, Batch 450, G Loss: 0.6929441690444946, D Loss: 1.3863189220428467\n",
            "Epoch 10, Batch 451, G Loss: 0.6929459571838379, D Loss: 1.3863276243209839\n",
            "Epoch 10, Batch 452, G Loss: 0.6929692625999451, D Loss: 1.3862661123275757\n",
            "Epoch 10, Batch 453, G Loss: 0.6929917335510254, D Loss: 1.3862935304641724\n",
            "Epoch 10, Batch 454, G Loss: 0.6930257678031921, D Loss: 1.386289358139038\n",
            "Epoch 10, Batch 455, G Loss: 0.6930632591247559, D Loss: 1.3863494396209717\n",
            "Epoch 10, Batch 456, G Loss: 0.6931379437446594, D Loss: 1.3863261938095093\n",
            "Epoch 10, Batch 457, G Loss: 0.6932367086410522, D Loss: 1.386307716369629\n",
            "Epoch 10, Batch 458, G Loss: 0.6933486461639404, D Loss: 1.3862780332565308\n",
            "Epoch 10, Batch 459, G Loss: 0.6934230327606201, D Loss: 1.3862831592559814\n",
            "Epoch 10, Batch 460, G Loss: 0.693463921546936, D Loss: 1.3862853050231934\n",
            "Epoch 10, Batch 461, G Loss: 0.693467915058136, D Loss: 1.3862850666046143\n",
            "Epoch 10, Batch 462, G Loss: 0.693446695804596, D Loss: 1.3862700462341309\n",
            "Epoch 10, Batch 463, G Loss: 0.6934401988983154, D Loss: 1.3862569332122803\n",
            "Epoch 10, Batch 464, G Loss: 0.6934529542922974, D Loss: 1.386253833770752\n",
            "Epoch 10, Batch 465, G Loss: 0.6934733986854553, D Loss: 1.3862247467041016\n",
            "Epoch 10, Batch 466, G Loss: 0.6935163140296936, D Loss: 1.3862038850784302\n",
            "Epoch 10, Batch 467, G Loss: 0.6935768127441406, D Loss: 1.3861801624298096\n",
            "Epoch 10, Batch 468, G Loss: 0.6936495900154114, D Loss: 1.3861615657806396\n",
            "Epoch 10, Batch 469, G Loss: 0.6937299370765686, D Loss: 1.3860726356506348\n",
            "Epoch 10, Batch 470, G Loss: 0.6938280463218689, D Loss: 1.386199951171875\n",
            "Epoch 10, Batch 471, G Loss: 0.6938988566398621, D Loss: 1.3861544132232666\n",
            "Epoch 10, Batch 472, G Loss: 0.6939452886581421, D Loss: 1.386136770248413\n",
            "Epoch 10, Batch 473, G Loss: 0.693976104259491, D Loss: 1.3860993385314941\n",
            "Epoch 10, Batch 474, G Loss: 0.6939860582351685, D Loss: 1.3860821723937988\n",
            "Epoch 10, Batch 475, G Loss: 0.6939809322357178, D Loss: 1.3862266540527344\n",
            "Epoch 10, Batch 476, G Loss: 0.6939404606819153, D Loss: 1.3862547874450684\n",
            "Epoch 10, Batch 477, G Loss: 0.6938613653182983, D Loss: 1.3862186670303345\n",
            "Epoch 10, Batch 478, G Loss: 0.6937627792358398, D Loss: 1.38606858253479\n",
            "Epoch 10, Batch 479, G Loss: 0.6936619281768799, D Loss: 1.385601282119751\n",
            "Epoch 10, Batch 480, G Loss: 0.6936135292053223, D Loss: 1.385707139968872\n",
            "Epoch 10, Batch 481, G Loss: 0.6936133503913879, D Loss: 1.3856465816497803\n",
            "Epoch 10, Batch 482, G Loss: 0.6936363577842712, D Loss: 1.3861653804779053\n",
            "Epoch 10, Batch 483, G Loss: 0.6936391592025757, D Loss: 1.3860673904418945\n",
            "Epoch 10, Batch 484, G Loss: 0.6936550736427307, D Loss: 1.385850429534912\n",
            "Epoch 10, Batch 485, G Loss: 0.6936609745025635, D Loss: 1.385883092880249\n",
            "Epoch 10, Batch 486, G Loss: 0.693676233291626, D Loss: 1.3859188556671143\n",
            "Epoch 10, Batch 487, G Loss: 0.693683922290802, D Loss: 1.3858481645584106\n",
            "Epoch 10, Batch 488, G Loss: 0.6937059164047241, D Loss: 1.38583505153656\n",
            "Epoch 10, Batch 489, G Loss: 0.6937146782875061, D Loss: 1.385860800743103\n",
            "Epoch 10, Batch 490, G Loss: 0.6937388181686401, D Loss: 1.385839819908142\n",
            "Epoch 10, Batch 491, G Loss: 0.6937564015388489, D Loss: 1.38569974899292\n",
            "Epoch 10, Batch 492, G Loss: 0.6937710642814636, D Loss: 1.3858816623687744\n",
            "Epoch 10, Batch 493, G Loss: 0.6937887072563171, D Loss: 1.385495901107788\n",
            "Epoch 10, Batch 494, G Loss: 0.6938117742538452, D Loss: 1.3855292797088623\n",
            "Epoch 10, Batch 495, G Loss: 0.6938605308532715, D Loss: 1.3855359554290771\n",
            "Epoch 10, Batch 496, G Loss: 0.6939132213592529, D Loss: 1.3856244087219238\n",
            "Epoch 10, Batch 497, G Loss: 0.693958044052124, D Loss: 1.3858752250671387\n",
            "Epoch 10, Batch 498, G Loss: 0.6939827799797058, D Loss: 1.3859412670135498\n",
            "Epoch 10, Batch 499, G Loss: 0.6939765810966492, D Loss: 1.3857500553131104\n",
            "Epoch 10, Batch 500, G Loss: 0.6939666271209717, D Loss: 1.386253833770752\n",
            "Epoch 10, Batch 501, G Loss: 0.6939168572425842, D Loss: 1.3865852355957031\n",
            "Epoch 10, Batch 502, G Loss: 0.6938110589981079, D Loss: 1.3865880966186523\n",
            "Epoch 10, Batch 503, G Loss: 0.6936677694320679, D Loss: 1.386345624923706\n",
            "Epoch 10, Batch 504, G Loss: 0.6935235857963562, D Loss: 1.3861172199249268\n",
            "Epoch 10, Batch 505, G Loss: 0.6933753490447998, D Loss: 1.3868283033370972\n",
            "Epoch 10, Batch 506, G Loss: 0.6931760907173157, D Loss: 1.386814832687378\n",
            "Epoch 10, Batch 507, G Loss: 0.692999541759491, D Loss: 1.3873353004455566\n",
            "Epoch 10, Batch 508, G Loss: 0.6927723288536072, D Loss: 1.3868818283081055\n",
            "Epoch 10, Batch 509, G Loss: 0.6925550103187561, D Loss: 1.3861348628997803\n",
            "Epoch 10, Batch 510, G Loss: 0.6923921704292297, D Loss: 1.386303424835205\n",
            "Epoch 10, Batch 511, G Loss: 0.6923136115074158, D Loss: 1.386060118675232\n",
            "Epoch 10, Batch 512, G Loss: 0.6922922134399414, D Loss: 1.3856557607650757\n",
            "Epoch 10, Batch 513, G Loss: 0.692332923412323, D Loss: 1.3861770629882812\n",
            "Epoch 10, Batch 514, G Loss: 0.692449152469635, D Loss: 1.3861346244812012\n",
            "Epoch 10, Batch 515, G Loss: 0.6925910115242004, D Loss: 1.3866603374481201\n",
            "Epoch 10, Batch 516, G Loss: 0.6927316784858704, D Loss: 1.3870062828063965\n",
            "Epoch 10, Batch 517, G Loss: 0.6928357481956482, D Loss: 1.387099027633667\n",
            "Epoch 10, Batch 518, G Loss: 0.6929036974906921, D Loss: 1.3866066932678223\n",
            "Epoch 10, Batch 519, G Loss: 0.6929478645324707, D Loss: 1.3868056535720825\n",
            "Epoch 10, Batch 520, G Loss: 0.6929853558540344, D Loss: 1.3867342472076416\n",
            "Epoch 10, Batch 521, G Loss: 0.6930045485496521, D Loss: 1.3866755962371826\n",
            "Epoch 10, Batch 522, G Loss: 0.692995011806488, D Loss: 1.3862695693969727\n",
            "Epoch 10, Batch 523, G Loss: 0.6929966807365417, D Loss: 1.3860857486724854\n",
            "Epoch 10, Batch 524, G Loss: 0.6930311322212219, D Loss: 1.3861701488494873\n",
            "Epoch 10, Batch 525, G Loss: 0.6930866837501526, D Loss: 1.3858709335327148\n",
            "Epoch 10, Batch 526, G Loss: 0.693146824836731, D Loss: 1.3859174251556396\n",
            "Epoch 10, Batch 527, G Loss: 0.6932623982429504, D Loss: 1.3863210678100586\n",
            "Epoch 10, Batch 528, G Loss: 0.6933382153511047, D Loss: 1.3868353366851807\n",
            "Epoch 10, Batch 529, G Loss: 0.6933651566505432, D Loss: 1.3872451782226562\n",
            "Epoch 10, Batch 530, G Loss: 0.6933255791664124, D Loss: 1.3867886066436768\n",
            "Epoch 10, Batch 531, G Loss: 0.6932567954063416, D Loss: 1.3866100311279297\n",
            "Epoch 10, Batch 532, G Loss: 0.693157434463501, D Loss: 1.385989785194397\n",
            "Epoch 10, Batch 533, G Loss: 0.6930990219116211, D Loss: 1.38654363155365\n",
            "Epoch 10, Batch 534, G Loss: 0.6930188536643982, D Loss: 1.3866698741912842\n",
            "Epoch 10, Batch 535, G Loss: 0.6929488182067871, D Loss: 1.3864246606826782\n",
            "Epoch 10, Batch 536, G Loss: 0.6928825974464417, D Loss: 1.3862791061401367\n",
            "Epoch 10, Batch 537, G Loss: 0.6928393244743347, D Loss: 1.3867299556732178\n",
            "Epoch 10, Batch 538, G Loss: 0.6927787661552429, D Loss: 1.3868820667266846\n",
            "Epoch 10, Batch 539, G Loss: 0.6927177309989929, D Loss: 1.386730670928955\n",
            "Epoch 10, Batch 540, G Loss: 0.6926327347755432, D Loss: 1.386757254600525\n",
            "Epoch 10, Batch 541, G Loss: 0.6925605535507202, D Loss: 1.3867371082305908\n",
            "Epoch 10, Batch 542, G Loss: 0.6924974918365479, D Loss: 1.3866569995880127\n",
            "Epoch 10, Batch 543, G Loss: 0.6924424171447754, D Loss: 1.3866798877716064\n",
            "Epoch 10, Batch 544, G Loss: 0.6924023628234863, D Loss: 1.386681318283081\n",
            "Epoch 10, Batch 545, G Loss: 0.692384660243988, D Loss: 1.3866350650787354\n",
            "Epoch 10, Batch 546, G Loss: 0.6923729777336121, D Loss: 1.386595368385315\n",
            "Epoch 10, Batch 547, G Loss: 0.6923767328262329, D Loss: 1.3865907192230225\n",
            "Epoch 10, Batch 548, G Loss: 0.6924028396606445, D Loss: 1.3868000507354736\n",
            "Epoch 10, Batch 549, G Loss: 0.6924123167991638, D Loss: 1.3865246772766113\n",
            "Epoch 10, Batch 550, G Loss: 0.6924321055412292, D Loss: 1.386327862739563\n",
            "Epoch 10, Batch 551, G Loss: 0.6924964189529419, D Loss: 1.3863563537597656\n",
            "Epoch 10, Batch 552, G Loss: 0.6925737857818604, D Loss: 1.3863561153411865\n",
            "Epoch 10, Batch 553, G Loss: 0.6926708817481995, D Loss: 1.3865422010421753\n",
            "Epoch 10, Batch 554, G Loss: 0.6927566528320312, D Loss: 1.3865569829940796\n",
            "Epoch 10, Batch 555, G Loss: 0.692824125289917, D Loss: 1.386570930480957\n",
            "Epoch 10, Batch 556, G Loss: 0.6928588151931763, D Loss: 1.3863945007324219\n",
            "Epoch 10, Batch 557, G Loss: 0.6928964853286743, D Loss: 1.3863509893417358\n",
            "Epoch 10, Batch 558, G Loss: 0.6929295659065247, D Loss: 1.386437177658081\n",
            "Epoch 10, Batch 559, G Loss: 0.6929503679275513, D Loss: 1.3863768577575684\n",
            "Epoch 10, Batch 560, G Loss: 0.6929641962051392, D Loss: 1.38637375831604\n",
            "Epoch 10, Batch 561, G Loss: 0.692969560623169, D Loss: 1.3863232135772705\n",
            "Epoch 10, Batch 562, G Loss: 0.6929832696914673, D Loss: 1.3862969875335693\n",
            "Epoch 10, Batch 563, G Loss: 0.6930081844329834, D Loss: 1.3862972259521484\n",
            "Epoch 10, Batch 564, G Loss: 0.6930481195449829, D Loss: 1.3863015174865723\n",
            "Epoch 10, Batch 565, G Loss: 0.6931079030036926, D Loss: 1.3863314390182495\n",
            "Epoch 10, Batch 566, G Loss: 0.6931409239768982, D Loss: 1.386311650276184\n",
            "Epoch 10, Batch 567, G Loss: 0.6931122541427612, D Loss: 1.3862818479537964\n",
            "Epoch 10, Batch 568, G Loss: 0.6930509209632874, D Loss: 1.3862335681915283\n",
            "Epoch 10, Batch 569, G Loss: 0.6929613947868347, D Loss: 1.3862836360931396\n",
            "Epoch 10, Batch 570, G Loss: 0.6928801536560059, D Loss: 1.3862509727478027\n",
            "Epoch 10, Batch 571, G Loss: 0.6928091049194336, D Loss: 1.3862732648849487\n",
            "Epoch 10, Batch 572, G Loss: 0.6927533745765686, D Loss: 1.386005163192749\n",
            "Epoch 10, Batch 573, G Loss: 0.6926791667938232, D Loss: 1.3860175609588623\n",
            "Epoch 10, Batch 574, G Loss: 0.6926051378250122, D Loss: 1.3858474493026733\n",
            "Epoch 10, Batch 575, G Loss: 0.6925214529037476, D Loss: 1.3858510255813599\n",
            "Epoch 10, Batch 576, G Loss: 0.6924402713775635, D Loss: 1.3860561847686768\n",
            "Epoch 10, Batch 577, G Loss: 0.6923904418945312, D Loss: 1.3860111236572266\n",
            "Epoch 10, Batch 578, G Loss: 0.6923769116401672, D Loss: 1.3861677646636963\n",
            "Epoch 10, Batch 579, G Loss: 0.6923952698707581, D Loss: 1.3862414360046387\n",
            "Epoch 10, Batch 580, G Loss: 0.6924497485160828, D Loss: 1.3856377601623535\n",
            "Epoch 10, Batch 581, G Loss: 0.6925157904624939, D Loss: 1.3857027292251587\n",
            "Epoch 10, Batch 582, G Loss: 0.6925948858261108, D Loss: 1.3856018781661987\n",
            "Epoch 10, Batch 583, G Loss: 0.6926907300949097, D Loss: 1.385516881942749\n",
            "Epoch 10, Batch 584, G Loss: 0.6927810907363892, D Loss: 1.3858895301818848\n",
            "Epoch 10, Batch 585, G Loss: 0.6928676962852478, D Loss: 1.385979413986206\n",
            "Epoch 10, Batch 586, G Loss: 0.692976176738739, D Loss: 1.3861286640167236\n",
            "Epoch 10, Batch 587, G Loss: 0.6930699944496155, D Loss: 1.3863445520401\n",
            "Epoch 10, Batch 588, G Loss: 0.6931894421577454, D Loss: 1.3863649368286133\n",
            "Epoch 10, Batch 589, G Loss: 0.6932716965675354, D Loss: 1.386501431465149\n",
            "Epoch 10, Batch 590, G Loss: 0.6933783888816833, D Loss: 1.3856092691421509\n",
            "Epoch 10, Batch 591, G Loss: 0.6934563517570496, D Loss: 1.3855493068695068\n",
            "Epoch 10, Batch 592, G Loss: 0.6935165524482727, D Loss: 1.3857492208480835\n",
            "Epoch 10, Batch 593, G Loss: 0.6935485601425171, D Loss: 1.3860665559768677\n",
            "Epoch 10, Batch 594, G Loss: 0.6935731768608093, D Loss: 1.3861750364303589\n",
            "Epoch 10, Batch 595, G Loss: 0.6935790181159973, D Loss: 1.3866314888000488\n",
            "Epoch 10, Batch 596, G Loss: 0.6935651898384094, D Loss: 1.3856964111328125\n",
            "Epoch 10, Batch 597, G Loss: 0.693559467792511, D Loss: 1.3856701850891113\n",
            "Epoch 10, Batch 598, G Loss: 0.6935267448425293, D Loss: 1.385970950126648\n",
            "Epoch 10, Batch 599, G Loss: 0.6935093402862549, D Loss: 1.3861207962036133\n",
            "Epoch 10, Batch 600, G Loss: 0.6934705972671509, D Loss: 1.3864190578460693\n",
            "Epoch 10, Batch 601, G Loss: 0.6934399008750916, D Loss: 1.386314868927002\n",
            "Epoch 10, Batch 602, G Loss: 0.693412721157074, D Loss: 1.3865735530853271\n",
            "Epoch 10, Batch 603, G Loss: 0.6933807134628296, D Loss: 1.3860795497894287\n",
            "Epoch 10, Batch 604, G Loss: 0.693347692489624, D Loss: 1.3852816820144653\n",
            "Epoch 10, Batch 605, G Loss: 0.6933245062828064, D Loss: 1.3862273693084717\n",
            "Epoch 10, Batch 606, G Loss: 0.6933115124702454, D Loss: 1.3861103057861328\n",
            "Epoch 10, Batch 607, G Loss: 0.693305253982544, D Loss: 1.3860852718353271\n",
            "Epoch 10, Batch 608, G Loss: 0.6932967305183411, D Loss: 1.385826826095581\n",
            "Epoch 10, Batch 609, G Loss: 0.6933048963546753, D Loss: 1.3863359689712524\n",
            "Epoch 10, Batch 610, G Loss: 0.6933057904243469, D Loss: 1.3866298198699951\n",
            "Epoch 10, Batch 611, G Loss: 0.6933484673500061, D Loss: 1.3858975172042847\n",
            "Epoch 10, Batch 612, G Loss: 0.6933571696281433, D Loss: 1.3854093551635742\n",
            "Epoch 10, Batch 613, G Loss: 0.6933756470680237, D Loss: 1.3854583501815796\n",
            "Epoch 10, Batch 614, G Loss: 0.6933751702308655, D Loss: 1.3857240676879883\n",
            "Epoch 10, Batch 615, G Loss: 0.6934038996696472, D Loss: 1.3858832120895386\n",
            "Epoch 10, Batch 616, G Loss: 0.6934117674827576, D Loss: 1.38582181930542\n",
            "Epoch 10, Batch 617, G Loss: 0.6934189796447754, D Loss: 1.3856465816497803\n",
            "Epoch 10, Batch 618, G Loss: 0.6934524774551392, D Loss: 1.38615083694458\n",
            "Epoch 10, Batch 619, G Loss: 0.6934523582458496, D Loss: 1.3858747482299805\n",
            "Epoch 10, Batch 620, G Loss: 0.6934581398963928, D Loss: 1.3858866691589355\n",
            "Epoch 10, Batch 621, G Loss: 0.693483293056488, D Loss: 1.386577844619751\n",
            "Epoch 10, Batch 622, G Loss: 0.6934720277786255, D Loss: 1.38759183883667\n",
            "Epoch 10, Batch 623, G Loss: 0.693500280380249, D Loss: 1.3866815567016602\n",
            "Epoch 10, Batch 624, G Loss: 0.6935068368911743, D Loss: 1.3870866298675537\n",
            "Epoch 10, Batch 625, G Loss: 0.6935023069381714, D Loss: 1.386193037033081\n",
            "Epoch 10, Batch 626, G Loss: 0.6935242414474487, D Loss: 1.3863909244537354\n",
            "Epoch 10, Batch 627, G Loss: 0.6934961676597595, D Loss: 1.3862073421478271\n",
            "Epoch 10, Batch 628, G Loss: 0.6935153007507324, D Loss: 1.3861801624298096\n",
            "Epoch 10, Batch 629, G Loss: 0.693497896194458, D Loss: 1.3864327669143677\n",
            "Epoch 10, Batch 630, G Loss: 0.6934844255447388, D Loss: 1.3869104385375977\n",
            "Epoch 10, Batch 631, G Loss: 0.6934780478477478, D Loss: 1.3869268894195557\n",
            "Epoch 10, Batch 632, G Loss: 0.6934898495674133, D Loss: 1.385854721069336\n",
            "Epoch 10, Batch 633, G Loss: 0.6934714913368225, D Loss: 1.385708212852478\n",
            "Epoch 10, Batch 634, G Loss: 0.6934596300125122, D Loss: 1.385892391204834\n",
            "Epoch 10, Batch 635, G Loss: 0.6934404969215393, D Loss: 1.3863085508346558\n",
            "Epoch 10, Batch 636, G Loss: 0.6934417486190796, D Loss: 1.3868423700332642\n",
            "Epoch 10, Batch 637, G Loss: 0.69342041015625, D Loss: 1.3867785930633545\n",
            "Epoch 10, Batch 638, G Loss: 0.6934082508087158, D Loss: 1.3871628046035767\n",
            "Epoch 10, Batch 639, G Loss: 0.6934236884117126, D Loss: 1.38657546043396\n",
            "Epoch 10, Batch 640, G Loss: 0.6933972835540771, D Loss: 1.3870418071746826\n",
            "Epoch 10, Batch 641, G Loss: 0.693419337272644, D Loss: 1.3868944644927979\n",
            "Epoch 10, Batch 642, G Loss: 0.6934322714805603, D Loss: 1.3868677616119385\n",
            "Epoch 10, Batch 643, G Loss: 0.6934416890144348, D Loss: 1.3868107795715332\n",
            "Epoch 10, Batch 644, G Loss: 0.6934589743614197, D Loss: 1.3864476680755615\n",
            "Epoch 10, Batch 645, G Loss: 0.6934480667114258, D Loss: 1.3869091272354126\n",
            "Epoch 10, Batch 646, G Loss: 0.6934862732887268, D Loss: 1.3864566087722778\n",
            "Epoch 10, Batch 647, G Loss: 0.6934813857078552, D Loss: 1.3864320516586304\n",
            "Epoch 10, Batch 648, G Loss: 0.6934852004051208, D Loss: 1.3865910768508911\n",
            "Epoch 10, Batch 649, G Loss: 0.6934714317321777, D Loss: 1.3870573043823242\n",
            "Epoch 10, Batch 650, G Loss: 0.6934820413589478, D Loss: 1.3864802122116089\n",
            "Epoch 10, Batch 651, G Loss: 0.6934723854064941, D Loss: 1.386790156364441\n",
            "Epoch 10, Batch 652, G Loss: 0.6934669613838196, D Loss: 1.387373447418213\n",
            "Epoch 10, Batch 653, G Loss: 0.6934802532196045, D Loss: 1.387500524520874\n",
            "Epoch 10, Batch 654, G Loss: 0.693511426448822, D Loss: 1.387042760848999\n",
            "Epoch 10, Batch 655, G Loss: 0.6935460567474365, D Loss: 1.3864619731903076\n",
            "Epoch 10, Batch 656, G Loss: 0.6935663819313049, D Loss: 1.3864226341247559\n",
            "Epoch 10, Batch 657, G Loss: 0.6935672163963318, D Loss: 1.3862552642822266\n",
            "Epoch 10, Batch 658, G Loss: 0.6935582160949707, D Loss: 1.3862738609313965\n",
            "Epoch 10, Batch 659, G Loss: 0.6935330033302307, D Loss: 1.3864102363586426\n",
            "Epoch 10, Batch 660, G Loss: 0.6934968829154968, D Loss: 1.3864456415176392\n",
            "Epoch 10, Batch 661, G Loss: 0.6934583187103271, D Loss: 1.3864048719406128\n",
            "Epoch 10, Batch 662, G Loss: 0.6934258341789246, D Loss: 1.3865883350372314\n",
            "Epoch 10, Batch 663, G Loss: 0.6933923959732056, D Loss: 1.3865078687667847\n",
            "Epoch 10, Batch 664, G Loss: 0.6933649778366089, D Loss: 1.386636734008789\n",
            "Epoch 10, Batch 665, G Loss: 0.6933537721633911, D Loss: 1.386663556098938\n",
            "Epoch 10, Batch 666, G Loss: 0.693365216255188, D Loss: 1.386231541633606\n",
            "Epoch 10, Batch 667, G Loss: 0.6933583617210388, D Loss: 1.3861908912658691\n",
            "Epoch 10, Batch 668, G Loss: 0.6933286786079407, D Loss: 1.3863880634307861\n",
            "Epoch 10, Batch 669, G Loss: 0.6933051943778992, D Loss: 1.3863170146942139\n",
            "Epoch 10, Batch 670, G Loss: 0.6932798027992249, D Loss: 1.386364459991455\n",
            "Epoch 10, Batch 671, G Loss: 0.6932569146156311, D Loss: 1.3864495754241943\n",
            "Epoch 10, Batch 672, G Loss: 0.6932485103607178, D Loss: 1.386544942855835\n",
            "Epoch 10, Batch 673, G Loss: 0.6932776570320129, D Loss: 1.3864760398864746\n",
            "Epoch 10, Batch 674, G Loss: 0.6933322548866272, D Loss: 1.3864601850509644\n",
            "Epoch 10, Batch 675, G Loss: 0.6934090256690979, D Loss: 1.3863263130187988\n",
            "Epoch 10, Batch 676, G Loss: 0.693467915058136, D Loss: 1.3863351345062256\n",
            "Epoch 10, Batch 677, G Loss: 0.6935184001922607, D Loss: 1.3863309621810913\n",
            "Epoch 10, Batch 678, G Loss: 0.6935628652572632, D Loss: 1.3863153457641602\n",
            "Epoch 10, Batch 679, G Loss: 0.6935999393463135, D Loss: 1.3863039016723633\n",
            "Epoch 10, Batch 680, G Loss: 0.6936264038085938, D Loss: 1.386292815208435\n",
            "Epoch 10, Batch 681, G Loss: 0.6936366558074951, D Loss: 1.386256217956543\n",
            "Epoch 10, Batch 682, G Loss: 0.6936486959457397, D Loss: 1.3862661123275757\n",
            "Epoch 10, Batch 683, G Loss: 0.6936488151550293, D Loss: 1.3862398862838745\n",
            "Epoch 10, Batch 684, G Loss: 0.6936419606208801, D Loss: 1.386223554611206\n",
            "Epoch 10, Batch 685, G Loss: 0.6936295032501221, D Loss: 1.3861584663391113\n",
            "Epoch 10, Batch 686, G Loss: 0.6936233043670654, D Loss: 1.3861134052276611\n",
            "Epoch 10, Batch 687, G Loss: 0.693632185459137, D Loss: 1.3860516548156738\n",
            "Epoch 10, Batch 688, G Loss: 0.6936561465263367, D Loss: 1.3859659433364868\n",
            "Epoch 10, Batch 689, G Loss: 0.6937042474746704, D Loss: 1.3861409425735474\n",
            "Epoch 10, Batch 690, G Loss: 0.6937394142150879, D Loss: 1.385981798171997\n",
            "Epoch 10, Batch 691, G Loss: 0.6937790513038635, D Loss: 1.3860373497009277\n",
            "Epoch 10, Batch 692, G Loss: 0.6938102841377258, D Loss: 1.386103868484497\n",
            "Epoch 10, Batch 693, G Loss: 0.6938285231590271, D Loss: 1.3861050605773926\n",
            "Epoch 10, Batch 694, G Loss: 0.6938267350196838, D Loss: 1.3858853578567505\n",
            "Epoch 10, Batch 695, G Loss: 0.6938281655311584, D Loss: 1.3860983848571777\n",
            "Epoch 10, Batch 696, G Loss: 0.6938062310218811, D Loss: 1.3859398365020752\n",
            "Epoch 10, Batch 697, G Loss: 0.6937828660011292, D Loss: 1.3859543800354004\n",
            "Epoch 10, Batch 698, G Loss: 0.6937707662582397, D Loss: 1.3858959674835205\n",
            "Epoch 10, Batch 699, G Loss: 0.6937460899353027, D Loss: 1.3857839107513428\n",
            "Epoch 10, Batch 700, G Loss: 0.6937289834022522, D Loss: 1.385819435119629\n",
            "Epoch 10, Batch 701, G Loss: 0.6937294006347656, D Loss: 1.3856327533721924\n",
            "Epoch 10, Batch 702, G Loss: 0.6937414407730103, D Loss: 1.3858938217163086\n",
            "Epoch 10, Batch 703, G Loss: 0.6937588453292847, D Loss: 1.386107087135315\n",
            "Epoch 10, Batch 704, G Loss: 0.6937385201454163, D Loss: 1.3859684467315674\n",
            "Epoch 10, Batch 705, G Loss: 0.6937279105186462, D Loss: 1.3861098289489746\n",
            "Epoch 10, Batch 706, G Loss: 0.69368976354599, D Loss: 1.3859844207763672\n",
            "Epoch 10, Batch 707, G Loss: 0.6936460137367249, D Loss: 1.3859702348709106\n",
            "Epoch 10, Batch 708, G Loss: 0.6936054825782776, D Loss: 1.3860788345336914\n",
            "Epoch 10, Batch 709, G Loss: 0.6935498118400574, D Loss: 1.3858203887939453\n",
            "Epoch 10, Batch 710, G Loss: 0.6935242414474487, D Loss: 1.3857264518737793\n",
            "Epoch 10, Batch 711, G Loss: 0.6934959292411804, D Loss: 1.3853569030761719\n",
            "Epoch 10, Batch 712, G Loss: 0.6935238838195801, D Loss: 1.3859837055206299\n",
            "Epoch 10, Batch 713, G Loss: 0.6935600638389587, D Loss: 1.385746717453003\n",
            "Epoch 10, Batch 714, G Loss: 0.6935821771621704, D Loss: 1.3858381509780884\n",
            "Epoch 10, Batch 715, G Loss: 0.6936325430870056, D Loss: 1.3860185146331787\n",
            "Epoch 10, Batch 716, G Loss: 0.6936594247817993, D Loss: 1.3859031200408936\n",
            "Epoch 10, Batch 717, G Loss: 0.6936812996864319, D Loss: 1.3860095739364624\n",
            "Epoch 10, Batch 718, G Loss: 0.6936792135238647, D Loss: 1.3854200839996338\n",
            "Epoch 10, Batch 719, G Loss: 0.693719208240509, D Loss: 1.3849413394927979\n",
            "Epoch 10, Batch 720, G Loss: 0.693781852722168, D Loss: 1.385300874710083\n",
            "Epoch 10, Batch 721, G Loss: 0.6938881874084473, D Loss: 1.385232925415039\n",
            "Epoch 10, Batch 722, G Loss: 0.694000780582428, D Loss: 1.3856217861175537\n",
            "Epoch 10, Batch 723, G Loss: 0.6940842270851135, D Loss: 1.3858472108840942\n",
            "Epoch 10, Batch 724, G Loss: 0.6941562294960022, D Loss: 1.3857460021972656\n",
            "Epoch 10, Batch 725, G Loss: 0.6941872239112854, D Loss: 1.3864846229553223\n",
            "Epoch 10, Batch 726, G Loss: 0.6941596865653992, D Loss: 1.38605797290802\n",
            "Epoch 10, Batch 727, G Loss: 0.6940760612487793, D Loss: 1.386038064956665\n",
            "Epoch 10, Batch 728, G Loss: 0.6939612627029419, D Loss: 1.3861840963363647\n",
            "Epoch 10, Batch 729, G Loss: 0.693828821182251, D Loss: 1.3864288330078125\n",
            "Epoch 10, Batch 730, G Loss: 0.6937041282653809, D Loss: 1.3870141506195068\n",
            "Epoch 10, Batch 731, G Loss: 0.693487823009491, D Loss: 1.3866064548492432\n",
            "Epoch 10, Batch 732, G Loss: 0.6932522654533386, D Loss: 1.3869355916976929\n",
            "Epoch 10, Batch 733, G Loss: 0.6930487751960754, D Loss: 1.386786937713623\n",
            "Epoch 10, Batch 734, G Loss: 0.6928176283836365, D Loss: 1.3865537643432617\n",
            "Epoch 10, Batch 735, G Loss: 0.6926249265670776, D Loss: 1.3864203691482544\n",
            "Epoch 10, Batch 736, G Loss: 0.6924787163734436, D Loss: 1.3864977359771729\n",
            "Epoch 10, Batch 737, G Loss: 0.6923737525939941, D Loss: 1.386580228805542\n",
            "Epoch 10, Batch 738, G Loss: 0.6923431754112244, D Loss: 1.385746955871582\n",
            "Epoch 10, Batch 739, G Loss: 0.6923703551292419, D Loss: 1.3856632709503174\n",
            "Epoch 10, Batch 740, G Loss: 0.6924962997436523, D Loss: 1.3855788707733154\n",
            "Epoch 10, Batch 741, G Loss: 0.6926664113998413, D Loss: 1.3869297504425049\n",
            "Epoch 10, Batch 742, G Loss: 0.692847728729248, D Loss: 1.3868625164031982\n",
            "Epoch 10, Batch 743, G Loss: 0.6929817795753479, D Loss: 1.38667631149292\n",
            "Epoch 10, Batch 744, G Loss: 0.6930861473083496, D Loss: 1.3876290321350098\n",
            "Epoch 10, Batch 745, G Loss: 0.6931527853012085, D Loss: 1.3878381252288818\n",
            "Epoch 10, Batch 746, G Loss: 0.6931077837944031, D Loss: 1.3873040676116943\n",
            "Epoch 10, Batch 747, G Loss: 0.69301837682724, D Loss: 1.3869960308074951\n",
            "Epoch 10, Batch 748, G Loss: 0.692897379398346, D Loss: 1.3870726823806763\n",
            "Epoch 10, Batch 749, G Loss: 0.6927717328071594, D Loss: 1.387037754058838\n",
            "Epoch 10, Batch 750, G Loss: 0.6926558613777161, D Loss: 1.3865089416503906\n",
            "Epoch 10, Batch 751, G Loss: 0.6925532817840576, D Loss: 1.387561321258545\n",
            "Epoch 10, Batch 752, G Loss: 0.6924211382865906, D Loss: 1.3869268894195557\n",
            "Epoch 10, Batch 753, G Loss: 0.6923237442970276, D Loss: 1.3874636888504028\n",
            "Epoch 10, Batch 754, G Loss: 0.6922134757041931, D Loss: 1.3873014450073242\n",
            "Epoch 10, Batch 755, G Loss: 0.6920871138572693, D Loss: 1.3872613906860352\n",
            "Epoch 10, Batch 756, G Loss: 0.6919979453086853, D Loss: 1.3870965242385864\n",
            "Epoch 10, Batch 757, G Loss: 0.6919289231300354, D Loss: 1.3870655298233032\n",
            "Epoch 10, Batch 758, G Loss: 0.6918644309043884, D Loss: 1.386832356452942\n",
            "Epoch 10, Batch 759, G Loss: 0.6918646097183228, D Loss: 1.3870165348052979\n",
            "Epoch 10, Batch 760, G Loss: 0.6918787956237793, D Loss: 1.3866957426071167\n",
            "Epoch 10, Batch 761, G Loss: 0.6919357776641846, D Loss: 1.3867746591567993\n",
            "Epoch 10, Batch 762, G Loss: 0.6920117735862732, D Loss: 1.386824607849121\n",
            "Epoch 10, Batch 763, G Loss: 0.6920925974845886, D Loss: 1.386760950088501\n",
            "Epoch 10, Batch 764, G Loss: 0.6921936273574829, D Loss: 1.3866113424301147\n",
            "Epoch 10, Batch 765, G Loss: 0.6923136711120605, D Loss: 1.3861048221588135\n",
            "Epoch 10, Batch 766, G Loss: 0.6925077438354492, D Loss: 1.3861751556396484\n",
            "Epoch 10, Batch 767, G Loss: 0.6927111148834229, D Loss: 1.3863239288330078\n",
            "Epoch 10, Batch 768, G Loss: 0.6929168701171875, D Loss: 1.3865180015563965\n",
            "Epoch 10, Batch 769, G Loss: 0.6930928230285645, D Loss: 1.386545181274414\n",
            "Epoch 10, Batch 770, G Loss: 0.6932176947593689, D Loss: 1.3864713907241821\n",
            "Epoch 10, Batch 771, G Loss: 0.6933158040046692, D Loss: 1.3863966464996338\n",
            "Epoch 10, Batch 772, G Loss: 0.693378746509552, D Loss: 1.386387825012207\n",
            "Epoch 10, Batch 773, G Loss: 0.6934044361114502, D Loss: 1.3864277601242065\n",
            "Epoch 10, Batch 774, G Loss: 0.6933939456939697, D Loss: 1.3863835334777832\n",
            "Epoch 10, Batch 775, G Loss: 0.693356990814209, D Loss: 1.3863604068756104\n",
            "Epoch 10, Batch 776, G Loss: 0.6933008432388306, D Loss: 1.3863526582717896\n",
            "Epoch 10, Batch 777, G Loss: 0.69322669506073, D Loss: 1.386305809020996\n",
            "Epoch 10, Batch 778, G Loss: 0.6931584477424622, D Loss: 1.386357069015503\n",
            "Epoch 10, Batch 779, G Loss: 0.6930684447288513, D Loss: 1.386317491531372\n",
            "Epoch 10, Batch 780, G Loss: 0.6929868459701538, D Loss: 1.3863155841827393\n",
            "Epoch 10, Batch 781, G Loss: 0.6929196715354919, D Loss: 1.3863213062286377\n",
            "Epoch 10, Batch 782, G Loss: 0.6928807497024536, D Loss: 1.3863123655319214\n",
            "Epoch 10, Batch 783, G Loss: 0.6928578615188599, D Loss: 1.3863046169281006\n",
            "Epoch 10, Batch 784, G Loss: 0.6928464770317078, D Loss: 1.3862826824188232\n",
            "Epoch 10, Batch 785, G Loss: 0.6928390264511108, D Loss: 1.3862919807434082\n",
            "Epoch 10, Batch 786, G Loss: 0.692844569683075, D Loss: 1.3862861394882202\n",
            "Epoch 10, Batch 787, G Loss: 0.6928591132164001, D Loss: 1.3862690925598145\n",
            "Epoch 10, Batch 788, G Loss: 0.6928797364234924, D Loss: 1.3862838745117188\n",
            "Epoch 10, Batch 789, G Loss: 0.6929078102111816, D Loss: 1.3862706422805786\n",
            "Epoch 10, Batch 790, G Loss: 0.6929412484169006, D Loss: 1.3862284421920776\n",
            "Epoch 10, Batch 791, G Loss: 0.692970335483551, D Loss: 1.3862849473953247\n",
            "Epoch 10, Batch 792, G Loss: 0.6930040121078491, D Loss: 1.386293888092041\n",
            "Epoch 10, Batch 793, G Loss: 0.6930400133132935, D Loss: 1.386223316192627\n",
            "Epoch 10, Batch 794, G Loss: 0.6930682063102722, D Loss: 1.3861291408538818\n",
            "Epoch 10, Batch 795, G Loss: 0.69307941198349, D Loss: 1.3862247467041016\n",
            "Epoch 10, Batch 796, G Loss: 0.6930859684944153, D Loss: 1.3861544132232666\n",
            "Epoch 10, Batch 797, G Loss: 0.6930789351463318, D Loss: 1.3862338066101074\n",
            "Epoch 10, Batch 798, G Loss: 0.6930748820304871, D Loss: 1.3862553834915161\n",
            "Epoch 10, Batch 799, G Loss: 0.6930725574493408, D Loss: 1.3861141204833984\n",
            "Epoch 10, Batch 800, G Loss: 0.6930618286132812, D Loss: 1.3860125541687012\n",
            "Epoch 10, Batch 801, G Loss: 0.6930370926856995, D Loss: 1.3858985900878906\n",
            "Epoch 10, Batch 802, G Loss: 0.6929990649223328, D Loss: 1.3860068321228027\n",
            "Epoch 10, Batch 803, G Loss: 0.6929616928100586, D Loss: 1.3860344886779785\n",
            "Epoch 10, Batch 804, G Loss: 0.6929227113723755, D Loss: 1.3861148357391357\n",
            "Epoch 10, Batch 805, G Loss: 0.6928924322128296, D Loss: 1.3859279155731201\n",
            "Epoch 10, Batch 806, G Loss: 0.6928715705871582, D Loss: 1.386225700378418\n",
            "Epoch 10, Batch 807, G Loss: 0.6928645372390747, D Loss: 1.3859636783599854\n",
            "Epoch 10, Batch 808, G Loss: 0.6928601861000061, D Loss: 1.3862404823303223\n",
            "Epoch 10, Batch 809, G Loss: 0.6928789019584656, D Loss: 1.386326789855957\n",
            "Epoch 10, Batch 810, G Loss: 0.6929163932800293, D Loss: 1.3861849308013916\n",
            "Epoch 10, Batch 811, G Loss: 0.6929702162742615, D Loss: 1.3863787651062012\n",
            "Epoch 10, Batch 812, G Loss: 0.6930288076400757, D Loss: 1.3860998153686523\n",
            "Epoch 10, Batch 813, G Loss: 0.693083643913269, D Loss: 1.3862794637680054\n",
            "Epoch 10, Batch 814, G Loss: 0.6931495070457458, D Loss: 1.3866214752197266\n",
            "Epoch 10, Batch 815, G Loss: 0.6932200193405151, D Loss: 1.3863043785095215\n",
            "Epoch 10, Batch 816, G Loss: 0.6932812333106995, D Loss: 1.3864758014678955\n",
            "Epoch 10, Batch 817, G Loss: 0.6933436989784241, D Loss: 1.3865761756896973\n",
            "Epoch 10, Batch 818, G Loss: 0.6933995485305786, D Loss: 1.3862258195877075\n",
            "Epoch 10, Batch 819, G Loss: 0.6934447288513184, D Loss: 1.3863592147827148\n",
            "Epoch 10, Batch 820, G Loss: 0.6934713125228882, D Loss: 1.3856167793273926\n",
            "Epoch 10, Batch 821, G Loss: 0.6934727430343628, D Loss: 1.3860397338867188\n",
            "Epoch 10, Batch 822, G Loss: 0.693452775478363, D Loss: 1.3860613107681274\n",
            "Epoch 10, Batch 823, G Loss: 0.6934342384338379, D Loss: 1.3862617015838623\n",
            "Epoch 10, Batch 824, G Loss: 0.6934025287628174, D Loss: 1.3863869905471802\n",
            "Epoch 10, Batch 825, G Loss: 0.6933642029762268, D Loss: 1.386508584022522\n",
            "Epoch 10, Batch 826, G Loss: 0.6933189034461975, D Loss: 1.3862097263336182\n",
            "Epoch 10, Batch 827, G Loss: 0.6932996511459351, D Loss: 1.3860584497451782\n",
            "Epoch 10, Batch 828, G Loss: 0.6932563781738281, D Loss: 1.3859295845031738\n",
            "Epoch 10, Batch 829, G Loss: 0.6932214498519897, D Loss: 1.3858108520507812\n",
            "Epoch 10, Batch 830, G Loss: 0.6931777000427246, D Loss: 1.3857901096343994\n",
            "Epoch 10, Batch 831, G Loss: 0.693139910697937, D Loss: 1.3863099813461304\n",
            "Epoch 10, Batch 832, G Loss: 0.6931060552597046, D Loss: 1.3860118389129639\n",
            "Epoch 10, Batch 833, G Loss: 0.693088948726654, D Loss: 1.3861771821975708\n",
            "Epoch 10, Batch 834, G Loss: 0.6930776834487915, D Loss: 1.3863990306854248\n",
            "Epoch 10, Batch 835, G Loss: 0.6930875182151794, D Loss: 1.3862019777297974\n",
            "Epoch 10, Batch 836, G Loss: 0.6930829882621765, D Loss: 1.3864061832427979\n",
            "Epoch 10, Batch 837, G Loss: 0.6931136846542358, D Loss: 1.3863368034362793\n",
            "Epoch 10, Batch 838, G Loss: 0.6931536197662354, D Loss: 1.3866357803344727\n",
            "Epoch 10, Batch 839, G Loss: 0.6931968331336975, D Loss: 1.3863177299499512\n",
            "Epoch 10, Batch 840, G Loss: 0.6932298541069031, D Loss: 1.3864150047302246\n",
            "Epoch 10, Batch 841, G Loss: 0.6932821273803711, D Loss: 1.3867180347442627\n",
            "Epoch 10, Batch 842, G Loss: 0.6933323740959167, D Loss: 1.3863935470581055\n",
            "Epoch 10, Batch 843, G Loss: 0.6933599710464478, D Loss: 1.38607919216156\n",
            "Epoch 10, Batch 844, G Loss: 0.6933989524841309, D Loss: 1.3860292434692383\n",
            "Epoch 10, Batch 845, G Loss: 0.6934051513671875, D Loss: 1.386240839958191\n",
            "Epoch 10, Batch 846, G Loss: 0.693414568901062, D Loss: 1.3862056732177734\n",
            "Epoch 10, Batch 847, G Loss: 0.6934168338775635, D Loss: 1.386364459991455\n",
            "Epoch 10, Batch 848, G Loss: 0.6934149861335754, D Loss: 1.3866705894470215\n",
            "Epoch 10, Batch 849, G Loss: 0.6934024095535278, D Loss: 1.3866586685180664\n",
            "Epoch 10, Batch 850, G Loss: 0.693396270275116, D Loss: 1.385988712310791\n",
            "Epoch 10, Batch 851, G Loss: 0.6933749318122864, D Loss: 1.3861795663833618\n",
            "Epoch 10, Batch 852, G Loss: 0.6933643817901611, D Loss: 1.3858925104141235\n",
            "Epoch 10, Batch 853, G Loss: 0.6933231949806213, D Loss: 1.3860268592834473\n",
            "Epoch 10, Batch 854, G Loss: 0.6932805776596069, D Loss: 1.3860458135604858\n",
            "Epoch 10, Batch 855, G Loss: 0.6932396292686462, D Loss: 1.3861050605773926\n",
            "Epoch 10, Batch 856, G Loss: 0.6932029128074646, D Loss: 1.3861069679260254\n",
            "Epoch 10, Batch 857, G Loss: 0.6931655406951904, D Loss: 1.3860933780670166\n",
            "Epoch 10, Batch 858, G Loss: 0.6931437253952026, D Loss: 1.3865097761154175\n",
            "Epoch 10, Batch 859, G Loss: 0.6931291818618774, D Loss: 1.386304497718811\n",
            "Epoch 10, Batch 860, G Loss: 0.6931208372116089, D Loss: 1.3860857486724854\n",
            "Epoch 10, Batch 861, G Loss: 0.6931228637695312, D Loss: 1.3863359689712524\n",
            "Epoch 10, Batch 862, G Loss: 0.6931236386299133, D Loss: 1.3862042427062988\n",
            "Epoch 10, Batch 863, G Loss: 0.6931337714195251, D Loss: 1.3862706422805786\n",
            "Epoch 10, Batch 864, G Loss: 0.6931660175323486, D Loss: 1.3862040042877197\n",
            "Epoch 10, Batch 865, G Loss: 0.6931846737861633, D Loss: 1.3864960670471191\n",
            "Epoch 10, Batch 866, G Loss: 0.6932101249694824, D Loss: 1.3864948749542236\n",
            "Epoch 10, Batch 867, G Loss: 0.6932522654533386, D Loss: 1.3864033222198486\n",
            "Epoch 10, Batch 868, G Loss: 0.6932787895202637, D Loss: 1.3860063552856445\n",
            "Epoch 10, Batch 869, G Loss: 0.6933057308197021, D Loss: 1.386170744895935\n",
            "Epoch 10, Batch 870, G Loss: 0.6933216452598572, D Loss: 1.3856160640716553\n",
            "Epoch 10, Batch 871, G Loss: 0.6933164596557617, D Loss: 1.385791301727295\n",
            "Epoch 10, Batch 872, G Loss: 0.6933031678199768, D Loss: 1.3858131170272827\n",
            "Epoch 10, Batch 873, G Loss: 0.6932675242424011, D Loss: 1.386282205581665\n",
            "Epoch 10, Batch 874, G Loss: 0.6932477355003357, D Loss: 1.3863415718078613\n",
            "Epoch 10, Batch 875, G Loss: 0.6932212710380554, D Loss: 1.386326551437378\n",
            "Epoch 10, Batch 876, G Loss: 0.6932166814804077, D Loss: 1.3863799571990967\n",
            "Epoch 10, Batch 877, G Loss: 0.693199634552002, D Loss: 1.3864552974700928\n",
            "Epoch 10, Batch 878, G Loss: 0.693193793296814, D Loss: 1.386322021484375\n",
            "Epoch 10, Batch 879, G Loss: 0.6931999921798706, D Loss: 1.3862996101379395\n",
            "Epoch 10, Batch 880, G Loss: 0.6932052373886108, D Loss: 1.3867149353027344\n",
            "Epoch 10, Batch 881, G Loss: 0.693231999874115, D Loss: 1.3867864608764648\n",
            "Epoch 10, Batch 882, G Loss: 0.6932604312896729, D Loss: 1.3867980241775513\n",
            "Epoch 10, Batch 883, G Loss: 0.6933050155639648, D Loss: 1.3865821361541748\n",
            "Epoch 10, Batch 884, G Loss: 0.6933484673500061, D Loss: 1.3864524364471436\n",
            "Epoch 10, Batch 885, G Loss: 0.6933869123458862, D Loss: 1.3865681886672974\n",
            "Epoch 10, Batch 886, G Loss: 0.6934183835983276, D Loss: 1.3865671157836914\n",
            "Epoch 10, Batch 887, G Loss: 0.6934540271759033, D Loss: 1.3860154151916504\n",
            "Epoch 10, Batch 888, G Loss: 0.6934627890586853, D Loss: 1.3862509727478027\n",
            "Epoch 10, Batch 889, G Loss: 0.693456768989563, D Loss: 1.386277198791504\n",
            "Epoch 10, Batch 890, G Loss: 0.6934313178062439, D Loss: 1.3865464925765991\n",
            "Epoch 10, Batch 891, G Loss: 0.6934170126914978, D Loss: 1.3866831064224243\n",
            "Epoch 10, Batch 892, G Loss: 0.6934170126914978, D Loss: 1.386974573135376\n",
            "Epoch 10, Batch 893, G Loss: 0.693419337272644, D Loss: 1.3867073059082031\n",
            "Epoch 10, Batch 894, G Loss: 0.6934333443641663, D Loss: 1.386522650718689\n",
            "Epoch 10, Batch 895, G Loss: 0.6934385299682617, D Loss: 1.3863952159881592\n",
            "Epoch 10, Batch 896, G Loss: 0.6934375166893005, D Loss: 1.386447548866272\n",
            "Epoch 10, Batch 897, G Loss: 0.6934316158294678, D Loss: 1.3864094018936157\n",
            "Epoch 10, Batch 898, G Loss: 0.693419337272644, D Loss: 1.3864879608154297\n",
            "Epoch 10, Batch 899, G Loss: 0.6934124827384949, D Loss: 1.3863193988800049\n",
            "Epoch 10, Batch 900, G Loss: 0.6933886408805847, D Loss: 1.3863646984100342\n",
            "Epoch 10, Batch 901, G Loss: 0.6933608055114746, D Loss: 1.3864835500717163\n",
            "Epoch 10, Batch 902, G Loss: 0.6933423280715942, D Loss: 1.386592149734497\n",
            "Epoch 10, Batch 903, G Loss: 0.6933407783508301, D Loss: 1.3865687847137451\n",
            "Epoch 10, Batch 904, G Loss: 0.693357527256012, D Loss: 1.3865594863891602\n",
            "Epoch 10, Batch 905, G Loss: 0.6933900713920593, D Loss: 1.3864400386810303\n",
            "Epoch 10, Batch 906, G Loss: 0.6934238076210022, D Loss: 1.3864622116088867\n",
            "Epoch 10, Batch 907, G Loss: 0.6934619545936584, D Loss: 1.386350393295288\n",
            "Epoch 10, Batch 908, G Loss: 0.6934940814971924, D Loss: 1.386438012123108\n",
            "Epoch 10, Batch 909, G Loss: 0.6935338973999023, D Loss: 1.386413812637329\n",
            "Epoch 10, Batch 910, G Loss: 0.6935808658599854, D Loss: 1.3863309621810913\n",
            "Epoch 10, Batch 911, G Loss: 0.6936116218566895, D Loss: 1.3863146305084229\n",
            "Epoch 10, Batch 912, G Loss: 0.6936133503913879, D Loss: 1.386308193206787\n",
            "Epoch 10, Batch 913, G Loss: 0.6935954093933105, D Loss: 1.3863188028335571\n",
            "Epoch 10, Batch 914, G Loss: 0.693540632724762, D Loss: 1.3863213062286377\n",
            "Epoch 10, Batch 915, G Loss: 0.6934607625007629, D Loss: 1.3862472772598267\n",
            "Epoch 10, Batch 916, G Loss: 0.6934009194374084, D Loss: 1.3861737251281738\n",
            "Epoch 10, Batch 917, G Loss: 0.6933809518814087, D Loss: 1.3861348628997803\n",
            "Epoch 10, Batch 918, G Loss: 0.6934007406234741, D Loss: 1.386083722114563\n",
            "Epoch 10, Batch 919, G Loss: 0.6934619545936584, D Loss: 1.3861219882965088\n",
            "Epoch 10, Batch 920, G Loss: 0.6935392618179321, D Loss: 1.3863375186920166\n",
            "Epoch 10, Batch 921, G Loss: 0.6935788989067078, D Loss: 1.3862369060516357\n",
            "Epoch 10, Batch 922, G Loss: 0.6935985088348389, D Loss: 1.3862452507019043\n",
            "Epoch 10, Batch 923, G Loss: 0.6935996413230896, D Loss: 1.3861479759216309\n",
            "Epoch 10, Batch 924, G Loss: 0.6935943365097046, D Loss: 1.3861160278320312\n",
            "Epoch 10, Batch 925, G Loss: 0.6935887336730957, D Loss: 1.386177897453308\n",
            "Epoch 10, Batch 926, G Loss: 0.6935679912567139, D Loss: 1.3862419128417969\n",
            "Epoch 10, Batch 927, G Loss: 0.6935315132141113, D Loss: 1.3864963054656982\n",
            "Epoch 10, Batch 928, G Loss: 0.6934539079666138, D Loss: 1.387202262878418\n",
            "Epoch 10, Batch 929, G Loss: 0.69326251745224, D Loss: 1.3877153396606445\n",
            "Epoch 10, Batch 930, G Loss: 0.6929357051849365, D Loss: 1.3865954875946045\n",
            "Epoch 10, Batch 931, G Loss: 0.6926144957542419, D Loss: 1.3859589099884033\n",
            "Epoch 10, Batch 932, G Loss: 0.6923937201499939, D Loss: 1.3861719369888306\n",
            "Epoch 10, Batch 933, G Loss: 0.6922550797462463, D Loss: 1.3863599300384521\n",
            "Epoch 10, Batch 934, G Loss: 0.6921749711036682, D Loss: 1.3861894607543945\n",
            "Epoch 10, Batch 935, G Loss: 0.6921709179878235, D Loss: 1.3863725662231445\n",
            "Epoch 10, Batch 936, G Loss: 0.6922095417976379, D Loss: 1.3864645957946777\n",
            "Epoch 10, Batch 937, G Loss: 0.6922861337661743, D Loss: 1.385936975479126\n",
            "Epoch 10, Batch 938, G Loss: 0.6924450993537903, D Loss: 1.3859869241714478\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Set parameters\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "latent = 100  # Update as per your latent dimension\n",
        "\n",
        "# Train the GAN\n",
        "train_GAN(batch_size, epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "lMxzGmY1xryc",
        "outputId": "dc8bd79f-4941-45d2-a23d-cf595b483d5a"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-66de1daf09f8>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Test the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtest_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-26-66de1daf09f8>\u001b[0m in \u001b[0;36mtest_discriminator\u001b[0;34m(discriminator)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtest_images_resized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images_resized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrue_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'predict'"
          ]
        }
      ],
      "source": [
        "# Load and preprocess MNIST data\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = np.expand_dims(x_train, axis=-1).astype('float32') / 255.0\n",
        "x_test = np.expand_dims(x_test, axis=-1).astype('float32') / 255.0\n",
        "\n",
        "# Test the model on the test dataset\n",
        "def test_discriminator(discriminator):\n",
        "    test_images_resized = tf.image.resize(x_test, (height, width)).numpy()\n",
        "    predictions = discriminator.predict(test_images_resized)\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "    true_labels = y_test\n",
        "    accuracy = np.mean(predicted_labels == true_labels)\n",
        "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Train the GAN\n",
        "# train_GAN(100, 7)\n",
        "\n",
        "# Test the discriminator\n",
        "test_discriminator(discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DpeFL1i5BZ-",
        "outputId": "3b7da21e-e8fb-48eb-cfd1-b5998de45210"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "<ipython-input-1-890cf22b52bd>:92: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "  deconv1 = tf.layers.conv2d_transpose(z, 512, kernel_size = [4,4],\n",
            "<ipython-input-1-890cf22b52bd>:95: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  batch1 = tf.layers.batch_normalization(deconv1, training = is_training)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/keras/src/layers/normalization/batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "<ipython-input-1-890cf22b52bd>:100: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "  deconv2 = tf.layers.conv2d_transpose(dropout1, 256, kernel_size = [4,4],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Generator architecture: \n",
            "(?, 4, 4, 512)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-1-890cf22b52bd>:103: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  batch2 = tf.layers.batch_normalization(deconv2, training = is_training)\n",
            "<ipython-input-1-890cf22b52bd>:108: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "  deconv3 = tf.layers.conv2d_transpose(dropout2, 128, kernel_size = [4,4],\n",
            "<ipython-input-1-890cf22b52bd>:111: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "  batch3 = tf.layers.batch_normalization(deconv3, training = is_training)\n",
            "<ipython-input-1-890cf22b52bd>:116: UserWarning: `tf.layers.conv2d_transpose` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "  deconv4 = tf.layers.conv2d_transpose(dropout3, 1, kernel_size = [4,4],\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(?, 16, 16, 256)\n",
            "(?, 32, 32, 128)\n",
            "(?, 64, 64, 1)\n",
            "Discriminator architecture: \n",
            "(?, 32, 32, 128)\n",
            "(?, 16, 16, 256)\n",
            "(?, 4, 4, 512)\n",
            "(?, 2, 2, 1024)\n",
            "Discriminator architecture: \n",
            "(?, 32, 32, 128)\n",
            "(?, 16, 16, 256)\n",
            "(?, 4, 4, 512)\n",
            "(?, 2, 2, 1024)\n",
            "...Training begins...\n",
            "Batch evaluated: 1\n",
            "Batch evaluated: 2\n",
            "Batch evaluated: 3\n",
            "Batch evaluated: 4\n",
            "Batch evaluated: 5\n",
            "Batch evaluated: 6\n",
            "Batch evaluated: 7\n",
            "Batch evaluated: 8\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-890cf22b52bd>\u001b[0m in \u001b[0;36m<cell line: 393>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_D_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_G_losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 393\u001b[0;31m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_GAN\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m128\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-890cf22b52bd>\u001b[0m in \u001b[0;36mtrain_GAN\u001b[0;34m(batch_size, epochs)\u001b[0m\n\u001b[1;32m    359\u001b[0m           \u001b[0;31m#The label provided in dict are one hot encoded in 10 classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m           \u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m           \u001b[0mG_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1628\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m     \"\"\"\n\u001b[0;32m-> 1630\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m \u001b[0mgradient_registry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4532\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4533\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4534\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    973\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    974\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1216\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1217\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1396\u001b[0m                            run_metadata)\n\u001b[1;32m   1397\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1400\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1402\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1403\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1404\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1383\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1385\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1386\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1476\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1477\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1478\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1479\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m                                             run_metadata)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "############ Imports ############\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from PIL import Image\n",
        "import time\n",
        "\n",
        "############ Initializations ############\n",
        "\n",
        "num_classes = 10\n",
        "channels = 1\n",
        "height = 64\n",
        "width = 64\n",
        "# MNIST was resized to 64 * 64 for discriminator and generator architecture fitting\n",
        "latent = 100\n",
        "epsilon = 1e-7\n",
        "labeled_rate = 0.2 # For initial testing\n",
        "\n",
        "############ Importing MNIST data ############\n",
        "\n",
        "def get_data():\n",
        "    (train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "    # Expanding the dimensions to add a channel dimension and converting to float\n",
        "    train_images = np.expand_dims(train_images, axis=-1).astype('float32')\n",
        "    test_images = np.expand_dims(test_images, axis=-1).astype('float32')\n",
        "\n",
        "    # Converting labels to one-hot encoded format\n",
        "    train_labels = tf.keras.utils.to_categorical(train_labels, 10)\n",
        "    test_labels = tf.keras.utils.to_categorical(test_labels, 10)\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "\n",
        "                ############ Normalizing data ############\n",
        "# Scaling in range (-1,1) for generator tanh output\n",
        "def scale(x):\n",
        "    # normalize data\n",
        "    x = (x - 0.5) / 0.5\n",
        "    return x\n",
        "\n",
        "\"\"\"Discriminator and Generator architecture should mirror each other\"\"\"\n",
        "\n",
        "############ Defining Discriminator ############\n",
        "\n",
        "def discriminator(x, dropout_rate = 0., is_training = True, reuse = False):\n",
        "    with tf.variable_scope('Discriminator', reuse=reuse):\n",
        "        print('Discriminator architecture: ')\n",
        "\n",
        "        # Layer 1\n",
        "        conv1 = tf.keras.layers.Conv2D(filters=128, kernel_size=[4,4], strides=[2,2], padding='same', activation=tf.nn.leaky_relu, name='conv1')(x)\n",
        "        dropout1 = tf.nn.dropout(conv1, dropout_rate)\n",
        "        print(conv1.shape)\n",
        "\n",
        "        # Layer 2\n",
        "        conv2 = tf.keras.layers.Conv2D(filters=256, kernel_size=[4,4], strides=[2,2], padding='same', activation=tf.nn.leaky_relu, name='conv2')(dropout1)\n",
        "        batch2 = tf.keras.layers.BatchNormalization()(conv2, training=is_training)\n",
        "        dropout2 = tf.nn.dropout(batch2, dropout_rate)\n",
        "        print(conv2.shape)\n",
        "\n",
        "        # Layer 3\n",
        "        conv3 = tf.keras.layers.Conv2D(filters=512, kernel_size=[4,4], strides=[4,4], padding='same', activation=tf.nn.leaky_relu, name='conv3')(dropout2)\n",
        "        batch3 = tf.keras.layers.BatchNormalization()(conv3, training=is_training)\n",
        "        dropout3 = tf.nn.dropout(batch3, dropout_rate)\n",
        "        print(conv3.shape)\n",
        "\n",
        "        # Layer 4\n",
        "        conv4 = tf.keras.layers.Conv2D(filters=1024, kernel_size=[3,3], strides=[1,1], padding='valid', activation=tf.nn.leaky_relu, name='conv4')(dropout3)\n",
        "        print(conv4.shape)\n",
        "\n",
        "        # Flatten and Dense Layer\n",
        "        flatten = tf.reduce_mean(conv4, axis=[1, 2])\n",
        "        logits_D = tf.keras.layers.Dense(1 + num_classes)(flatten)\n",
        "\n",
        "        out_D = tf.nn.softmax(logits_D)\n",
        "\n",
        "    return flatten, logits_D, out_D\n",
        "\n",
        "\n",
        "############ Defining Generator ############\n",
        "\n",
        "def generator(z, dropout_rate = 0., is_training = True, reuse = False):\n",
        "    # input latent z -> image x\n",
        "\n",
        "    with tf.variable_scope('Generator', reuse = reuse):\n",
        "      print('\\n Generator architecture: ')\n",
        "\n",
        "      #Layer 1\n",
        "      deconv1 = tf.layers.conv2d_transpose(z, 512, kernel_size = [4,4],\n",
        "                                         strides = [1,1], padding = 'valid',\n",
        "                                        activation = tf.nn.relu, name = 'deconv1') # ?*4*4*512\n",
        "      batch1 = tf.layers.batch_normalization(deconv1, training = is_training)\n",
        "      dropout1 = tf.nn.dropout(batch1, dropout_rate)\n",
        "      print(deconv1.shape)\n",
        "\n",
        "      #Layer 2\n",
        "      deconv2 = tf.layers.conv2d_transpose(dropout1, 256, kernel_size = [4,4],\n",
        "                                         strides = [4,4], padding = 'same',\n",
        "                                        activation = tf.nn.relu, name = 'deconv2')# ?*16*16*256\n",
        "      batch2 = tf.layers.batch_normalization(deconv2, training = is_training)\n",
        "      dropout2 = tf.nn.dropout(batch2, dropout_rate)\n",
        "      print(deconv2.shape)\n",
        "\n",
        "      #Layer 3\n",
        "      deconv3 = tf.layers.conv2d_transpose(dropout2, 128, kernel_size = [4,4],\n",
        "                                         strides = [2,2], padding = 'same',\n",
        "                                        activation = tf.nn.relu, name = 'deconv3')# ?*32*32*256\n",
        "      batch3 = tf.layers.batch_normalization(deconv3, training = is_training)\n",
        "      dropout3 = tf.nn.dropout(batch3, dropout_rate)\n",
        "      print(deconv3.shape)\n",
        "\n",
        "      #Output layer\n",
        "      deconv4 = tf.layers.conv2d_transpose(dropout3, 1, kernel_size = [4,4],\n",
        "                                        strides = [2,2], padding = 'same',\n",
        "                                        activation = None, name = 'deconv4')# ?*64*64*1\n",
        "      out = tf.nn.tanh(deconv4)\n",
        "      print(deconv4.shape)\n",
        "\n",
        "    return out\n",
        "\n",
        "############ Building model ############\n",
        "\n",
        "def build_GAN(x_real, z, dropout_rate, is_training):\n",
        "\n",
        "    fake_images = generator(z, dropout_rate, is_training)\n",
        "\n",
        "    D_real_features, D_real_logits, D_real_prob = discriminator(x_real, dropout_rate,\n",
        "                                                              is_training)\n",
        "\n",
        "    D_fake_features, D_fake_logits, D_fake_prob = discriminator(fake_images, dropout_rate,\n",
        "                                                                is_training, reuse = True)\n",
        "    #Setting reuse=True this time for using variables trained in real batch training\n",
        "\n",
        "    return D_real_features, D_real_logits, D_real_prob, D_fake_features, D_fake_logits, D_fake_prob, fake_images\n",
        "\n",
        "############ Preparing Mask ############\n",
        "\n",
        "# Preparing a binary label_mask to be multiplied with real labels\n",
        "def get_labeled_mask(labeled_rate, batch_size):\n",
        "    labeled_mask = np.zeros([batch_size], dtype=np.float32)\n",
        "    labeled_count = int(batch_size * labeled_rate)\n",
        "    labeled_mask[range(labeled_count)] = 1.0\n",
        "    np.random.shuffle(labeled_mask)\n",
        "    return labeled_mask\n",
        "\n",
        "############ Preparing Extended label ############\n",
        "\n",
        "def prepare_extended_label(label):\n",
        "    # add extra label for fake data\n",
        "    extended_label = tf.concat([tf.zeros([tf.shape(label)[0], 1]), label], axis = 1)\n",
        "\n",
        "    return extended_label\n",
        "\n",
        "############ Defining losses ############\n",
        "\n",
        "# The total loss inculcates  D_L_Unsupervised + D_L_Supervised + G_feature_matching loss + G_R/F loss\n",
        "\n",
        "def loss_accuracy(D_real_features, D_real_logit, D_real_prob, D_fake_features,\n",
        "                  D_fake_logit, D_fake_prob, extended_label, labeled_mask):\n",
        "\n",
        "                    ### Discriminator loss ###\n",
        "\n",
        "    # Supervised loss -> which class the real data belongs to\n",
        "\n",
        "    temp = tf.nn.softmax_cross_entropy_with_logits_v2(logits = D_real_logit,\n",
        "                                                  labels = extended_label)\n",
        "    # Don't confuse labeled_rate with labeled_mask\n",
        "    # Labeled_mask and temp are of same size = batch_size where temp is softmax\n",
        "    # cross_entropy calculated over whole batch\n",
        "\n",
        "    D_L_Supervised = tf.reduce_sum(tf.multiply(temp,labeled_mask)) / tf.reduce_sum(labeled_mask)\n",
        "\n",
        "    # Multiplying temp with labeled_mask gives supervised loss on labeled_mask\n",
        "    # data only, calculating mean by dividing by no of labeled samples\n",
        "\n",
        "    # Unsupervised loss -> R/F\n",
        "\n",
        "    D_L_RealUnsupervised = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits = D_real_logit[:, 0], labels = tf.zeros_like(D_real_logit[:, 0], dtype=tf.float32)))\n",
        "\n",
        "    D_L_FakeUnsupervised = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits = D_fake_logit[:, 0], labels = tf.ones_like(D_fake_logit[:, 0], dtype=tf.float32)))\n",
        "\n",
        "    D_L = D_L_Supervised + D_L_RealUnsupervised + D_L_FakeUnsupervised\n",
        "\n",
        "\n",
        "                    ### Generator loss ###\n",
        "\n",
        "    # G_L_1 -> Fake data wanna be real\n",
        "\n",
        "    G_L_1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "            logits = D_fake_logit[:, 0],labels = tf.zeros_like(D_fake_logit[:, 0], dtype=tf.float32)))\n",
        "\n",
        "    # G_L_2 -> Feature matching\n",
        "    data_moments = tf.reduce_mean(D_real_features, axis = 0)\n",
        "    sample_moments = tf.reduce_mean(D_fake_features, axis = 0)\n",
        "    G_L_2 = tf.reduce_mean(tf.square(data_moments-sample_moments))\n",
        "\n",
        "\n",
        "    G_L = G_L_1 + G_L_2\n",
        "\n",
        "    prediction = tf.equal(tf.argmax(D_real_prob[:, 1:], 1),\n",
        "                                  tf.argmax(extended_label[:, 1:], 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
        "\n",
        "    return D_L, G_L, accuracy\n",
        "\n",
        "############ Defining Optimizer ############\n",
        "\n",
        "def optimizer(D_Loss, G_Loss, learning_rate, beta1):\n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    with tf.control_dependencies(update_ops):\n",
        "        all_vars = tf.trainable_variables()\n",
        "        D_vars = [var for var in all_vars if var.name.startswith('Discriminator')]\n",
        "        G_vars = [var for var in all_vars if var.name.startswith('Generator')]\n",
        "\n",
        "        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1,\n",
        "                                             name = 'd_optimiser').minimize(D_Loss, var_list=D_vars)\n",
        "        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1,\n",
        "                                             name = 'g_optimiser').minimize(G_Loss, var_list=G_vars)\n",
        "\n",
        "    return d_train_opt, g_train_opt\n",
        "\n",
        "############ Plotting Results ############\n",
        "\n",
        "def show_result(test_images, num_epoch, show = True, save = False, path = 'result.png'):\n",
        "\n",
        "    size_figure_grid = 5\n",
        "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
        "    for i in range(0, size_figure_grid):\n",
        "      for j in range(0, size_figure_grid):\n",
        "        ax[i, j].get_xaxis().set_visible(False)\n",
        "        ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "    for k in range(size_figure_grid*size_figure_grid):\n",
        "        i = k // size_figure_grid\n",
        "        j = k % size_figure_grid\n",
        "        ax[i, j].cla()\n",
        "        ax[i, j].imshow(np.reshape(test_images[k], (64, 64)), cmap='gray')\n",
        "\n",
        "    label = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0.04, label, ha='center')\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
        "\n",
        "\n",
        "    x = range(len(hist['D_losses']))\n",
        "\n",
        "    y1 = hist['D_losses']\n",
        "    y2 = hist['G_losses']\n",
        "\n",
        "    plt.plot(x, y1, label='D_loss')\n",
        "    plt.plot(x, y2, label='G_loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "############ TRAINING ############\n",
        "\n",
        "def train_GAN(batch_size, epochs):\n",
        "\n",
        "    train_hist = {}\n",
        "    train_hist['D_losses'] = []\n",
        "    train_hist['G_losses'] = []\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "\n",
        "    x = tf.placeholder(tf.float32, shape=[None, height, width, channels], name='x')\n",
        "    z = tf.placeholder(tf.float32, shape=[None, 1, 1, latent], name='z')\n",
        "    label = tf.placeholder(tf.float32, shape=[None, num_classes], name='label')\n",
        "    labeled_mask = tf.placeholder(tf.float32, shape=[None], name='labeled_mask')\n",
        "    dropout_rate = tf.placeholder(tf.float32, name='dropout_rate')\n",
        "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
        "\n",
        "    lr_rate = 2e-4\n",
        "\n",
        "    model = build_GAN(x, z, dropout_rate, is_training)\n",
        "    D_real_features, D_real_logit, D_real_prob, D_fake_features, D_fake_logit, D_fake_prob, fake_data = model\n",
        "\n",
        "    extended_label = prepare_extended_label(label)\n",
        "\n",
        "    # Fake_data of size = batch_size*28*28*1\n",
        "\n",
        "    loss_acc = loss_accuracy(D_real_features, D_real_logit, D_real_prob,\n",
        "                                  D_fake_features, D_fake_logit, D_fake_prob,\n",
        "                                  extended_label, labeled_mask)\n",
        "    D_L, G_L, accuracy = loss_acc\n",
        "\n",
        "    D_optimizer, G_optimizer = optimizer(D_L, G_L, lr_rate, beta1 = 0.5)\n",
        "\n",
        "    print ('...Training begins...')\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "\n",
        "      train_images, train_labels, test_images, test_labels = get_data()\n",
        "      no_of_batches = int(train_images.shape[0] / batch_size)\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "\n",
        "        train_accuracies, train_D_losses, train_G_losses = [], [], []\n",
        "            # Shuffle dataset at the beginning of each epoch\n",
        "        indices = np.arange(train_images.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        shuffled_train_images = train_images[indices]\n",
        "        shuffled_train_labels = train_labels[indices]\n",
        "        for it in range(no_of_batches):\n",
        "\n",
        "          start = it * batch_size\n",
        "          end = min(start + batch_size, train_images.shape[0])\n",
        "          batch_images = shuffled_train_images[start:end]\n",
        "          batch_labels = shuffled_train_labels[start:end]\n",
        "\n",
        "          # Resize images\n",
        "          batch_images_resized = np.array([np.array(Image.fromarray(img.squeeze(), 'L').resize((64, 64))) for img in batch_images])\n",
        "          batch_images_resized = np.expand_dims(batch_images_resized, axis=-1)\n",
        "\n",
        "          # Normalize images\n",
        "          batch_images_normalized = scale(batch_images_resized)\n",
        "\n",
        "          # Generate random noise for generator\n",
        "          batch_z = np.random.normal(0, 1, (batch_images_resized.shape[0], 1, 1, latent))\n",
        "\n",
        "          # Create mask for labeled data\n",
        "          mask = get_labeled_mask(labeled_rate, batch_images_resized.shape[0])\n",
        "\n",
        "          # Create feed dictionary\n",
        "          train_feed_dict = {\n",
        "              x: batch_images_normalized,\n",
        "              z: batch_z,\n",
        "              label: batch_labels,\n",
        "              labeled_mask: mask,\n",
        "              dropout_rate: 0.7,\n",
        "              is_training: True\n",
        "          }\n",
        "          #The label provided in dict are one hot encoded in 10 classes\n",
        "\n",
        "          D_optimizer.run(feed_dict = train_feed_dict)\n",
        "          G_optimizer.run(feed_dict = train_feed_dict)\n",
        "\n",
        "          train_D_loss = D_L.eval(feed_dict = train_feed_dict)\n",
        "          train_G_loss = G_L.eval(feed_dict = train_feed_dict)\n",
        "          train_accuracy = accuracy.eval(feed_dict = train_feed_dict)\n",
        "\n",
        "          train_D_losses.append(train_D_loss)\n",
        "          train_G_losses.append(train_G_loss)\n",
        "          train_accuracies.append(train_accuracy)\n",
        "          print('Batch evaluated: ' +str(it+1))\n",
        "\n",
        "        tr_GL = np.mean(train_G_losses)\n",
        "        tr_DL = np.mean(train_D_losses)\n",
        "        tr_acc = np.mean(train_accuracies)\n",
        "\n",
        "        print ('After epoch: '+ str(epoch+1) + ' Generator loss: '\n",
        "                       + str(tr_GL) + ' Discriminator loss: ' + str(tr_DL) + ' Accuracy: ' + str(tr_acc))\n",
        "\n",
        "        gen_samples = fake_data.eval(feed_dict = {z : np.random.normal(0, 1, (25, 1, 1, latent)), dropout_rate : 0.7, is_training : False})\n",
        "        # Dont train batch-norm while plotting => is_training = False\n",
        "        test_images = tf.image.resize_images(gen_samples, [64, 64]).eval()\n",
        "        show_result(test_images, (epoch + 1), show = True, save = False, path = '')\n",
        "\n",
        "        train_hist['D_losses'].append(np.mean(train_D_losses))\n",
        "        train_hist['G_losses'].append(np.mean(train_G_losses))\n",
        "\n",
        "      show_train_hist(train_hist, show=True, save = True, path = 'train_hist.png')\n",
        "      sess.close()\n",
        "\n",
        "    return train_D_losses,train_G_losses\n",
        "\n",
        "key = train_GAN( 128 , 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4OVDHvT8H5o"
      },
      "source": [
        "To measure the performance of our GAN model on the test set, we will need to evaluate the discriminator's ability to classify real images from the test set accurately. Since GANs are typically used for generation tasks, their performance is often assessed qualitatively through the visual inspection of generated images. However, for the discriminator in a GAN, we can quantify its performance using classification accuracy on real images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d5BbjwF8HOT"
      },
      "outputs": [],
      "source": [
        "def preprocess_test_data(test_images):\n",
        "    # Resize images\n",
        "    test_images_resized = np.array([np.array(Image.fromarray(img.squeeze(), 'L').resize((64, 64))) for img in test_images])\n",
        "    test_images_resized = np.expand_dims(test_images_resized, axis=-1)\n",
        "\n",
        "    # Normalize images\n",
        "    test_images_normalized = scale(test_images_resized)\n",
        "    return test_images_normalized\n",
        "\n",
        "def evaluate_discriminator(sess, x, dropout_rate, is_training, D_real_logits, test_images, test_labels):\n",
        "    test_images_preprocessed = preprocess_test_data(test_images)\n",
        "\n",
        "    real_prediction = tf.nn.sigmoid(D_real_logits[:, 0])  # Assuming first logit is for real/fake classification\n",
        "    predictions = sess.run(real_prediction, feed_dict={x: test_images_preprocessed, dropout_rate: 0.0, is_training: False})\n",
        "    predicted_labels = (predictions > 0.5).astype(np.int32)\n",
        "    true_labels = (np.argmax(test_labels, axis=1) == 0).astype(np.int32)  # Assuming real images are labeled as class 0\n",
        "\n",
        "    accuracy = np.mean(predicted_labels == true_labels)\n",
        "    return accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bzw-HIXsvZ6w"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}